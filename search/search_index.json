{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Run:ai Documentation Library","text":"<p>Welcome to the Run:ai documentation area. For an introduction about what is the Run:ai Platform see Run:ai platform on the run.ai website.</p> <p>The Run:ai documentation is targeting three personas:</p> <ul> <li> <p>Run:ai Administrator - Is responsible for the setup and the day-to-day administration of the product. Administrator documentation can be found here.</p> </li> <li> <p>Researcher - Using Run:ai to submit Jobs. Researcher documentation can be found here.</p> </li> <li> <p>Developer - Using various APIs to manipulate Jobs and integrate with other systems. Developer documentation can be found here.</p> </li> </ul>"},{"location":"#how-to-get-support","title":"How to get support","text":"<p>To get support use the following channels:</p> <ul> <li> <p>On the Run:ai user interface at <code>&lt;company-name&gt;.run.ai</code>, use the 'Contact Support' link on the top right.</p> </li> <li> <p>Or submit a ticket by clicking the button below:</p> </li> </ul> <p>Submit a Ticket</p>"},{"location":"#community","title":"Community","text":"<p>Run:ai provides its customers with access to the Run:ai Customer Community portal in order to submit tickets, track ticket progress and update support cases.</p> <p>Customer Community Portal</p> <p>Reach out to customer support for credentials.</p>"},{"location":"#runai-cloud-status-page","title":"Run:ai Cloud Status Page","text":"<p>Run:ai cloud availability is monitored at status.run.ai.</p>"},{"location":"#collect-logs-to-send-to-support","title":"Collect Logs to Send to Support","text":"<p>As an IT Administrator, you can collect Run:ai logs to send to support:</p> <ul> <li>Install the Run:ai Administrator command-line interface.</li> <li>Run <code>runai-adm collect-logs</code>. The command will generate a compressed file containing all of the existing Run:ai log files.</li> </ul> <p>Note</p> <p>The tar file packages the logs of Run:ai components only. It does not include logs of researcher containers that may contain private information. </p>"},{"location":"#example-code","title":"Example Code","text":"<p>Code for the Docker images referred to on this site is available at https://github.com/run-ai/docs/tree/master/quickstart.</p> <p>The following images are used throughout the documentation:</p> Image Description Source gcr.io/run-ai-demo/quickstart Basic training image. Multi-GPU support https://github.com/run-ai/docs/tree/master/quickstart/main gcr.io/run-ai-demo/quickstart-distributed Distributed training using MPI and Horovod https://github.com/run-ai/docs/tree/master/quickstart/distributed zembutsu/docker-sample-nginx Build (interactive) with Connected Ports https://github.com/zembutsu/docker-sample-nginx gcr.io/run-ai-demo/quickstart-hpo Hyperparameter Optimization https://github.com/run-ai/docs/tree/master/quickstart/hpo gcr.io/run-ai-demo/quickstart-x-forwarding Use X11 forwarding from Docker image https://github.com/run-ai/docs/tree/master/quickstart/x-forwarding gcr.io/run-ai-demo/pycharm-demo Image used for tool integration (PyCharm and VSCode) https://github.com/run-ai/docs/tree/master/quickstart/python%2Bssh gcr.io/run-ai-demo/example-triton-client and  gcr.io/run-ai-demo/example-triton-server Basic Inference https://github.com/run-ai/models/tree/main/models/triton"},{"location":"#contributing-to-the-documentation","title":"Contributing to the documentation","text":"<p>This documentation is made better by a number of individuals from our customer and partner community. If you see something worth fixing, please comment at the bottom of the page or create a pull request via GitHub. The public GitHub repository can be found on the top-right of this page. </p>"},{"location":"Researcher/overview-researcher/","title":"Overview: Researcher Documentation","text":"<p>Researchers use Run:ai to submit jobs. </p> <p>As part of the Researcher documentation you will find:</p> <ul> <li>Quickstart Guides which provide step-by-step guides to Run:ai technology.</li> <li>Command line interface reference documentation.</li> <li>Best Practices for Deep Learning with Run:ai.</li> <li>Information about the Run:ai Scheduler.</li> <li>The Run:ai Python Researcher Library which you can optionally use in your container to get additional reporting and further resource optimization.</li> <li>Using Run:ai with various developer tools. </li> </ul>"},{"location":"Researcher/use-cases/","title":"Use Cases","text":"<p>This is a collection of various client-requested use cases. Each use case is accompanied by a short live-demo video, along with all the files used.</p> <p>Note</p> <p>For the most up-to-date information, check out the official Run:ai use-cases GitHub page.  </p> <ul> <li>MLflow with Run:ai: experiment management is important for Data Scientists. This is a demo of how to set up and use MLflow with Run:ai.  </li> <li>Introduction to Docker: Run:ai runs using Docker images. This is a brief introduction to Docker, image creation, and how to use them in the context of Run:ai. Please also check out the Persistent Environments use case if you wish to keep the creation of Docker images to a minimum.  </li> <li>Tensorboard with Jupyter (ResNet demo): Many Data Scientists like to use Tensorboard to keep an eye on the their current training experiments. They also like to have it side-by-side with Jupyter. In this demo, we will show how to integrate Tensorboard and Jupyter Lab within the context of Run:ai.  </li> <li>Persistent Environments (with Conda/Mamba &amp; Jupyter): Some Data Scientists find creating Docker images for every single one of their environments a bit of a hindrance. They would often prefer the ability to create and alter environments on the fly and to have those environments remain, even after an image has finished running in a job. This demo shows users how they can create and persist Conda/Mamba environments using an NFS.  </li> <li>Weights &amp; Biases with Run:ai: W&amp;B (Weights &amp; Biases) is one of the best tools for experiment tracking and management. W&amp;B is an official Run:ai partner. In this tutorial, we will demo how to use W&amp;B alongside Run:ai</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/","title":"Quickstart: Launch an Inference Workload","text":""},{"location":"Researcher/Walkthroughs/quickstart-inference/#introduction","title":"Introduction","text":"<p>Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output. </p> <p>With Inference, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time. </p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster. There are additional prerequisites for running inference. See cluster installation prerequisites for more information. </li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> <li>You must have ML Engineer access rights. See Adding, Updating and Deleting Users for more information. </li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/quickstart-inference/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named \"team-a\".</li> <li>Allocate 2 GPUs to the Project.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#run-an-inference-workload","title":"Run an Inference Workload","text":"<ul> <li>In the Run:ai user interface go to <code>Deployments</code>. If you do not see the <code>Deployments</code> section you may not have the required access control, or the inference module is disabled. </li> <li>Select <code>New Deployment</code> on the top right.</li> <li>Select <code>team-a</code> as a project and add an arbitrary name. Use the image <code>gcr.io/run-ai-demo/example-triton-server</code>.</li> <li>Under <code>Resources</code> add 0.5 GPUs.</li> <li>Under <code>Auto Scaling</code> select a minimum of 1, a maximum of 2. Use the <code>concurrency</code> autoscaling threshold method. Add a threshold of 3.</li> <li>Add a <code>Container port</code> of <code>8000</code>.</li> </ul> <p>This would start an inference workload for team-a with an allocation of a single GPU. Follow up on the Job's progress using the Deployment list in the user interface or by running <code>runai list jobs</code></p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#query-the-inference-server","title":"Query the Inference Server","text":"<p>The specific inference server we just created is accepting queries over port 8000. You can use the Run:ai Triton demo client to send requests to the server:</p> <ul> <li>Find an IP address by running <code>kubectl get svc -n runai-team-a</code>. Use the <code>inference1-00001-private</code> Cluster IP.</li> <li>Replace <code>&lt;IP&gt;</code> below and run: </li> </ul> <pre><code> runai submit inference-client  -i gcr.io/run-ai-demo/example-triton-client \\\n    -- perf_analyzer -m inception_graphdef  -p 3600000 -u  &lt;IP&gt;\n</code></pre> <ul> <li>To see the result, run the following:</li> </ul> <pre><code>runai logs inference-client\n</code></pre>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#view-status-on-the-runai-user-interface","title":"View status on the Run:ai User Interface","text":"<ul> <li>Open the Run:ai user interface.</li> <li>Under Deployments you can view the new Workload. When clicking the workload, note the utilization graphs go up. </li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#stop-workload","title":"Stop Workload","text":"<p>Use the user interface to delete the workload.</p>"},{"location":"Researcher/Walkthroughs/quickstart-inference/#see-also","title":"See also","text":"<ul> <li>You can also create Inference deployments via API. For more information see Submitting Workloads via YAML.</li> <li>See Deployment user interface.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-mig/","title":"Quickstart: Launch Workloads with NVIDIA Dynamic MIG","text":""},{"location":"Researcher/Walkthroughs/quickstart-mig/#introduction","title":"Introduction","text":"<p>A single GPU has a significant amount of memory. Ranging from a couple of gigabytes in older generations and up to 80GB per GPU in the later models of the latest NVIDIA GPU technology. A single GPU also has a vast amount of computing power. </p> <p>This amount of memory and computing power is important for processing large amounts of data, such as in training deep learning models. However, there are quite a few applications that do not need this power. Examples can be inference workloads and the model-creation phase. It would thus be convenient if we could divide up a GPU between various workloads, thus achieving better GPU utilization. </p> <p>Run:ai provides two alternatives for splitting GPUs: Fractions and Dynamic MIG allocation. The focus of this article is Dynamic MIG allocation.  A detailed explanation of the two Run:ai offerings can be found here.</p>"},{"location":"Researcher/Walkthroughs/quickstart-mig/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> <li>A machine with a single available NVIDIA A100 GPU. This can be achieved by allocating filler workloads to the other GPUs on the node, or by using Google Cloud which allows for the creation of a virtual node with a single A100 GPU. </li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-mig/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/quickstart-mig/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Allocate 2 GPUs to the Project.</li> <li>Mark the node as a dynamic MIG node as described here.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-mig/#run-an-inference-workload-single-replica","title":"Run an Inference Workload - Single Replica","text":"<p>At the GPU node level, run: <code>nvidia-smi</code>:</p> <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                   On |\n| N/A   32C    P0    42W / 400W |      0MiB / 40536MiB |     N/A      Default |\n|                               |                      |              Enabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  No MIG devices found                                                       |\n+-----------------------------------------------------------------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>In the highlighted text above, note that:</p> <ul> <li>MIG is enabled (if <code>Enabled</code> has a star next to it, you need to reboot your machine).</li> <li>The GPU is not yet divided into devices.</li> </ul> <p>At the command-line run:</p> <pre><code>runai config project team-a\nrunai submit mig1 -i gcr.io/run-ai-demo/quickstart-cuda  --gpu-memory 10GB\nrunai submit mig2 -i gcr.io/run-ai-demo/quickstart-cuda  --mig-profile 2g.10gb \nrunai submit mig3 -i gcr.io/run-ai-demo/quickstart-cuda  --mig-profile 2g.10gb \n</code></pre> <p>We used two different methods to create MIG partitions: </p> <ol> <li>Stating the amount of GPU memory we require </li> <li>Requiring a partition of explicit size using NVIDIA terminology. </li> </ol> <p>Both methods achieve the same effect. They result in three MIG partitions of 10GB each. You can verify that by running <code>nvidia-smi</code>, at the GPU node level:</p> <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                   On |\n| N/A   47C    P0   194W / 400W |  27254MiB / 40536MiB |     N/A      Default |\n|                               |                      |              Enabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| MIG devices:                                                                |\n+------------------+----------------------+-----------+-----------------------+\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n|                  |                      |        ECC|                       |\n|==================+======================+===========+=======================|\n|  0    3   0   0  |   9118MiB /  9984MiB | 28      0 |  2   0    1    0    0 |\n|                  |      4MiB / 16383MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n|  0    4   0   1  |   9118MiB /  9984MiB | 28      0 |  2   0    1    0    0 |\n|                  |      4MiB / 16383MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n|  0    5   0   2  |   9016MiB /  9984MiB | 28      0 |  2   0    1    0    0 |\n|                  |      2MiB / 16383MiB |           |                       |\n+------------------+----------------------+-----------+-----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0    3    0     142213      C   ./quickstart                     9111MiB |\n|    0    4    0     146799      C   ./quickstart                     9111MiB |\n|    0    5    0     132219      C   ./quickstart                     9009MiB |\n+-----------------------------------------------------------------------------+\n</code></pre> <ul> <li>Highlighted above is a list of 3 MIG devices, each 10GB large. Total of 30GB (out of the 40GB on the GPU)</li> <li>You can also run the same command inside one of the containers: <code>runai exec mig1 nvidia-smi</code>. This will show a single device (the only one that the container sees from its point of view).</li> <li>Run: <code>runai list</code> to see the 3 jobs in <code>Running</code> state.</li> </ul> <p>We now want to allocate an interactive job with 20GB. Interactive jobs take precedence over the default training jobs:</p> <p><pre><code>runai submit mig1-int -i gcr.io/run-ai-demo/quickstart-cuda \\\n    --interactive --gpu-memory 20G \n</code></pre> or similarly, <pre><code>runai submit mig1-int -i gcr.io/run-ai-demo/quickstart-cuda \\\n    --interactive --mig-profile 3g.20gb  \n</code></pre></p> <p>Using <code>runai list</code> and <code>nvidia-smi</code> on the host machine, you can see that:</p> <ul> <li>One training job is preempted, and its device is deleted.</li> <li>The new, interactive job starts running.</li> </ul>"},{"location":"Researcher/Walkthroughs/quickstart-overview/","title":"Run:ai Quickstart Guides","text":"<p>Below are a set of Quickstart documents. The purpose of these documents is to get you acquainted with an aspect of Run:ai in the simplest possible form. Follow the Quickstart documents below to learn more:</p> <ul> <li>Unattended training sessions</li> <li>Interactive build sessions</li> <li>Interactive build sessions with externalized services</li> <li>Using GPU Fractions</li> <li>Distributed Training</li> <li>Hyperparameter Optimization</li> <li>Over-Quota, Basic Fairness &amp; Bin Packing</li> <li>Fairness</li> <li>Inference</li> <li>Dynamic MIG</li> </ul> <p>Most quickstarts rely on an image called <code>gcr.io/run-ai-demo/quickstart</code>. The image is based on  TensorFlow Release 20-08. This TensorFlow image has minimal requirements for CUDA and NVIDIA Compute Capability. </p> <p>If your GPUs do not meet these requirements, use <code>gcr.io/run-ai-demo/quickstart:legacy</code> instead. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/","title":"Quickstart: Launch Interactive Build Workloads with Connected Ports","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#introduction","title":"Introduction","text":"<p>This Quickstart is an extension of the Quickstart document: Start and Use Interactive Build Workloads </p> <p>When starting a container with the Run:ai Command-Line Interface (CLI), it is sometimes needed to expose internal ports to the user. Examples are: accessing a Jupyter notebook, using the container from a development environment such as PyCharm. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#exposing-a-container-port","title":"Exposing a Container Port","text":"<p>There are four ways to expose ports in Kubernetes: Port Forwarding, NodePort, and LoadBalancer. The first two will always work. The others require a special setup by your administrator. The four methods are explained here. </p> <p>The document below provides an example based on Port Forwarding.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#port-forwarding-step-by-step-walkthrough","title":"Port Forwarding, Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named <code>team-a</code>.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#run-workload","title":"Run Workload","text":"<ul> <li>At the command-line run:</li> </ul> <pre><code>runai config project team-a\nrunai submit nginx-test -i zembutsu/docker-sample-nginx --interactive \\\n--service-type portforward --port 8080:80 </code></pre> <ul> <li>The Job is based on a sample NGINX webserver docker image <code>zembutsu/docker-sample-nginx</code>. Once accessed via a browser, the page shows the container name. </li> <li>Note the interactive flag which means the Job will not have a start or end. It is the Researcher's responsibility to close the Job.  </li> <li>In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8080 to localhost as long as the <code>runai submit</code> command is not stopped</li> <li>It is possible to forward traffic from multiple IP addresses by using the \"--address\" parameter. Check the CLI reference for further details. </li> </ul> <p>The result will be:</p> <pre><code>The job 'nginx-test-0' has been submitted successfully\nYou can run `runai describe job nginx-test-0 -p team-a` to check the job status\nWaiting for pod to start running...\nINFO[0023] Job started\nOpen access point(s) to service from localhost:8080\nForwarding from 127.0.0.1:8080 -&gt; 80\nForwarding from [::1]:8080 -&gt; 80\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#access-the-webserver","title":"Access the Webserver","text":"<p>Open the following in the browser at http://localhost:8080.</p> <p>You should see a web page with the name of the container.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#stop-workload","title":"Stop Workload","text":"<p>Press Ctrl-C in the shell to stop port forwarding. Then delete the Job by running <code>runai delete job nginx-test</code></p>"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#see-also","title":"See Also","text":"<ul> <li>Develop on Run:ai using Visual Studio Code</li> <li>Develop on Run:ai using PyCharm</li> <li>Use a Jupyter notbook with Run:ai.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/","title":"Quickstart: Launch Interactive Build Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build/#introduction","title":"Introduction","text":"<p>Deep learning workloads can be divided into two generic types:</p> <ul> <li>Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. </li> <li>Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results.</li> </ul> <p>With this Quickstart you will learn how to:</p> <ul> <li>Use the Run:ai command-line interface (CLI) to start a deep learning Build workload</li> <li>Open an ssh session to the Build workload</li> <li>Stop the Build workload</li> </ul> <p>It is also possible to open ports to specific services within the container. See \"Next Steps\" at the end of this article.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#step-by-step-quickstart","title":"Step by Step Quickstart","text":""},{"location":"Researcher/Walkthroughs/walkthrough-build/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named \"team-a\".</li> <li>Allocate 2 GPUs to the Project.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#run-workload","title":"Run Workload","text":"<ul> <li> <p>At the command-line run:</p> <pre><code>runai config project team-a\nrunai submit build1 -i ubuntu -g 1 --interactive -- sleep infinity\n</code></pre> </li> <li> <p>The job is based on a sample docker image <code>ubuntu</code></p> </li> <li>We named the job build1.</li> <li>Note the interactive flag which means the job will not have a start or end. It is the Researcher's responsibility to close the job. </li> <li>The job is assigned to team-a with an allocation of a single GPU. </li> <li>The command provided is <code>sleep infinity</code>. You must provide a command or the container will start and then exit immediately. Alternatively, replace these flags with <code>--attach</code> to attach immediately to a session.</li> </ul> <p>Follow up on the job's status by running:</p> <pre><code>runai list jobs\n</code></pre> <p>The result:</p> <p></p> <p>Typical statuses you may see:</p> <ul> <li>ContainerCreating - The docker container is being downloaded from the cloud repository</li> <li>Pending - the job is waiting to be scheduled</li> <li>Running - the job is running</li> </ul> <p>A full list of Job statuses can be found here</p> <p>To get additional status on your job run:</p> <pre><code>runai describe job build1\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#get-a-shell-to-the-container","title":"Get a Shell to the container","text":"<p>Run:</p> <pre><code>runai bash build1\n</code></pre> <p>This should provide a direct shell into the computer</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#view-status-on-the-runai-user-interface","title":"View status on the Run:ai User Interface","text":"<ul> <li>Open the Run:ai user interface.</li> <li>Under \"Jobs\" you can view the new Workload:</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> <pre><code>runai delete job build1\n</code></pre> <p>This would stop the training workload. You can verify this by running <code>runai list jobs</code> again.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-build/#next-steps","title":"Next Steps","text":"<ul> <li>Expose internal ports to your interactive build workload: Quickstart Launch an Interactive Build Workload with Connected Ports.</li> <li>Follow the Quickstart document: Launch Unattended Training Workloads.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/","title":"Quickstart: Launch Distributed Training Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#introduction","title":"Introduction","text":"<p>Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker node. Worker nodes work in parallel to speed up model training. Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container.</p> <p>Getting Distributed Training to work is more complex than multi-GPU training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Several Deep Learning frameworks support Distributed Training. Horovod is a good example.</p> <p>Run:ai provides the ability to run, manage, and view Distributed Training workloads. The following is a Quickstart document for such a scenario.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>During the installation, you have installed the Kubeflow MPI Operator as specified here</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named \"team-a\".</li> <li>Allocate 2 GPUs to the Project.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-training-distributed-workload","title":"Run Training Distributed Workload","text":"<ul> <li>At the command-line run:</li> </ul> <pre><code>runai config project team-a\nrunai submit-mpi dist --processes=2 -g 1 \\\n-i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 -e RUNAI_SLEEP_SECS=60\n</code></pre> <ul> <li>We named the Job dist</li> <li>The Job is assigned to team-a</li> <li>There will be two worker processes (--processes=2), each allocated with a single GPU (-g 1)</li> <li>The Job is based on a sample docker image <code>gcr.io/run-ai-demo/quickstart-distributed:v0.3.0</code>.</li> <li>The image contains a startup script that runs a deep learning Horovod-based workload.</li> </ul> <p>Follow up on the Job's status by running:</p> <pre><code>    runai list jobs\n</code></pre> <p>The result:</p> <p></p> <p>The Run:ai scheduler ensures that all processes can run together. You can see the list of workers as well as the main \"launcher\" process by running:</p> <pre><code>    runai describe job dist\n</code></pre> <p>You will see two worker processes (pods) their status and on which node they run:</p> <p></p> <p>To see the merged logs of all pods run:</p> <pre><code>    runai logs dist\n</code></pre> <p>Finally, you can delete the distributed training workload by running:</p> <pre><code>    runai delete job dist\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-an-interactive-distributed-workload","title":"Run an Interactive Distributed Workload","text":"<p>It is also possible to run a distributed training Job as \"interactive\". This is useful if you want to test your distributed training Job before committing on a long, unattended training session. To run such a session use:</p> <pre><code>runai submit-mpi dist-int --processes=2 -g 1 \\\n-i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 --interactive \\\n-- sh -c \"sleep infinity\" </code></pre> <p>When the workers are running run:</p> <pre><code>    runai bash dist-int\n</code></pre> <p>This will provide shell access to the launcher process. From there, you can run your distributed workload. For Horovod version smaller than 0.17.0 run:</p> <pre><code>horovodrun -np $RUNAI_MPI_NUM_WORKERS \\\npython scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n--model=resnet20 --num_batches=1000000 --data_name cifar10 \\\n--data_dir /cifar10 --batch_size=64 --variable_update=horovod\n</code></pre> <p>For Horovod version 0.17.0 or later, add the <code>-hostfile</code> flag as follows:</p> <pre><code>horovodrun -np $RUNAI_MPI_NUM_WORKERS -hostfile /etc/mpi/hostfile \\\npython scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n--model=resnet20 --num_batches=1000000 --data_name cifar10 \\\n--data_dir /cifar10 --batch_size=64 --variable_update=horovod </code></pre> <p>The environment variable <code>RUNAI_MPI_NUM_WORKERS</code> is passed by Run:ai and contains the number of worker processes provided to the <code>runai submit-mpi</code> command (in the above example the value is 2).</p>"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#see-also","title":"See Also","text":"<ul> <li>The source code of the image used in this Quickstart document is in Github</li> <li>For a full list of the <code>submit-mpi</code> options see runai submit-mpi</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/","title":"Quickstart: Launch Workloads with GPU Fractions","text":""},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#introduction","title":"Introduction","text":"<p>Run:ai provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs.</p> <p>Run:ai\u2019s fractional GPU system effectively creates logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves.</p> <p>A typical use-case could see 2-8 Jobs running on the same GPU, meaning you could do eight times the work with the same hardware. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named \"team-a\".</li> <li>Allocate 1 GPU to the Project.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#run-workload","title":"Run Workload","text":"<ul> <li> <p>At the command-line run:</p> <pre><code>runai config project team-a\n\nrunai submit frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 --interactive\nrunai submit frac03 -i gcr.io/run-ai-demo/quickstart -g 0.3\n</code></pre> </li> <li> <p>The Jobs are based on a sample docker image <code>gcr.io/run-ai-demo/quickstart</code> the image contains a startup script that runs a deep learning TensorFlow-based workload.</p> </li> <li>We named the Jobs frac05 and frac03 respectively. </li> <li>Note that fractions may or may not use the <code>--interactive</code> flag. Setting the flag means that the Job will not automatically finish. Rather, it is the Researcher's responsibility to delete the Job. Fractions support both Interactive and non-interactive Jobs. </li> <li>The Jobs are assigned to team-a with an allocation of a single GPU. </li> </ul> <p>Follow up on the Job's status by running:</p> <pre><code>runai list jobs\n</code></pre> <p>The result:</p> <p></p> <p>Note that both Jobs were allocated to the same node.</p> <p>When both Jobs are running, bash into one of them:</p> <pre><code>runai bash frac05\n</code></pre> <p>Now, inside the container,  run: </p> <pre><code>nvidia-smi\n</code></pre> <p>The result:</p> <p></p> <p>Notes:</p> <ul> <li>The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs.</li> <li>The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#use-exact-gpu-memory","title":"Use Exact GPU Memory","text":"<p>Instead of requesting a fraction of the GPU, you can ask for specific GPU memory requirements. For example:</p> <pre><code>runai submit  -i gcr.io/run-ai-demo/quickstart --gpu-memory 5G\n</code></pre> <p>Which will provide 5GB of GPU memory. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/","title":"Quickstart: Hyperparameter Optimization","text":""},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#introduction","title":"Introduction","text":"<p>Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter can be a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers.</p> <p>To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine results to decide what works best.</p> <p>There are several strategies for searching the hyperparameter space. Most notable are Random search and Grid search. The former, as its name implies, selects parameters at random while the latter does an exhaustive search from a list of pre-selected values.</p> <p>Run:ai provides the ability to run, manage, and view HPO runs. The following is a Quickstart of such a scenario.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named \"team-a\".</li> <li>Allocate 2 GPUs to the Project.</li> <li>On shared storage create a library to store HPO results. E.g. <code>/nfs/john/hpo</code>.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#pods","title":"Pods","text":"<p>With HPO, we introduce the concept of Pods. Pods are units of work within a Job. </p> <ul> <li>Typically, each Job has a single Pod. However, with HPO as well as with Distributed Training there are multiple Pods per Job. </li> <li>Pods are independent</li> <li>All Pods execute with the same arguments as added via <code>runai submit</code>. E.g. The same image name, the same code script, the same number of Allocated GPUs, and memory.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#hpo-sample-code","title":"HPO Sample Code","text":"<p>The Quickstart code uses the Run:ai HPO pythong library github.com/run-ai/docs. And needs to be installed within the image. Below are some highlights of the code: </p> <pre><code># import Run:ai HPO library\nimport runai.hpo\n# select Random search or grid search\nstrategy = runai.hpo.Strategy.GridSearch\n# initialize the Run:ai HPO library. Send the NFS directory used for sync\nrunai.hpo.init(\"/nfs\")\n# pick a configuration for this HPO experiment\n# we pass the options of all hyperparameters we want to test\n# `config` will hold a single value for each parameter\nconfig = runai.hpo.pick(\ngrid=dict(\nbatch_size=[32, 64, 128],\nlr=[1, 0.1, 0.01, 0.001]),\nstrategy=strategy)\n....\n# Use the selected configuration within your code\noptimizer = keras.optimizers.SGD(lr=config['lr'])\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#run-an-hpo-workload","title":"Run an HPO Workload","text":"<ul> <li>At the command-line run:</li> </ul> <pre><code>runai config project team-a \nrunai submit hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1 \\\n    --parallelism 3 --completions 12 -v /nfs/john/hpo:/nfs\n</code></pre> <ul> <li>We named the Job hpo1</li> <li>The Job is assigned to team-a</li> <li>The Job will be complete when 12 pods will run (--completions 12), each allocated with a single GPU (-g 1)</li> <li>At most, there will be 3 pods running concurrently (--parallelism 3)</li> <li>The Job is based on a sample docker image <code>gcr.io/run-ai-demo/quickstart-hpo</code>. The image contains a startup script that selects a set of hyperparameters and then uses them, as described above. </li> <li>The command maps a shared volume <code>/nfs/john/hpo</code> to a directory in the container <code>/nfs</code>. The running pods will use the directory to sync hyperparameters and save results.</li> </ul> <p>Follow up on the Job's status by running:</p> <pre><code>runai list jobs\n</code></pre> <p>The result:</p> <p></p> <p>Follow up on the Job's pods by running:</p> <pre><code>runai describe job hpo1 \n</code></pre> <p>You will see 3 running pods currently executing:</p> <p></p> <p>Once the 3 pods are done, they will be replaced by new ones from the 12 completions. This process will continue until all 12 have run.</p> <p>You can also submit Jobs on another Project until only 2 GPUs remain. This will preempt 1 pod and will henceforth limit the HPO Job to run on 2 pods only. Preempted pods will be picked up and ran later.</p> <p>You can see logs of specific pods by running :</p> <pre><code>runai logs hpo1 --pod &lt;POD-NAME&gt;\n</code></pre> <p>where <code>&lt;&lt;POD-NAME&gt;&gt;</code> is a pod name as appears above in the <code>runai describe job hpo1</code> output </p> <p>The logs will contain a couple of lines worth noting:</p> <p>Picked HPO experiment #4</p> <p>...</p> <p>Using HPO directory /hpo</p> <p>Using configuration: {'batch_size': 32, 'lr': 0.001}</p>"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#examine-the-results","title":"Examine the Results","text":"<p>The Run:ai HPO library saves the experiment variations and the experiment results to a single file, making it easier to pick the best HPO run. The file can be found in the shared folder. Below is a snapshot of the file for two experiments with two epochs each:</p> <pre><code>creationTime: 24/08/2020 08:50:06\nexperiments:\n- config:\nbatch_size: 32\nlr: 1\nid: 1\nmodificationTime: 24/08/2020 08:50:06\nreports:\n- epoch: 0\nmetrics:\nacc: 0.09814\nloss: 2.310984723968506\nval_acc: 0.1\nval_loss: 2.3098626373291014\nreportTime: 24/08/2020 08:52:11\n- epoch: 1\nmetrics:\nacc: 0.09914\nloss: 2.30994320602417\nval_acc: 0.1\nval_loss: 2.3110838134765626\nreportTime: 24/08/2020 08:54:10\n- config:\nbatch_size: 32\nlr: 0.1\nid: 2\nmodificationTime: 24/08/2020 08:50:36\nreports:\n- epoch: 0\nmetrics:\nacc: 0.11012\nloss: 2.2979678358459474\nval_acc: 0.1667\nval_loss: 2.268467852783203\nreportTime: 24/08/2020 08:52:44\n- epoch: 1\nmetrics:\nacc: 0.2047\nloss: 2.0894255745697023\nval_acc: 0.2833\nval_loss: 1.8615504817962647\nreportTime: 24/08/2020 08:54:45\n</code></pre> <p>Finally, you can delete the HPO Job by running:</p> <pre><code>    runai delete job hpo1\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#see-also","title":"See Also","text":"<p>For further information on the Run:ai HPO support library see:</p> <ul> <li>The Run:ai HPO Support Library</li> <li>Sample code in Github</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/","title":"Quickstart: Over-Quota and Bin Packing","text":""},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#goals","title":"Goals","text":"<p>The goal of this Quickstart is to explain the concepts of over-quota and bin-packing (consolidation) and how they help in maximizing cluster utilization: </p> <ul> <li>Show the simplicity of resource provisioning, and how resources are abstracted from users.</li> <li>Show how the system eliminates compute bottlenecks by allowing teams/users to go over their resource quota if there are free GPUs in the cluster.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#setup-and-configuration","title":"Setup and configuration:","text":"<ul> <li>4 GPUs on 2 machines with 2 GPUs each</li> <li>2 Projects: team-a and team-b with 2 allocated GPUs each</li> <li>Run:ai canonical image gcr.io/run-ai-demo/quickstart</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-i-over-quota","title":"Part I: Over-quota","text":"<p>Run the following commands:</p> <pre><code>runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a\nrunai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a\nrunai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>team-a has 3 GPUs allocated. Which is over its quota by 1 GPU. </li> <li>The system allows this over-quota as long as there are available resources</li> <li>The system is at full capacity with all GPUs utilized. </li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-2-basic-fairness-via-preemption","title":"Part 2: Basic Fairness via Preemption","text":"<p>Run the following command:</p> <pre><code>runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>team-a can no longer remain in over-quota. Thus, one Job, must be preempted: moved out to allow team-b to grow.</li> <li>Run:ai scheduler chooses to preempt Job a1.</li> <li>It is important that unattended Jobs will save checkpoints. This will ensure that whenever Job a1 resume, it will do so from where it left off.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-3-bin-packing","title":"Part 3: Bin Packing","text":"<p>Run the following command:</p> <pre><code>runai delete job a2 -p team-a\n</code></pre> <p>a1 is now going to start running again.</p> <p>Run:</p> <pre><code>runai list jobs -A\n</code></pre> <p>You have two Jobs that are running on the first node and one Job that is running alone the second node. </p> <p>Choose one of the two Jobs from the full node and delete it:</p> <pre><code>runai delete job &lt;job-name&gt; -p &lt;project&gt;\n</code></pre> <p>The status now is: </p> <p>Now, run a 2 GPU Job:</p> <pre><code>runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a\n</code></pre> <p>The status now is: </p> <p>Discussion</p> <p>Note that Job a1 has been preempted and then restarted on the second node, in order to clear space for the new a2 Job. This is bin-packing or consolidation</p>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/","title":"Quickstart: Queue Fairness","text":""},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#goal","title":"Goal","text":"<p>The goal of this Quickstart is to explain fairness. The over-quota Quickstart shows basic fairness where allocated GPUs per Project are adhered to such that if a Project is in over-quota, its Job will be preempted once another Project requires its resources.</p> <p>This Quickstart is about queue fairness. It shows that Jobs will be scheduled fairly regardless of the time they have been submitted. As such, if a person in Project A has submitted 50 Jobs and soon after that, a person in Project B has submitted 25 Jobs, the Jobs in the queue will be processed fairly.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#setup-and-configuration","title":"Setup and configuration:","text":"<ul> <li>4 GPUs on 2 machines with 2 GPUs each.</li> <li>2 Projects: team-a and team-b with 1 allocated GPU each.</li> <li>Run:ai canonical image gcr.io/run-ai-demo/quickstart</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-i-immediate-displacement-of-over-quota","title":"Part I: Immediate Displacement of Over-Quota","text":"<p>Run the following commands:</p> <pre><code>runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a\nrunai submit a2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a\nrunai submit a3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a\nrunai submit a4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <p>team-a, even though it has a single GPU as quota, is now using all 4 GPUs.</p> <p>Run the following commands:</p> <pre><code>runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\nrunai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\nrunai submit b3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\nrunai submit b4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b\n</code></pre> <p>System status after run: </p> <p>Discussion</p> <ul> <li>Two team-b Jobs have immediately displaced team-a. </li> <li>team-a and team-b each have a quota of 1 GPU, thus the remaining over-quota (2 GPUs) is distributed equally between the Projects.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-2-queue-fairness","title":"Part 2: Queue Fairness","text":"<p>Now lets start deleting Jobs. Alternatively, you can wait for Jobs to complete.</p> <pre><code>runai delete job b2 -p team-b\n</code></pre> <p>Discussion</p> <p>As the quotas are equal (1 for each Project, the remaining pending Jobs will get scheduled one by one alternating between Projects, regardless of the time in which they were submitted. </p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/","title":"Quickstart: Launch Unattended Training Workloads","text":""},{"location":"Researcher/Walkthroughs/walkthrough-train/#introduction","title":"Introduction","text":"<p>Deep learning workloads can be divided into two generic types:</p> <ul> <li>Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly.</li> <li>Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results.</li> </ul> <p>With this Quickstart you will learn how to:</p> <ul> <li>Use the Run:ai command-line interface (CLI) to start a deep learning training workload.</li> <li>View training status and resource consumption using the Run:ai user interface and the Run:ai CLI.</li> <li>View training logs.</li> <li>Stop the training.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#prerequisites","title":"Prerequisites","text":"<p>To complete this Quickstart you must have:</p> <ul> <li>Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster</li> <li>Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#step-by-step-walkthrough","title":"Step by Step Walkthrough","text":""},{"location":"Researcher/Walkthroughs/walkthrough-train/#setup","title":"Setup","text":"<ul> <li>Login to the Projects area of the Run:ai user interface.</li> <li>Add a Project named \"team-a\".</li> <li>Allocate 2 GPUs to the Project.</li> </ul>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#run-workload","title":"Run Workload","text":"<ul> <li>At the command-line run:<pre><code>runai config project team-a\nrunai submit train1 -i gcr.io/run-ai-demo/quickstart -g 1\n</code></pre> </li> </ul> <p>This would start an unattended training Job for team-a with an allocation of a single GPU. The Job is based on a sample docker image <code>gcr.io/run-ai-demo/quickstart</code>. We named the Job <code>train1</code></p> <ul> <li>Follow up on the Job's progress by running:<pre><code>runai list jobs\n</code></pre> </li> </ul> <p>The result:</p> <p></p> <p>Typical statuses you may see:</p> <ul> <li>ContainerCreating - The docker container is being downloaded from the cloud repository</li> <li>Pending - the Job is waiting to be scheduled</li> <li>Running - the Job is running</li> <li>Succeeded - the Job has ended</li> </ul> <p>A full list of Job statuses can be found here </p> <p>To get additional status on your Job run:</p> <pre><code>runai describe job train1\n</code></pre>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-logs","title":"View Logs","text":"<p>Run the following:</p> <pre><code>runai logs train1\n</code></pre> <p>You should see a log of a running deep learning session:</p> <p></p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-status-on-the-runai-user-interface","title":"View status on the Run:ai User Interface","text":"<ul> <li>Open the Run:ai user interface.</li> <li>Under \"Jobs\" you can view the new Workload:</li> </ul> <p>The image we used for training includes the Run:ai Training library. Among other features, this library allows the reporting of metrics from within the deep learning Job. Metrics such as progress, accuracy, loss, and epoch and step numbers.  </p> <ul> <li>Progress can be seen in the status column above. </li> <li>To see other metrics, press the settings wheel on the top right  and select additional deep learning metrics from the list</li> </ul> <p>Under Nodes you can see node utilization:</p> <p></p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#stop-workload","title":"Stop Workload","text":"<p>Run the following:</p> <pre><code>runai delete job train1\n</code></pre> <p>This would stop the training workload. You can verify this by running <code>runai list jobs</code> again.</p>"},{"location":"Researcher/Walkthroughs/walkthrough-train/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Quickstart document: Launch Interactive Workloads</li> <li>Use your container to run an unattended training workload</li> </ul>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/","title":"Best Practice: From Bare Metal to Docker Images","text":""},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#introduction","title":"Introduction","text":"<p>Some Researchers do data science on bare metal. The term bare-metal relates to connecting to a server and working directly on its operating system and disks.</p> <p>This is the fastest way to start working, but it introduces problems when the data science organization scales:</p> <ul> <li>More Researchers mean that the machine resources need to be efficiently shared</li> <li>Researchers need to collaborate and share data, code, and results</li> </ul> <p>To overcome that, people working on bare-metal typically write scripts to gather data, code as well as code dependencies. This soon becomes an overwhelming task.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#why-use-docker-images","title":"Why Use Docker Images?","text":"<p>Docker images and containerization in general provide a level of abstraction which, by large, frees developers and Researchers from the mundane tasks of setting up an environment. The image is an operating system by itself and thus the 'environment' is by large, a part of the image.</p> <p>When a docker image is instantiated, it creates a container. A container is the running manifestation of a docker image.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#moving-a-data-science-environment-to-docker","title":"Moving a Data Science Environment to Docker","text":"<p>A data science environment typically includes:</p> <li>Training data</li> <li>Machine Learning (ML) code and inputs</li> <li>Libraries: Code dependencies that must be installed before the ML code can be run</li>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#training-data","title":"Training data","text":"<p>Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system.</p> <p>The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the Researcher is currently using, allowing the Researcher to easily migrate between machines. </p> <p>Organizations without a shared file system typically write scripts to copy data from machine to machine.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#machine-learning-code-and-inputs","title":"Machine Learning Code and Inputs","text":"<p>As a rule, code needs to be saved and versioned in a code repository.</p> <p>There are two alternative practices:</p> <ul> <li>The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code.</li> <li>When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. </li> </ul> <p>Both practices are valid.</p> <p>Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#code-dependencies","title":"Code Dependencies","text":"<p>Any code has code dependencies. These libraries must be installed for the code to run. As the code is changing, so do the dependencies.</p> <p>ML Code is typically python and python dependencies are typically declared together in a single <code>requirements.txt</code> file which is saved together with the code.</p> <p>The best practice is to have your docker startup script (see below) run this file using <code>pip install -r requirements.txt</code>. This allows the flexibility of adding and removing code dependencies dynamically.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#ml-lifecycle-build-and-train","title":"ML Lifecycle: Build and Train","text":"<p>Deep learning workloads can be divided into two generic types:</p> <li>Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm, or similar and accesses GPU resources directly. Build workloads are typically meant for debugging and development sessions. </li> <li>Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results. A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. </li> <p>Getting your docker ready is also a matter of which type of workload you are currently running.</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#build-workloads","title":"Build Workloads","text":"<p>With \"build\" you are actually coding and debugging small experiments. You are interactive. In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow) and use it directly.</p> <p>Start a docker container by running:</p> <pre><code>docker run -it .... \"the well known image\" -v /where/my/code/resides bash </code></pre> <p>You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh.</p> <p>You can also access the container remotely from tools such as PyCharm, Jupyter Notebook, and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service).</p>"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#training-workloads","title":"Training Workloads","text":"<p>For training workloads, you can use a well-known image (e.g. the TensorFlow image from the link above) but more often than not, you want to create your own docker image. The best practice is to use the well-known image (e.g. TensorFlow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile. A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.:</p> <ol><li>Base image is nvidia-tensorflow</li> <li>Install popular software</li> <li>(Optional) Run a script</li> </ol> <p>The script can be part of the image or can be provided as part of the command line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. </p> <p>The best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training Job. For further information on how to set up and parameterize a training workload via docker or Run:ai see Converting your Workload to use Unattended Training Execution.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/","title":"Best Practice: Convert your Workload to Run Unattended","text":""},{"location":"Researcher/best-practices/convert-to-unattended/#motivation","title":"Motivation","text":"<p>Run:ai allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this kind of flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires Researchers to switch workloads from running interactively, to running unattended, thus allowing Run:ai to pause/resume the run.</p> <p>Unattended workloads are a good fit for long-duration runs, or sets of smaller hyperparameter optimization runs.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#best-practices","title":"Best Practices","text":""},{"location":"Researcher/best-practices/convert-to-unattended/#docker-image","title":"Docker Image","text":"<p>A docker container is based on a docker image. Some Researchers use generic images such as ones provided by Nvidia, for example: NVIDIA NGC TensorFlow.  Others, use generic images as the base image to a more customized image using Dockerfiles.</p> <p>Realizing that Researchers are not always proficient with building docker files, as a best practice, you will want to:</p> <ul> <li>Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image.</li> <li>Leave some degree of flexibility, which allows the Researcher to add/remove python dependencies without re-creating images.</li> </ul>"},{"location":"Researcher/best-practices/convert-to-unattended/#code-location","title":"Code Location","text":"<p>You will want to minimize the cycle of code change-and-run. There are a couple of best practices which you can choose from:</p> <ol> <li>Code resides on the network file storage. This way you can change the code and immediately run the Job. The Job picks up the new files from the network.</li> <li>Use the <code>runai submit</code> flag <code>--git-sync</code>. The flag allows the Researcher to provide details of a Git repository. The repository will be automatically cloned into a specified directory when the container starts.</li> <li>The code can be embedded within the image. In this case, you will want to create an automatic CI/CD process, which packages the code into a modified image. </li> </ol> <p>The document below assumes option #1. </p>"},{"location":"Researcher/best-practices/convert-to-unattended/#create-a-startup-script","title":"Create a Startup Script","text":"<p>Gather the commands you ran inside the interactive Job into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john).</p> <p>An example of a common startup script start.sh:</p> <pre><code>pip install -r requirements.txt\n...\npython training.py\n</code></pre> <p>The first line of this script is there to make sure that all required python libraries are installed before the training script executes, it also allows the Researcher to add/remove libraries without needing changes to the image itself.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#support-variance-between-different-runs","title":"Support Variance Between Different Runs","text":"<p>Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods:</p> <ol> <li> <p>Your script can read arguments passed to the script:</p> <p><pre><code>python training.py --number-of-epochs=30</code></pre></p> </li> </ol> <p>In which case, change your start.sh script to:</p> <pre><code>pip install -r requirements.txt\n...\npython training.py $@</code></pre> <ol> <li>Your script can read from environment variables during script execution. In case you use environment variables, the variables will be passed to the training script automatically. No special action is required in this case.</li> </ol>"},{"location":"Researcher/best-practices/convert-to-unattended/#checkpoints","title":"Checkpoints","text":"<p>Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs).</p> <p>TensorFlow, Pytorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch).</p> <p>It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node</p> <p>For more information on best practices for saving checkpoints, see Saving Deep Learning Checkpoints.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#running-the-job","title":"Running the Job","text":"<p>Using <code>runai submit</code>, drop the flag <code>--interactive</code>. For submitting a Job using the script created above, please use <code>-- [COMMAND]</code> flag to specify a command, use the <code>--</code> syntax to pass arguments, and pass environment variables using the flag <code>--environment</code>.</p> <p>Example with Environment variables:</p> <pre><code>runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3  \n    -v /nfs/john:/mydir -g 1  --working-dir /mydir/  \n    -e 'EPOCHS=30'  -e 'LEARNING_RATE=0.02'  \n    -- ./startup.sh  \n</code></pre> <p>Example with Command-line arguments:</p> <pre><code>runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3  \n    -v /nfs/john:/mydir -g 1  --working-dir /mydir/  \n    -- ./startup.sh batch-size=64 number-of-epochs=3\n</code></pre> <p>Please refer to Command-Line Interface, runai submit for a list of all arguments accepted by the Run:ai CLI.</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#use-cli-policies","title":"Use CLI Policies","text":"<p>Different run configurations may vary significantly and can be tedious to be written each time on the command-line. To make life easier, our CLI offers a way to set administrator policies for these configurations and use pre-configured configuration when submitting a Workload. Please refer to Configure Command-Line Interface Policies. </p>"},{"location":"Researcher/best-practices/convert-to-unattended/#attached-files","title":"Attached Files","text":"<p>The 3 relevant files mentioned in this document can be downloaded from Github</p>"},{"location":"Researcher/best-practices/convert-to-unattended/#see-also","title":"See Also","text":"<p>See the unattended training Quickstart: Launch Unattended Training Workloads</p>"},{"location":"Researcher/best-practices/env-variables/","title":"Environment Variables inside a Run:ai Workload","text":""},{"location":"Researcher/best-practices/env-variables/#identifying-a-job","title":"Identifying a Job","text":"<p>There may be use cases where your container may need to uniquely identify the Job it is currently running in. A typical use case is for saving Job artifacts under a unique name.  Run:ai provides pre-defined environment variables you can use. These variables are guaranteed to be unique even if the Job is preempted or evicted and then runs again. </p> <p>Run:ai provides the following environment variables:</p> <ul> <li><code>JOB_NAME</code> - the name of the Job.</li> <li><code>JOB_UUID</code> - a unique identifier for the Job. </li> </ul> <p>Note that the Job can be deleted and then recreated with the same name. A Job UUID will be different even if the Job names are the same.</p>"},{"location":"Researcher/best-practices/env-variables/#identifying-a-pod","title":"Identifying a Pod","text":"<p>With Hyperparameter Optimization, experiments are run as Pods within the Job. Run:ai provides the following environment variables to identify the Pod.</p> <ul> <li><code>POD_INDEX</code> -  An index number (0, 1, 2, 3....) for a specific Pod within the Job. This is useful for Hyperparameter Optimization to allow easy mapping to individual experiments. The Pod index will remain the same if restarted (due to a failure or preemption). Therefore, it can be used by the Researcher to identify experiments. </li> <li><code>POD_UUID</code> - a unique identifier for the Pod. if the Pod is restarted, the Pod UUID will change.</li> </ul>"},{"location":"Researcher/best-practices/env-variables/#gpu-allocation","title":"GPU Allocation","text":"<p>Run:ai provides an environment variable, visible inside the container, to help identify the number of GPUs allocated for the container. Use <code>RUNAI_NUM_OF_GPUS</code></p>"},{"location":"Researcher/best-practices/env-variables/#node-name","title":"Node Name","text":"<p>There may be use cases where your container may need to identify the node it is currently running on. Run:ai provides an environment variable, visible inside the container, to help identify the name of the node on which the pod was scheduled. Use <code>NODE_NAME</code></p>"},{"location":"Researcher/best-practices/env-variables/#usage-example-in-python","title":"Usage Example in Python","text":"<pre><code>import os\njobName = os.environ['JOB_NAME']\njobUUID = os.environ['JOB_UUID']\n</code></pre>"},{"location":"Researcher/best-practices/save-dl-checkpoints/","title":"Best Practice: Save Deep-Learning Checkpoints","text":""},{"location":"Researcher/best-practices/save-dl-checkpoints/#introduction","title":"Introduction","text":"<p>Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs).</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#how-to-save-checkpoints","title":"How to Save Checkpoints","text":"<p>TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch).</p> <p>This document uses Keras as an example. The code itself can be found here</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#where-to-save-checkpoints","title":"Where to Save Checkpoints","text":"<p>It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node. Example:</p> <pre><code>runai submit train-with-checkpoints -i tensorflow/tensorflow:1.14.0-gpu-py3 \\\n  -v /mnt/nfs_share/john:/mydir -g 1  --working-dir /mydir --command -- ./startup.sh\n</code></pre> <p>The command saves the checkpoints in an NFS checkpoints folder <code>/mnt/nfs_share/john</code></p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#when-to-save-checkpoints","title":"When to Save Checkpoints","text":""},{"location":"Researcher/best-practices/save-dl-checkpoints/#save-periodically","title":"Save Periodically","text":"<p>It is a best practice to save checkpoints at intervals. For example, every epoch as the Keras code below shows:</p> <pre><code>checkpoints_file = \"weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(checkpoints_file, monitor='val_acc', verbose=1, \nsave_best_only=True, mode='max')\n</code></pre>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#save-on-exit-signal","title":"Save on Exit Signal","text":"<p>If periodic checkpoints are not enough, you can use a signal-hook provided by Run:ai (via Kubernetes). The hook is python code that is called before your Job is suspended and allows you to save your checkpoints as well as other state data you may wish to store.</p> <pre><code>import signal\nimport time\ndef graceful_exit_handler(signum, frame):\n# save your checkpoints to shared storage\n# exit with status \"1\" is important for the Job to return later.  \nexit(1)\nsignal.signal(signal.SIGTERM, graceful_exit_handler)\n</code></pre> <p>By default, you will have 30 seconds to save your checkpoints.</p> <p>Important</p> <p>For the signal to be captured, it must be propagated from the startup script to the python child process. See code here</p>"},{"location":"Researcher/best-practices/save-dl-checkpoints/#resuming-using-saved-checkpoints","title":"Resuming using Saved Checkpoints","text":"<p>A Run:ai unattended workload that is resumed, will run the same startup script as on the first run. It is the responsibility of the script developer to add code that:</p> <ul> <li>Checks if saved checkpoints exist (see above)</li> <li>If saved checkpoints exist, load them and start the run using these checkpoints</li> </ul> <pre><code>import os\ncheckpoints_file = \"weights.best.hdf5\"\nif os.path.isfile(checkpoints_file):\nprint(\"loading checkpoint file: \" + checkpoints_file)\nmodel.load_weights(checkpoints_file)\n</code></pre>"},{"location":"Researcher/cli-reference/Introduction/","title":"Introduction","text":"<p>The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc.</p> <p>To install and configure the Run:ai CLI see Researcher Setup - Start Here</p>"},{"location":"Researcher/cli-reference/runai-attach/","title":"runai attach","text":""},{"location":"Researcher/cli-reference/runai-attach/#description","title":"Description","text":"<p>Attach to a running Job.</p> <p>The command attaches to the standard input, output, and error streams of a running Job. If the Job has multiple pods the job will attach to the first pod unless otherwise set.</p>"},{"location":"Researcher/cli-reference/runai-attach/#synopsis","title":"Synopsis","text":"<pre><code>runai attach &lt;job-name&gt;\n    [--no-stdin ]\n    [--no-tty]   \n    [--pod string]\n    .\n    [--loglevel value] \n    [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-attach/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-attach/#-no-stdin","title":"--no-stdin","text":"<p>Do not attach STDIN.</p>"},{"location":"Researcher/cli-reference/runai-attach/#-no-tty","title":"--no-tty","text":"<p>Do not allocate a pseudo-TTY</p>"},{"location":"Researcher/cli-reference/runai-attach/#-pod-string","title":"--pod string","text":"<p>Attach to a specific pod within the Job. To find the list of pods run <code>runai describe job &lt;job-name&gt;</code> and then use the pod name with the <code>--pod</code> flag.</p>"},{"location":"Researcher/cli-reference/runai-attach/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-attach/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-attach/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-attach/#output","title":"Output","text":"<p>None</p>"},{"location":"Researcher/cli-reference/runai-bash/","title":"runai bash","text":""},{"location":"Researcher/cli-reference/runai-bash/#description","title":"Description","text":"<p>Get a bash session inside a running Job</p> <p>This command is a shortcut to runai exec (<code>runai exec -it job-name bash</code>). See runai exec for full documentation of the exec command.</p>"},{"location":"Researcher/cli-reference/runai-bash/#synopsis","title":"Synopsis","text":"<pre><code>runai bash &lt;job-name&gt; [--pod string]\n[--loglevel value] [--project string | -p string] [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-bash/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-bash/#-pod-string","title":"--pod string","text":"<p>Specify a pod of a running Job. To get a list of the pods of a specific Job, run <code>runai describe job &lt;job-name&gt;</code> command</p>"},{"location":"Researcher/cli-reference/runai-bash/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-bash/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>"},{"location":"Researcher/cli-reference/runai-bash/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-bash/#-help-h","title":"--help | -h","text":"<p>Show help text</p>"},{"location":"Researcher/cli-reference/runai-bash/#output","title":"Output","text":"<p>The command will access the container that should be currently running in the current cluster and attempt to create a command-line shell based on bash.</p> <p>The command will return an error if the container does not exist or has not been in a running state yet.</p>"},{"location":"Researcher/cli-reference/runai-bash/#see-also","title":"See also","text":"<p>Build Workloads. See Quickstart document: Launch Interactive Build Workloads.</p>"},{"location":"Researcher/cli-reference/runai-config/","title":"runai config","text":""},{"location":"Researcher/cli-reference/runai-config/#description","title":"Description","text":"<p>Set a default Project or Cluster</p>"},{"location":"Researcher/cli-reference/runai-config/#synopsis","title":"Synopsis","text":"<pre><code>runai  config project &lt;project-name&gt;\n    [--loglevel value] [--help | -h]\nrunai  config cluster &lt;cluster-name&gt;\n    [--loglevel value] [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-config/#options","title":"Options","text":"<p>&lt;project-name&gt;  - The name of the Project you want to set as default. Mandatory.</p> <p>&lt;cluster-name&gt; - The name of the cluster you want to set as the current cluster. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-config/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-config/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-config/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-config/#output","title":"Output","text":"<p>None</p>"},{"location":"Researcher/cli-reference/runai-delete/","title":"runai delete","text":""},{"location":"Researcher/cli-reference/runai-delete/#description","title":"Description","text":"<p>Delete a Workload and its associated Pods.</p> <p>Note that once you delete a Workload, its entire data will be gone:</p> <ul> <li>You will no longer be able to enter it via bash.</li> <li>You will no longer be able to access logs.</li> <li>Any data saved on the container and not stored in a shared location will be lost.</li> </ul>"},{"location":"Researcher/cli-reference/runai-delete/#synopsis","title":"Synopsis","text":"<pre><code>runai delete job &lt;job-name&gt; [--all | -A]\n[--loglevel value] [--project string | -p string] [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-delete/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Workload to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-delete/#-all-a","title":"--all | -A","text":"<p>Delete all Workloads.</p>"},{"location":"Researcher/cli-reference/runai-delete/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-delete/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-delete/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-delete/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-delete/#output","title":"Output","text":"<ul> <li> <p>The Workload will be deleted and not available via the command runai list jobs.</p> </li> <li> <p>The Workloads will show as <code>deleted</code> from the Run:ai user interface Job list.</p> </li> </ul>"},{"location":"Researcher/cli-reference/runai-delete/#see-also","title":"See Also","text":"<ul> <li> <p>Build Workloads. See Quickstart document: Launch Interactive Build Workloads.</p> </li> <li> <p>Training Workloads. See Quickstart document:  Launch Unattended Training Workloads.</p> </li> </ul>"},{"location":"Researcher/cli-reference/runai-describe/","title":"runai describe","text":""},{"location":"Researcher/cli-reference/runai-describe/#description","title":"Description","text":"<p>Display details of a Workload or Node.</p>"},{"location":"Researcher/cli-reference/runai-describe/#synopsis","title":"Synopsis","text":"<pre><code>runai describe job &lt;job-name&gt; [--output value | -o value]  [--loglevel value] [--project string | -p string] [--help | -h]\n[--output string | -o string]  runai describe node [node-name] [--loglevel value] [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-describe/#options","title":"Options","text":"<ul> <li>&lt;job-name&gt; - The name of the Workload to run the command with. Mandatory.</li> <li>&lt;node-name&gt; - The name of the Node to run the command with. If a Node name is not specified, a description of all Nodes is shown.</li> </ul> <p>-o | --output</p> <p>Output format. One of: json|yaml|wide. Default is 'wide'</p>"},{"location":"Researcher/cli-reference/runai-describe/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-describe/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-describe/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project, use: <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-describe/#-help-h","title":"--help | -h","text":"<p>Show help text</p>"},{"location":"Researcher/cli-reference/runai-describe/#output","title":"Output","text":"<ul> <li>The <code>runai describe job</code> command will show Workload properties and status as well as lifecycle events and the list of related resources and pods.</li> <li>The <code>runai describe node</code> command will show Node properties. </li> </ul>"},{"location":"Researcher/cli-reference/runai-exec/","title":"runai exec","text":""},{"location":"Researcher/cli-reference/runai-exec/#description","title":"Description","text":"<p>Execute a command inside a running Job</p> <p>Note: to execute a bash command, you can also use the shorthand runai bash</p>"},{"location":"Researcher/cli-reference/runai-exec/#synopsis","title":"Synopsis","text":"<pre><code>runai exec &lt;job-name&gt; &lt;command&gt; [--stdin | -i] [--tty | -t]\n[--loglevel value] [--project string | -p string] [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-exec/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p> <p>&lt;command&gt; the command itself (e.g. bash).</p>"},{"location":"Researcher/cli-reference/runai-exec/#-stdin-i","title":"--stdin | -i","text":"<p>Keep STDIN open even if not attached.</p>"},{"location":"Researcher/cli-reference/runai-exec/#-tty-t","title":"--tty | -t","text":"<p>Allocate a pseudo-TTY.</p>"},{"location":"Researcher/cli-reference/runai-exec/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-exec/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-exec/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-exec/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-exec/#output","title":"Output","text":"<p>The command will run in the context of the container.</p>"},{"location":"Researcher/cli-reference/runai-exec/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-list/","title":"runai list","text":""},{"location":"Researcher/cli-reference/runai-list/#description","title":"Description","text":"<p>Show lists of Workloads, Projects, Clusters or Nodes.</p>"},{"location":"Researcher/cli-reference/runai-list/#synopsis","title":"Synopsis","text":"<pre><code>runai list jobs [--all-projects | -A]  [--loglevel value] [--project string | -p string] [--help | -h]\nrunai list projects [--loglevel value] [--help | -h]\nrunai list clusters  [--loglevel value] [--help | -h]\nrunai list nodes [node-name]\n[--loglevel value] [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-list/#options","title":"Options","text":"<p><code>node-name</code> - Name of a specific node to list (optional).</p>"},{"location":"Researcher/cli-reference/runai-list/#-all-projects-a","title":"--all-projects | -A","text":"<p>Show Workloads from all Projects.</p>"},{"location":"Researcher/cli-reference/runai-list/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-list/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-list/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-list/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-list/#output","title":"Output","text":"<ul> <li>A list of Workloads, Nodes, Projects, or Clusters. </li> <li>To filter 'runai list nodes' for a specific Node, add the Node name.</li> </ul>"},{"location":"Researcher/cli-reference/runai-list/#see-also","title":"See Also","text":"<p>To show details for a specific Workload or Node see runai describe.</p>"},{"location":"Researcher/cli-reference/runai-login/","title":"runai login","text":""},{"location":"Researcher/cli-reference/runai-login/#description","title":"Description","text":"<p>Login to Run:ai</p> <p>When Researcher Authentication is enabled, you will need to login to Run:ai using your username and password before accessing resources </p>"},{"location":"Researcher/cli-reference/runai-login/#synopsis","title":"Synopsis","text":"<pre><code>runai login [--loglevel value]\n[--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-login/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-login/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-login/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-login/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-login/#output","title":"Output","text":"<p>You will be prompted for a user name and password</p>"},{"location":"Researcher/cli-reference/runai-login/#see-also","title":"See Also","text":"<ul> <li>runai logout.</li> </ul>"},{"location":"Researcher/cli-reference/runai-logout/","title":"runai logout","text":""},{"location":"Researcher/cli-reference/runai-logout/#description","title":"Description","text":"<p>Log out from Run:ai</p>"},{"location":"Researcher/cli-reference/runai-logout/#synopsis","title":"Synopsis","text":"<pre><code>runai logout [--loglevel value]\n[--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-logout/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-logout/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-logout/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-logout/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-logout/#output","title":"Output","text":"<p>You will be logged out from Run:ai</p>"},{"location":"Researcher/cli-reference/runai-logout/#see-also","title":"See Also","text":"<ul> <li>runai login.</li> </ul>"},{"location":"Researcher/cli-reference/runai-logs/","title":"runai logs","text":""},{"location":"Researcher/cli-reference/runai-logs/#description","title":"Description","text":"<p>Show the logs of a Job.</p>"},{"location":"Researcher/cli-reference/runai-logs/#synopsis","title":"Synopsis","text":"<pre><code>runai logs &lt;job-name&gt; [--follow | -f] [--pod string | -p string] [--since duration] [--since-time date-time] [--tail int | -t int] [--timestamps]  [--loglevel value] [--project string | -p string] [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-logs/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-follow-f","title":"--follow | -f","text":"<p>Stream the logs.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-pod-p","title":"--pod | -p","text":"<p>Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-instance-string-i-string","title":"--instance (string) | -i (string)","text":"<p>Show logs for a specific instance in cases where a Job contains multiple pods.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-since-duration","title":"--since (duration)","text":"<p>Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-since-time-date-time","title":"--since-time (date-time)","text":"<p>Return logs after specified date. Date format should be RFC3339, example: <code>2020-01-26T15:00:00Z</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-tail-int-t-int","title":"--tail (int) | -t (int)","text":"<p># of lines of recent log file to display.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-timestamps","title":"--timestamps","text":"<p>Include timestamps on each line in the log output.</p>"},{"location":"Researcher/cli-reference/runai-logs/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-logs/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-logs/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-logs/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-logs/#output","title":"Output","text":"<p>The command will show the logs of the first process in the container. For training Jobs, this would be the command run at startup. For interactive Jobs, the command may not show anything.</p>"},{"location":"Researcher/cli-reference/runai-logs/#see-also","title":"See Also","text":"<ul> <li>Training Workloads. See Quickstart document:  Launch Unattended Training Workloads.</li> </ul>"},{"location":"Researcher/cli-reference/runai-resume/","title":"runai resume","text":""},{"location":"Researcher/cli-reference/runai-resume/#description","title":"Description","text":"<p>Resume a suspended Job</p> <p>Resuming a previously suspended Job will return it to the queue for scheduling. The Job may or may not start immediately, depending on available resources. </p> <p>Suspend and resume do not work with mpi Jobs. </p>"},{"location":"Researcher/cli-reference/runai-resume/#synopsis","title":"Synopsis","text":"<pre><code>runai resume &lt;job-name&gt;\n    [--all | -A]\n[--loglevel value]\n[--project string | -p string]\n[--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-resume/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-resume/#-all-a","title":"--all | -A","text":"<p>Resume all suspended Jobs in the current Project.</p>"},{"location":"Researcher/cli-reference/runai-resume/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-resume/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-resume/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-resume/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-resume/#output","title":"Output","text":"<ul> <li>The Job will be resumed. When running runai list jobs the Job status will no longer by Suspended.</li> </ul>"},{"location":"Researcher/cli-reference/runai-resume/#see-also","title":"See Also","text":"<ul> <li>Suspending Jobs: Suspend.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-mpi/","title":"runai submit-mpi","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#description","title":"Description","text":"<p>Submit a Distributed Training (MPI) Run:ai Job for execution.</p> <p>Note</p> <p>To use distributed training you need to have installed the Kubeflow MPI Operator as specified here</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#synopsis","title":"Synopsis","text":"<p><pre><code>runai submit-mpi\n    [--attach]\n[--backoff-limit int] [--command]\n[--cpu double] [--cpu-limit double] [--create-home-dir]\n[--environment stringArray | -e stringArray] [--git-sync string]\n[--gpu double | -g double] [--gpu-memory string]\n[--host-ipc] [--host-network] [--image string | -i string] [--interactive] [--job-name-prefix string]\n[--large-shm] [--local-image] [--memory string] [--memory-limit string] [--mount-propagation]\n[--name string]\n[--node-pool string]\n[--node-type string] [--prevent-privilege-escalation]\n[--processes int] [--pvc [StorageClassName]:Size:ContainerMountPath:[ro]]\n[--run-as-user]\n[--s3 string]\n[--stdin]\n[--toleration string]\n[--tty | -t]\n[--volume stringArray | -v stringArray] [--nfs-server string]\n[--working-dir]  [--loglevel string] [--project string | -p string] [--help | -h]\n-- [COMMAND] [ARGS...] [options]\n</code></pre>  Syntax notes:</p> <ul> <li>Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#examples","title":"Examples","text":"<p>start an unattended mpi training Job of name dist1, based on Project team-a using a quickstart-distributed image:</p> <pre><code>runai submit-mpi --name dist1 --processes=2 -g 1 \\\n    -i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 -e RUNAI_SLEEP_SECS=60\n</code></pre> <p>(see: distributed training Quickstart).</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#aliases-and-shortcuts","title":"Aliases and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#-name","title":"--name","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-interactive","title":"--interactive","text":"<p>Mark this Job as Interactive. Interactive Jobs are not terminated automatically by the system.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-job-name-prefix-string","title":"--job-name-prefix string","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#container-related","title":"Container Related","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach. </p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true. </p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example: </p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-e-stringarray-environment-stringarray","title":"-e stringArray | --environment stringArray","text":"<p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).   </p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-git-sync-string","title":"--git-sync string","text":"<p>Clone a git repository into the container running the Job. The parameter should follow the syntax: <code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code>.</p> <p>Note that source and target fields are mandatory.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-image-string-i-string","title":"--image string | -i string","text":"<p>Image to use when creating the container for this Job</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-image-pull-policy-string","title":"--image-pull-policy string","text":"<p>Pulling policy of the image When starting a container. Options are: </p> <ul> <li><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded.</li> <li><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</li> <li><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</li> </ul> <p>For more information see Kubernetes documentation.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-local-image-deprecated","title":"--local-image (deprecated)","text":"<p>Deprecated. Please use <code>image-pull-policy=never</code> instead.  Use a local image for this Job. A local image is an image that exists on all local servers of the Kubernetes Cluster.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-stdin","title":"--stdin","text":"<p>Keep stdin open for the container(s) in the pod, even if nothing is attached.</p> <p>-t, --tty</p> <p>Allocate a pseudo-TTY</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-working-dir-string","title":"--working-dir string","text":"<p>Starts the container with the specified directory as the current directory.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#-cpu-double","title":"--cpu double","text":"<p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-cpu-limit-double","title":"--cpu-limit double","text":"<p>Limitations on the number of CPUs consumed by the Job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-gpu-double-g-double","title":"--gpu double | -g double","text":"<p>Number of GPUs to allocate for the Job. The default is no allocated GPUs. the GPU value can be an integer or a fraction between 0 and 1.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-gpu-memory","title":"--gpu-memory","text":"<p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-large-shm","title":"--large-shm","text":"<p>Mount a large /dev/shm device. An shm is a shared file system mounted on RAM.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-memory-string","title":"--memory string","text":"<p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-memory-limit-string","title":"--memory-limit string","text":"<p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>","text":"<p>Mount a persistent volume claim into a container.</p> <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p> <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-volume-sourcecontainer_mount_pathronfs-host","title":"--volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'","text":"<p>Volumes to mount into the container.</p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code> Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code> Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-nfs-server-string","title":"--nfs-server string","text":"<p>Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-mount-propagation","title":"--mount-propagation","text":"<p>The flag allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. When the flag is set, Run:ai will set mount propagation to the value of <code>HostToContainer</code> as documented here. With <code>HostToContainer</code> the volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-git-sync-string_1","title":"--git-sync string","text":"<p>Clone a git repository into the container running the job. The parameter should follow the syntax:</p> <p><code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code></p> <p>Note that source and target fields are mandatory.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-s3-string","title":"--s3 string","text":"<p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#-host-ipc","title":"--host-ipc","text":"<p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference documentation.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-host-network","title":"--host-network","text":"<p>Use the host's network stack inside the container. For further information see docker run referencedocumentation.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#-backoff-limit-int","title":"--backoff-limit int","text":"<p>The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the <code>--interactive</code> flag is not specified).</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-processes-int","title":"--processes int","text":"<p>Number of distributed training processes. The default is 1.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-prevent-privilege-escalation","title":"--prevent-privilege-escalation","text":"<p>Prevent the Job\u2019s container and all launched processes from gaining additional privileges after the Job starts. Default is <code>false</code>. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-run-as-user","title":"--run-as-user","text":"<p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#-node-type-string","title":"--node-type string","text":"<p>Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group.</p> <p>This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see: Working with Projects.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-node-pools-string","title":"--node-pools string","text":"<p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-toleration-string","title":"--toleration string","text":"<p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string: <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre></p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit-mpi/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\")</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#output","title":"Output","text":"<p>The command will attempt to submit an mpi Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit-mpi/#see-also","title":"See Also","text":"<ul> <li>See Quickstart document Running Distributed Training.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit/","title":"runai submit","text":""},{"location":"Researcher/cli-reference/runai-submit/#description","title":"Description","text":"<p>Submit a Run:ai Job for execution.</p>"},{"location":"Researcher/cli-reference/runai-submit/#synopsis","title":"Synopsis","text":"<pre><code>runai submit [--attach]\n[--backoff-limit int] [--completions int]\n[--cpu double] [--cpu-limit double] [--create-home-dir]\n[--environment stringArray | -e stringArray] [--git-sync string]\n[--gpu double | -g double] [--gpu-memory string]\n[--host-ipc] [--host-network] [--image string | -i string]\n[--imagePullPolicy string] [--interactive] [--jupyter] [--job-name-prefix string]\n[--large-shm] [--local-image] [--memory string] [--memory-limit string] [--mount-propagation]\n[--mps] [--name string]\n[--node-pool string]\n[--node-type string] [--parallelism int]\n[--port stringArray] [--preemptible] [--prevent-privilege-escalation]\n[--pvc [StorageClassName]:Size:ContainerMountPath:[ro]]\n[--run-as-user] [--s3 string]\n[--service-type string | -s string] [--stdin]\n[--toleration string]\n[--tty | -t]\n[--volume stringArray | -v stringArray] [--nfs-server string]\n[--working-dir] [--loglevel string] [--project string | -p string] [--help | -h]\n-- [COMMAND] [ARGS...] [options]\n</code></pre> <p>Syntax notes:</p> <ul> <li>Flags of type stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.</li> </ul>"},{"location":"Researcher/cli-reference/runai-submit/#examples","title":"Examples","text":"<p>All examples assume a Run:ai Project has been set using <code>runai config project &lt;project-name&gt;</code>.</p> <p>Start an interactive Job:</p> <pre><code>runai submit -i ubuntu --interactive --attach -g 1\n</code></pre> <p>Or</p> <pre><code>runai submit --name build1 -i ubuntu -g 1 --interactive -- sleep infinity \n</code></pre> <p>(see: build Quickstart).</p> <p>Externalize ports:</p> <pre><code>runai submit --name build-remote -i rastasheep/ubuntu-sshd:14.04 --interactive \\\n    --service-type=nodeport --port 30022:22\n    -- /usr/sbin/sshd -D\n</code></pre> <p>(see: build with ports Quickstart).</p> <p>Start a Training Job</p> <pre><code>runai submit --name train1 -i gcr.io/run-ai-demo/quickstart -g 1\n</code></pre> <p>(see: training Quickstart).</p> <p>Use GPU Fractions</p> <pre><code>runai submit --name frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5\n</code></pre> <p>(see: GPU fractions Quickstart).</p> <p>Hyperparameter Optimization</p> <pre><code>runai submit --name hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1  \\\n    --parallelism 3 --completions 12 -v /nfs/john/hpo:/hpo\n</code></pre> <p>(see: hyperparameter optimization Quickstart).</p> <p>Submit a Job without a name (automatically generates a name)</p> <pre><code>runai submit -i gcr.io/run-ai-demo/quickstart -g 1\n</code></pre> <p>Submit a Job without a name with a pre-defined prefix and an incremental index suffix</p> <pre><code>runai submit --job-name-prefix -i gcr.io/run-ai-demo/quickstart -g 1\n</code></pre>"},{"location":"Researcher/cli-reference/runai-submit/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-submit/#aliases-and-shortcuts","title":"Aliases and Shortcuts","text":""},{"location":"Researcher/cli-reference/runai-submit/#-name","title":"--name","text":"<p>The name of the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-interactive","title":"--interactive","text":"<p>Mark this Job as Interactive. Interactive Jobs are not terminated automatically by the system.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-jupyter","title":"--jupyter","text":"<p>Shortcut for running a Jupyter notebook container. Uses a pre-created image and a default notebook configuration. </p> <p>Example:</p> <p><code>runai submit --name jup1 --jupyter -g 0.5 --service-type=nodeport</code> will start an interactive session named jup1 and use an nodeport load balancer to connect to it. The output of the command is an access token for the notebook. Run <code>runai list jobs</code> to find the URL for the notebook.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-job-name-prefix-string","title":"--job-name-prefix string","text":"<p>The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional <code>--job-name-prefix flag</code> creates Job names with the provided prefix</p>"},{"location":"Researcher/cli-reference/runai-submit/#container-related","title":"Container Related","text":""},{"location":"Researcher/cli-reference/runai-submit/#-attach","title":"--attach","text":"<p>Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach.</p> <p>The --attach flag also sets <code>--tty</code> and <code>--stdin</code> to true. </p>"},{"location":"Researcher/cli-reference/runai-submit/#-command","title":"--command","text":"<p>Overrides the image's entry point with the command supplied after '--'. When not using the <code>--command</code> flag, the entry point will not be overrided and the string after <code>--</code> will be appended as arguments to the entry point command.</p> <p>Example: </p> <p><code>--command -- run.sh 1 54</code> will start the docker and run <code>run.sh 1 54</code></p> <p><code>-- script.py 10000</code> will augment <code>script.py 10000</code> to the entry point command (e.g. <code>python</code>)</p>"},{"location":"Researcher/cli-reference/runai-submit/#-e-stringarray-environment-stringarray","title":"-e stringArray | --environment stringArray","text":"<p>Define environment variables to be set in the container. To set multiple values add the flag multiple times (<code>-e BATCH_SIZE=50 -e LEARNING_RATE=0.2</code>).</p>"},{"location":"Researcher/cli-reference/runai-submit/#-image-string-i-string","title":"--image string | -i string","text":"<p>Image to use when creating the container for this Job</p>"},{"location":"Researcher/cli-reference/runai-submit/#-image-pull-policy-string","title":"--image-pull-policy string","text":"<p>Pulling policy of the image When starting a container. Options are: </p> <ul> <li> <p><code>Always</code> (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded. </p> </li> <li> <p><code>IfNotPresent</code>: the image is pulled only if it is not already present locally.</p> </li> <li> <p><code>Never</code>: the image is assumed to exist locally. No attempt is made to pull the image.</p> </li> </ul> <p>For more information see Kubernetes documentation.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-local-image-deprecated","title":"--local-image (deprecated)","text":"<p>Deprecated. Please use <code>image-pull-policy=never</code> instead.  Use a local image for this Job. A local image is an image that exists on all local servers of the Kubernetes Cluster. </p> <p></p>"},{"location":"Researcher/cli-reference/runai-submit/#-mps","title":"--mps","text":"<p>Use NVIDIA MPS. NIVDIA MPS is useful with Inference workloads for optimizing multiple processes running on a single GPU. The <code>--mps</code> flag only works in conjunction with GPU fractions (--gpu <code>&lt;num&gt;</code> where <code>&lt;num&gt;</code> is not an integer or using --gpu-memory).</p>"},{"location":"Researcher/cli-reference/runai-submit/#-stdin","title":"--stdin","text":"<p>Keep stdin open for the container(s) in the pod, even if nothing is attached.</p> <p>-t, --tty</p> <p>Allocate a pseudo-TTY.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-working-dir-string","title":"--working-dir string","text":"<p>Starts the container with the specified directory as the current directory.</p>"},{"location":"Researcher/cli-reference/runai-submit/#resource-allocation","title":"Resource Allocation","text":""},{"location":"Researcher/cli-reference/runai-submit/#-cpu-double","title":"--cpu double","text":"<p>CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-cpu-limit-double","title":"--cpu-limit double","text":"<p>Limitations on the number of CPUs consumed by the Job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of CPUs.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-gpu-double-g-double","title":"--gpu double | -g double","text":"<p>Number of GPUs to allocate for the Job. The default is no allocated GPUs. The GPU value can be an integer or a fraction between 0 and 1.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-gpu-memory","title":"--gpu-memory","text":"<p>GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-large-shm","title":"--large-shm","text":"<p>Mount a large /dev/shm device. An shm is a shared file system mounted on RAM.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-memory-string","title":"--memory string","text":"<p>CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-memory-limit-string","title":"--memory-limit string","text":"<p>CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.</p>"},{"location":"Researcher/cli-reference/runai-submit/#storage","title":"Storage","text":""},{"location":"Researcher/cli-reference/runai-submit/#-pvc-storage_class_namesizecontainer_mount_pathro","title":"--pvc <code>[Storage_Class_Name]:Size:Container_Mount_Path:[ro]</code>","text":""},{"location":"Researcher/cli-reference/runai-submit/#-pvc-pvc_namecontainer_mount_pathro","title":"--pvc <code>Pvc_Name:Container_Mount_Path:[ro]</code>","text":"<p>Mount a persistent volume claim of Network Attached Storage into a container.</p> <p>The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both.</p> <p>Storage_Class_Name is a storage class name that can be obtained by running <code>kubectl get storageclasses.storage.k8s.io</code>. This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. </p> <p>Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes</p> <p>Container_Mount_Path. A path internal to the container where the storage will be mounted</p> <p>Pvc_Name. The name of a pre-existing Persistent Volume Claim to mount into the container</p> <p>Examples:</p> <p><code>--pvc :3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the default Storage class. Mount it to <code>/tmp/john</code> as read-only </p> <p><code>--pvc my-storage:3Gi:/tmp/john:ro</code>  - Allocate <code>3GB</code> from the <code>my-storage</code> storage class. Mount it to /tmp/john as read-only </p> <p><code>--pvc :3Gi:/tmp/john</code> - Allocate <code>3GB</code> from the default storage class. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc:/tmp/john</code> - Use a Persistent Volume Claim named <code>my-pvc</code>. Mount it to <code>/tmp/john</code> as read-write </p> <p><code>--pvc my-pvc-2:/tmp/john:ro</code> - Use a Persistent Volume Claim named <code>my-pvc-2</code>. Mount it to <code>/tmp/john</code> as read-only</p>"},{"location":"Researcher/cli-reference/runai-submit/#-volume-sourcecontainer_mount_pathronfs-host","title":"--volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'","text":"<p>Volumes to mount into the container.  </p> <p>Examples:</p> <p><code>-v /raid/public/john/data:/root/data:ro</code>  Mount /root/data to local path /raid/public/john/data for read-only access.</p> <p><code>-v /public/data:/root/data::nfs.example.com</code> Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-nfs-server-string","title":"--nfs-server string","text":"<p>Use this flag to specify the default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume  individually (see --volume for details).</p>"},{"location":"Researcher/cli-reference/runai-submit/#-mount-propagation","title":"--mount-propagation","text":"<p>The flag allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. When the flag is set, Run:ai will set mount propagation to the value of <code>HostToContainer</code> as documented here. With <code>HostToContainer</code> the volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-git-sync-string","title":"--git-sync string","text":"<p>Clone a git repository into the container running the job. The parameter should follow the syntax:</p> <p><code>source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE</code></p> <p>Note that source and target fields are mandatory.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-s3-string","title":"--s3 string","text":"<p>Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax:</p> <p><code>bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH</code></p> <p>All the fields, except url=URL, are mandatory. Default for url is</p> <p><code>url=https://s3.amazon.com</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#network","title":"Network","text":""},{"location":"Researcher/cli-reference/runai-submit/#-host-ipc","title":"--host-ipc","text":"<p>Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack.</p> <p>For further information see docker run reference.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-host-network","title":"--host-network","text":"<p>Use the host's network stack inside the container. For further information see docker run reference.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-port-stringarray","title":"--port stringArray","text":"<p>Expose ports from the Job container. Used together with <code>--service-type</code>.  </p> <p>Example:  </p> <p><code>--port 8080:80 --service-type portforward</code></p>"},{"location":"Researcher/cli-reference/runai-submit/#-service-type-string-s-string","title":"--service-type string | -s string","text":"<p>Service exposure method for interactive Job. Options are: <code>portforward</code>, <code>loadbalancer</code>, <code>nodeport</code>.  Use the command runai list to obtain the endpoint to use the service when the Job is running. Different service methods have different endpoint structures.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-address-string","title":"--address string","text":"<p>Comma separated list of IP addresses to listen to when running interactive job  with portforward service type (default is localhost).</p> <p>Example:</p> <p><code>--interactive --service-type portforward --address \"localhost,192.168.1.2\" --port 8888:5555</code></p> <p>Any traffic on port 8888 of both localhost and 192.168.1.2 will be forwarded to port 5555 of the running job.</p>"},{"location":"Researcher/cli-reference/runai-submit/#job-lifecycle","title":"Job Lifecycle","text":""},{"location":"Researcher/cli-reference/runai-submit/#-backoff-limit-int","title":"--backoff-limit int","text":"<p>The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the <code>--interactive</code> flag is not specified).</p>"},{"location":"Researcher/cli-reference/runai-submit/#-completions-int","title":"--completions int","text":"<p>The number of successful pods required for this Job to be completed. Used for Hyperparameter optimization. Use together with <code>--parallelism</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-parallelism-int","title":"--parallelism int","text":"<p>The number of pods this Job tries to run in parallel at any time.  Used for Hyperparameter optimization. Use together with <code>--completions</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit/#access-control","title":"Access Control","text":""},{"location":"Researcher/cli-reference/runai-submit/#-create-home-dir","title":"--create-home-dir","text":"<p>Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-prevent-privilege-escalation","title":"--prevent-privilege-escalation","text":"<p>Prevent the Job\u2019s container and all launched processes from gaining additional privileges after the Job starts. Default is <code>false</code>. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-run-as-user","title":"--run-as-user","text":"<p>Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML , you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers.</p>"},{"location":"Researcher/cli-reference/runai-submit/#scheduling","title":"Scheduling","text":""},{"location":"Researcher/cli-reference/runai-submit/#-node-type-string","title":"--node-type string","text":"<p>Allows defining specific nodes (machines) or a group of nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group. This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see: Working with Projects.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-node-pools-string","title":"--node-pools string","text":"<p>Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool. You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool.</p> <p>This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-preemptible","title":"--preemptible","text":"<p>Mark an interactive Job as preemptible. Preemptible Jobs can be scheduled above the guaranteed quota but may be reclaimed at any time.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-toleration-string","title":"--toleration string","text":"<p>Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node.  This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide.</p> <p>The format of the string: <pre><code>operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]\n</code></pre></p>"},{"location":"Researcher/cli-reference/runai-submit/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-submit/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-submit/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-submit/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-submit/#output","title":"Output","text":"<p>The command will attempt to submit a Job. You can follow up on the Job by running <code>runai list jobs</code> or <code>runai describe job &lt;job-name&gt;</code>.</p> <p>Note that the submit call may use a policy to provide defaults to any of the above flags.</p>"},{"location":"Researcher/cli-reference/runai-submit/#see-also","title":"See Also","text":"<ul> <li>See any of the Quickstart documents here:.</li> <li>See policy configuration for a description on how policies work.</li> </ul>"},{"location":"Researcher/cli-reference/runai-suspend/","title":"runai suspend","text":""},{"location":"Researcher/cli-reference/runai-suspend/#description","title":"Description","text":"<p>Suspend a Job</p> <p>Suspending a Running Job will stop the Job and will not allow it to be scheduled until it is resumed using <code>runai resume</code>. This means that,</p> <ul> <li>You will no longer be able to enter it via <code>runai bash</code>.</li> <li>The Job logs will be deleted.</li> <li>Any data saved on the container and not stored in a shared location will be lost.</li> </ul> <p>Technically, the command deletes the Kubernetes pods associated with the Job and marks the Job as suspended until it is manually released. </p> <p>Suspend and resume do not work with MPI and Inference </p>"},{"location":"Researcher/cli-reference/runai-suspend/#synopsis","title":"Synopsis","text":"<pre><code>runai suspend &lt;job-name&gt;\n    [--all | -A]\n[--loglevel value]\n[--project string | -p string]\n[--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-suspend/#options","title":"Options","text":"<p>&lt;job-name&gt; - The name of the Job to run the command with. Mandatory.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-all-a","title":"--all | -A","text":"<p>Suspend all Jobs in the current Project.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-suspend/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-project-p-string","title":"--project | -p (string)","text":"<p>Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use <code>runai config project &lt;project-name&gt;</code>.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-suspend/#output","title":"Output","text":"<ul> <li>The Job will be suspended. When running runai list jobs the Job will be marked as Suspended.</li> </ul>"},{"location":"Researcher/cli-reference/runai-suspend/#see-also","title":"See Also","text":"<ul> <li>Resuming Jobs: Resume.</li> </ul>"},{"location":"Researcher/cli-reference/runai-top-job/","title":"runai top job","text":""},{"location":"Researcher/cli-reference/runai-top-job/#description","title":"Description","text":"<p>Show list of Jobs, their resource requirements and utilization.</p>"},{"location":"Researcher/cli-reference/runai-top-job/#synopsis","title":"Synopsis","text":"<pre><code>runai top job \n    [--help | -h]\n    [--details | -d]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-top-job/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-top-job/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-top-job/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-top-job/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-top-job/#-details-d","title":"--details | -d","text":"<p>Show additional details.</p>"},{"location":"Researcher/cli-reference/runai-top-job/#output","title":"Output","text":"<p>Shows a list of Jobs their resource requirements and utilization.</p>"},{"location":"Researcher/cli-reference/runai-top-job/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-top-node/","title":"runai top node","text":""},{"location":"Researcher/cli-reference/runai-top-node/#description","title":"Description","text":"<p>Show list of Nodes (machines), their capacity and utilization.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#synopsis","title":"Synopsis","text":"<pre><code>runai top node \n    [--help | -h]\n    [--details | -d]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-top-node/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-top-node/#global-flags","title":"Global Flags","text":""},{"location":"Researcher/cli-reference/runai-top-node/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-top-node/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#-details-d","title":"--details | -d","text":"<p>Show additional details.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#output","title":"Output","text":"<p>Shows a list of Nodes their capacity and utilization.</p>"},{"location":"Researcher/cli-reference/runai-top-node/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-update/","title":"runai update","text":""},{"location":"Researcher/cli-reference/runai-update/#description","title":"Description","text":"<p>Find and install the latest version of the runai command-line utility. The command must be run with sudo permissions.</p> <pre><code>sudo runai update\n</code></pre>"},{"location":"Researcher/cli-reference/runai-update/#synopsis","title":"Synopsis","text":"<pre><code>runai update [--loglevel value] [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-update/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-update/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-update/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-update/#output","title":"Output","text":"<p>Update of the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-update/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-version/","title":"runai version","text":""},{"location":"Researcher/cli-reference/runai-version/#description","title":"Description","text":"<p>Show the version of this utility.</p>"},{"location":"Researcher/cli-reference/runai-version/#synopsis","title":"Synopsis","text":"<pre><code>runai version [--loglevel value] [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-version/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-version/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-version/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-version/#output","title":"Output","text":"<p>The version of the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-version/#see-also","title":"See Also","text":""},{"location":"Researcher/cli-reference/runai-whoami/","title":"runai whoami","text":""},{"location":"Researcher/cli-reference/runai-whoami/#description","title":"Description","text":"<p>Show the user name currently logged in</p>"},{"location":"Researcher/cli-reference/runai-whoami/#synopsis","title":"Synopsis","text":"<pre><code>runai whoami [--loglevel value] [--help | -h]\n</code></pre>"},{"location":"Researcher/cli-reference/runai-whoami/#options","title":"Options","text":""},{"location":"Researcher/cli-reference/runai-whoami/#-loglevel-string","title":"--loglevel (string)","text":"<p>Set the logging level. One of: debug | info | warn | error (default \"info\").</p>"},{"location":"Researcher/cli-reference/runai-whoami/#-help-h","title":"--help | -h","text":"<p>Show help text.</p>"},{"location":"Researcher/cli-reference/runai-whoami/#output","title":"Output","text":"<p>The name of the User currently logged in with the Run:ai command-line interface.</p>"},{"location":"Researcher/cli-reference/runai-whoami/#see-also","title":"See Also","text":""},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/","title":"Allocation of CPU and Memory","text":""},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#introduction","title":"Introduction","text":"<p>When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But two additional resources are no less important:</p> <ul> <li>CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run.</li> <li>Memory. Has a direct influence on the quantities of data a training run can process in batches.</li> </ul> <p>GPU servers tend to come installed with a significant amount of memory and CPUs.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#requesting-cpu-memory","title":"Requesting CPU &amp; Memory","text":"<p>When submitting a Job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example:</p> <pre><code>runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G\n</code></pre> <p>The system guarantees that if the Job is scheduled, you will be able to receive this amount of CPU and memory.</p> <p>For further details on these flags see: runai submit</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#cpu-over-allocation","title":"CPU over allocation","text":"<p>The number of CPUs your Job will receive is guaranteed to be the number defined using the <code>--cpu</code> flag. In practice, however, you may receive more CPUs than you have asked for:</p> <ul> <li>If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined.</li> <li>However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the <code>--cpu</code> flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 cpus, the workloads will receive 10 and 30 CPUs respectively. If the flag <code>--cpu</code> is not specified, it will be taken from the cluster default (see the section below)</li> </ul>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#memory-over-allocation","title":"Memory over allocation","text":"<p>The amount of Memory your Job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above.</p> <p>It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your Job may receive an out-of-memory exception and terminate.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#cpu-and-memory-limits","title":"CPU and Memory limits","text":"<p>You can limit your Job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example:</p> <pre><code>runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\\n    --memory 1G --memory-limit 4G\n</code></pre> <p>The limit behavior is different for CPUs and memory.</p> <ul> <li>Your Job will never be allocated with more than the amount stated in the <code>--cpu-limit</code> flag</li> <li>If your Job tries to allocate more than the amount stated in the <code>--memory-limit</code> flag it will receive an out-of-memory exception.</li> </ul> <p>The limit (for both CPU and memory) overrides the cluster default described in the section below</p> <p>For further details on these flags see: runai submit</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#flag-defaults","title":"Flag Defaults","text":""},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-cpu-flag","title":"Defaults for --cpu flag","text":"<p>If your Job has not specified <code>--cpu</code>, the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs.</p> <p>If, for example, the default has been defined as 1:6 and your Job has specified <code>--gpu 2</code> and has not specified <code>--cpu</code>, then the implied <code>--cpu</code> flag value is 12 CPUs.</p> <p>The system comes with a cluster-wide default of 1:1. To change the ratio see below.</p> <p>If you didn't request any GPUs for your job and has not specified <code>--cpu</code>, the default is defined as a ratio of CPU limit to CPUs.</p> <p>If, for example, the default has been defined as 1:0.2 and your Job has specified <code>--cpu-limit 10</code> and has not specified <code>--cpu</code>, then the implied <code>--cpu</code> flag value is 2 CPUs.</p> <p>The system comes with a cluster-wide default of 1:0.1. To change the ratio see below.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-memory-flag","title":"Defaults for --memory flag","text":"<p>If your Job has not specified <code>--memory</code>, the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs.</p> <p>The system comes with a cluster-wide default of 100MiB of allocated CPU memory per GPU. To change the ratio see below.</p> <p>If you didn't request any GPUs for your job and has not specified <code>--memory</code>, the default is defined as a ratio of CPU Memory limit to CPU Memory Request.</p> <p>The system comes with a cluster-wide default of 1:0.1. To change the ratio see below.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-cpu-limit-flag","title":"Defaults for --cpu-limit flag","text":"<p>If your Job has not specified <code>--cpu-limit</code>, then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to CPUs. See below on how to change the ratio.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-memory-limit-flag","title":"Defaults for --memory-limit flag","text":"<p>If your Job has not specified <code>--memory-limit</code>, then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to Memory. See below on how to change the ratio.</p>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#changing-the-ratios","title":"Changing the ratios","text":"<p>To change the cluster wide-ratio use the following process. The example shows: </p> <ul> <li>a CPU request with a default ratio of 2:1 CPUs to GPUs.</li> <li>a CPU Memory request with a default ratio of 200MB per GPU.</li> <li>a CPU limit with a default ratio of 4:1 CPU to GPU.</li> <li>a Memory limit with a default ratio of 2GB per GPU.</li> <li>a CPU request with a default ratio of 0.1 CPUs per 1 CPU limit.</li> <li>a CPU Memory request with a default ratio of 0.1:1 request per CPU Memory limit.</li> </ul> <p>You must edit the cluster installation values file:</p> <ul> <li>When installing the Run:ai cluster, edit the values file.</li> <li>On an existing installation, use the upgrade cluster instructions to modify the values file.</li> <li>You must specify at least the first 4 values as follows: </li> </ul> <pre><code>runai-operator:\nconfig:\nlimitRange:\ncpuDefaultRequestGpuFactor: 2\nmemoryDefaultRequestGpuFactor: 200Mi\ncpuDefaultLimitGpuFactor: 4\nmemoryDefaultLimitGpuFactor: 2Gi\ncpuDefaultRequestCpuLimitFactorNoGpu: 0.1\nmemoryDefaultRequestMemoryLimitFactorNoGpu: 0.1\n</code></pre>"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#validating-cpu-memory-allocations","title":"Validating CPU &amp; Memory Allocations","text":"<p>To review CPU &amp; Memory allocations you need to look into Kubernetes. A Run:ai Job creates a Kubernetes pod. The pod declares its resource requests and limits. To see the memory and CPU consumption in Kubernetes:</p> <ul> <li>Get the pod name for the Job by running: <pre><code>runai describe job &lt;JOB_NAME&gt;\n</code></pre> </li> </ul> <p>the pod will appear under the <code>PODS</code> category. </p> <ul> <li>Run:<pre><code>kubectl describe pod &lt;POD_NAME&gt;\n</code></pre> </li> </ul> <p>The information will appear under <code>Requests</code> and <code>Limits</code>. For example:</p> <pre><code>Limits:\nnvidia.com/gpu:  2\nRequests:\ncpu:             1\nmemory:          104857600\nnvidia.com/gpu:  2\n</code></pre>"},{"location":"Researcher/scheduling/fractions/","title":"Allocation of GPU Fractions","text":""},{"location":"Researcher/scheduling/fractions/#introduction","title":"Introduction","text":"<p>A single GPU has a significant amount of memory. Ranging from a couple of gigabytes in older generations and up to 80GB per GPU in the later models of the latest NVIDIA GPU technology. A single GPU also has a vast amount of computing power. </p> <p>This amount of memory and computing power is important for processing large amounts of data, such as in training deep learning models. However, there are quite a few applications that do not need this power. Examples can be inference workloads and the model-creation phase. It would thus be convenient if we could divide up a GPU between various workloads, thus achieving better GPU utilization. </p> <p>This article describes two complementary technologies that allow the division of GPUs and how to use them with Run:ai.</p> <ol> <li>Run:ai Fractions. </li> <li>Dynamic allocation using NVIDIA Multi-instance GPU (MIG)</li> </ol>"},{"location":"Researcher/scheduling/fractions/#runai-fractions","title":"Run:ai Fractions","text":"<p>Run:ai provides the capability to allocate a container with a specific amount of GPU RAM. As a researcher, if you know that your code needs 4GB of RAM. You can submit a job using the flag <code>--gpu-memory 4G</code> to specify the exact portion of the GPU memory that you need. Run:ai will allocate your container that specific amount of GPU RAM. Attempting to reach beyond your allotted RAM will result in an out-of-memory exception. </p> <p>You can also use the flag <code>--gpu 0.2</code> to get 20% of the GPU memory on the GPU assigned for you. </p> <p>For more details on Run:ai fractions see the fractions quickstart.</p> <p>Limitation</p> <p>With the fraction technology all running workloads, which utilize the GPU, share the compute in parallel and on average get an even share of the compute. For example, assuming two containers, one with 0.25 GPU workload and the other with 0.75 GPU workload - both will get (on average) an equal part of the computation power. If one of the workloads does not utilize the GPU, the other workload will get the entire GPU's compute power.</p> <p>Info</p> <p>For interoperability with other Kubernetes schedulers, Run:ai creates special reservation pods. Once a workload has been allocated a fraction of a GPU, Run:ai will create a pod in a dedicated <code>runai-reservation</code> namespace with the full GPU as a resource. This would cause other schedulers to understand that the GPU is reserved.    </p>"},{"location":"Researcher/scheduling/fractions/#dynamic-mig","title":"Dynamic MIG","text":"<p>NVIDIA MIG allows GPUs based on the NVIDIA Ampere architecture (such as NVIDIA A100) to be partitioned into separate GPU Instances:</p> <ul> <li>When divided, the portion acts as a fully independent GPU.</li> <li>The division is static, in the sense that you have to call NVIDIA API or the <code>nvidia-smi</code> command to create or remove the MIG partition. </li> <li>The division is both of compute and memory.</li> <li>The division has fixed sizes.  Up to 7 units of compute and memory in fixed sizes. The various MIG profiles can be found in the NVIDIA documentation. A typical profile can be <code>MIG 2g.10gb</code> which provides 2/7 of the compute power and 10GB of RAM</li> <li>Reconfiguration of MIG profiles on the GPU requires administrator permissions and the draining of all running workloads. </li> </ul> <p>Run:ai provides a way to dynamically create a MIG partition:</p> <ul> <li>Using the same experience as the Fractions technology above, if you know that your code needs 4GB of RAM. You can use the flag <code>--gpu-memory 4G</code> to specify the portion of the GPU memory that you need. Run:ai will call the NVIDIA MIG API to generate the smallest possible MIG profile for your request, and allocate it to your container. </li> <li>MIG is configured on the fly according to workload demand, without needing to drain workloads or to involve an IT administrator.</li> <li>Run:ai will automatically deallocate the partition when the workload finishes. This happens in a lazy fashion in the sense that the partition will not be removed until the scheduler decides that it is needed elsewhere. </li> <li>Run:ai provides an additional flag to dynamically create the specific MIG partition in NVIDIA terminology. As such, you can specify <code>--mig-profile 2g.10gb</code>.  </li> <li>In a single GPU cluster you have some MIG nodes that are dynamically allocated and some that are not.</li> </ul> <p>For more details on Run:ai fractions see the dynamic MIG quickstart.</p>"},{"location":"Researcher/scheduling/fractions/#setting-up-dynamic-mig","title":"Setting up Dynamic MIG","text":"<p>As described above, MIG is only available in the latest NVIDIA architecture. </p> <ul> <li>When working with Kubernetes, NVIDIA defines a concept called MIG Strategy. With Run:ai you must set the MIG strategy to <code>mixed</code>. See NVIDIA prerequisites on how to set this flag. </li> <li> <p>The administrator needs to specifically enable dynamic MIG on the node by running: </p> <p><pre><code>runai-adm set node-role --dynamic-mig-enabled &lt;node-name&gt;\n</code></pre> (use <code>runai-adm remove</code> to unset)</p> </li> <li> <p>Make sure that MIG is enabled on the node level (see dynamic MIG quickstart for details) and set:     <pre><code>kubectl label node &lt;node-name&gt; node-role.kubernetes.io/runai-mig-enabled=true\n</code></pre>    (use <code>kubectl</code> to unset)</p> </li> </ul> <p>Limitations</p> <ul> <li>Once a node has been marked as dynamic MIG enabled, it can only be used via the Run:ai scheduler.</li> <li>Run:ai currently supports H100 or A100 nodes with 40GB/80GB RAM.</li> <li>GPU utilization, shown on the Run:ai dashboards, may not be accurate while MIG jobs are running.</li> </ul>"},{"location":"Researcher/scheduling/fractions/#mixing-fractions-and-dynamic-mig","title":"Mixing Fractions and Dynamic MIG","text":"<p>Given a specific node, the IT administrator can decide whether to use one technology or the other. When the Researcher asks for a specific amount of GPU memory, Run:ai will either provide it on an annotated node by dynamically allocating a MIG partition, or use a different node using the fractions technology.</p>"},{"location":"Researcher/scheduling/fractions/#see-also","title":"See Also","text":"<ul> <li>Fractions quickstart.</li> <li>Dynamic MIG quickstart</li> </ul>"},{"location":"Researcher/scheduling/hpo/","title":"Researcher Library: Hyperparameter Optimization Support","text":"<p>The Run:ai Researcher Library is a python library you can add to your deep learning python code. The hyperparameter optimization(HPO) support module of the library is a helper library for hyperparameter optimization (HPO) experiments</p> <p>Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. Example hyperparameters: Learning rate, Batch size, Different optimizers, number of layers.</p> <p>To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine the results to decide what works best.</p> <p>With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch, and more. In addition, you can externalize custom metrics of your choosing.</p>"},{"location":"Researcher/scheduling/hpo/#getting-started","title":"Getting Started","text":""},{"location":"Researcher/scheduling/hpo/#prerequisites","title":"Prerequisites","text":"<p>Run:ai HPO library is dependent on PyYAML. Install it using the command:</p> <pre><code>pip install pyyaml\n</code></pre>"},{"location":"Researcher/scheduling/hpo/#installing","title":"Installing","text":"<p>Install the <code>runai</code> Python library using <code>pip</code> using the following command:</p> <pre><code>pip install runai\n</code></pre> <p>Make sure to use the correct <code>pip</code> installer (you might need to use <code>pip3</code> for Python3)</p>"},{"location":"Researcher/scheduling/hpo/#usage","title":"Usage","text":"<ul> <li>Import the <code>runai.hpo</code> package.</li> </ul> <pre><code>import runai.hpo\n</code></pre> <ul> <li>Initialize the Run:ai HPO library with a path to a directory shared between all cluster nodes (typically using an NFS server). We recommend specifying a unique name for the experiment, the name will be used to create a sub-directory on the shared folder. To do so, we recommend using the environment variables <code>JOB_NAME</code> and <code>JOB_UUID</code> which are injected to the container by Run:ai.</li> </ul> <pre><code>hpo_root = '/path/to/nfs'\nhpo_experiment = '%s_%s' % (os.getenv('JOB_NAME'), os.getenv('JOB_UUID'))\nrunai.hpo.init(hpo_root, hpo_experiment)\n</code></pre> <ul> <li>Decide on an HPO strategy:<ul> <li>Random search - randomly pick a set of hyperparameter values</li> <li>Grid search - pick the next set of hyperparameter values, iterating through all sets across multiple experiments</li> </ul> </li> </ul> <pre><code>strategy = runai.hpo.Strategy.GridSearch\n</code></pre> <ul> <li>Call the Run:ai HPO library to specify a set of hyperparameters and pick a specific configuration for this experiment.</li> </ul> <pre><code>config = runai.hpo.pick(\ngrid=dict(\nbatch_size=[32, 64, 128],\nlr=[1, 0.1, 0.01, 0.001]),\nstrategy=strategy)\n</code></pre> <ul> <li>Use the returned configuration in your code. For example:</li> </ul> <pre><code>optimizer = keras.optimizers.SGD(lr=config['lr'])\n</code></pre> <p>Metrics could be reported and saved in the experiment directory under the fule <code>runai.yaml</code> using <code>runai.hpo.report</code>. You should pass the epoch number and a dictionary with metrics to be reported. For example:</p> <pre><code>runai.hpo.report(epoch=5, metrics={ 'accuracy': 0.87 })\n</code></pre>"},{"location":"Researcher/scheduling/hpo/#see-also","title":"See Also","text":"<ul> <li>See hyperparameter Optimization Quickstart</li> <li>Sample code in Github</li> </ul>"},{"location":"Researcher/scheduling/job-statuses/","title":"Job Statuses","text":""},{"location":"Researcher/scheduling/job-statuses/#introduction","title":"Introduction","text":"<p>The runai submit function and its sibling the runai submit-mpi function submit Run:ai Jobs for execution. </p> <p>A Job has a status. Once a Job is submitted it goes through several statuses before ending in an End State. Most of these statuses originate in the underlying Kubernetes infrastructure, but some are Run:ai-specific. </p> <p>The purpose of this document is to explain these statuses as well as the lifecycle of a Job. </p>"},{"location":"Researcher/scheduling/job-statuses/#successful-flow","title":"Successful Flow","text":"<p>A regular, training Job that has no errors and executes without preemption would go through the following statuses:</p> <p></p> <ul> <li>Pending - the Job is waiting to be scheduled.</li> <li>ContainerCreating - the Job has been scheduled, the Job docker image is now downloading.</li> <li>Running - the Job is now executing.</li> <li>Succeeded - the Job has finished with exit code 0 (success).</li> </ul> <p>The Job can be preempted, in which case it can go through other statuses:</p> <ul> <li>Terminating - the Job is now being preempted.</li> <li>Pending - the Job is waiting in queue again to receive resources.</li> </ul> <p>An interactive Job, by definition, needs to be closed by the Researcher and will thus never reach the Succeeded status. Rather, it would be moved by the Researcher to status Deleted.</p> <p>For a further explanation of the additional statuses, see the table below.</p>"},{"location":"Researcher/scheduling/job-statuses/#error-flow","title":"Error flow","text":"<p>A regular, training Job may encounter an error inside the running process (exit code is non-zero). In which case the following will happen:</p> <p></p> <p>The Job enters an Error status and then immediately tries to reschedule itself for another attempted run. The reschedule can happen on another node in the system. After a specified number of retries, the Job will enter a final status of Fail</p> <p>An interactive Job, enters an Error status and then moves immediately to CrashLoopBackOff trying to reschedule itself. The reschedule attempt has no 'back-off' limit and will continue to retry indefinitely </p> <p></p> <p>Jobs may be submitted with an image that cannot be downloaded. There are special statuses for such Jobs. See table below </p>"},{"location":"Researcher/scheduling/job-statuses/#status-table","title":"Status Table","text":"<p>Below is a list of statuses. For each status the list shows:</p> <ul> <li> <p>Name</p> </li> <li> <p>End State - this status is the final status in the lifecycle of the Job</p> </li> <li> <p>Resource Allocation - when the Job is in this status, does the system allocate resources to it</p> </li> <li> <p>Description</p> </li> <li> <p>Color - Status color as can be seen in the Run:ai User Interface Job list</p> </li> </ul> <p>Status</p> <p>End State</p> <p>Resource Allocation</p> <p>Description</p> <p>Color</p> <p>Running</p> <p></p> <p>Yes</p> <p>Job is running successfully</p> <p></p> <p>Terminating</p> <p></p> <p>Yes</p> <p>Pod is being evicted at the moment (e.g. due to an over-quota allocation, the reason will be written once eviction finishes). A new pod will be created shortly</p> <p></p> <p>ContainerCreating</p> <p></p> <p>Yes</p> <p>Image is being pulled from registry.</p> <p></p> <p>Pending</p> <p></p> <p>-</p> <p>Job is pending. Possible reasons:</p> <p>- Not enough resources</p> <p>- Waiting in Queue (over quota etc).</p> <p></p> <p>Succeeded</p> <p>Yes</p> <p>-</p> <p>An Unattended (training) Job has ran and finished successfully.</p> <p></p> <p>Deleted</p> <p>Yes</p> <p>-</p> <p>Job has been deleted.</p> <p></p> <p>TimedOut</p> <p>Yes</p> <p>-</p> <p>Interactive Job has reached the defined timeout of the project.</p> <p></p> <p>Preempted</p> <p>Yes</p> <p>-</p> <p>Interactive preemptible Job has been evicted.</p> <p></p> <p>ContainerCannotRun</p> <p>Yes</p> <p>-</p> <p>Container has failed to start running. This is typically a problem within the docker image itself.</p> <p></p> <p>Error</p> <p></p> <p>Yes for interactive only </p> <p>The Job has returned an exit code different than zero. It is now waiting for another run attempt (retry).</p> <p></p> <p>Fail</p> <p>Yes</p> <p>-</p> <p>Job has failed after a number of retries (according to \"--backoffLimit\" field) and will not be trying again.</p> <p></p> <p>CrashLoopBackOff</p> <p></p> <p>Yes</p> <p>Interactive Only: During backoff after Error, before a retry attempt to run pod on the same node.</p> <p></p> <p>ErrImagePull, ImagePullBackOff</p> <p></p> <p>Yes</p> <p>Failing to retrieve docker image</p> <p></p> <p>Unknown</p> <p>Yes</p> <p>-</p> <p>The Run:ai Scheduler wasn't running when the Job has finished.</p> <p></p>"},{"location":"Researcher/scheduling/job-statuses/#how-to-get-more-information","title":"How to get more information","text":"<p>The system stores various events during the Job's lifecycle. These events can be helpful in diagnosing issues around Job scheduling. To view these events run:</p> <pre><code>runai describe job &lt;workload-name&gt;\n</code></pre> <p>Sometimes, useful information can be found by looking at  logs emitted from the process running inside the container. For example, Jobs that have exited with an exit code different than zero may write an exit reason in this log. To see Job logs run:</p> <pre><code>runai logs &lt;job-name&gt;\n</code></pre>"},{"location":"Researcher/scheduling/job-statuses/#distributed-training-mpi-jobs","title":"Distributed Training (mpi) Jobs","text":"<p>A distributed (mpi) Job, which has no errors will be slightly more complicated and has additional statuses associated with it. </p> <ul> <li> <p>Distributed Jobs start with an \"init container\" which sets the stage for a distributed run.</p> </li> <li> <p>When the init container finishes, the main \"launcher\" container is created. The launcher is responsible for coordinating between the different workers</p> </li> <li> <p>Workers run and do the actual work.</p> </li> </ul> <p>A successful flow of distribute training would look as:</p> <p></p> <p>Additional Statuses:</p> <p>Status</p> <p>End State</p> <p>Resource Allocation</p> <p>Description</p> <p>Color</p> <p>Init:&lt;number A&gt;/&lt;number B&gt;</p> <p></p> <p>Yes</p> <p>The Pod has B Init Containers, and A have completed so far.</p> <p></p> <p>PodInitializing</p> <p></p> <p>Yes</p> <p>The pod has finished executing Init Containers. The system is creating the main 'launcher' container</p> <p></p> <p>Init:Error</p> <p></p> <p></p> <p>An Init Container has failed to execute.</p> <p></p> <p>Init:CrashLoopBackOff</p> <p></p> <p></p> <p>An Init Container has failed repeatedly to execute</p> <p></p>"},{"location":"Researcher/scheduling/strategies/","title":"Scheduling Strategies","text":""},{"location":"Researcher/scheduling/strategies/#introduction","title":"Introduction","text":"<p>When the Run:ai scheduler schedules Jobs, it can use two alternate placement strategies:</p> Strategy Description Bin Packing Fill up a GPU and/or a node before moving on to the next one Spreading Equally spread Jobs amongst GPUs and nodes"},{"location":"Researcher/scheduling/strategies/#bin-packing","title":"Bin Packing","text":"<p>Bin packing is the default strategy. With bin packing, the scheduler tries to:</p> <ul> <li>Fill up a node with Jobs before allocating Jobs to second and third nodes.</li> <li>In a multi GPU node, when using fractions, fill up a GPU before allocating Jobs to a second GPU. </li> </ul> <p>The advantage of this strategy is that the scheduler, over time, can package more Jobs into the cluster. As the strategy minimizes fragmentation. </p> <p>For example, if we have 2 GPUs in a single node on the cluster, and 2 tasks requiring 0.5 GPUs each, using bin-packing, we would place both Jobs on the same GPU and remain with a full GPU ready for the next Job. </p>"},{"location":"Researcher/scheduling/strategies/#spreading","title":"Spreading","text":"<p>There are disadvantages to bin-packing: </p> <ul> <li>Within a single GPU, two fractional Jobs compete for the same onboard compute power. </li> <li>Within a single node, two Jobs (even on separate GPUs) compete for networking resources, compute power and memory. </li> </ul> <p>When there are more resources available than requested, it sometimes makes sense to spread Jobs amongst nodes and GPUs, to allow higher utilization of computing resources and network resources. </p> <p>Returning to the example above, if we have 2 GPUs in a single node on the cluster, and 2 Jobs requiring 0.5 GPUs each, using spread scheduling we would place each Job on a separate GPU, allowing both to benefit from the computing power of a full GPU.</p>"},{"location":"Researcher/scheduling/strategies/#changing-scheduler-strategy","title":"Changing Scheduler Strategy","text":"<p>The strategy affects the entire cluster. To change the strategy run:</p> <pre><code>kubectl edit runaiconfig -n runai\n</code></pre> <p>Find `runai-scheduler' and add:</p> <pre><code>runai-scheduler:\n  placementStrategy: spread\n</code></pre>"},{"location":"Researcher/scheduling/the-runai-scheduler/","title":"The Run:ai Scheduler","text":""},{"location":"Researcher/scheduling/the-runai-scheduler/#introduction","title":"Introduction","text":"<p>At the heart of the Run:ai solution is the Run:ai scheduler. The scheduler is the gatekeeper of your organization's hardware resources. It makes decisions on resource allocations according to pre-created rules.</p> <p>The purpose of this document is to describe the Run:ai scheduler and explain how resource management works.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#terminology","title":"Terminology","text":""},{"location":"Researcher/scheduling/the-runai-scheduler/#workload-types","title":"Workload Types","text":"<p>Run:ai differentiates between three types of deep learning workloads:</p> <ul> <li>Interactive build workloads. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Build workloads typically do not tax the GPU for a long duration. There are also typically real users behind an interactive workload that need an immediate scheduling response.</li> <li> <p>Unattended (or \"non-interactive\") training workloads. Training is characterized by a deep learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. Training workloads typically utilize large percentages of the GPU. During the execution, the Researcher can examine the results. A Training session can take anything from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored.  It follows that a good practice for the Researcher is to save checkpoints and allow the code to restore from the last checkpoint.</p> </li> <li> <p>Inference workloads. These are production workloads that serve requests. The Run:ai scheduler treats these workloads as Interactive workloads.</p> </li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#projects","title":"Projects","text":"<p>Projects are quota entities that associate a Project name with a deserved GPU quota as well as other preferences.</p> <p>A Researcher submitting a workload must associate a Project with any workload request. The Run:ai scheduler will then compare the request against the current allocations and the Project's deserved quota and determine whether the workload can be allocated with resources or whether it should remain in a pending state.</p> <p>For further information on Projects and how to configure them, see: Working with Projects</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#departments","title":"Departments","text":"<p>A Department is the second hierarchy of resource allocation above Project. A Department quota supersedes a Project quota in the sense that if the sum of Project quotas for Department A exceeds the Department quota -- the scheduler will use the Department quota rather than the Projects' quota.  </p> <p>For further information on Departments and how to configure them, see: Working with Departments</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#pods","title":"Pods","text":"<p>Pods are units of work within a Job. </p> <ul> <li>Typically, each Job has a single Pod. However, in some scenarios (see Hyperparameter Optimization and Distribute Training below) there will be multiple Pods per Job. </li> <li>All Pods execute with the same arguments as added via <code>runai submit</code>. E.g. The same image name, the same code script, the same number of Allocated GPUs, memory.</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#basic-scheduling-concepts","title":"Basic Scheduling Concepts","text":""},{"location":"Researcher/scheduling/the-runai-scheduler/#interactive-training-and-inference","title":"Interactive, Training and Inference","text":"<p>The Researcher uses the --interactive flag to specify whether the workload is an unattended \"train\" workload or an interactive \"build\" workload.</p> <ul> <li>Interactive &amp; Inference workloads will get precedence over training workloads.</li> <li>Training workloads can be preempted when the scheduler determines a more urgent need for resources. Interactive workloads are never preempted.</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#guaranteed-quota-and-over-quota","title":"Guaranteed Quota and Over-Quota","text":"<p>There are two use cases for Quota and Over-Quota:</p> <p>Node pools are disabled</p> <p>Every new workload is associated with a Project. The Project contains a deserved GPU quota. During scheduling:</p> <ul> <li>If the newly required resources, together with currently used resources, end up within the Project's quota, then the workload is ready to be scheduled as part of the guaranteed quota.</li> <li>If the newly required resources together with currently used resources end up above the Project's quota, the workload will only be scheduled if there are 'spare' GPU resources. There are nuances in this flow that are meant to ensure that a Project does not end up with an over-quota made fully of interactive workloads. For additional details see below.</li> </ul> <p>Node pools are enabled</p> <p>Every new workload is associated with a Project. The Project contains a deserved GPU quota that is the sum off all node pools GPU quotas. During scheduling:</p> <ul> <li>If the newly required resources, together with currently used resources, end up within the overall Project's quota and the requested node pool(s) quota, then the workload is ready to be scheduled as part of the guaranteed quota.</li> <li>If the newly required resources together with currently used resources end up above the Project's quota or the requested node pool(s) quota, the workload will only be scheduled if there are 'spare' GPU resources within the same node pool but not part of this Project. There are nuances in this flow that are meant to ensure that a Project does not end up with an over-quota made entirely of interactive workloads. For additional details see below.</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#quota-with-multiple-resources","title":"Quota with Multiple Resources","text":"<p>A project may have a quota set for more than one resource (GPU, CPU or CPU Memory). For a project to be \"Over Quota\" it will have to have at least one resource over its quota. For a project to be under quota it needs to have all of its resources under quota.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#scheduler-details","title":"Scheduler Details","text":""},{"location":"Researcher/scheduling/the-runai-scheduler/#allocation-preemption","title":"Allocation &amp; Preemption","text":"<p>The Run:ai scheduler wakes up periodically to perform allocation tasks on pending workloads:</p> <ul> <li>The scheduler looks at each Project separately and selects the most 'deprived' Project.</li> <li> <p>For this deprived Project it chooses a single workload to work on:</p> <ul> <li>Interactive &amp; Inference workloads are tried first, but only up to the Project's guaranteed quota. If such a workload exists, it is scheduled even if it means preempting a running unattended workload in this Project.</li> <li>Else, it looks for an unattended workload and schedules it on guaranteed quota or over-quota.</li> </ul> </li> <li> <p>The scheduler then recalculates the next 'deprived' Project and continues with the same flow until it finishes attempting to schedule all workloads</p> </li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#node-pools","title":"Node Pools","text":"<p>A Node Pool is a set of nodes grouped by an Administrator into a distinct group of resources from which resources can be allocated to Projects and Departments. By default, any node pool created in the system is automatically associated with all Projects and Departments using zero quota resource (GPUs, CPUs, Memory) allocation. This allows any Project and Department to use any node pool with Over-Quota (for Preemptible workloads), thus maximizing the system resource utilization.</p> <ul> <li>An Administrator can allocate resources from a specific node pool to chosen Projects and Departments. See Project Setup</li> <li>The Researcher can use node pools in two ways. The first one is where a Project has guaranteed resources on node pools - The Researcher can then submit a workload and specify a single node pool or a prioritized list of node pools to use and receive guaranteed resources.  The second is by using node-pool(s) with no guaranteed resource for that Project (zero allocated resources), and in practice using Over-Quota resources of node-pools. This means a Workload must be Preemptible as it uses resources out of the Project or node pool quota. The same scenario occurs if a Researcher uses more resources than allocated to a specific node pool and goes Over-Quota.</li> <li>By default, if a Researcher doesn't specify a node-pool to use by a workload, the scheduler assigns the workload to run using the Project's 'Default node-pool list'.</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#node-affinity","title":"Node Affinity","text":"<p>Both the Administrator and the Researcher can provide limitations as to which nodes can be selected for the Job. Limits are managed via Kubernetes labels:</p> <ul> <li>The Administrator can set limits at the Project level. Example: Project <code>team-a</code> can only run <code>interactive</code> Jobs on machines with a label of <code>v-100</code> or <code>a-100</code>. See Project Setup for more information.</li> <li>The Researcher can set a limit at the Job level, by using the command-line interface flag <code>--node-type</code>. The flag acts as a subset to the Project setting. </li> </ul> <p>Node affinity constraints are used during the Allocation phase to filter out candidate nodes for running the Job. For more information on how nodes are filtered see the <code>Filtering</code> section under Node selection in kube-scheduler. The Run:ai scheduler works similarly.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#reclaim","title":"Reclaim","text":"<p>During the above process, there may be a pending workload whose Project is below the deserved capacity. Still, it cannot be allocated due to the lack of GPU resources. The scheduler will then look for alternative allocations at the expense of another Project which has gone over-quota while preserving fairness between Projects.</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#fairness","title":"Fairness","text":"<p>The Run:ai scheduler determines fairness between multiple over-quota Projects according to their GPU quota. Consider for example two Projects, each spawning a significant amount of workloads (e.g. for Hyperparameter tuning) all of which wait in the queue to be executed. The Run:ai Scheduler allocates resources while preserving fairness between the different Projects regardless of the time they entered the system. The fairness works according to the relative portion of the GPU quota for each Project. To further illustrate that, suppose that:</p> <ul> <li>Project A has been allocated a quota of 3 GPUs.</li> <li>Project B has been allocated a quota of 1 GPU.</li> </ul> <p>Then, if both Projects go over quota, Project A will receive 75% (=3/(1+3)) of the idle GPUs and Project B will receive 25% (=1/(1+3)) of the idle GPUs. This ratio will be recalculated every time a new Job is submitted to the system or an existing Job ends.</p> <p>This fairness equivalence will also be maintained amongst running Jobs. The scheduler will preempt training sessions to maintain this equivalence </p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#over-quota-priority","title":"Over-Quota Priority","text":"<p>When the Over Quota Priority feature is enabled, The Run:ai scheduler allocates GPUs within-quota and over-quota using different weights. Within quota, GPUs are allocated based on assigned GPUs. The remaining over-quota GPUs are allocated based on their relative portion of GPU Over Quota Priority for each Project.  GPUs Over-Quota Priority values are translated into numeric values as follows: None-0, Low-1, Medium-2, High-3.</p> <p>Let's examine the previous example with Over-Quota Weights:</p> <ul> <li>Project A has been allocated with a quota of 3 GPUs and GPU over quota weight is set to Low.</li> <li>Project B has been allocated with a quota of 1 GPU and GPU over quota weight is set to High.</li> </ul> <p>Then, Project A is allocated with 3 GPUs and project B is allocated with 1 GPU. if both Projects go over quota, Project A will receive an additional 25% (=1/(1+3)) of the idle GPUs and Project B will receive an additional 75% (=3/(1+3)) of the idle GPUs.</p> <p>With the addition of node pools, the principles of Over-Quota and Over-Quota priority remain unchanged. However, the number of resources that are allocated with Over-Quota and Over-Quota Priority is calculated against node pool resources instead of the whole Project resources.</p> <ul> <li>Note: Over-Quota On/Off and Over-Quota Priority settings remain at the Project and Department level.  </li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#bin-packing-consolidation","title":"Bin-packing &amp; Consolidation","text":"<p>Part of an efficient scheduler is the ability to eliminate fragmentation:</p> <ul> <li>The first step in avoiding fragmentation is bin packing: try and fill nodes (machines) up before allocating workloads to new machines.</li> <li>The next step is to consolidate Jobs on demand. If a workload cannot be allocated due to fragmentation, the scheduler will try and move unattended workloads from node to node in order to get the required amount of GPUs to schedule the pending workload.</li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#advanced","title":"Advanced","text":""},{"location":"Researcher/scheduling/the-runai-scheduler/#gpu-fractions","title":"GPU Fractions","text":"<p>Run:ai provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU.</p> <p>Run:ai\u2019s fractional GPU system effectively creates logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. </p> <p>One important thing to note is that fraction scheduling divides up GPU memory. As such the GPU memory is divided up between Jobs. If a Job asks for 0.5 GPU, and the GPU has 32GB of memory, then the Job will see only 16GB. An attempt to allocate more than 16GB will result in an out-of-memory exception.</p> <p>GPU Fractions are scheduled as regular GPUs in the sense that:</p> <ul> <li>Allocation is made using fractions such that the total of the GPU allocation for a single GPU is smaller or equal to 1.</li> <li>Preemption is available for non-interactive workloads.  </li> <li>Bin-packing &amp; Consolidation work the same for fractions.</li> </ul> <p>Support: </p> <ul> <li>Hyperparameter Optimization supports fractions. </li> </ul>"},{"location":"Researcher/scheduling/the-runai-scheduler/#distributed-training","title":"Distributed Training","text":"<p>Distributed Training, is the ability to split the training of a model among multiple processors. It is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Each such split is a pod (see definition above). Run:ai spawns an additional launcher process that manages and coordinates the other worker pods.</p> <p>Distribute Training utilizes a practice sometimes known as Gang Scheduling:</p> <ul> <li>The scheduler must ensure that multiple pods are started on what are typically multiple Nodes before the Job can start. </li> <li>If one pod is preempted, the others are also immediately preempted.</li> <li>When node pools are enabled, all pods must be scheduled to the same node pool.</li> </ul> <p>Gang Scheduling essentially prevents scenarios where part of the pods are scheduled while other pods belonging to the same Job are pending for resources to become available; scenarios that can cause deadlock situations and major inefficiencies in cluster utilization. </p> <p>The Run:ai system provides:</p> <ul> <li>Inter-pod communication. </li> <li>Command-line interface to access logs and an interactive shell. </li> </ul> <p>For more information on Distributed Training in Run:ai see here</p>"},{"location":"Researcher/scheduling/the-runai-scheduler/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, and the number of layers.</p> <p>To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine the results to decide what works best.</p> <p>With HPO, the Researcher provides a single script that is used with multiple, varying, parameters. Each run is a pod (see definition above). Unlike Gang Scheduling, with HPO, pods are independent. They are scheduled independently, started, and end independently, and if preempted, the other pods are unaffected. The scheduling behavior for individual pods is exactly as described in the Scheduler Details section above for Jobs.  In case node pools are enabled, if the HPO workload has been assigned with more than one node pool, the different pods might end up running on different node pools. </p> <p>For more information on Hyperparameter Optimization in Run:ai see here</p>"},{"location":"Researcher/scheduling/using-node-pools/","title":"Using Node Pools","text":""},{"location":"Researcher/scheduling/using-node-pools/#introduction","title":"Introduction","text":"<p> Version 2.8 and up.</p> <p>Node pools assist in managing heterogeneous resources effectively. A node pool is a set of nodes grouped into a bucket of resources using a predefined (e.g. GPU-Type) or administrator-defined label (key &amp; value). Typically, those nodes share a common feature or property, such as GPU type or other HW capability (such as Infiniband connectivity) or represent a proximity group (i.e. nodes interconnected via a local ultra-fast switch). Those nodes would typically be used by researchers to run specific workloads on specific resource types, or by MLops engineers to run specific Inference workloads that require specific node types. </p>"},{"location":"Researcher/scheduling/using-node-pools/#enabling-node-pools","title":"Enabling Node-Pools","text":"<p>The \u2018Node Pools\u2019 feature is disabled by default:</p> <ul> <li>To use node pools - enable this feature under <code>Settings</code> | <code>General</code>. Turn on <code>Enable Node Pools</code>.</li> <li>To manage CPU resources - enable this feature under  <code>Settings</code> | <code>General</code>. Turn on <code>Enable CPU Resources Quota</code>.</li> </ul> <p>Once the feature is enabled by the administrator, all nodes in each of your upgraded clusters are associated with the <code>Default</code> node pool.</p>"},{"location":"Researcher/scheduling/using-node-pools/#creating-and-using-node-pools","title":"Creating and using Node-Pools","text":"<p>An administrator creates logical groups of nodes by specifying a unique label (key &amp; value) and associating it with a node pool. Run:ai allows an administrator to use any label key and value as the designated node-pool label (e.g. <code>gpu-type = A100</code> or <code>faculty = computer-science</code>). Each node pool has a unique name and label used to identify and group nodes into a node pool. Once a new node pool is created, it is automatically assigned to all Projects and Departments with a quota of zero GPU resources and CPU resources. This allows any Project and Department to use any node pool when over-quota is enabled, even if the administrator has not assigned a quota for a specific node pool in a Project or Department.</p> <p>Using resources with over-quota means these resources might be reclaimed by other Projects or Departments that have an assigned quota in place for those node pools. On the other hand, this pattern allows for maximizing the utilization of GPU and CPU resources by the system. An administrator should assign resources from a node pool to a project for which the administrator wants to guarantee reserved resources on that node pool. The reservation should be done for GPU resources and CPU resources. Projects and Departments with no reserved resources for a specific node pool can still use node pool resources, but the resources are not reserved and can be reclaimed by the resources owner Project (or Department).</p> <p>Creating a new node pool and assigning resources from a node pool to Projects and Departments is an operation limited to Administrators only. Researchers can use node pools when submitting a new workload. By specifying the node pool from which a workload allocates resources, the scheduler shell launch that workload on a node that is part of the specified node pool. If no node-pool is selected by a workload, the \u2018Default\u2019 node-pool is used.</p>"},{"location":"Researcher/scheduling/using-node-pools/#multiple-node-pools-selection","title":"Multiple Node Pools Selection","text":"<p> Version 2.9 and up</p> <p>Starting version 2.9, Run:ai system supports scheduling workloads to a node pool using a list of prioritized node pools. The scheduler will try to schedule the workload to the most prioritized node pool first, if it fails, it will try the second one and so forth. If the scheduler tried the entire list and failed to schedule the workload, it will start from the most prioritized node pool again. This pattern allows for maximizing the odds that a workload will be scheduled. </p>"},{"location":"Researcher/scheduling/using-node-pools/#defining-project-level-default-node-pool-priority-list","title":"Defining Project level 'default node pool priority list'","text":"<p>If the Researcher did not specify any node pool within the workload specification, the system will use the default node pool priority list as defined by the administrator. If the administrator did not define the default node pool priority list, the system will use the <code>Default</code> node pool.</p>"},{"location":"Researcher/scheduling/using-node-pools/#node-pools-best-practices","title":"Node-Pools Best Practices","text":"<p>Node pools give administrators the ability to manage quotas in a more granular manner than the Project level, allowing them to specify which Projects are assigned guaranteed resources on specific sets of nodes to be then used by Workloads that need specific node characteristics. Any Project can use any node pool, even if a quota was not assigned to the Node-Pool, it can still be used in an Over-Quota manner.</p> <p>As a rule of thumb, it is best for the administrator to split the organization's GPU deployment to the smallest number of node pools that still serves its purpose, this would help in keeping each pool large enough and minimize the probability that the Run:ai scheduler would not be able to find available resources on a specific node-pool.</p> <p>It is a good practice for researchers to use multiple node pools where applicable, to maximize their workload odds to get scheduled promptly or in cases where resources are scarce in a specific node pool.</p> <p>Administrators should set Projects' default node pool priority list' to make sure that in case a workload was scheduled with no node pool selection, it is scheduled to the preferences of the Administrator, and to increase the workload's odds to get scheduled and promptly.</p>"},{"location":"Researcher/scheduling/using-node-pools/#common-use-cases","title":"Common use-cases","text":"<ul> <li>Training workloads that require specific GPU-type nodes, either because of the scale of parameters (computation time) or for other specific GPU capabilities</li> <li>Inference workloads that require specific GPU-type nodes to comply with constraints such as execution time</li> <li>Workloads that require proximity of nodes for purposes of local ultra-fast networking</li> <li>Organizations where specific nodes belong to specific a  department, and while assuring quota for that department and its subordinated projects, the administrator also wants to let other departments and projects use those nodes when not used by the resource owner</li> <li>Projects that need to use specific resources, but also ensure others will not occupy those resources</li> </ul> <p>While the upper use cases refer to a single node pool, they are also applicable to multiple node pools. In cases where a workload's specification could be satisfied by more than one type of node, using multiple node pool selection potentially increases the probability of a workload finding resources to allocate and shortening the time it will take to get those resources.</p>"},{"location":"Researcher/tools/dev-jupyter/","title":"Use a Jupyter Notebook with a Run:ai Job","text":"<p>A Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code. Uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Jupyter Notebooks are popular with Researchers as a way to code and run deep-learning code. A Jupyter Notebook runs inside the user container. </p> <p>This document is about accessing the remote container created by Run:ai via such a notebook. Alternatively, Run:ai provides integration with JupyterHub. JupyterHub is a separate service that makes it possible to serve pre-configured data science environments. For more information see Connecting JupyterHub with Run:ai.</p>"},{"location":"Researcher/tools/dev-jupyter/#submit-a-jupyter-notebook-workload","title":"Submit a Jupyter Notebook Workload","text":"<p>There are two ways to submit a Jupyter Notebook Job: via the Command-line interface or the user interface</p>"},{"location":"Researcher/tools/dev-jupyter/#submit-via-the-user-interface","title":"Submit via the User interface","text":"<ul> <li>Within the user interface go to the Job list.</li> <li>Select <code>New Job</code> on the top right.</li> <li>Select <code>Interactive</code> at the top. </li> <li>Add an image that supports Jupyter Notebook. For example <code>jupyter/scipy-notebook</code>.</li> <li>Select the <code>Jupyter Notebook</code> button.</li> </ul> <p>Submit the Job. When running, select the job and press <code>Connect</code> on the top right.</p>"},{"location":"Researcher/tools/dev-jupyter/#submit-a-workload","title":"Submit a Workload","text":"<p>Run the following command to connect to the Jupyter Notebook container as if it were running locally:</p> <pre><code>runai submit build-jupyter --jupyter -g 1\n</code></pre> <p>The terminal will show the following: </p> <pre><code>~&gt; runai submit build-jupyter --jupyter -g 1 --attach\nINFO[0001] Exposing default jupyter notebook port 8888\nINFO[0001] Using default jupyter notebook image \"jupyter/scipy-notebook\"\nINFO[0001] Using default jupyter notebook service type portforward\nThe job 'build-jupyter' has been submitted successfully\nYou can run `runai describe job build-jupyter -p team-a` to check the job status\nINFO[0006] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0081] Job started\nJupyter notebook token: 428dc561a5431bd383eff17714460de478d673deec57c045\nOpen access point(s) to service from localhost:8888\nForwarding from 127.0.0.1:8888 -&gt; 8888\nForwarding from [::1]:8888 -&gt; 8888\n</code></pre> <ul> <li>The Job starts a Jupyter notebook container.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 8888</li> </ul> <p>Browse to http://localhost:8888. Use the token in the output to log into the notebook. </p>"},{"location":"Researcher/tools/dev-jupyter/#alternatives","title":"Alternatives","text":"<p>The above flag <code>--jupyter</code> is a shortcut with a predefined image. If you want to run your own notebook, use the quickstart on running a build workload with connected ports. </p>"},{"location":"Researcher/tools/dev-pycharm/","title":"Use PyCharm with a Run:ai Job","text":"<p>Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook</p> <p>This document is about accessing the remote container created by Run:ai, from JetBrain's PyCharm. </p>"},{"location":"Researcher/tools/dev-pycharm/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For the purposes of this document, we have created an image named <code>gcr.io/run-ai-demo/pycharm-demo</code>. The image runs both python and ssh. Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to connect to the container as if it were running locally:</p> <pre><code>runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive  \\\n        --service-type=portforward --port 2222:22\n</code></pre> <p>The terminal will show the connection: </p> <pre><code>The job 'build-remote' has been submitted successfully\nYou can run `runai describe job build-remote -p team-a` to check the job status\nINFO[0007] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0045] Job started\nOpen access point(s) to service from localhost:2222\nForwarding from [::1]:2222 -&gt; 22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul> <p>Note<p>It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run:</p> <pre><code>runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The Job redirects the external port 30022 to port 22 and uses a Node Port service type.</li> <li> <p>Run: <code>runai list worklaods</code></p> </li> <li> <p>Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222 </p> </li> </ul> </p>"},{"location":"Researcher/tools/dev-pycharm/#pycharm","title":"PyCharm","text":"<ul> <li>Under PyCharm | Preferences go to: Project | Python Interpreter </li> <li>Add a new SSH Interpreter. </li> <li>As Host, use the IP address above. Change the port to the above and use the Username <code>root</code></li> <li>You will be prompted for a password. Enter <code>root</code></li> <li>Apply settings and run the code via this interpreter. You will see your project uploaded to the container and running remotely. </li> </ul>"},{"location":"Researcher/tools/dev-tensorboard/","title":"Connecting to TensorBoard","text":"<p>Once you launch a Deep Learning workload using Run:ai, you may want to view its progress. A popular tool for viewing progress is TensorBoard.</p> <p>The document below explains how to use TensorBoard to view the progress or a Run:ai Job.</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-a-workload","title":"Submit a Workload","text":"<p>When you submit a workload, your workload must save TensorBoard logs which can later be viewed. Follow this document on how to do this. You can also view the Run:ai sample code here.</p> <p>The code shows:</p> <ul> <li>A reference to a log directory:</li> </ul> <pre><code>log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n</code></pre> <ul> <li>A registered Keras callback for TensorBoard:</li> </ul> <pre><code>tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\nmodel.fit(x_train, y_train,\n....\ncallbacks=[..., tensorboard_callback])\n</code></pre> <p>The <code>logs</code> directory must be saved on a Network File Server such that it can be accessed by the TensorBoard Job. For example, by running the Job as follows:</p> <pre><code>runai submit train-with-logs -i tensorflow/tensorflow:1.14.0-gpu-py3 \\\n  -v /mnt/nfs_share/john:/mydir -g 1  --working-dir /mydir --command -- ./startup.sh\n</code></pre> <p>Note the volume flag (<code>-v</code>) and working directory flag (<code>--working-dir</code>). The logs directory will be created on <code>/mnt/nfs_share/john/logs/fit</code>.</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-a-tensorboard-workload","title":"Submit a TensorBoard Workload","text":"<p>There are two ways to submit a TensorBoard Workload: via the Command-line interface or the user interface</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-via-the-user-interface","title":"Submit via the User interface","text":"<ul> <li>Within the user interface go to the Job list.</li> <li>Select <code>New Job</code> on the top right.</li> <li>Select <code>Interactive</code> at the top. </li> <li>Add an image that supports TensorBoard. For example: <code>tensorflow/tensorflow:latest</code>.</li> <li>Select the <code>TensorBoard</code> button.</li> <li>Add a mounted volume on which TensorBoard logs exist. The example above uses <code>/mnt/nfs_share/john</code>. Map to <code>/mydir</code></li> <li>Add <code>/mydir</code> to the <code>TensorBoard Logs Directory</code>. </li> </ul> <p>Submit the Job. When running, select the job and press <code>Connect</code> on the top right.</p>"},{"location":"Researcher/tools/dev-tensorboard/#submit-via-the-command-line-interface","title":"Submit via the Command-line interface","text":"<p>Run the following:</p> <pre><code>runai submit tb -i tensorflow/tensorflow:latest --interactive --service-type=portforward --port 8888:8888  --working-dir /mydir  -v /mnt/nfs_share/john:/mydir  -- tensorboard --logdir logs/fit --port 8888 --host 0.0.0.0\n</code></pre> <p>The terminal will show the following: </p> <pre><code>The job 'tb' has been submitted successfully\nYou can run `runai describe job tb -p team-a` to check the job status\nINFO[0006] Waiting for job to start\nWaiting for job to start\nINFO[0014] Job started\nOpen access point(s) to service from localhost:8888\nForwarding from 127.0.0.1:8888 -&gt; 8888\nForwarding from [::1]:8888 -&gt; 8888\n</code></pre> <p>Browse to http://localhost:8888/ to view TensorBoard.</p> <p>Note</p> <p>A single TensorBoard Job can be used to view multiple deep learning Jobs, provided it has access to the logs directory for these Jobs. </p> <p>You can also submit a TensorBoard Job via the user interface. In which case, instead of <code>portforward</code> you will need to select a different service type. If the URL to the TensorBoard job includes a path, you may need to use the TensorBoard flag <code>--path_prefix</code>. For example, if your access point is acme.com/tensorboard1 add  <code>--path_prefix /tensorboard1</code>.</p>"},{"location":"Researcher/tools/dev-vscode/","title":"Use Visual Studio Code with a Run:ai Job","text":"<p>Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook</p> <p>This document is about accessing the remote container created by Run:ai, from Visual Studio Code. </p>"},{"location":"Researcher/tools/dev-vscode/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For the purposes of this document, we have created an image named <code>gcr.io/run-ai-demo/pycharm-demo</code>. The image runs both python and ssh. Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to connect to the container as if it were running locally:</p> <pre><code>runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive  \\\n        --service-type=portforward --port 2222:22\n</code></pre> <p>The terminal will show the connection: </p> <pre><code>The job 'build-remote' has been submitted successfully\nYou can run `runai describe job build-remote -p team-a` to check the job status\nINFO[0007] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0045] Job started\nOpen access point(s) to service from localhost:2222\nForwarding from [::1]:2222 -&gt; 22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul> <p>Note<p>It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run:</p> <pre><code>runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The Job redirects the external port 30022 to port 22 and uses a Node Port service type.</li> <li> <p>Run: <code>runai list jobs</code></p> </li> <li> <p>Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222 </p> </li> </ul> </p>"},{"location":"Researcher/tools/dev-vscode/#visual-studio-code","title":"Visual Studio Code","text":"<ul> <li>Under Visual Studio code install the Remote SSH extension.</li> <li>Create an ssh entry to the service by editing .ssh/config file or use the command Remote-SSH: Connect to Host... from the Command Palette.  Enter the IP address and port from above (e.g. ssh root@35.34.212.12 -p 30022 or ssh root@127.0.0.1 -p 2222). User and password are <code>root</code> </li> <li>Using VS Code, install the Python extension on the remote machine </li> <li>Write your first python code and run it remotely.</li> </ul>"},{"location":"Researcher/tools/dev-x11forward-pycharm/","title":"Use PyCharm with X11 Forwarding and Run:ai","text":"<p>X11 is a window system for the Unix operating systems. X11 forwarding allows executing a program remotely through an SSH connection. Meaning, the executable file itself is hosted on a different machine than where the graphical interface is being displayed. The graphical windows are forwarded to your local machine through the SSH connection.</p> <p>This section is about setting up X11 forwarding from a Run:ai-based container to a PyCharm IDE on a remote machine.</p>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#submit-a-workload","title":"Submit a Workload","text":"<p>You will need your image to run an SSH server  (e.g OpenSSH). For the purposes of this document, we have created an image named <code>gcr.io/run-ai-demo/quickstart-x-forwarding</code>. The image runs:</p> <ul> <li>Python</li> <li>SSH Daemon configured for X11Forwarding </li> <li>OpenCV python library for image handling</li> </ul> <p>Details on how to create the image are here. The image is configured to use the <code>root</code> user and password for SSH.</p> <p>Run the following command to connect to the container as if it were running locally:</p> <pre><code>runai submit xforward-remote -i gcr.io/run-ai-demo/quickstart-x-forwarding --interactive  \\\n        --service-type=portforward --port 2222:22\n</code></pre> <p>The terminal will show the connection:</p> <pre><code>The job 'xforward-remote' has been submitted successfully\nYou can run `runai describe job xforward-remote -p team-a` to check the job status\nINFO[0007] Waiting for job to start\nWaiting for job to start\nWaiting for job to start\nWaiting for job to start\nINFO[0045] Job started\nOpen access point(s) to service from localhost:2222\nForwarding from [::1]:2222 -&gt; 22\n</code></pre> <ul> <li>The Job starts an sshd server on port 22.</li> <li>The connection is redirected to the local machine (127.0.0.1) on port 2222</li> </ul>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#setup-the-x11-forwarding-tunnel","title":"Setup the X11 Forwarding Tunnel","text":"<p>Connect to the new Job by running:</p> <pre><code>ssh -X root@127.0.0.1 -p 2222\n</code></pre> <p>Note the <code>-X</code> flag. </p> <p>Run:</p> <p><pre><code>echo $DISPLAY\n</code></pre> Copy the value. It will be used as a PyCharm environment variable.</p> <p>Important</p> <p>The ssh terminal should remain active throughout the session.</p>"},{"location":"Researcher/tools/dev-x11forward-pycharm/#pycharm","title":"PyCharm","text":"<ul> <li>Under PyCharm | Preferences go to: Project | Python Interpreter</li> <li>Add a new SSH Interpreter.</li> <li>As Host, use <code>localhost</code>. Change the port to the above (<code>2222</code>) and use the Username <code>root</code>.</li> <li>You will be prompted for a password. Enter <code>root</code>.</li> <li>Make sure to set the correct path of the Python binary. In our case it's <code>/usr/local/bin/python</code>.</li> <li> <p>Apply your settings.</p> </li> <li> <p>Under PyCharm configuration set the following environment variables:</p> <ol> <li><code>DISPLAY</code> - set environment variable you copied before</li> <li><code>HOME</code> - In our case it's <code>/root</code>. This is required for the X11 authentication to work.</li> </ol> </li> </ul> <p>Run your code. You can use our sample code here.</p>"},{"location":"Researcher/user-interface/workspaces/overview/","title":"Getting familiar with workspaces","text":"<p> Version 2.9</p> <p>Workspace is a simplified tool for researchers to conduct experiments, build AI models, access standard MLOps tools, and collaborate with their peers.</p> <p>Run:ai workspaces abstract complex concepts related to running containerized workloads in a Kubernetes environment. Aspects such as networking, storage, and secrets, are built from predefined abstracted setups, that ease and streamline the researcher's AI model development.</p> <p>A workspace consists of all the setup and configuration needed for the research, including container images, data sets, resource requests, as well as all required tools for the research, in a single place.  This setup is set to facilitate the research needs and yet to ensure infrastructure owners keep control and efficiency when supporting the various needs.</p> <p>A workspace is associated with a specific Run:ai project (internally: a Kubernetes namespace). A researcher can create multiple workspaces under a specific project.</p> <p>Researchers can only view and use workspaces that are created under projects they are assigned to.</p> <p></p> <p>Workspaces can be created with just a few clicks of a button. See Workspace creation.  </p> <p>Workspaces can be stopped and started to save expensive resources without losing complex environment configurations.</p> <p>Only when a workspace is in status active (see also Workspace Statuses) does it consume resources. </p> <p>When the workspace is active it exposes the connections to the tools (for example, a Jupyter notebook) within the workspace. </p> <p></p> <p>An active workspace is a Run:ai interactive workload. The interactive workload starts when the workspace is started and stopped when the workspace is stopped. </p> <p>Workspaces can be used via the user interface or programmatically via the Run:ai Admin API. Workspaces are not supported via the command line interface. You can still run an interactive workload via the command line. </p>"},{"location":"Researcher/user-interface/workspaces/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Workspaces are made from building blocks. Read about the various building block</li> <li>See how to create a Workspace.  </li> </ul>"},{"location":"Researcher/user-interface/workspaces/statuses/","title":"Workspace Statuses","text":"<p>The Workspace\u2019s status mechanism displays the state of the workspace by aggregating various Kubernetes statuses into the following list:</p> Status Description Pending The workspace is waiting in queue and does not consume any resources. Initializing The workspace has been scheduled and it is consuming resources. Active The workspace is ready to be used and allows the researcher to connect. Stopped The workspace is currently unused and does not consume any resources Failed Something went wrong and the workspace is not usable. <p>This allows the researcher to quickly understand whether the workspace is ready to use and if resources are allocated to it. You can hover over the status column to see additional details about the workspace status.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/statuses/#pending-workspace","title":"Pending workspace","text":"<p>The Pending status indicates that the workspace is waiting in queue and does not consume any resources. The workspace will always end up in this state if the workspace was successfully activated but the relevant resources are unavailable.</p>"},{"location":"Researcher/user-interface/workspaces/statuses/#initializing-workspace","title":"Initializing workspace","text":"<p>The Initializing status indicates that the workspace has been scheduled and is consuming resources. However, it is not active yet as its container is still initializing (so it is not possible to connect to the container tools). This step can take anything from a few seconds to a couple of minutes depending on several factors such as the image size to be pulled. The workspace always goes through this state before the workspace turns active.</p>"},{"location":"Researcher/user-interface/workspaces/statuses/#active-workspace","title":"Active workspace","text":"<p>The Active status indicates that the workspace is ready to be used and allows the researcher to connect to its tools. At this status, the workspace is consuming resources and affecting the project\u2019s quota. The workspace will turn to active status once the <code>Active</code> button is pressed, the activation process ends up successfully and relevant resources are available and vacant.</p>"},{"location":"Researcher/user-interface/workspaces/statuses/#stopped-workspace","title":"Stopped workspace","text":"<p>The Stopped status indicates that the workspace is currently unused and does not consume any resources. A workspace can be stopped either manually, or automatically if triggered by idleness criteria set by the admin (see Limit duration of interactive Jobs).</p>"},{"location":"Researcher/user-interface/workspaces/statuses/#failed-workspace","title":"Failed workspace","text":"<p>The Failed status indicates that something went wrong and the workspace is not usable. You must recreate the workspace and try again.</p>"},{"location":"Researcher/user-interface/workspaces/statuses/#transitioning-states","title":"Transitioning states","text":"<p>When the user attempts to delete, stop, or activate a workspace, the status column indicates a transition state which will either be successful or will fail. If the action fails, the workspace will stay in its original status. For example, if the user tries to delete an active workspace and fails, the workspace is left in active status. Transitioning states are only visible in the browser of the user.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/blocks/building-blocks/","title":"Workspace Building Blocks","text":"<p>Workspace building blocks are a layer that abstracts complex containers and Kubernetes concepts and provides simple and reusable tools to quickly allocate resources to the workspace. This way researchers need to interact only with the building blocks, and do not need to be aware of technical setups and configurations.</p> <p>Workspaces are built from the following building blocks:</p> <ol> <li>Environment</li> <li>Data source</li> <li>Compute resource</li> </ol> <p></p> <p>When a workspace is created, the researcher chooses from preconfigured building blocks or can create a new one on the fly. For example, a workspace can be composed of the following blocks:</p> <ul> <li>Environment: Jupyter, Tensor Board and Cude 11.2</li> <li>Compute resource: 0.5 GPU, 8 cores and 200 Megabytes of CPU memory</li> <li>Data source: A Git branch with the relevant dataset needed</li> </ul> <p></p> <p>A building block has a scope. The scope links a building block to a specific Run:ai project or to all projects:   </p> <ul> <li>When a building block scope is a specific project. It can be viewed and used only within the project.</li> <li>A building block scope can also be set to all projects (current projects and also any future ones).</li> </ul> <p></p> <p>Typically, building blocks are created by the administrator and then assigned to a project. You can grant permission to the researchers to create their own building blocks. These building blocks will only be available to the projects that are assigned to the researcher that created them.</p>"},{"location":"Researcher/user-interface/workspaces/blocks/building-blocks/#next-steps","title":"Next Steps","text":"<p>Read about the various building blocks Environments, Compute Resources and Data Sources.</p>"},{"location":"Researcher/user-interface/workspaces/blocks/compute/","title":"Compute resource introduction","text":"<p>A compute resource building block represents a resource request to be used by the workspace (for example 0.5 GPU, 8 cores and 200 Megabytes of CPU memory). When a workspace is activated, the scheduler looks for a node that can fullfil the request. </p> <p>The compute resource is a mandatory building block for Workspace. A request is composed of the following resources: </p> <ul> <li>GPU resources</li> <li>CPU memory resources</li> <li>CPU cores resources</li> </ul> <p></p> <p>Note</p> <p>GPU resources can be requested as either a memory request, a full GPU request or a fraction of a GPU. A fraction of a GPU also supports the selection of a dynamic MIG profile if configured</p>"},{"location":"Researcher/user-interface/workspaces/blocks/compute/#see-also","title":"See Also","text":"<ul> <li>Create a Compute resource. </li> </ul>"},{"location":"Researcher/user-interface/workspaces/blocks/datasources/","title":"Data source introduction","text":"<p>A data source is a location where data sets relevant to the research are stored. Workspaces can be attached to several data sources for reading and writing. The data can be located locally or in the cloud. Run:ai data sources can use a variety of storage technologies such as Git, S3, NFS, PVC, and more.  </p> <p>The data source is an optional building block for the creation of a workspace.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/blocks/datasources/#see-also","title":"See Also","text":"<ul> <li>Create a Data source. </li> </ul>"},{"location":"Researcher/user-interface/workspaces/blocks/environments/","title":"Environment introduction","text":"<p>The environment block consists of the URL path for the container image and the image pull policy. It exposes all the necessary tools (open source, 3rd party, or custom tools) along with their connection interfaces (See also external node port and the container ports.</p> <p>An environment is a mandatory building block for the creation of a workspace. </p> <p></p> <p>You can also include commands, arguments, and environment variables, as well as the user identity with permission to run the commands in the container.</p> <p>Note</p> <p>Additional arguments and environment variables can be added to workspaces even if they were not defined in the environment building block used by the workspace. This ensures that the same environment can still serve many workspaces, even if they differ in their arguments and environment variables.</p>"},{"location":"Researcher/user-interface/workspaces/blocks/environments/#see-also","title":"See Also","text":"<ul> <li>Create an Environment. </li> </ul>"},{"location":"Researcher/user-interface/workspaces/create/create-compute/","title":"Create a new Compute Resource","text":"<p>To create a compute resource:</p> <ul> <li>Select the <code>New Compute Resource</code> button</li> <li>Select the project the resource will reside in</li> <li>Give the resource a meaningful name.</li> </ul> <p></p> <p>A compute resource, is assigned to a single project or all projects (current and future ones). The latter option can only be created by a Run:ai administrator. A compute resource, by design, is shared with all project members.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-compute/#set-the-resources-request","title":"Set the resources request","text":"<p>A resources request is composed of 3 types of resources:</p> <ol> <li>GPU</li> <li>CPU Memory</li> <li>CPU Compute</li> </ol> <p>The user can select one or more resources. For example, one compute resource may consist of a CPU resource request only, whereas a different request can consist of a CPU memory request and a GPU request.</p> <p></p> <p>Note</p> <p>Selecting resources more than the cluster can supply will result in a permanently failed workspace.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-compute/#set-gpu-resources","title":"Set GPU resources","text":"<p>GPU resources can be expressed in various ways:</p> <ol> <li>Request GPU devices: this option supports whole GPUs (e.g. 1 GPU, 2 GPUs, 3 GPUs) or a fraction of GPU (e.g. 0.1 GPU, 0.5 GPU, 0.93 GPU, etc.) </li> <li>Request partial memory of a single GPU device: this option allows to explicitly state the amount of memory needed (e.g. 5GB GPU RAM). </li> <li>Request a MIG profile: this option will dynamically provision the requested MIG profile (if the relevant hardware exists). </li> </ol> <p>Note</p> <ul> <li>Selecting a GPU fraction (e.g. 0.5 GPU) in a heterogeneous cluster may result in inconsistent results: For example, half of a V100 16GB GPU memory is different than A100 with 40GB). In such scenarios. Requesting specific GPU memory is a better strategy.</li> <li>When selecting partial memory of a single GPU device, if NVIDIA MIG is enabled on a node, then the memory can be provided as a MIG profile. For more information see Dynamic MIG. </li> <li>If GPUs are not requested, they will not be allocated even if resources are available. In that case, the project's GPU quota will not be affected.</li> </ul>"},{"location":"Researcher/user-interface/workspaces/create/create-compute/#set-cpu-resources","title":"Set CPU resources","text":"<p>A CPU resource consists of cores and memory. When GPU resources are requested the user interface will automatically present a proportional amount of CPU cores and memory (as set on the cluster side). </p> <p>Note</p> <p>If no GPU, CPU and memory resources are defined, the request will not be allocated any GPUs. The scheduler will create a container with no minimal CPU and memory. Such a job will run but is likely to be preempted at any time by other jobs. The scheme is relevant for testing and debugging purposes.  </p>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/","title":"Create a new data source","text":"<p>When you select <code>New Compute Resource</code> you will be presented with various data source options described below.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-an-nfs-data-source","title":"Create an NFS data source","text":"<p>To create an NFS data source, provide:</p> <ul> <li>A data source name</li> <li>A Run:ai project scope</li> <li>An NFS server </li> <li>The path to the data within the server. </li> <li>The path within the container where the data will be mounted.</li> </ul> <p>The data can be set as read-write or limited to read-only permission regardless of any other user privileges. </p>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-a-pvc-data-source","title":"Create a PVC data source","text":"<p>To create an PVC data source, provide:</p> <ul> <li>A data source name</li> <li>A Run:ai project scope</li> <li>Select an existing PVC or create a new one by providing a claim name, a storage class, access mode, and a required storage size. </li> <li>The path within the container where the data will be mounted.</li> </ul>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-an-s3-data-source","title":"Create an S3 data source","text":"<p>S3 storage saves data in buckets. S3 is typically attributed to AWS cloud service but can also be used as a separate service unrelated to Amazon. </p> <p>To create an S3 data source, provide</p> <ul> <li>A data source name</li> <li>A Run:ai project scope</li> <li>The relevant S3 service URL server</li> <li>The bucket name of the data. </li> <li>The path within the container where the data will be mounted.</li> </ul> <p>Note that an S3 data source can be public or private. For the latter option, please select the relevant credentials associated with the project to allow access to the data.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-a-git-data-source","title":"Create a Git data source","text":"<p>To create a Git data source, provide:</p> <ul> <li>A data source name</li> <li>A Run:ai project scope</li> <li>The relevant repository URL. </li> <li>The path within the container where the data will be mounted.</li> </ul> <p>The Git data source can be public or private. To allow access to a private Git data source, you must select the relevant credentials associated with the project. </p>"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-a-host-path-data-source","title":"Create a host path data source","text":"<p>To create a host path data source, provide:</p> <ul> <li>A data source name</li> <li>A Run:ai project scope</li> <li>The relevant path on the host. </li> <li>The path within the container where the data will be mounted.</li> </ul> <p>Note that the data can be limited to read-only permission regardless of any other user privileges. </p>"},{"location":"Researcher/user-interface/workspaces/create/create-env/","title":"Creating a new environment","text":"<p>To create an environment:</p> <ul> <li>Select the <code>New Environment</code> button.</li> <li>Give the environment a meaningful name.</li> <li>Select the project the environment will reside in.</li> </ul> <p></p> <p>An Environment is assigned to a single project or all projects (current and future ones). The latter option can only be created by a Run:ai administrator. A compute resource, by design, is shared with all project members.</p>"},{"location":"Researcher/user-interface/workspaces/create/create-env/#set-the-container-image","title":"Set the container image","text":"<p>Enter the image URL path as well as a policy for pulling the image from the image repository.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/create/create-env/#select-the-relevant-tools","title":"Select the relevant tools","text":"<p>In a single environment, it is possible to add as many tools as needed (or none at all).</p> <p>Tools can be:</p> <ul> <li>Different applications such as Code editor IDEs (e.g VS Code), Experiment tracking (e.g. Weight and Biases), visualization tools (e.g. Tensor Board), and more.</li> <li>Open source tools (e.g Jupyter notebook) or commercial 3rd party tools (e.g. MATLAB)</li> </ul> <p></p> <p>It is also possible to set up a custom tool used by the organization.</p> <p>For each tool, you must set the type of connection interface and port. If not set, default values are provided. The supported connection types are:</p> <ul> <li>External URL:  This connection type allows you to connect to your tool either by inserting a custom URL or having one generated for you. Either way, the URL should be unique per workspace as many workspaces may use the same environment. If the URL type was set to custom, the URL will be requested from the Researcher upon creating the workspace.</li> <li>External node port: A NodePort exposes your application externally on every host of the cluster, access the tool using <code>http://&lt;HOST_IP&gt;:&lt;NODEPORT&gt;</code> (e.g http://203.0.113.20:30556).</li> </ul> <p></p> <p>Note</p> <p>Selecting a tool is not sufficient to have it up and running. To actually configure a tool you need additional steps:</p> <ul> <li>The container image needs to support the tool. </li> <li>The administrator must configure a DNS record and certificate as described here.</li> </ul>"},{"location":"Researcher/user-interface/workspaces/create/create-env/#configure-runtime-settings","title":"Configure runtime settings","text":"<p>Per environment, the creating user (either Researcher or administrator) is allowed to set the command running in the container. This command will be visible in the workspace creation form, although it will not be editable (e.g. a python command). In addition, the researcher can add arguments that can be edited upon creating a workspace using this environment. Similarly, environment variables can be added to the environments, but these can be edited in the workspace creation form.</p> <p>Note</p> <p>The value of an environment variable can be left empty for the researcher to fill in upon workplace creation.</p> <p>Examples:</p> <ol> <li>WANDB</li> <li>UID and GID of the user when launching a Jupyter notebook. </li> </ol> <p>In addition, in the environment, it is possible to set the path to the working directory that will be used as the current directory when the container running the created workload starts.</p> <p></p> <p>It is possible to either use the exact UID and GID defined in the image. However, in many cases, it can be with root privileges so it is possible to override it. If single sign on is configured, the UID and GID to be used will be the ones of the logged researcher that creates the workspace, otherwise, the researcher creating the workspace will be guided to provide it upon workspace creation form.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/","title":"Workspaces actions and use cases","text":""},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-new-workspace","title":"Create a new workspace","text":"<p>A Workspace is assigned to a project and is affected by the project\u2019s quota just like any other workload. A workspace is shared with all project members for collaboration.</p> <p>To create a workspace, you must provide:</p> <ul> <li>At least one project </li> <li>A researcher assigned to at least one project</li> </ul> <p>To create a workspace, the researcher must select building blocks  in one of two ways:</p> <ul> <li>Create a workspace from scratch:  this allows you to either select an existing building block or create them on the fly (pending the right permissions).</li> <li>Create a workspace from a template: a template contains a set of predefined building blocks as well as additional configurations which allow the user to immediately create a templated-based workspace.</li> </ul> <p>To create a workspace:</p> <ul> <li>Press <code>New Workspace</code> </li> <li>Select a project for the new workspace. The project visualization contains information about the project such as how much of the quota is being allocated and indicates the likelihood of the workspace being scheduled or left in the queue</li> </ul> <p></p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-new-workspace-from-scratch","title":"Create a new workspace from scratch","text":"<p>See picture:</p> <p></p> <p>Note</p> <p>The building block can also be created (and then selected) directly from within the workspace creation form.</p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#select-an-environment-for-a-new-workspace","title":"Select an Environment for a new workspace","text":"<p>An environment is a mandatory element of a workspace. All environments created for the project will be shown to researchers in the form of a gallery view (see also Creating a new environment). Each tile shows the tools as well as the image. When selecting an environment, the command, arguments and environment variables defined in the environment are visible for review. The researcher can edit arguments and environment variables that are specific to the current workspace and that are not part of the common shared environment. In some cases, it would even be expected that the researcher will provide additional information (for example, values for environment variables) to successfully create the workspace (see also Create new environment).</p> <p></p> <p>You can also decide whether the workspace is preemptable or not (see also create a preemptable worksapce). By default, interactive sessions are limited to the project\u2019s GPU, meaning that they can only be scheduled (and activated) when there is an available and sufficient GPU quota.  With the following parameter, the researcher can determine whether the workspace is allowed to go over quota with the understanding that it can be preempted if other projects would demand back their quota.</p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#select-a-compute-resource-for-a-new-workspace","title":"Select a compute resource for a new workspace","text":"<p>Selecting compute resources for the workspace is a mandatory step. If compute resources are created for the project (see also creating a new compute resource), those will be offered to researchers in the form of a gallery view. Each tile shows the amount of GPU, CPU and Memory in the request.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#select-a-data-source-for-a-new-workspace","title":"Select a data source for a new workspace","text":"<p>Selecting a data source for the workspace is a non-mandatory step. If data sources are created for the project (see also creating a new compute resource), those will be offered to researchers in the form of a gallery view. Each tile shows the unique name of the building block and the type of data source.</p> <p></p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-new-workspace-from-a-template","title":"Create a new workspace from a template","text":"<p>Templates ease the way of creating a new workspace in a few clicks. In contrast to creating a workspace from scratch (selecting manually which building blocks to use in your workspace), a template aggregates all building blocks under a single entity for researchers to use for the creation of workspaces.</p> <p></p> <p>A Template consists of the building blocks and other parameters that are exposed in a workspace creation form. Templates can be fully defined to a point researcher can select and create the workspace without providing any additional information or partially defined, hence, leaving some degree of freedom in the creation of the workspace via the template. This can help in cases where only part of the configuration is selected in the template and the rest is expected to be provided by the user creating a workspace from the template. </p> <p>Few examples: </p> <ul> <li>A template can have the value of an environment variable empty for the researcher to edit later during the workspace creation.</li> <li>A template can consist of an environment with a tool that requests a custom URL. This URL field stays empty until the researcher fills it upon creating the workspace</li> </ul> <p>For collaboration purposes, templates are assigned to a specific project and are shared with all project members by design.</p>"},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-preemptible-workspace","title":"Create a preemptible workspace","text":"<p>For a better experience, workspaces, as they are built for interactive research, are designed to not be preempted (because the researchers actively interact with GPU resources). Thus, non-preemptable workspaces can be only scheduled if the project has a sufficient vacant quota. However, if that\u2019s not the case (the project does not have a sufficient vacant quota) and the researcher still needs to create and activate a workspace (if cluster resources are available) he/she can allow the workspace to go over quota, thus be scheduled, but with the cost of preemption without prior notice.</p> <p></p>"},{"location":"admin/overview-administrator/","title":"Overview: Administrator Documentation","text":"<p>The role of Administrators is to set up Run:ai and perform day-to-day monitoring and maintenance. </p> <p>As part of the Administrator documentation you will find:</p> <ul> <li>Run:ai Setup How to set up and modify a GPU cluster with Run:ai.</li> <li>Researcher Setup How to set up Researchers to work with Run:ai.</li> <li>Setting and maintaining the cluster via the  Run:ai User Interface.</li> <li>Integrations of Run:ai with a variety of other systems.</li> </ul>"},{"location":"admin/admin-ui-setup/admin-ui-users/","title":"Adding, Updating and Deleting Users","text":""},{"location":"admin/admin-ui-setup/admin-ui-users/#introduction","title":"Introduction","text":"<p>The Run:ai User Interface allows the creation of Run:ai Users. Run:ai Users can receive varying levels of access to the Administration UI and submit Jobs on the Cluster.</p> <p>Tip</p> <p>It is possible to connect the Run:ai user interface to the organization's directory and use single sign-on. This allows you to set Run:ai roles for users and groups from the organizational directory. For further information see single sign-on configuration.</p>"},{"location":"admin/admin-ui-setup/admin-ui-users/#working-with-users","title":"Working with Users","text":"<p>You can create users, as well as update and delete users. </p>"},{"location":"admin/admin-ui-setup/admin-ui-users/#create-a-user","title":"Create a User","text":"<p>Note</p> <p>To be able to review, add, update and delete users, you must have an Administrator access. If you do not have such access, please contact an Administrator. </p> <ol> <li>Login to the Users area of the Run:ai User interface at <code>company-name.run.ai</code>.</li> <li>On the top right, select \"Add New Users\".</li> <li>Choose a User name and email. </li> <li>Select Roles. More than one role can be selected. The available roles are:<ul> <li>Administrator: Can manage Users and install Clusters. </li> <li>Editor: Can manage Projects and Departments.</li> <li>Viewer: View-only access to the Run:ai User Interface.</li> <li>Researcher: Can submit ML workloads. Setting a user as a Researcher also requires assigning the user to projects.</li> <li>Research Manager: Can act as Researcher in all projects, including new ones to be created in the future. </li> <li>ML Engineer: Can view and manage deployments and cluster resources. Available only when Inference module is installed.</li> </ul> </li> <li>(Optional) Select Cluster(s). This determines what Clusters are accessible to this User.</li> <li>Press \"Save\".</li> </ol> <p>You will get the new user credentials and have the option to send the credentials by email. </p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/","title":"Dashboard Analysis","text":"<p>The Run:ai Administration User Interface provides a set of dashboards that help you monitor Clusters, Cluster Nodes, Projects, and Jobs. This document provides the key metrics to monitor, how to assess them as well as suggested actions. </p> <p>There are 3 dashboards:</p> <ul> <li>Overview dashboard - Provides information about what is happening right now in the cluster</li> <li>Analytics dashboard - Provides long term analysis of cluster behavior</li> <li>Multi-Cluster Overview dashboard - Provides a more holistic, multi-cluster view of what is happening right now. The dashboard is intended for organizations that have more than one connected cluster.</li> </ul>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#overview-dashboard","title":"Overview Dashboard","text":"<p>The Overview dashboard provides information about what is happening right now in the cluster.  Administrators can view high-level information on the state of the cluster, including:</p> <ul> <li>The number of available and allocated resources and their cluster-wide utilization</li> <li>The number of running and pending Jobs, their utilization, information on Jobs with errors or Jobs with idle GPUs</li> <li>Active Projects, their assigned and allocated GPUs and number of running and pending Jobs</li> </ul> <p>Cluster administrators can use the Overview dashboard to find issues and fix them. Below are a few examples:</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#jobs-with-idle-gpus","title":"Jobs with idle GPUs","text":"<p>Locate Jobs with idle GPUs, defined as GPUs with 0% GPU utilization for more than 5 minutes.\u00a0</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> Review Analysis  &amp; Actions Interactive Jobs are too frequently idle *  Consider setting time limits for interactive Jobs through the Projects tab.\u00a0 *  Consider also reducing GPU quotas for specific Projects to encourage users to run more training Jobs as opposed to interactive Jobs (note that interactive Jobs can not use more than the GPU quota assigned to their Project). Training Jobs are too frequently idle Identify and notify the right users and work with them to improve the utilization of their training scripts"},{"location":"admin/admin-ui-setup/dashboard-analysis/#jobs-with-an-error","title":"Jobs with an Error","text":"<p>Search for Jobs with an error status. These Jobs may be holding GPUs without actually using them.\u00a0</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>Search for Jobs with an Error status on the Jobs view and discuss with the Job owner. Consider deleting these Jobs to free up the resources for other users.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#jobs-with-a-long-duration","title":"Jobs with a Long Duration","text":"<p>View list of 5 longest Jobs. </p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> Review Analysis &amp; Actions Training Jobs run for too long Ask users to view their Jobs and analyze whether useful work is being done. If needed, stop their Jobs. Interactive Jobs run for too long Consider setting time limits for interactive Jobs via the Project editor."},{"location":"admin/admin-ui-setup/dashboard-analysis/#job-queue","title":"Job Queue","text":"<p>Identify queueing bottlenecks.</p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> Review Analysis &amp; Actions Cluster is fully loaded Go over the table of active Projects and check that fairness between Projects was enforced, by reviewing the number of allocated GPUs for each Project, ensuring each Project was allocated with its fair-share portion of the cluster. Cluster is not fully loaded Go to the Jobs view to review the resources requested for that Job (CPU, CPU memory, GPU, GPU memory). Go to the Nodes view to verify that there is no Node with enough free resources that can host that Job. <p>Also, check the command that the user used to submit the job. The Researcher may have requested a specific Node for that Job.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#analytics-dashboard","title":"Analytics Dashboard","text":"<p>The Analytics dashboard provides means for viewing historical data on cluster information such as:</p> <ul> <li>Utilization across the cluster</li> <li>GPU usage by different Projects, including allocation and utilization, broken down into interactive and training Jobs</li> <li>Breakdown of running Jobs into interactive, training, and GPU versus CPU-only Jobs, including information on queueing (number of pending Jobs and requested GPUs),</li> <li>Status of Nodes in terms of availability and allocated and utilized resources.</li> </ul> <p>The information presented in Analytics can be used in different ways for identifying problems and fixing them. Below are a few examples.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#node-downtime","title":"Node Downtime","text":"<p>View the overall available resources per Node and identify cases where a Node is down and there was a reduction in the number of available resources.</p> <p>How to: view the following panel.</p> <p></p> <p>Analysis and Suggested actions:</p> <p>Filter according to time range to understand for how long the Node is down.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#gpu-allocation","title":"GPU Allocation","text":"<p>Track GPU allocation across time.</p> <p>How to: view the following panels. </p> <p></p> <p>The panel on the right-hand side shows the cluster-wide GPU allocation and utilization versus time, whereas the panels on the left-hand side show the cluster-wide GPU allocation and utilization averaged across the filtered time range.</p> <p>Analysis and Suggested actions:</p> <p>If the allocation is too low for a long period, work with users to run more workloads and to better utilize the Cluster.</p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#track-gpu-utilization","title":"Track GPU utilization","text":"<p>Track whether Researchers efficiently use the GPU resources they have allocated for themselves. </p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>If utilization is too low for a long period, you will want to identify the source of the problem:</p> <ul> <li>Go to \u201cAverage GPU Allocation &amp; Utilization\u201d </li> <li>Look for Projects with large GPU allocations for interactive Jobs or Projects that poorly utilize their training Jobs. Users tend to poorly utilize their GPUs in interactive sessions because of the dev &amp; debug nature of their work which typically is an iterative process with long idle GPU time. On many occasions users also don\u2019t shut down their interactive Jobs, holding their GPUs idle and preventing others from using them. </li> </ul> Review Analysis &amp; Actions Low GPU utilization is due to interactive Jobs being used too frequently Consider setting time limits for interactive Jobs through the Projects tab or reducing GPU quotas to encourage users to run more training Jobs as opposed to interactive Jobs (note that interactive Jobs can not use more than the GPU quota assigned to their Project). Low GPU utilization is due to users poorly utilizing their GPUs in training sessions Identify Projects with bad GPU utilization in training Jobs, notify the users and work with them to improve their code and the way they utilize their GPUs."},{"location":"admin/admin-ui-setup/dashboard-analysis/#training-vs-interactive-researcher-maturity","title":"Training vs. Interactive -- Researcher maturity","text":"<p>Track the number of running Jobs and the breakdown into interactive, training, and CPU-only Jobs. </p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>We would want to encourage users to run more training Jobs than interactive Jobs, as it is the key to achieving high GPU utilization across the Cluster:</p> <ul> <li>Training Jobs run to completion and free up their resources automatically when training ends</li> <li>Training Jobs can be preempted, queued, and resumed automatically by the Run:ai system according to predefined policies which increases fairness and Cluster utilization.</li> </ul>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#pending-queue-size","title":"Pending Queue Size","text":"<p>Track how long is the queue for pending Jobs</p> <p>How to: view the following panels:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>Consider buying more GPUs if, </p> <ul> <li>Too many Jobs are waiting in queue for too long </li> <li>With a large number of requested GPUs </li> <li>While the Cluster is fully loaded and well utilized. </li> </ul>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#cpu-memory-utilization","title":"CPU &amp; Memory Utilization","text":"<p>Track CPU and memory Node utilization and identify times where the load on specific Nodes is high. </p> <p>How to: view the following panel:</p> <p></p> <p>Analysis and Suggested actions:</p> <p>If the load on specific Nodes is too high, it may cause problems with the proper operation of the Cluster and the way jobs are running. </p> <p>Consider adding more CPUs, or adding additional CPU-only nodes for Jobs that do only CPU processing. </p>"},{"location":"admin/admin-ui-setup/dashboard-analysis/#multi-cluster-overview-dashboard","title":"Multi-Cluster Overview Dashboard","text":"<p>Provides a holistic, aggregated view across Clusters, including information about Cluster and Node utilization, available resources, and allocated resources. With this dashboard, you can identify Clusters that are down or underutilized and go to the Overview of that Cluster to explore further. </p> <p></p>"},{"location":"admin/admin-ui-setup/department-setup/","title":"Departments","text":""},{"location":"admin/admin-ui-setup/department-setup/#introduction","title":"Introduction","text":"<p>Researchers submit Jobs. To streamline resource allocation and prioritize work, Run:ai introduces the concept of Projects. Projects are the tool to implement resource allocation policies as well as create segregation between different initiatives. A project in most cases represents a team, an individual, or an initiative that shares resources or has a specific resources budget (quota).</p> <p>A Researcher submitting a Job needs to associate a Project name with the request. The Run:ai scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in the queue for future allocation.</p> <p>In some organizations, Projects may not be enough, this is because:</p> <ul> <li>There are simply too many individual entities that are attached to a quota.</li> <li>There are organizational quotas at a higher level. </li> </ul>"},{"location":"admin/admin-ui-setup/department-setup/#departments","title":"Departments","text":"<p>Departments create a secondary hierarchy of resource allocation:</p> <ul> <li>A Project is associated with a single Department. Multiple Projects can be associated with the same Department.</li> <li>A Department, like a Project is associated with a Quota. </li> <li>It is recommended that a Department's quota supersedes the sum of all its associated Projects' quota.</li> </ul>"},{"location":"admin/admin-ui-setup/department-setup/#node-pools-and-quota-settings","title":"Node Pools and Quota settings","text":"<p>For detailed information on node pools, see Using node pools.</p> <p>By default, all nodes in a cluster are part of the <code>Default</code> node pool. The administrator can choose to create new node pools and include a set of nodes in a node pool by associating the nodes with a label.</p> <p>If the node pools feature is disabled, all GPU and CPU resources are directly associated with the Department's Quota. </p> <p>Once an Administrator enables node pools, all GPU and CPU resources will be included in the <code>Default</code> node pool and summed up to the Department's overall Quotas.</p> <p>An administrator can create a new node pool and associate nodes into this pool. Any new pool is automatically associated with all Departments and Projects within a cluster, with a GPU and CPU resource Quota of zero. The Administrator can then change the Quota of any node-pool resource per Department and Project. The Quota of node-pool X within Department Y should be at least the sum of the same node-pool X Quota across all associated Projects. This means an administrator should carefully plan the resource Quota allocation from the Department to its descendent Projects. The overall Quota of the Department is the sum of all its associated node pools. </p>"},{"location":"admin/admin-ui-setup/department-setup/#over-quota-behavior","title":"Over-quota behavior","text":"<p>Consider an example from an academic use case: the Computer Science Department and the GeoPhysics Department have each purchased 10 nodes with 8 GPUs for each node, totaling a cluster of 160 GPUs for both departments. The two Departments do not mind sharing GPUs as long as they always get their 80 GPUs when they truly need them. As such, there could be many Projects in the GeoPhysics Department, totaling an allocation of 100 GPUs, but anything above 80 GPUs will be considered by the Run:ai scheduler as over-quota. For more details on over-quota scheduling see the Run:ai Scheduler. In case node pools are enabled, the same rule applies per node pool, i.e. if a job tries to use resources that supersede a node pool Department's quota - it will be considered as Over-Quota.</p> <p>Important</p> <p>Best practice: As a rule, the sum of the Departments' Quota allocations should be equal to the number of GPUs in the cluster.</p>"},{"location":"admin/admin-ui-setup/department-setup/#creating-and-managing-departments","title":"Creating and Managing Departments","text":""},{"location":"admin/admin-ui-setup/department-setup/#enable-departments","title":"Enable Departments","text":"<p>Departments are disabled by default. To start working with Departments:</p> <ul> <li>Go to Settings | General</li> <li>Enable Departments </li> </ul> <p>Once Departments are enabled, the left-side menu will have a new item named \"Departments\".</p> <p>Under Departments there will be a single Department named default. All Projects created before the Department feature was enabled will belong to the default Department.</p>"},{"location":"admin/admin-ui-setup/department-setup/#adding-departments","title":"Adding Departments","text":"<p>You can add new Departments by pressing the Add New Department at the top right of the Department view. Add Department name and quota allocation.</p>"},{"location":"admin/admin-ui-setup/department-setup/#assigning-projects-to-departments","title":"Assigning Projects to Departments","text":"<p>Under Projects edit an existing Project. You will see a new Department drop-down with which you can associate a Project with a Department.</p>"},{"location":"admin/admin-ui-setup/deployments/","title":"Viewing and Submitting Deployments","text":"<p>The Run:ai User interface Deployment area allows the viewing and submitting of Deployments for serving inference workloads. Submitting inference workloads can only be done if your user has <code>ML Engineer</code> access.</p>"},{"location":"admin/admin-ui-setup/deployments/#deployment-list","title":"Deployment list","text":"<p>The main view shows a list of Deployments:</p> <p></p>"},{"location":"admin/admin-ui-setup/deployments/#submit-a-deployment","title":"Submit a Deployment","text":"<p>On the top right, you can choose to Submit a new Deployment. </p> <p>Note</p> <p>If knative is not installed in your cluster the button will be grayed out.</p> <p>A Deployment form will open: </p> <p></p> <p>Note</p> <p>If the Deploy button is disabled or does not exist, then your cluster is not installed or configured to connect to the cluster see here for more information.</p>"},{"location":"admin/admin-ui-setup/deployments/#deployment-properties","title":"Deployment Properties","text":"<p>When selecting a single Deployment, a right-pane appears:</p> <p></p> <p>This multi-tab view provides information about Deployment details, related Pods, Deployment status history, and various utilization graphs. </p>"},{"location":"admin/admin-ui-setup/jobs/","title":"Viewing and Submitting Jobs","text":"<p>The Run:ai User interface Job area allows the viewing of Jobs and Job details. It also allows the Researcher to submit Jobs, suspend and resume Jobs and delete Jobs.</p>"},{"location":"admin/admin-ui-setup/jobs/#job-list","title":"Job list","text":"<p>The main view shows a list of Jobs. The list can be filtered and sorted:</p> <p></p>"},{"location":"admin/admin-ui-setup/jobs/#submit-job","title":"Submit Job","text":"<p>On the top right, you can choose to Submit a new Job. A Job form will open: </p> <p></p> <p>Note</p> <p>If the Submit Job button is disabled or does not exist, then your cluster is not installed or configured to connect to the cluster see here for more information.</p>"},{"location":"admin/admin-ui-setup/jobs/#job-properties","title":"Job Properties","text":"<p>When selecting a single Job, a right-pane appears:</p> <p></p> <p>This multi-tab view provides information about Job details, related Pods, Job status history, and various utilization graphs. You can also view internal Job logs as shown here:</p> <p></p>"},{"location":"admin/admin-ui-setup/jobs/#other-operations","title":"Other operations","text":"<p>You can also delete a selected Job or suspend/resume a selected Job. </p>"},{"location":"admin/admin-ui-setup/overview/","title":"User Interface Overview","text":"<p>Run:ai provides a single user interface that, depending on your role, serves both as a control-plane management tool and a researcher workbench. </p> <p>The control-plane part of the tool allows the administrator to:</p> <ul> <li>Analyze cluster status using dashboards.</li> <li>Manage Run:ai metadata such as users, departments, and projects. </li> <li>View Job details to be able to help researchers solve Job-related issues.</li> </ul> <p>The researcher workbench part of the tool allows Researchers to submit, delete and pause Jobs, view Job logs etc.</p>"},{"location":"admin/admin-ui-setup/overview/#setup","title":"Setup","text":"<p>The cluster installation process requires configuring a new cluster and downloading a YAML file.  On SaaS-based installations, the cluster creation wizard requires a URL to the cluster as explained here.</p>"},{"location":"admin/admin-ui-setup/overview/#architecture","title":"Architecture","text":"<ul> <li>Run:ai saves metadata such as users, projects, departments, clusters, and tenant settings, in the control plane residing on the Run:ai cloud.</li> <li>Workload information resides on (sometimes multiple) GPU clusters. </li> <li>The Run:ai user interface needs to work with both sources of information. </li> </ul> <p>As such, the chosen architecture of the user interface is:</p> <p></p> <ul> <li>The user interface is served from the management backend.</li> <li>The user interface connects directly to multiple GPU clusters using cross-origin access. This works using CORS: Cross-origin resource sharing. This allows submitting workloads and getting extended logging information directly from the GPU clusters. </li> <li>Meta-data, such as Projects, Settings, and Job information is synced into the management backend via a cluster-sync service. Cluster-sync creates an outbound-only channel with no incoming HTTPS connections.  </li> </ul> <p>Important</p> <p>One corollary of this architecture is that for SaaS-based tenants, the user interface will only be able to access the cluster when the browser is inside the corporate firewall. When working outside the firewall. Workload-related functionality such as Submitting a Job, viewing Job lots etc, is disabled. </p>"},{"location":"admin/admin-ui-setup/project-setup/","title":"Projects","text":""},{"location":"admin/admin-ui-setup/project-setup/#introduction","title":"Introduction","text":"<p>Researchers submit Jobs. To streamline resource allocation and prioritize work, Run:ai introduces the concept of Projects. Projects are the tool to implement resource allocation policies as well as create segregation between different initiatives. A project in most cases represents a team, an individual, or an initiative that shares resources or has a specific resources budget (quota).</p> <p>A Researcher submitting a Job needs to associate a Project name with the request. The Run:ai scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in the queue for future allocation.</p>"},{"location":"admin/admin-ui-setup/project-setup/#modeling-projects","title":"Modeling Projects","text":"<p>As an Admin, you need to determine how to model Projects. You can:</p> <ul> <li>Set a Project per user.</li> <li>Set a Project per team of users.</li> <li>Set a Project per a real organizational Project.</li> </ul>"},{"location":"admin/admin-ui-setup/project-setup/#node-pools","title":"Node Pools","text":"<p>For detailed information on node pools, see Using node pools.</p> <p>By default, all nodes in a cluster are part of the <code>Default</code> node pool. The administrator can choose to create new node pools and include a set of nodes in a node pool by associating the nodes with a label.</p> <p>Each node pool is automatically associated with all Projects and Departments with zero resource allocation (Quotas).  When submitting a Job (or Deployment), the Researcher can choose one or more node pools. When choosing more than one node pool, the researcher sets the order of priority between the chosen node pools. The scheduler will try to schedule the Job to the first node pool. If not successful the scheduler will try the second node pool in the list, and so forth until it finds a node pool that can provide the Job's specification.</p> <p>An administrator can set a Project's <code>default priority list</code> of node pools. In case the Researcher did not specify any node pool (or node pool list), the scheduler will use the Project's default node pool priority list to determine the order that the scheduler will use when scheduling the Job.</p>"},{"location":"admin/admin-ui-setup/project-setup/#project-quotas","title":"Project Quotas","text":"<p>Each Project is associated with a total quota of GPU and CPU resources (CPU Compute &amp; CPU Memory) that can be allocated for the Project at the same time. This total is the sum of all node pools' quotas associated with this Project. This is guaranteed quota in the sense that Researchers using this Project are guaranteed to get this amount of GPU and CPU resources, no matter what the status in the cluster is.</p> <p>Beyond that, a user of this Project can receive an over-quota (The administrator needs to enable over quota per project). As long as GPUs are unused, a Researcher using this Project can get more GPUs. However, these GPUs can be taken away at a moment's notice. When the node pools flag is enabled, over-quota is effective and calculated per node pool, this means that a workload requesting resources from a certain node pool can get its resources from a quota that belongs to another Project for the same node pool if the resources are exhausted for this Project and available on another Project. For more details on over-quota scheduling see the Run:ai Scheduler.</p> <p>Important</p> <p>Best practice: As a rule, the sum of the Projects' allocations should be equal to the number of GPUs in the cluster.</p>"},{"location":"admin/admin-ui-setup/project-setup/#controlling-over-quota-behavior","title":"Controlling Over-Quota Behavior","text":"<p>By default, the amount of over-quota available for Project members is proportional to the original quota provided above. The Run:ai scheduler document provides further examples which show how over-quota is distributed amongst competing Projects. </p> <p>As an administrator, you may want to disconnect the two parameters. So, for example, a Project with a high quota will receive little or no over-quota. To perform this:</p> <ul> <li>Under <code>General | Settings</code> turn on the <code>Enable Over-quota Priority</code> feature</li> <li>When creating a new Project, you can now see a slider for over-quota priority ranging from <code>None</code> to <code>High</code> </li> </ul>"},{"location":"admin/admin-ui-setup/project-setup/#create-a-project","title":"Create a Project","text":"<p>Note</p> <p>To be able to create or edit Projects, you must have Editor access. See the Users documentation.</p> <ul> <li>Login to the Projects area of the Run:ai user interface at <code>&lt;company-name&gt;.run.ai</code>.</li> <li>On the top right, select \"Add New Project\"</li> <li>Choose a Project name and a Project quota </li> <li>Press \"Save\"</li> </ul>"},{"location":"admin/admin-ui-setup/project-setup/#assign-users-to-project","title":"Assign Users to Project","text":"<p>When Researcher Authentication is enabled, the Project form will contain an additional Access Control tab. The tab will allow you to assign Researchers to their Projects. </p> <p>If you are using Single-sign-on, you can also assign Groups </p>"},{"location":"admin/admin-ui-setup/project-setup/#other-project-properties","title":"Other Project Properties","text":""},{"location":"admin/admin-ui-setup/project-setup/#limit-jobs-to-run-on-specific-node-groups","title":"Limit Jobs to run on Specific Node Groups","text":"<p>You can assign a Project to run on specific nodes (machines). This is achieved by two different mechanisms:</p> <ul> <li> <p>Node Pools:          All node pools in the system are associated with each Project. Each node pool can allocate GPU and CPU resources (CPU Compute &amp; CPU Memory) to a Project. By associating a quota on specific node pools for a Project, you can control which nodes a Project can utilize and which default priority order the scheduler will use (in case the workload did choose so by itself). Each workload should choose the node pool(s) to use, if no choice is made, it will use the Project's default 'node pool priority list'. Note that node pools with zero resources associated with a Project or node pools with exhausted resources can still be used by a Project when the Over Quota flag is enabled.</p> </li> <li> <p>Node Affinities (aka Node Type)         Administrator can associate specific node sets characterized by a shared run-ai/node-type label value to a Project. This means descendant workloads can only use nodes from one of those node affinity groups. A workload can specify which node affinity to use, out of the list is bounded to its parent Project.</p> </li> </ul> <p>There are many use cases and reasons to use specific nodes for a Project and its descendant workloads, here are some examples:</p> <ul> <li>The project team needs specialized hardware (e.g. with enough memory).</li> <li>The project team is the owner of specific hardware which was acquired with a specialized budget.</li> <li>We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes.</li> </ul>"},{"location":"admin/admin-ui-setup/project-setup/#the-difference-between-node-pools-and-affinities","title":"The difference between Node Pools and Affinities","text":"<p>Node pools represent an independent scheduling domain per Project, therefore are completely segregated from each other. To use a specific node pool (or node pools), any workload must specify the node pool(s) it would like to use. While for affinities, workloads that ask for a specific affinity will only be scheduled to nodes marked with that affinity, while workloads that did not specify any affinity might be scheduled as well to those nodes with an affinity. Therefore the scheduler cannot guarantee quota for node affinities, only to node pools.</p> <p>Note that using node pools and affinities narrows down the scope of nodes a specific project is eligible to use. It, therefore, reduces the odds of a specific workload under that Project getting scheduled. In some cases, this may reduce the overall system utilization.</p>"},{"location":"admin/admin-ui-setup/project-setup/#grouping-nodes-using-node-pools","title":"Grouping Nodes using Node Pools","text":"<p>To create a node pool you must first annotate nodes with a label or use an existing node label, as the key for grouping nodes into pools. You can use any unique label (in the format <code>key:value</code>) to form a node pool. a node pool is characterized by a label but also has its own unique node pool name.</p> <p>To get the list of nodes and their current labels, run:</p> <pre><code>kubectl get nodes --show-labels\n</code></pre> <p>To annotate a specific node with the label <code>dgx-2</code>, run:</p> <p><pre><code>kubectl label node &lt;node-name&gt; node-model=dgx-2\n</code></pre> You can annotate multiple nodes with the same label.</p> <p>To create a node pool with the chosen common label use the create node pool Run:ai API.</p>"},{"location":"admin/admin-ui-setup/project-setup/#setting-node-pools-for-a-specific-project","title":"Setting Node Pools for a Specific Project","text":"<p>By default, all node pools are associated with every Project and Department using zero resource allocation. This means that by default any Project can use any node-pool if Over-Quota is set for that Project, but only for preemptible workloads (i.e. Training workloads or Interactive using Preemptible flag).</p> <ul> <li>To guarantee resources for all workloads including non-preemptible workloads, the administrator should allocate resources in node pools.</li> <li>Go to the Node Pools tab under Project and set a quota to any of the node pools (GPU resources, CPU resources) you want to use.</li> <li>To set the Project's default node pool's order of priority, you should set the precedence of each node pool, this is done in the Project's node pool tab.</li> <li>The node pool default priority order is used if the workload did not specify its preferred node pool(s) list of priority.</li> <li>To mandate a Workload to run on a specific node pool, the Researcher should specify the node pool to use for a workload. </li> <li>If no node-pool is specified - the Project's 'Default' node-pool priority list is used. </li> <li>Press 'Save' to save your changes.</li> </ul>"},{"location":"admin/admin-ui-setup/project-setup/#grouping-nodes-using-node-affinities","title":"Grouping Nodes using Node Affinities","text":"<p>To set node affinities, you must first annotate nodes with labels. These labels will later be associated with Projects. </p> <p>To get the list of nodes, run:</p> <pre><code>kubectl get nodes\n</code></pre> <p>To annotate a specific node with the label \"dgx-2\", run:</p> <pre><code>kubectl label node &lt;node-name&gt; run.ai/type=dgx-2\n</code></pre> <ul> <li>Each node can only be annotated with a single label.</li> <li>You can annotate multiple nodes with the same label.</li> </ul>"},{"location":"admin/admin-ui-setup/project-setup/#setting-affinity-for-a-specific-project","title":"Setting Affinity for a Specific Project","text":"<p>To mandate training Jobs to run on specific node groups:</p> <ul> <li>Create a Project or edit an existing Project.</li> <li>Go to the Node Affinity tab and set a limit to specific node groups.</li> <li>If the label does not yet exist, press the + sign and add the label.</li> <li>Press Enter to save the label.</li> <li>Select the label.</li> </ul> <p>To mandate interactive Jobs to run on specific node groups, perform the same steps under the \"interactive\" section in the Project dialog.</p>"},{"location":"admin/admin-ui-setup/project-setup/#further-affinity-refinement-by-the-researcher","title":"Further Affinity Refinement by the Researcher","text":"<p>The Researcher can limit the selection of node groups by using the CLI flag <code>--node-type</code> with a specific label. When setting specific Project affinity, the CLI flag can only be used with a node group out of the previously chosen list.  See CLI reference for further information runai submit </p>"},{"location":"admin/admin-ui-setup/project-setup/#limit-duration-of-interactive-and-training-jobs","title":"Limit Duration of Interactive and Training Jobs","text":"<p>As interactive sessions involve human interaction, Run:ai provides an additional tool to enforce a policy that sets the time limit for such sessions. This policy is often used to handle situations like researchers leaving sessions open even when they do not need to access the resources.</p> <p>Warning</p> <p>This feature will cause containers to automatically stop. Any work not saved to a shared volume will be lost</p> <p>To set a duration limit for interactive Jobs:</p> <ul> <li>Create a Project or edit an existing Project.</li> <li>Go to the Time Limit tab</li> <li>You can limit interactive Jobs using two criteria:<ul> <li>Set a hard time limit (day, hour, minute) to an Interactive Job, regardless of the activity of this Job, e.g. stop the Job after 1 day of work.</li> <li>Set a time limit for Idle Interactive Jobs, i.e. an Interactive Job idle for X time is stopped. Idle means no GPU activity.</li> <li>You can set if this idle time limit is effective for Interactive Jobs that are Preemptible, non-Preemptible, or both. </li> </ul> </li> </ul> <p>The setting only takes effect for Jobs that have started after the duration has been changed.</p> <p>In some use cases, you would like to stop Training Jobs if X time elapsed since they have started to run. This can be to clean up stale Training Jobs or Jobs that are running for too long probably because of wrong parameters set or other errors of the model.</p> <p>To set a duration limit for Training Jobs:</p> <ul> <li>Create a Project or edit an existing Project.</li> <li>Go to the Time Limit tab:<ul> <li>Set a time limit for Idle Training Jobs, i.e. a Training Job idle for X time is stopped. Idle means no GPU activity.</li> </ul> </li> </ul> <p>The setting only takes effect for Jobs that have started after the duration has been changed. </p>"},{"location":"admin/admin-ui-setup/project-setup/#see-also","title":"See Also","text":"<p>Run:ai supports an additional (optional) level of resource allocation called Departments. </p>"},{"location":"admin/integration/airflow/","title":"Integrate Run:ai with Apache Airflow","text":"<p>Airflow is a platform to programmatically author, schedule, and monitor workflows. Specifically, it is used in Machine Learning to create pipelines.  </p>"},{"location":"admin/integration/airflow/#airflow-dag","title":"Airflow DAG","text":"<p>In Airflow, a DAG \u2013 or a Directed Acyclic Graph \u2013 is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies.</p> <p>A DAG is defined in a Python script, which represents the DAGs structure (tasks and their dependencies) as code.</p> <p>For example, a simple DAG could consist of three tasks: A, B, and C. It could say that A has to run successfully before B can run, but C can run anytime. It could say that task A times out after 5 minutes, and B can be restarted up to 5 times in case it fails. It might also say that the workflow will run every night at 10 pm, but shouldn\u2019t start until a certain date.</p> <p>Airflow tasks are sent for execution. Specifically, the Airflow - Kubernetes integration allows Airflow tasks to be scheduled on a Kubernetes cluster. </p>"},{"location":"admin/integration/airflow/#runai-airflow-integration","title":"Run:ai - Airflow Integration","text":"<p>DAGs are defined in Python. Airflow tasks based on Kubernetes are defined via the KubernetesPodOperator class.  To run an Airflow task with Run:ai you must provide additional, Run:ai-related, properties to </p> <p><pre><code>dag = DAG(...)\nresources = {\n\"limit_gpu\": &lt;number-of-GPUs&gt;\n}\njob = KubernetesPodOperator(\nnamespace='runai-&lt;project-name&gt;',\nimage='&lt;image-name&gt;',\nlabels={\"project\": '&lt;project-name&gt;'},\nname='&lt;task-name&gt;',\ntask_id='&lt;task-name&gt;',\nget_logs=True,\nschedulername='runai-scheduler',\nresources=resources,\ndag=dag\n)\n</code></pre> The code:</p> <ul> <li>Specifies the runai-scheduler which directs the task to be scheduled with the Run:ai scheduler</li> <li>Specifies a Run:ai Project. A Project in Run:ai specifies guaranteed GPU &amp; CPU quota.  </li> </ul> <p>Once you run the DAG, you can see Airflow tasks shown in the Run:ai UI. </p>"},{"location":"admin/integration/argo-workflows/","title":"Integrate Run:ai with Argo Workflows","text":"<p>Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes.</p> <p>This document describes the process of using Argo Workflows in conjunction with Run:ai. Argo Workflows submits jobs that are scheduled via Run:ai.</p>"},{"location":"admin/integration/argo-workflows/#install-argo-workflows","title":"Install Argo Workflows","text":"<p>Use the default installation to install Argo Workflows. As described in the documentation, open the Argo Workflows UI by running: </p> <pre><code>kubectl -n argo port-forward deployment/argo-workflows-server 2746:2746\n</code></pre> <p>Then browse to localhost:2746</p>"},{"location":"admin/integration/argo-workflows/#create-a-runai-project","title":"Create a Run:ai Project","text":"<p>Using the Run:ai user interface, create a Run:ai Project. A Project named <code>team-a</code> will create a Kubernetes namespace named <code>runai-team-a</code>.</p>"},{"location":"admin/integration/argo-workflows/#run-an-argo-workflow-with-runai","title":"Run an Argo Workflow with Run:ai","text":""},{"location":"admin/integration/argo-workflows/#create-an-argo-workflows-template","title":"Create an Argo Workflows Template","text":"<p>Within the Argo Workflows user interface, go to <code>Templates</code> and create a new Template. Add the following metadata:</p> <pre><code>spec:\ntemplates:\n- name: &lt;WORKFLOW-NAME&gt;\nmetadata:\nlabels:\nproject: team-a # (1)\n</code></pre> <ol> <li>Name of Project.</li> </ol>"},{"location":"admin/integration/argo-workflows/#create-and-run-the-workflow","title":"Create and Run the Workflow","text":"<p>Create an Argo Workflow from the template and run it. Open the Run:ai user interface, go to <code>Jobs</code>, and verify that you can see the new Job. </p>"},{"location":"admin/integration/argo-workflows/#using-gpu-fractions-with-argo-workflows","title":"Using GPU Fractions with Argo Workflows","text":"<p>To run an Argo Workflow using GPU Fractions, you will need to add an <code>annotation</code>:</p> <pre><code>spec:\ntemplates:\n- name: &lt;WORKFLOW-NAME&gt;\nmetadata:\nannotations:\ngpu-fraction: '0.5' # (1)\nlabels:\nproject: team-a # (2)\n</code></pre> <ol> <li>Size of required GPU Fraction.</li> <li>Name of Project.</li> </ol>"},{"location":"admin/integration/clearml/","title":"Integrate Run:ai with ClearML","text":"<p>ClearML is an open-source and commercial platform to manage the ML lifecycle. The purpose of this document is to explain how to run Jobs with MLflow using the Run:ai scheduler. </p>"},{"location":"admin/integration/clearml/#overview","title":"Overview","text":"<p>ClearML concepts are discussed here. Specifically see ClearML Kubernetes architecture.</p>"},{"location":"admin/integration/clearml/#terminology","title":"Terminology","text":"<ul> <li>Run:ai uses Projects. A Project is assigned to users and contains information such as quota, affinity, and more. A Run:ai Project is implemented as a Kubernetes namespace. </li> <li>ClearML allows the Reesearcher to run Experiments. Experiment is equivalent to a Run:ai Job. A ClearML Experiment is sent to a ClearML Queue for execution. </li> <li>ClearML execute Agents. An agent runs on a Kubernetes namespace. An Agent is configured to watch a Queue. The Agent fetches an experiment from the queue for execution within the Kubernetes namespace.</li> </ul>"},{"location":"admin/integration/clearml/#step-by-step-instructions","title":"Step by Step Instructions","text":""},{"location":"admin/integration/clearml/#prerequisites","title":"Prerequisites","text":"<ul> <li>A working Run:ai cluster.</li> <li>Install ClearML via ClearML helm charts. Once ClearML is installed, verify that the installation is working by running:</li> </ul> <pre><code>kubectl get pod -n clearml\n</code></pre> <p>See that all pods are up. </p>"},{"location":"admin/integration/clearml/#preparations","title":"Preparations","text":"<p>To prepare a Run:ai Project and a ClearML Queue do the following:</p> <ul> <li>In ClearML, create a queue named <code>runai-clearml</code>.</li> <li>In Run:ai, create a project named <code>clearml</code>. This will create a namespace called <code>runai-clearml</code></li> <li>Associate the queue and the project by running:</li> </ul> <pre><code>kubectl get role -n clearml k8sagent-pods-access -ojson | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | kubectl create -f -\nkubectl get rolebinding -n clearml k8sagent-pods-access -ojson | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | kubectl create -f -\nkubectl get secret -n clearml clearml-conf -ojson | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | kubectl create -f -\nkubectl get configmap -n clearml k8sagent-pod-template -ojson | sed 's@tolerations:\\\\n    {}@tolerations:\\\\n    []@g' | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | jq '.data[\"template.yaml\"]=(.data[\"template.yaml\"] + \"  schedulerName: runai-scheduler\")' | kubectl create -f -\nkubectl get deployment -n clearml clearml-k8sagent -ojson | sed 's/clearml-apiserver/clearml-apiserver.clearml.svc.cluster.local/; s/clearml-webserver/clearml-webserver.clearml.svc.cluster.local/; s/clearml-fileserver/clearml-fileserver.clearml.svc.cluster.local/; s@--template-yaml /root/template/template.yaml@--template-yaml /root/template/template.yaml --namespace runai-clearml@; s/k8s-agent/runai-k8s-agent/; s/aws-instances/runai-clearml/' | jq 'del(.status)' | jq 'del(.metadata.creationTimestamp)' | jq 'del(.metadata.generation)' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq '.metadata.namespace=\"runai-clearml\"' | kubectl create -f -\n</code></pre> <p>Note</p> <p>The script is hardcoded for the above queue name and Run:ai Project name. You can change the script accordingly.</p> <p>Validate that the Queue and the Project are connected by running:</p> <pre><code>kubectl get pod -n runai-clearml\n</code></pre> <p>You should see a ClearML agent running inside the Run:ai namespace. </p>"},{"location":"admin/integration/clearml/#running-an-experiment","title":"Running an Experiment","text":"<ul> <li>Using the ClearML interface create an experiment and enqueue it to the <code>runai-clearml</code> queue.</li> <li>Go to the Run:ai user interface. Under <code>Jobs</code> see that the job was created. </li> </ul>"},{"location":"admin/integration/jupyterhub/","title":"Connect JupyterHub with Run:ai","text":""},{"location":"admin/integration/jupyterhub/#overview","title":"Overview","text":"<p>A Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code. Uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Jupyter Notebooks are popular with Researchers as a way to code and run deep-learning code. A Jupyter Notebook runs inside the user container. For more information, see Using a Jupyter Notebook within a Run:ai Job.</p> <p>JupyterHub is a separate service that makes it possible to serve pre-configured data science environments. </p> <p>This document explains how to set up JupyterHub to integrate with Run:ai such that Notebooks spawned via JuptyerHub will use resources scheduled by Run:ai.</p>"},{"location":"admin/integration/jupyterhub/#installing-jupyterhub","title":"Installing JupyterHub","text":"<p>This document follows the JupyterHub installation documentation</p>"},{"location":"admin/integration/jupyterhub/#create-a-namespace","title":"Create a namespace","text":"<p>Run:</p> <pre><code>kubectl create namespace jhub\n</code></pre>"},{"location":"admin/integration/jupyterhub/#provide-access-roles","title":"Provide access roles","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/run-ai/docs/master/install/jupyterhub/jhubroles.yaml\n</code></pre>"},{"location":"admin/integration/jupyterhub/#create-storage","title":"Create storage","text":"<p>JupyterHub requires storage in the form of a PersistentVolume (PV). For an example of a local PV:</p> <ul> <li>Download https://raw.githubusercontent.com/run-ai/docs/master/install/jupyterhub/pv-example.yaml </li> <li>Replace <code>&lt;NODE-NAME&gt;</code> with one of your worker nodes. </li> <li>The example PV refers to <code>/srv/jupyterhub</code>. Log on to <code>&lt;NODE-NAME&gt;</code> and create the folder and run <code>sudo chmod 777 -R /srv/jupyterhub</code></li> </ul> <p>Then run:</p> <pre><code>kubectl apply -f pv-example.yaml \n</code></pre> <p>Note</p> <p>The JupyterHub installation will create a PersistentVolumeClaim named <code>hub-db-dir</code> that should be referred to by any PV you create.</p>"},{"location":"admin/integration/jupyterhub/#create-a-configuration-file","title":"Create a configuration file","text":"<p>Create a configuration file for JupyterHub. An example configuration file for Run:ai can be found in https://raw.githubusercontent.com/run-ai/docs/master/install/jupyterhub/config.yaml. It contains 3 sample Run:ai configurations. </p> <ul> <li>Download the file </li> <li>Replace <code>&lt;SECRET-TOKEN&gt;</code> with a random number generated, by running <code>openssl rand -hex 32</code></li> </ul>"},{"location":"admin/integration/jupyterhub/#install","title":"Install","text":"<p>Run:</p> <pre><code>helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/\nhelm repo update\nhelm install jhub jupyterhub/jupyterhub -n jhub  --version=0.11.1 --values config.yaml\n</code></pre>"},{"location":"admin/integration/jupyterhub/#verify-installation","title":"Verify Installation","text":"<p>Run: </p> <pre><code>kubectl get pods -n jhub\n</code></pre> <p>Verify that all pods are running</p>"},{"location":"admin/integration/jupyterhub/#access-jupyterhub","title":"Access JupyterHub","text":"<p>Run:</p> <pre><code>kubectl get service -n jhub proxy-public\n</code></pre> <p>Use the <code>External IP</code> of the service to access the service.</p> <p>Login with Run:ai Project name as user name.</p>"},{"location":"admin/integration/jupyterhub/#troubleshooting-the-jupyterhub-installation","title":"Troubleshooting the JupyterHub Installation","text":"<p>If the <code>External IP</code> of the proxy-public service remains in the <code>Pending</code> status, it might mean that this service is not configured with an <code>External IP</code> by default.</p> <p>To fix, find out which pod is the proxy pod running on.</p> <p>Run: </p> <pre><code>kubectl get pods -n jhub -l component=proxy -o=jsonpath='{.items[0].spec.nodeName}{\"\\n\"}'\n</code></pre> <p>This will print the node that the proxy pod is running on. You will need to get both the internal and external IPs of this node for the next step. </p> <p>Now, let's check the proxy-public service definition. Run:</p> <pre><code>kubectl edit svc proxy-public -n jhub\n</code></pre> <p>Under <code>spec</code> You should see a section <code>externalIPs</code>. If it does not exist, you must add it there. The section must contain both the external and the internal IPs of the proxy pod, for example:</p> <pre><code>spec:\nexternalIPs:\n- 35.224.44.230\n- 10.8.0.9\n</code></pre> <p>Save the file and then try to access JupyterHub by using the external IP from the previous step in your browser.</p> <p>Caution</p> <p>Jupyter hub integration does not currently work properly when the Run:ai Project name includes a hyphen ('-'). We are working to fix that. </p>"},{"location":"admin/integration/kubeflow/","title":"Integrate Run:ai with Kubeflow","text":"<p>Kubeflow is a platform for data scientists who want to build and experiment with ML pipelines. Kubeflow is also for ML engineers and operational teams who want to deploy ML systems to various environments for development, testing, and production-level serving.</p> <p>This document describes the process of using Kubeflow in conjunction with Run:ai. Kubeflow submits jobs that are scheduled via Run:ai.</p> <p>Kubeflow is a set of technologies. This document discusses Kubeflow Notebooks and Kubeflow Pipelines.</p>"},{"location":"admin/integration/kubeflow/#install-kubeflow","title":"Install Kubeflow","text":"<p>Use the default installation to install Kubeflow.</p>"},{"location":"admin/integration/kubeflow/#install-runai-cluster","title":"Install Run:ai Cluster","text":"<p>When installing Run:ai, customize the cluster installation as follows:</p> <ul> <li>Set <code>createNamespaces</code> to <code>false</code>, as Kubeflow uses its own namespace convention.</li> </ul>"},{"location":"admin/integration/kubeflow/#create-runai-projects","title":"Create Run:ai Projects","text":"<p>Kubeflow uses the namespace convention <code>kubeflow-&lt;username&gt;</code>. Use the 4 steps here to set up Run:ai projects and link them with Kubeflow namespaces. </p> <p>Verify that the association has worked by running:</p> <pre><code>kubectl get rolebindings -n &lt;KUBEFLOW-USER-NAMESPACE&gt;\n</code></pre> <p>See that role bindings starting with <code>runai-</code> were created.</p>"},{"location":"admin/integration/kubeflow/#kubeflow-users-and-kubernetes-namespaces","title":"Kubeflow, Users and Kubernetes Namespaces","text":"<p>Kubeflow has a multi-user architecture. A user has a Kubeflow profile which maps to a Kubernetes Namespace. This is similar to the Run:ai concept where a Run:ai Project is mapped to a Kubernetes namespace.</p>"},{"location":"admin/integration/kubeflow/#kubeflow-notebooks","title":"Kubeflow Notebooks","text":"<p>When starting a Kubeflow Notebook, you select a <code>Kubeflow configuration</code>. A Kubeflow configuration allows you to inject additional settings into the notebook, such as environment variables. To use Kubeflow with Run:ai you will use configurations to inject:</p> <ul> <li>The name of the Run:ai project</li> <li>Allocation of a fraction of a GPU, if required</li> </ul>"},{"location":"admin/integration/kubeflow/#whole-gpus","title":"Whole GPUs","text":"<p>To use Run:ai with whole GPUs (no fractions), apply the following configuration:</p> <pre><code>apiVersion: kubeflow.org/v1alpha1\nkind: PodDefault\nmetadata:\nname: runai-non-fractional\nnamespace: &lt;KUBEFLOW-USER-NAMESPACE&gt;\nspec:\ndesc: \"Use Run:ai scheduler (whole GPUs)\"\nenv:\n- name: RUNAI_PROJECT value: \"&lt;PROJECT&gt;\"\nselector:\nmatchLabels:\nrunai-non-fractional: \"true\"  # key must be identical to metadata.name\n</code></pre> <p>Where <code>&lt;KUBEFLOW-USER-NAMESPACE&gt;</code> is the name of the namespace associated with the Kubeflow user and <code>&lt;PROJECT&gt;</code> is the name of the Run:ai project.</p> <p>Important</p> <p>Jobs should not be submitted within the same namespace where Kubeflow Operator is installed.</p> <p>Within the Kubeflow Notebook creation form, select the new configuration as well as the number of GPUs required.</p>"},{"location":"admin/integration/kubeflow/#fractions","title":"Fractions","text":"<p>The Kubeflow Notebook creation form only allows the selection of 1, 2, 4, or 8 GPUs. It is not possible to select a portion of a GPU (e.g. 0.5). As such, within the form, select <code>None</code> in the GPU box together with the following configuration:</p> <p><pre><code>apiVersion: kubeflow.org/v1alpha1\nkind: PodDefault\nmetadata:\nname: runai-half-gpu\nnamespace: &lt;KUBEFLOW-USER-NAMESPACE&gt;\nspec:\ndesc: \"Allocate 0.5 GPUs via Run:ai scheduler\"\nenv:\n- name: RUNAI_PROJECT value: \"&lt;PROJECT&gt;\"\n- name: RUNAI_GPU_FRACTION\nvalue: \"0.5\"\nselector:\nmatchLabels:\nrunai-half-gpu: \"true\"  # key must be identical to metadata.name\n</code></pre> Similar configurations can be created for fractional configurations, other than 0.5. </p>"},{"location":"admin/integration/kubeflow/#kubeflow-pipelines","title":"Kubeflow Pipelines","text":"<p>Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.</p> <p>As with Kubeflow Notebooks, the goal of this section is to run pipelines jobs within the context of Run:ai.</p> <p>To create a Kubeflow pipeline, you:</p> <ul> <li>Write code using the Kubeflow Pipeline SDK. </li> <li>Package it into a single compressed file.</li> <li>Upload the file into Kubeflow and set it up.</li> </ul> <p>The example code provided here shows how to augment pipeline code to use Run:ai</p>"},{"location":"admin/integration/kubeflow/#whole-gpus_1","title":"Whole GPUs","text":"<p>To the pipeline code add:</p> <pre><code>_training = training_op()\n...\n_training.add_pod_label('runai', 'true')\n_training.add_pod_label('project', '&lt;PROJECT&gt;')\n</code></pre> <p>Where <code>&lt;Project&gt;</code> is the Run:ai project name. See example code here</p> <p>Compile the code by running:</p> <p><pre><code>dsl-compile --py kubeflow-runai-one-gpu.py --output kubeflow-runai-one-gpu.tar.gz\n</code></pre> (dsl-compile is part of the Kubeflow Pipeline Python SDK).</p>"},{"location":"admin/integration/kubeflow/#fractions_1","title":"Fractions","text":"<p>To allocate half a GPU, add the following to the pipeline code:</p> <pre><code>_training = training_op()\n...\n_training.add_pod_label('runai', 'true')\n_training.add_pod_label('project', '&lt;PROJECT&gt;')\n_training.add_pod_annotation('gpu-fraction', '0.5')\n</code></pre> <p>Where <code>&lt;Project&gt;</code> is the Run:ai project name. See example code here.</p> <p>Compile the code as described above. </p>"},{"location":"admin/integration/kubevirt/","title":"Scheduling Virtual Machines using Run:ai","text":"<p>Many organizations use virtual machines (VMs) to provide operating system abstraction to users. Containers are different than VMs but serve a similar purpose. Containers at a large scale are best managed by Kubernetes and Run:ai is based on Kubernetes. </p> <p>It is possible to mix and match containers and VMs to some extent using a technology called KubeVirt. KubeVirt allows running VMs inside containers on top of Kubernetes. </p> <p>This article describes how to use KubeVirt to schedule VMs with GPUs.</p>"},{"location":"admin/integration/kubevirt/#limitations","title":"Limitations","text":"<p>Each node in the cluster will be able to support either VMs or containers - not combined.</p> <p>GPU fractions are not supported. </p>"},{"location":"admin/integration/kubevirt/#preparations","title":"Preparations","text":"<p>Making GPUs visible to VMs is not trivial. It requires either a license for NVIDIA software called NVIDIA vGPU or creating a GPU passthrough by the explicit mapping of GPU devices to virtual machines. This guide relates to the latter option. </p>"},{"location":"admin/integration/kubevirt/#install-kubevirt","title":"Install KubeVirt","text":"<p>Install KubeVirt using the following guide.</p>"},{"location":"admin/integration/kubevirt/#dedicate-specific-nodes-for-vms","title":"Dedicate specific nodes for VMs","text":"<p>Dedicate specific nodes within the cluster to be used for VMs and not containers - following the guide.</p> <p>Specifically, restrict <code>virt-controller</code>, <code>virt-api</code> and <code>virt-handler</code> pods to only run on the nodes you want to be used for VMs.</p>"},{"location":"admin/integration/kubevirt/#assign-host-devices-to-virtual-machines","title":"Assign host devices to virtual machines","text":"<p>For each node in the cluster that we want to use with VMs we must:</p> <ul> <li>Identify all GPU cards we want to dedicate to be used by VMs.</li> <li>Map GPU cards for KubeVirt to pick up (called assigning host devices to a virtual machine).</li> </ul> <p>Instructions for identifying GPU cards are operating-system-specific. For Ubuntu 20.04 run:</p> <pre><code>lspci -nnk -d 10de:\n</code></pre> <p>Search for GPU cards that are marked with the text Kernel driver in use. Save the PCI Address, for example: 10de:1e04</p> <p>Important</p> <p>Once exposed, these GPUs cannot be used by regular pods. Only VMs. </p> <p>To expose the GPUs and map them to KubeVirt follow the instructions here. Specifically, run:</p> <pre><code>kubectl edit kubevirt -n kubevirt -o yaml\n</code></pre> <p>And add all of the PCI Addresses of all GPUs of all Nodes concatenated by commas, with the resource name kubevirt/vmgpu:</p> <pre><code>spec:\ncertificateRotateStrategy: {}\nconfiguration:\ndeveloperConfiguration:\nfeatureGates:\n- GPU\n- HostDevices\npermittedHostDevices:\npciHostDevices:\n- pciVendorSelector: &lt;PCI-ADDRESS&gt;,&lt;PCI-ADDRESS&gt;,\nresourceName: kubevirt/vmgpu\n</code></pre>"},{"location":"admin/integration/kubevirt/#assign-gpus-to-vms","title":"Assign GPUs to VMs","text":"<p>You must create a CRD called vm for each virtual machine. <code>vm</code> is a reference to a virtual machine and its capabilities.</p> <p>The Run:ai project is matched to a Kubernetes namespace. Unless manually configured, the namespace is <code>runai-&lt;PROJECT-NAME&gt;</code>. Per Run:ai Project, create a <code>vm</code> object. See KubeVirt documentation example. Specifically, the created YAML should look like this:</p> <pre><code>spec:\nrunning: false\ntemplate:\nmetadata:\ncreationTimestamp: null\nlabels:\n....\npriorityClassName: &lt;WORKLOAD-TYPE&gt;\nproject: &lt;PROJECT-NAME&gt;\nspec:\nschedulerName: runai-scheduler\ndomain:\ndevices:\ngpus:\n- deviceName: kubevirt/vmgpu # identical name to resourceName above\nname: gpu1  # name here is arbitrary and is not used. \n</code></pre> <p>Where <code>&lt;WORKLOAD-TYPE&gt;</code> is <code>train</code> or <code>build</code></p>"},{"location":"admin/integration/kubevirt/#turn-on-kubevirt-feature-in-runai","title":"Turn on KubeVirt feature in Run:ai","text":"<ul> <li> <p>If you want to upgrade the runai cluster, use the instructions. </p> <ul> <li>During the upgrade, customize the cluster installation by adding the following to the values.yaml file:</li> </ul> <pre><code>global:\nkubevirtCluster:\nenabled: true\n</code></pre> </li> <li> <p>If you don't want to upgrade the whole cluster, you can add those values to your existing values.yaml file.</p> <ul> <li>Then, run the command:</li> </ul> <pre><code>helm upgrade runai-cluster runai/runai-cluster -n runai -f values.yaml\n</code></pre> </li> <li> <p>Make sure the <code>kubevirtCluster: enabled</code> flag is still turned on in <code>runaiconfig</code>:</p> <pre><code>kubectl edit runaiconfig runai -n runai\n</code></pre> </li> </ul>"},{"location":"admin/integration/kubevirt/#start-a-vm","title":"Start a VM","text":"<p>Run:</p> <pre><code>virtctl start testvm -n runai-test\n</code></pre> <p>You can now see the VMs pod in Run:ai.</p> <pre><code>runai list -A\nNAME    STATUS   AGE  NODE         IMAGE                                   TYPE  PROJECT  USER  GPUs Allocated (Requested)  PODs Running (Pending)  SERVICE URL(S)\ntestvm  Running  0s   master-node  quay.io/kubevirt/virt-launcher:v0.47.1        test           1 (1)                       1 (0)\n</code></pre>"},{"location":"admin/integration/mlflow/","title":"Integrate Run:ai with MLflow","text":"<p>MLflow is an open-source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. The purpose of this document is to explain how to run Jobs with MLflow using the Run:ai scheduler. </p>"},{"location":"admin/integration/mlflow/#overview","title":"Overview","text":"<p>MLflow concepts and alternative architectures are discussed here. MLflow can run on various platforms. To work with Run:ai we would use the MLflow Kubernetes integration.</p> <p>The MLflow documentation describes the Kubernetes integration as such:</p> <p>Quote</p> <p>When you run an MLflow Project on Kubernetes, MLflow constructs a new Docker image containing the Project\u2019s contents; this image inherits from the Project\u2019s Docker environment. MLflow then pushes the new Project image to your specified Docker registry and starts a Kubernetes Job on your specified Kubernetes cluster. This Kubernetes Job downloads the Project image and starts a corresponding Docker container. Finally, the container invokes your Project\u2019s entry point, logging parameters, tags, metrics, and artifacts to your MLflow tracking server.</p> <p>To run an MLflow job via Kubernetes, you specify an MLflow Kubernetes configuration file that contains a template. Here is an example from the MLflow documentation:</p> <pre><code>{\n\"kube-context\": ...,\n\"repository-uri\": ...,\n\"kube-job-template-path\": \"/username/path/to/kubernetes_job_template.yaml\"\n}\n</code></pre> <p>The essence of the Run:ai integration is the modification of the <code>kubernetes_job_template.yaml</code> file. Specifically adding the Run:ai scheduler name and the Run:ai Project (Kubernetes namespace).</p>"},{"location":"admin/integration/mlflow/#step-by-step-instructions","title":"Step by Step Instructions","text":""},{"location":"admin/integration/mlflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install MLflow.</li> <li>Make sure you have push access to a Docker repository from your local machine.</li> <li>Make sure you are connected to Run:ai via the Run:ai Command-line interface.</li> </ul>"},{"location":"admin/integration/mlflow/#the-sample-mlflow-project","title":"The sample MLflow Project","text":"<p>The relevant sample files are here. These contain:</p> <ul> <li>A <code>Dockerfile</code>. This file builds a base docker image containing python3 and the required MLflow dependencies. The Docker file is already compiled and available at <code>gcr.io/run-ai-demo/mlflow-demo</code>.</li> <li>An MLflow project file <code>MLproject</code>. The project file contains the base image above as well as the python command-line to run. </li> <li>The training python code <code>train.py</code></li> <li>MLflow Kubernetes configuration files as in the MLflow documentation.<ul> <li>Kubernetes configuration file <code>kubernetes_config.json</code></li> <li>An MLflow Kubernetes Job template <code>kubernetes_job_template.yaml</code> </li> </ul> </li> </ul>"},{"location":"admin/integration/mlflow/#preparations","title":"Preparations","text":"<ul> <li>Edit <code>kubernetes_config.json</code>. <ul> <li>Set <code>kube-context</code> to the name of the Kubernetes context. You can find the context name by running <code>runai list clusters</code> or <code>kubectl config get-contexts</code>.</li> <li>Set <code>repository-uri</code> to a repository and name of a docker image that will be used by MLflow (this is a different image than the base docker image described above). Your local machine needs permissions to be able to push this image to the Docker registry.</li> </ul> </li> <li>Edit <code>kubernetes_job_template.yaml</code>. <ul> <li>Set the value of <code>namespace</code> to <code>runai-&lt;name of Run:ai project&gt;</code>. </li> <li>Note the last line which adds the Run:ai scheduler to the configuration. </li> <li>Do not change the lines marked by <code>{replaced with...</code>.</li> <li>Set the requested resources including GPUs. You can use the <code>--dry-run</code> flag of the runai submit command to gain insight on additional configurations</li> </ul> </li> </ul>"},{"location":"admin/integration/mlflow/#running","title":"Running","text":"<ul> <li>Perform <code>docker login</code> if required.</li> <li>Run:</li> </ul> <pre><code>mlflow run mlproject -P alpha=5.0  -P l1-ratio=0.1  \\\n    --backend kubernetes --backend-config kubernetes_config.json\n</code></pre>"},{"location":"admin/integration/mlflow/#mlflow-tracking","title":"MLflow Tracking","text":"<p>The sample training code above does not contain references to an MLflow tracking server. This has been done to simplify the required setup. With MLflow-Kubernetes you will need a remote server architecture. Once you have such an architecture set up, you can use MLflow Tracking in your code.</p>"},{"location":"admin/integration/mlflow/#using-interactive-workloads","title":"Using Interactive Workloads","text":"<p>With Run:ai you can also run interactive workloads. To run the Job as interactive, add the following to <code>kubernetes_job_template.yaml</code>: </p> <pre><code>metadata:\nlabels:\npriorityClassName: \"build\"\n</code></pre>"},{"location":"admin/integration/mlflow/#see-also","title":"See Also","text":"<ul> <li>You can use MLflow together with Fractional GPUs. For more information see Launch Job via YAML.</li> <li>To map additional Run:ai options to the YAML, see Launch Job via YAML.</li> </ul>"},{"location":"admin/integration/seldon/","title":"Integrate Run:ai with Seldon Core","text":"<p>Seldon Core is software that deploys machine learning models to production over Kubernetes. The purpose of this document is to explain how to use Seldon Core together with Run:ai.  </p> <p>Of special importance, is the usage of Seldon together with the Run:ai fractions technology: Machine learning production tends to take less GPU Memory. As such, allocating a fraction of the GPU per job allows for better GPU Utilization. </p>"},{"location":"admin/integration/seldon/#prerequisites","title":"Prerequisites","text":"<p>Install Seldon Core as described here. We recommend using the helm-based installation of both Seldon Core and Istio.</p>"},{"location":"admin/integration/seldon/#create-a-seldon-deployment","title":"Create a Seldon deployment","text":"<p>The instructions below follow a sample machine learning model that tests the Run:ai - Seldon Core integration.  Save the following in a file named <code>&lt;FILE-NAME&gt;.yaml</code></p> <pre><code>apiVersion: machinelearning.seldon.io/v1\nkind: SeldonDeployment\nmetadata:\nname: seldon-model\nnamespace: runai-&lt;PROJECT-NAME&gt;\nspec:\nname: test-deployment\npredictors:\n- componentSpecs:\n- spec:\ncontainers:\n- name: classifier\nimage: seldonio/mock_classifier:1.5.0-dev\nresources:\nlimits:\nnvidia.com/gpu: &lt;GPUs&gt;\nschedulerName: runai-scheduler\ngraph:\nchildren: []\nendpoint:\ntype: REST\nname: classifier\ntype: MODEL\nname: example\nreplicas: 1\n</code></pre> <p>apiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata:   name: seldon-model   namespace: runai- spec:   name: test-deployment   predictors:   - componentSpecs:     - spec:         containers:         - name: classifier           image: seldonio/mock_classifier:1.0           resources:             limits:               nvidia.com/gpu:          schedulerName: runai-scheduler     graph:       children: []       endpoint:         type: REST       name: classifier       type: MODEL     name: example     replicas: 1 <p>Replace <code>&lt;PROJECT-NAME&gt;</code> with the Run:ai projects and <code>&lt;GPUs&gt;</code> with the amount of GPUs you want to allocate (e.g. 0.5 GPUs).</p> <pre><code>kubectl apply -f &lt;FILE-NAME&gt;.yaml\n</code></pre>"},{"location":"admin/integration/seldon/#verification","title":"Verification","text":"<p>Run: <code>runai list jobs</code> and verify that the job is running</p>"},{"location":"admin/integration/seldon/#delete-a-deployment","title":"Delete a deployment","text":"<p>Run: </p> <pre><code>kubectl delete -f &lt;FILE-NAME&gt;.yaml\n</code></pre>"},{"location":"admin/integration/weights-and-biases/","title":"Workspace Weights and Biases","text":"<p>Weights and Biases is a commercial tool that provides experiment tracking, model visualization, and collaboration for machine learning projects. It helps researchers and developers keep track of their experiments, visualize their results, and compare different models to make informed decisions.</p> <p>When Wights and Biases are integrated into Run:ai Workspaces, researchers can easily create their custom work environments and have access to a toolbox of many researcher-relevant tools in a single place. Researchers can now create useful connectivity between the running Workspace and the relevant project using Weights Biases for experiment tracking.</p> <p>To configure Weights and Biases:</p> <ol> <li>Login to your account in Weights and Biases. If you do not have a valid account, you will need to create one.</li> <li>Setup your Weights and Biases account here</li> <li>In your Run:ai account, create an environment and set Weights and Biases as a tool then:</li> <li>Link it to https://wandb-c.ACME.com/${WANDB_PROJECT}</li> <li>Add an environment variable:<pre><code>```Key = WANDB_PROJECT```\n\n```Value =``` leave empty for researcher to fill it in when creating a Workspace in the next step.\n</code></pre> <ol> <li>Create a Workspace using the Environment you just created.</li> <li>In the Workspace, add the URL for your project in your Weights and Biases account to the <code>value</code> environment variable. </li> </ol> </li> </ol> <p>This will create a link, that will automatically open a new tab directly from your Workspace to your exact Weights and Biases project.</p>"},{"location":"admin/researcher-setup/cli-install/","title":"Install the Run:ai Command-line Interface","text":"<p>The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc.</p> <p>The instructions below will guide you through the process of installing the CLI. The Run:ai CLI runs on Mac and Linux. You can run the CLI on Windows by using Docker for Windows. See the end of this document.</p>"},{"location":"admin/researcher-setup/cli-install/#researcher-authentication","title":"Researcher Authentication","text":"<p>When enabled, Researcher authentication requires additional setup when installing the CLI. To configure authentication see Setup Project-based Researcher Access Control. Use the modified Kubernetes configuration file described in the article.</p>"},{"location":"admin/researcher-setup/cli-install/#prerequisites","title":"Prerequisites","text":"<ul> <li>When installing the command-line interface, it is worth considering future upgrades:<ul> <li>Install the CLI on a dedicated Jumpbox machine. Researchers will connect to the Jumpbox from which they can submit Run:ai commands</li> <li>Install the CLI on a shared directory that is mounted on Researchers' machines.  </li> </ul> </li> <li>A Kubernetes configuration file. </li> </ul>"},{"location":"admin/researcher-setup/cli-install/#setup","title":"Setup","text":""},{"location":"admin/researcher-setup/cli-install/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<ul> <li>On the Researcher's root folder, create a directory .kube. Copy the Kubernetes configuration file into the directory. Each Researcher should have a separate copy of the configuration file. The Researcher should have write access to the configuration file as it stores user defaults. </li> <li>If you choose to locate the file at a different location than <code>~/.kube/config</code>, you must create a shell variable to point to the configuration file as follows:</li> </ul> <pre><code>export KUBECONFIG=&lt;Kubernetes-config-file&gt;\n</code></pre> <ul> <li>Test the connection by running:</li> </ul> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#install-runai-cli","title":"Install Run:ai CLI","text":"Mac or LinuxWindows (Run:ai Version 2.9)Windows (Run:ai Version 2.8 or lower) <ul> <li>Go to the Run:ai user interface. On the top right select <code>Researcher Command Line Interface</code>.</li> <li>Select <code>Mac</code> or <code>Linux</code>. </li> <li>Download directly using the button or copy the command and run it on a remote machine</li> <li>Run:</li> </ul> <pre><code>chmod +x runai\nsudo mv runai /usr/local/bin/runai\n</code></pre> <ul> <li>Go to the Run:ai user interface. On the top right select <code>Researcher Command Line Interface</code>.</li> <li>Select <code>Windows</code></li> <li>Download directly using the button or copy the command and run it on a remote machine</li> <li>Rename the downloaded file to have a <code>.exe</code> extension and move the file to a folder that is a part of the <code>PATH</code>.</li> </ul> <ul> <li>Install Docker for Windows.</li> <li>Get the following folder from GitHub: https://github.com/run-ai/docs/tree/master/cli/windows.</li> <li>Replace <code>config</code> with your Kubernetes Configuration file.</li> <li>Replace <code>&lt;CLUSTER-URL&gt;</code> in the Dockerfile with the URL of the cluster. The URL can be found in the <code>Clusters</code> view of the Run:ai user interface. </li> <li>Run: <code>build.sh</code> to create a docker image named <code>runai-cli</code>.</li> </ul> <p>Test the image by running:</p> <pre><code>docker run -it runai-cli bash\n</code></pre> <p>If you get an error <code>x509: certificate signed by unknown authority</code> please contact Run:ai customer support </p> <p>Try and connect to your cluster from inside the docker by running a Run:ai CLI command. E.g. <code>runai list projects</code>.</p> <p>Distribute the image to Windows users.</p> <ul> <li>In case you want to use port-forward feature please use the following command</li> </ul> <pre><code>docker run -it -p &lt;PORT&gt;:&lt;PORT&gt; runai-cli bash\n</code></pre> <p>And when using <code>runai submit</code> command add the following flag:  <pre><code>--address 0.0.0.0\n</code></pre></p> <p>Note</p> <p>An alternative way of downloading the CLI is provided under the CLI Troubleshooting section.</p> <p>To verify the installation run:</p> <pre><code>runai list jobs\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#install-command-auto-completion","title":"Install Command Auto-Completion","text":"<p>It is possible to configure your Linux/Mac shell to complete Run:ai CLI commands. This feature works on bash and zsh shells only.</p>"},{"location":"admin/researcher-setup/cli-install/#zsh","title":"Zsh","text":"<p>Edit the file <code>~/.zshrc</code>. Add the lines:</p> <pre><code>autoload -U compinit; compinit -i\nsource &lt;(runai completion zsh)\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#bash","title":"Bash","text":"<p>Install the bash-completion package:</p> <ul> <li>Mac: <code>brew install bash-completion</code></li> <li>Ubuntu/Debian: <code>sudo apt-get install bash-completion</code></li> <li>Fedora/Centos: <code>sudo yum install bash-completion</code></li> </ul> <p>Edit the file <code>~/.bashrc</code>. Add the lines:</p> <pre><code>[[ -r \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d ]] &amp;&amp; . \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d\nsource &lt;(runai completion bash)\n</code></pre>"},{"location":"admin/researcher-setup/cli-install/#troubleshoot-the-cli-installation","title":"Troubleshoot the CLI Installation","text":"<p>See Troubleshooting a CLI installation</p>"},{"location":"admin/researcher-setup/cli-install/#update-the-runai-cli","title":"Update the Run:ai CLI","text":"<p>To update the CLI to the latest version perform the same install process again.</p>"},{"location":"admin/researcher-setup/cli-install/#delete-the-runai-cli","title":"Delete the Run:ai CLI","text":"<p>If you have installed using the default path, run:</p> <pre><code>sudo rm /usr/local/bin/runai\n</code></pre>"},{"location":"admin/researcher-setup/docker-registry-config/","title":"Using a Docker Registry with Credentials","text":""},{"location":"admin/researcher-setup/docker-registry-config/#why","title":"Why?","text":"<p>Some Docker images are stored in private docker registries. For the Researcher to access the images, we will need to provide credentials for the registry.</p>"},{"location":"admin/researcher-setup/docker-registry-config/#how","title":"How?","text":"<p>There could be two business scenarios:</p> <ol> <li>All researchers use single credentials for the registry. </li> <li>There exist separate registry credentials per Run:ai Project. </li> </ol>"},{"location":"admin/researcher-setup/docker-registry-config/#single-credentials","title":"Single Credentials","text":"<p>For each private registry you must perform the following (The example below uses Docker Hub):</p> <pre><code>kubectl create secret docker-registry &lt;secret_name&gt; -n runai \\ \n    --docker-server=https://index.docker.io/v1/ \\\n    --docker-username=&lt;user_name&gt; --docker-password=&lt;password&gt;\n</code></pre> <p>Then:</p> <pre><code>kubectl label secret &lt;secret_name&gt; runai/cluster-wide=\"true\" -n runai\n</code></pre> <ul> <li><code>&lt;secret_name&gt;</code> may be any arbitrary string</li> <li><code>&lt;user_name&gt;</code> and <code>&lt;password&gt;</code> are the repository user and password</li> </ul> <p>Notes<ul> <li>The secret may take up to a minute to update in the system.</li> <li>The above scheme relies on the cluster setting <code>clusterWideSecret</code> to be set to <code>true</code></li> </ul> </p>"},{"location":"admin/researcher-setup/docker-registry-config/#credentials-per-project","title":"Credentials per Project","text":"<p>For each Run:ai Project create a secret:</p> <pre><code>kubectl create secret docker-registry &lt;secret_name&gt; -n &lt;NAMESPACE&gt; \\ \n    --docker-server=https://index.docker.io/v1/ \\\n    --docker-username=&lt;user_name&gt; --docker-password=&lt;password&gt;\n</code></pre> <p>Where <code>&lt;NAMESPACE&gt;</code> is the namespace associated with the Project (typically its <code>runai-&lt;PROJECT-NAME&gt;</code>).</p> <p>Then apply the secret to Run:ai by running:</p> <pre><code>kubectl patch serviceaccount default -n &lt;NAMESPACE&gt; -p '{\"imagePullSecrets\": [{\"name\": \"&lt;secret_name&gt;\"}]}'\n</code></pre>"},{"location":"admin/researcher-setup/docker-registry-config/#google-cloud-registry","title":"Google Cloud Registry","text":"<p>Follow the steps below to access private images in the Google Container Registry (GCR):</p> <ul> <li>Create a service-account in GCP. Provide it <code>Viewer</code> permissions and download a JSON key.</li> <li>Under GCR, go to image and locate the domain name. Example GCR domains can be <code>gcr.io</code>, <code>eu.gcr.io</code> etc. </li> <li> <p>On your local machine, log in to docker with the new credentials: <pre><code>docker login -u _json_key -p \"$(cat &lt;config.json&gt;)\" &lt;gcr-domain&gt;\n</code></pre>  Where <code>&lt;gcr-domain&gt;</code> is the GCR domain we have located, <code>&lt;config.json&gt;</code> is the GCP configuration file. This will generate an entry for the GCR domain in your  <code>~/.docker/config.json file</code>.</p> </li> <li> <p>Open the <code>~/.docker/config.json</code> file.  Copy the JSON structure under the GCR domain into a new file called <code>~/docker-config.json</code>. When doing so, take care to remove all newlines. For example: <pre><code>{\"https://eu.gcr.io\": { \"auth\": \"&lt;key&gt;\"}}\n</code></pre></p> </li> <li> <p>Convert the file into base64: <pre><code>cat ~/docker-config.json | base64\n</code></pre></p> </li> <li>Create a new file called <code>secret.yaml</code>:</li> </ul> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: gcr-secret\nnamespace: runai\nlabels:\nrunai/cluster-wide: \"true\"\ndata:\n.dockerconfigjson: &lt;&lt; PASTE_HERE_THE_LONG_BASE64_ENCODED_STRING &gt;&gt;\ntype: kubernetes.io/dockerconfigjson\n</code></pre> <ul> <li>Apply to Kubernetes by running  the command: <pre><code>kubectl create -f ~/secret.yaml\n</code></pre></li> <li>Test your settings by submitting a which references an image from the GCR repository</li> </ul>"},{"location":"admin/researcher-setup/docker-to-runai/","title":"From Docker to Run:ai","text":""},{"location":"admin/researcher-setup/docker-to-runai/#dockers-images-and-kubernetes","title":"Dockers, Images, and Kubernetes","text":"<p>Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image.</p> <p>You create a container by starting a docker image on a machine.</p> <p>Run:ai is based on Kubernetes. At its core, Kubernetes is an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the Researcher's workflow as follows.</p>"},{"location":"admin/researcher-setup/docker-to-runai/#image-repository","title":"Image Repository","text":"<p>If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when runai submit workloads, the Researcher must use the flag <code>--local-image</code>).</p> <p>If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself.  It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub. Alternatively, the organization can install a private repository on-prem.</p> <p>Day-to-day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, <code>nvcr.io/nvidia/pytorch:19.12-py_3</code> is a PyTorch image that is located in nvcr.io. This is the Nvidia image repository as found on the web. </p>"},{"location":"admin/researcher-setup/docker-to-runai/#data","title":"Data","text":"<p>Deep learning is about data. It can be your code, the training data, saved checkpoints, etc.</p> <p>If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself.</p> <p>If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command).</p>"},{"location":"admin/researcher-setup/docker-to-runai/#working-with-containers","title":"Working with Containers","text":"<p>Starting a container using docker usually involves a single command-line with multiple flags. A typical example: </p> <pre><code>docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME='hostname' \\\n    -v /raid/public/my_datasets:/root/dataset:ro   -i  nvcr.io/nvidia/pytorch:19.12-py3\n</code></pre> <p>The docker command <code>docker run</code> should be replaced with a Run:ai command <code>runai submit</code>. The flags are usually the same but some adaptation is required. A complete list of flags can be found here: runai submit. </p> <p>There are similar commands to get a shell into the container (runai bash), get the container logs (runai logs), and more. For a complete list see the Run:ai CLI reference. </p>"},{"location":"admin/researcher-setup/docker-to-runai/#schedule-an-onboarding-session","title":"Schedule an Onboarding Session","text":"<p>It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline  Researchers' work as well as save money for the organization.</p>"},{"location":"admin/researcher-setup/limit-to-node-group/","title":"Group Nodes","text":""},{"location":"admin/researcher-setup/limit-to-node-group/#why","title":"Why?","text":"<p>In some business scenarios, you may want to direct the Run:ai scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions, Hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Another example is an inference workload that is optimized to a specific GPU type and must have dedicated resources reserved to ensure enough capacity.</p> <p>Run:ai provides two methods to designate, and group, specific resources:</p> <ul> <li>Node Pools: Run:ai allows administrators to group specific nodes into a node pool. A node pool is a group of nodes identified by a given name (node pool name) and grouped by any label (key and value combination). The label can be chosen by the administrator or can be an existing, pre-set, label (such as an NVIDIA GPU type label).</li> <li>Node Affinity: Run:ai allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag <code>--node-type &lt;label&gt;</code> to force this allocation.</li> </ul> <p>Important</p> <p>One can set and use both node pool and node affinity combined as a prerequisite to the scheduler, for example, if a researcher wants to use a T4 node with an Infiniband card - he or she can use a node pool of T4 and from that group, choose only the nodes with Infiniband card (node-type = infiniband).</p> <p>There is a tradeoff in place when allowing Researchers to designate specific nodes. Overuse of this feature limits the scheduler in finding an optimal resource and thus reduces overall cluster utilization.</p>"},{"location":"admin/researcher-setup/limit-to-node-group/#configuring-node-groups","title":"Configuring Node Groups","text":"<p>To configure a node pool:</p> <ul> <li>Find the label key &amp; value you want to use for Run:ai to create the node pool.</li> <li>Check that the nodes you want to group as a pool have a unique label to use, otherwise you should mark those nodes with your own uniquely identifiable label.</li> <li>Get the names of the nodes you want Run:ai to group together. To get a list of nodes, run:</li> </ul> <pre><code>kubectl get nodes\nKubectl get nodes --show-labels\n</code></pre> <ul> <li>If you chose to set your own label, run the following:</li> </ul> <pre><code>kubectl label node &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;\n</code></pre> <p>The same value can be set to a single node or multiple nodes. Node Pool can only use one label (key &amp; value) at a time. *   To create a node pool use the create node pool Run:ai API.</p> <p>To configure a node affinity:</p> <ul> <li>Get the names of the nodes where you want to limit Run:ai. To get a list of nodes, run:</li> </ul> <pre><code>kubectl get nodes\n</code></pre> <ul> <li>For each node run the following:</li> </ul> <pre><code>kubectl label node &lt;node-name&gt; run.ai/type=&lt;label&gt;\n</code></pre> <p>The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value.</p>"},{"location":"admin/researcher-setup/limit-to-node-group/#using-node-groups-via-the-cli","title":"Using Node Groups via the CLI","text":"<p>To use Run:ai node pool with a workload, use Run:ai CLI command \u2018node-pool\u2019: </p> <pre><code>runai submit job1 ... --node-pools \"my-pool\" ...\n</code></pre> <p>To use multiple node pools with a workload, use the Run:ai CLI command:</p> <pre><code>runai submit job1 ... --node-pools \"my-pool my-pool2 my-pool3\" ...\n</code></pre> <p>With multiple node pools, the researcher creates a list of prioritized node pools and lets the scheduler try and choose from any of the node pools in the list, according to the given priority. </p> <p>To use node affinity, use the node type label with the <code>--node-type</code> flag:</p> <pre><code>runai submit job1 ... --node-type \"my-nodes\"\n</code></pre> <p>A researcher may combine the two flags to select both a node pool and a specific set of nodes out of that node pool (e.g. gpu-type=t4 and node-type=infiniband):</p> <pre><code>runai submit job1 ... --node-pool-name \u201cmy pool\u201d --node-type \"my-nodes\"\n</code></pre> <p>Note</p> <p>When submitting a workload, if you choose a node pool label and a node affinity (node type) label which does not intersect, the Run:ai scheduler will not be able to schedule that workload as it represents an empty nodes group.</p> <p>See the runai submit documentation for further information.</p>"},{"location":"admin/researcher-setup/limit-to-node-group/#assigning-node-groups-to-a-project","title":"Assigning Node Groups to a Project","text":"<p>Node Pools are automatically assigned to all Projects and Departments with zero resource allocation as default. Allocating resources to a node pool can be done for each Project and Department. Submitting a workload to a node pool that has zero allocation for a specific project (or department) results in that workload running as an over-quota workload.</p> <p>To assign and configure specific node affinity groups or node pools to a Project see working with Projects.</p> <p>When the command-line interface flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the Project.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/","title":"Researcher Setup Overview","text":"<p>Following is a step-by-step guide for getting a new Researcher up to speed with Run:ai and Kubernetes.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#change-of-paradigms-from-docker-to-kubernetes","title":"Change of Paradigms: from Docker to Kubernetes","text":"<p>As part of Run:ai, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the Researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:ai CLI.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#setup-the-runai-command-line-interface","title":"Setup the Run:ai Command-Line Interface","text":"<p>Run:ai CLI needs to be installed on the Researcher's machine. This document provides step by step instructions.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#provide-the-researcher-with-a-gpu-quota","title":"Provide the Researcher with a GPU Quota","text":"<p>To submit workloads with Run:ai, the Researcher must be provided with a Project that contains a GPU quota. Please see Working with Projects document on how to create Projects and set a quota.</p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#provide-access-to-the-runai-user-interface","title":"Provide access to the Run:ai User Interface","text":"<p>See Setting up users for further information on how to provide access to users.  </p>"},{"location":"admin/researcher-setup/researcher-setup-intro/#schedule-an-onboarding-session","title":"Schedule an Onboarding Session","text":"<p>It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline the Researchers' work as well as save money for the organization. </p>"},{"location":"admin/runai-setup/installation-types/","title":"Installation Types","text":"<p>Run:ai consists of two components:</p> <ul> <li>The Run:ai Cluster. One or more data-science GPU clusters hosted by the customer (on-prem or cloud).</li> <li>The Run:ai Control plane. A single entity that monitors clusters, sets priorities, and business policies. </li> </ul> <p>There are two main installation options:</p> Installation Type Description Classic (SaaS) Run:ai is installed on the customer's data science GPU clusters. The cluster connects to the Run:ai control plane on the cloud (https://.run.ai).  With this installation, the cluster requires an outbound connection to the Run:ai cloud. Self-hosted The Run:ai control plane is also installed in the customer's data center <p>The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns. The self-hosted installation is priced differently. For further information please talk to Run:ai sales. </p>"},{"location":"admin/runai-setup/installation-types/#self-hosted-installation","title":"Self-hosted Installation","text":"<p>Run:ai self-hosting comes with two variants:</p> Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet"},{"location":"admin/runai-setup/installation-types/#self-hosting-with-kubernetes-vs-openshift","title":"Self-hosting with Kubernetes vs OpenShift","text":"<p>Kubernetes has many Certified Kubernetes Providers. Run:ai has been certified with several of them (see the Kubernetes prerequisites section). The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections:</p> <ul> <li>OpenShift-based installation. See Run:ai OpenShift installation.</li> <li>Kubernetes-based installation. See Run:ai Kubernetes installation.</li> </ul>"},{"location":"admin/runai-setup/installation-types/#secure-installation","title":"Secure Installation","text":"<p>In many organizations, Kubernetes is governed by IT compliance rules. In this scenario, there are strict access control rules during the installation and running of workloads:</p> <ul> <li>OpenShift is secured using Security Context Constraints (SCC). The Run:ai installation supports SCC.</li> <li>Kubernetes Pod Security Policy (PSP) has been deprecated by Kubernetes. Support for PSP will be removed on Run:ai versions higher than 2.8.</li> </ul>"},{"location":"admin/runai-setup/try-azure/","title":"Try Run:ai on Azure Cloud","text":"<p>You can try Run:ai by starting a virtual machine on Azure. This option is currently limited to a single GPU node. To install a cluster with multiple nodes or for running a formal pilot with Run:ai, use Cluster Installation.</p>"},{"location":"admin/runai-setup/try-azure/#prerequisites","title":"Prerequisites","text":"<p>You will need:</p> <ul> <li>An account in Azure with a quota for GPUs. Run:ai will work with any modern GPU.</li> <li>Tenant credentials and data, provided by Run:ai customer support. </li> </ul>"},{"location":"admin/runai-setup/try-azure/#create-an-instance-in-azure","title":"Create an instance in Azure","text":"<ul> <li>Go to Run:ai Quickstart in the Azure marketplace.  </li> <li>Press the \"Create\" button. </li> <li>Select a name, subscription, and machine size with GPUs. The machine should have at least 8 CPUs. </li> <li>Under the <code>Advanced</code> tab select <code>Enable user data</code>. Paste the user data provided by Run:ai customer support. It should be in the format: <pre><code>export RUNAI_TENANT=&lt;tenant-name&gt;\nexport RUNAI_CLIENTID=&lt;client-id&gt;\nexport RUNAI_SECRET=&lt;secret&gt;\n</code></pre></li> <li>Create the machine.</li> </ul>"},{"location":"admin/runai-setup/try-azure/#use-runai","title":"Use Run:ai","text":"<p>Go to <code>https://&lt;tenant-name&gt;.run.ai</code>. Use credentials provided by Run:ai support.</p> <p>After ~30 minutes you should have a working Run:ai cluster. You can submit Jobs via the user interface. Command-line is not provided.  </p>"},{"location":"admin/runai-setup/try-azure/#limitations","title":"Limitations","text":"<p>This setup does not support single-sign-on.</p>"},{"location":"admin/runai-setup/authentication/authentication-overview/","title":"Overview","text":""},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-overview","title":"Authentication Overview","text":"<p>To access Run:ai resources, you have to authenticate. The purpose of this document is to explain how authentication works at Run:ai.</p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-endpoints","title":"Authentication Endpoints","text":"<p>Generally speaking, there are two authentication endpoints:</p> <ul> <li>The Run:ai control plane.</li> <li>Run:ai GPU clusters.</li> </ul> <p>Both endpoints are accessible via APIs as well as a user interface. </p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#identity-service","title":"Identity Service","text":"<p>Run:ai includes an internal identity service. The identity service ensures users are who they claim to be and gives them the right kind of access to Run:ai.</p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#users","title":"Users","text":"<p>Out of the box, The Run:ai identity service provides a way to create users and associate them with access roles. </p> <p>It is also possible to configure the Run:ai identity service to connect to a company directory using the SAML protocol. For more information see single sign-on.</p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-method","title":"Authentication Method","text":"<p>Both endpoints described above are protected via time-limited oauth2-like JWT authentication tokens.</p> <p>There are two ways of getting a token:</p> <ul> <li>Using a user/password combination.</li> <li>Using client applications for API access.</li> </ul>"},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-flows","title":"Authentication Flows","text":""},{"location":"admin/runai-setup/authentication/authentication-overview/#runai-control-plane","title":"Run:ai control plane","text":"<p>You can use the Run:ai user interface to provide user/password. These are validated against the identity service. Run:ai will return a token with the right access rights for continued operation. </p> <p>You can also use a client application to get a token and then connect directly to the administration API endpoint. </p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#runai-gpu-clusters","title":"Run:ai GPU Clusters","text":"<p>The Run:ai GPU cluster is a Kubernetes cluster. All communication into Kubernetes flows through the Kubernetes API server.</p> <p>To facilitate authentication via Run:ai the Kubernetes API server must be configured to use the Run:ai identity service to validate authentication tokens. For more information on how to configure the Kubernetes API server see Kubernetes configuration under researcher authentication.</p>"},{"location":"admin/runai-setup/authentication/authentication-overview/#see-also","title":"See also","text":"<ul> <li>To configure authentication for researchers researcher authentication.</li> <li>To configure single sign-on, see single sign-on.</li> </ul>"},{"location":"admin/runai-setup/authentication/researcher-authentication/","title":"Setup Researcher Access Control","text":""},{"location":"admin/runai-setup/authentication/researcher-authentication/#introduction","title":"Introduction","text":"<p>The following instructions explain how to complete the configuration of access control for Researchers. Run:ai access control is at the Project level. When you assign Users to Projects - only these users are allowed to submit Jobs and access Jobs details. </p> <p>This requires several steps:</p> <ul> <li>Assign users to their Projects.</li> <li>(Mandatory) Modify the Kubernetes entry point (called the <code>Kubernetes API server</code>) to validate credentials of incoming requests against the Run:ai Authentication authority.</li> <li>(Command-line Interface usage only) Modify the Kubernetes profile to prompt the Researcher for credentials when running <code>runai login</code> (or <code>oc login</code> for OpenShift). </li> </ul>"},{"location":"admin/runai-setup/authentication/researcher-authentication/#administration-user-interface-setup","title":"Administration User Interface Setup","text":""},{"location":"admin/runai-setup/authentication/researcher-authentication/#enable-researcher-authentication","title":"Enable Researcher Authentication","text":"<ul> <li>Open the Run:ai user interface and navigate to <code>General | Settings</code>. </li> <li>Enable the flag Researcher Authentication (should be enabled by default for new tenants).</li> <li>There are values for <code>Realm</code>, <code>client configuration</code>, and <code>server configuration</code> which appear on the screen. Use them as below. </li> </ul>"},{"location":"admin/runai-setup/authentication/researcher-authentication/#assign-users-to-projects","title":"Assign Users to Projects","text":"<p>Assign Researchers to Projects:</p> <ul> <li>Open the Run:ai user interface and navigate to <code>Users</code>. Add a Researcher and assign it a <code>Researcher</code> role.</li> <li>Navigate to <code>Projects</code>. Edit or create a Project. Use the <code>Access Control</code> tab to assign the Researcher to the Project. </li> <li>If you are using Single Sign-On, you can also assign Groups. For more information see the Single Sign-On documentation.</li> </ul>"},{"location":"admin/runai-setup/authentication/researcher-authentication/#mandatory-kubernetes-configuration","title":"(Mandatory) Kubernetes Configuration","text":"<p>As described in authentication overview, you must direct the Kubernetes API server to authenticate via Run:ai. This requires adding flags to the Kubernetes API Server. Modifying the API Server configuration differs between Kubernetes distributions:</p> Native KubernetesOpenShiftRKERKE2GKEEKSBrightAKSOther <ul> <li>Locate the Kubernetes API Server configuration file. The file's location may defer between different Kubernetes distributions. The location for vanilla Kubernetes is <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code></li> <li>Edit the document, under the <code>command</code> tag, add the server configuration text from <code>General | Settings | Researcher Authentication</code> described above.   </li> <li>Verify that the <code>kube-apiserver-&lt;master-node-name&gt;</code> pod in the <code>kube-system</code> namespace has been restarted and that changes have been incorporated. Run the below and verify that the oidc flags you have added:</li> </ul> <pre><code>kubectl get pods -n kube-system kube-apiserver-&lt;master-node-name&gt; -o yaml\n</code></pre> <p>No configuration is needed. Instead, Run:ai assumes that an Identity Provider has been defined at the OpenShift level and that the Run:ai Cluster installation has set the <code>OpenshiftIdp</code> flag to true. For more information see the Run:ai OpenShift control-plane setup.</p> <p>Edit Rancher <code>cluster.yml</code> (with Rancher UI, follow this). Add the following:</p> cluster.yml<pre><code>kube-api:\nalways_pull_images: false\nextra_args:\noidc-client-id: runai  # (1)\noidc-issuer-url: https://example.com/auth\noidc-username-prefix: \"-\"\n</code></pre> <ol> <li>These are example parameters. Copy the actual parameters from from <code>General | Settings | Researcher Authentication</code> as described above.</li> </ol> <p>You can verify that the flags have been incorporated into the RKE cluster by following the instructions here and running <code>docker inspect &lt;kube-api-server-container-id&gt;</code>, where <code>&lt;kube-api-server-container-id&gt;</code> is the container ID of api-server via obtained in the Rancher document. </p> <p>If working via the RKE2 Quickstart, edit <code>/etc/rancher/rke2/config.yaml</code>. Add the parameters provided in the server configuration section as described above in the following fashion:</p> /etc/rancher/rke2/config.yaml<pre><code>kube-apiserver-arg:\n- \"oidc-client-id=&lt;CLIENT-ID&gt;\"\n- \"oidc-issuer-url=&lt;URL&gt;\"\n- \"oidc-username-prefix=-\"\n</code></pre> <p>If working via Rancher UI, need to add the flag as part of the cluster provisioning. At the time of writing, the flags cannot be changed after the cluster has been provisioned due to a Rancher bug. Under <code>Cluster Management | Create</code>, turn on RKE2 and select a platform. Under <code>Cluster Configuration | Advanced | Additional API Server Args</code>. Add the Run:ai flags as <code>&lt;key&gt;=&lt;value&gt;</code> (e.g. <code>oidc-username-prefix=-</code>).</p> <p>Install Anthos identity service by running:</p> <pre><code>gcloud container clusters update &lt;gke-cluster-name&gt; \\\n    --enable-identity-service --project=&lt;gcp-project-name&gt; --zone=&lt;gcp-zone-name&gt;\n</code></pre> <p>Install the yq utility and run:</p> <pre><code>kubectl get clientconfig default -n kube-public -o yaml &gt; login-config.yaml\nyq -i e \".spec +={\\\"authentication\\\":[{\\\"name\\\":\\\"oidc\\\",\\\"oidc\\\":{\\\"clientID\\\":\\\"$OIDC_CLIENT_ID\\\",\\\"issuerURI\\\":\\\"$OIDC_ISSUER_URL\\\",\\\"kubectlRedirectURI\\\":\\\"http://localhost:8000/callback\\\",\\\"userClaim\\\":\\\"sub\\\",\\\"userPrefix\\\":\\\"$OIDC_USERNAME_PREFIX\\\"}}]}\" login-config.yaml\nkubectl apply -f login-config.yaml\n</code></pre> <p>Where the <code>OIDC</code> flags are provided in the Run:ai server configuration section as described above. </p> <p>To create a kubeconfig profile for Researchers run:</p> <pre><code>kubectl oidc login --cluster=CLUSTER_NAME --login-config=login-config.yaml \\\n    --kubeconfig=developer-kubeconfig\n</code></pre> <p>Then modify the <code>developer-kubeconfig</code> file as described in the Command-line Inteface Access section below.</p> <ul> <li>In the AWS Console, under EKS, find your cluster.</li> <li>Go to <code>Configuration</code> and then to <code>Authentication</code>.</li> <li>Associate a new <code>identity provider</code>. Use the parameters provided in the server configuration section as described above. The process can take up to 30 minutes. </li> </ul> <p>Run the following. Replace <code>&lt;TENANT-NAME&gt;</code> and <code>&lt;REALM-NAME&gt;</code> with the appropriate values:</p> <pre><code># start cmsh\n[root@headnode ~]# cmsh\n# go to the configurationoverlay submode\n[headnode]% configurationoverlay\n\n[headnode-&gt;configurationoverlay]% list  # use list here to list overlays\n...\n\n# go to the overlay for kube master nodes\n[headnode-&gt;configurationoverlay]% use kube-default-master\n\n[headnode-&gt;configurationoverlay[kube-default-master]]% show  # use show here to show the selected overlay\n...\n\n# go to the kube apiserver role\n[headnode-&gt;configurationoverlay[kube-default-master]]% roles\n[headnode-&gt;configurationoverlay[kube-default-master]-&gt;roles]% list   # ... \n[headnode-&gt;configurationoverlay[kube-default-master]-&gt;roles]% use kubernetes::apiserver\n\n# we can check the current value of \"options\"\n[headnode-&gt;configurationoverlay[kube-default-master]-&gt;roles[Kubernetes::ApiServer]]% show  # ...\n[headnode-&gt;configurationoverlay[kube-default-master]-&gt;roles[Kubernetes::ApiServer]]% get options\n--anonymous-auth=false\n--service-account-issuer=https://kubernetes.default.svc.cluster.local\n--service-account-signing-key-file=/cm/local/apps/kubernetes/var/etc/sa-default.key\n--feature-gates=LegacyServiceAccountTokenNoAutoGeneration=false\n# we can append our flags like this\n[headnode-&gt;configurationoverlay[kube-default-master]-&gt;roles[Kubernetes::ApiServer]]% append options \"--oidc-client-id=runai\"\n[headnode-&gt;configurationoverlay*[kube-default-master*]-&gt;roles*[Kubernetes::ApiServer*]]% append options \"--oidc-issuer-url=https://app.run.ai/auth/realms/&lt;REALM-NAME&gt;\"\n[headnode-&gt;configurationoverlay*[kube-default-master*]-&gt;roles*[Kubernetes::ApiServer*]]% append options \"--oidc-username-prefix=-\"\n# commit the changes\n[headnode-&gt;configurationoverlay[kube-default-master]-&gt;roles[Kubernetes::ApiServer]]% ]]% commit\n\n# view updated list of options\n[headnode-&gt;configurationoverlay[kube-default-master]-&gt;roles[Kubernetes::ApiServer]]% get options\n--anonymous-auth=false\n--service-account-issuer=https://kubernetes.default.svc.cluster.local\n--service-account-signing-key-file=/cm/local/apps/kubernetes/var/etc/sa-default.key\n--feature-gates=LegacyServiceAccountTokenNoAutoGeneration=false\n--cors-allowed-origins=[\\\"https://&lt;TENANT-NAME&gt;.run.ai\\\"]\n--oidc-client-id=runai\n--oidc-issuer-url=https://app.run.ai/auth/realms/&lt;REALM-NAME&gt;\n--oidc-username-prefix=-\n</code></pre> <p>All nodes with the <code>kube api server</code> role will automatically restart with the new flag.</p> <p>Please contact Run:ai customer support.</p> <p>See specific instructions in the documentation of the Kubernetes distribution.  </p>"},{"location":"admin/runai-setup/authentication/researcher-authentication/#command-line-interface-access","title":"Command-line Interface Access","text":"<p>To control access to Run:ai (and Kubernetes) resources, you must modify the Kubernetes configuration file. The file is distributed to users as part of the Command-line interface installation. </p> <p>When making changes to the file, keep a copy of the original file to be used for cluster administration. After making the modifications, distribute the modified file to Researchers. </p> <ul> <li>Under the <code>~/.kube</code> directory edit the <code>config</code> file, remove the administrative user, and replace it with the client configuration text from <code>General | Settings | Researcher Authentication</code> described above. </li> <li>Under <code>contexts | context | user</code> change the user to <code>runai-authenticated-user</code>.</li> </ul>"},{"location":"admin/runai-setup/authentication/researcher-authentication/#test-via-command-line-interface","title":"Test via Command-line interface","text":"<ul> <li>Run: <code>runai login</code> (in OpenShift environments use <code>oc login</code> rather than <code>runai login</code>).</li> <li>You will be prompted for a username and password. In a single sign-on flow, you will be asked to copy a link to a browser, log in and return a code. </li> <li>Once login is successful, submit a Job.</li> <li>If the Job was submitted with a Project to which you have no access, your access will be denied. </li> <li>If the Job was submitted with a Project to which you have access, your access will be granted.</li> </ul> <p>You can also submit a Job from the Run:ai User interface and verify that the new job shows on the job list with your user name. </p>"},{"location":"admin/runai-setup/authentication/researcher-authentication/#test-via-user-interface","title":"Test via User Interface","text":"<ul> <li>Open the Run:ai user interface, go to <code>Jobs</code>.</li> <li>On the top-right, select <code>Submit Job</code>. </li> </ul> <p>Tip</p> <p>If you do not see the button or it is disabled, then you either do not have <code>Researcher</code> access or the cluster has not been set up correctly. For more information, refer to user interface overview.</p>"},{"location":"admin/runai-setup/authentication/sso/","title":"Single Sign-On","text":"<p>Single Sign-On (SSO) is an authentication scheme that allows a user to log in with a single ID to other, independent, software systems. SSO solves security issues involving multiple user/password data entries, multiple compliance schemes, etc. </p> <p>Run:ai supports SSO using the SAML 2.0 protocol. When SSO is configured, the system is accessible via single-sign-on only.</p> <p>Caution</p> <p>Single sign-on is only available with SaaS installations where the tenant has been created post-January 2022 or any Self-hosted installation of release 2.0.58 or upwards. If you are using single sign-on with older versions of Run:ai, please contact Run:ai customer support</p>"},{"location":"admin/runai-setup/authentication/sso/#terminology","title":"Terminology","text":"<p>The term Identity Provider (or IdP) below relates to the system which creates, maintains, and manages identity information. Example IdPs: Google, Keycloak, Salesforce, Auth0. </p>"},{"location":"admin/runai-setup/authentication/sso/#prerequisites","title":"Prerequisites","text":"<ul> <li>XML Metadata: You must have an XML Metadata file retrieved from your IdP. Upload the file to a web server such that you will have a URL to the file. The URL must have the XML file extension. For example, to connect using Google, you must create a custom SAML App here, download the Metadata file, and upload it to a web server.</li> <li>Organization Name: You must have a Run:ai Organization Name. This is the name that appears on the top right of the Run:ai user interface.</li> <li>Additional attribute mapping: Configure your IdP to map several IdP attributes: </li> </ul> IdP attribute Run:ai required name Description User email email (Mandatory) <code>e-mail</code> is the user identifier with Run:ai. User role groups GROUPS (Optional) If exists, allows assigning Run:ai role groups via the IdP. The IdP attribute must be of a type of list of strings. See more below Linux User ID UID (Optional) If exists in IdP, allows Researcher containers to start with the Linux User <code>UID</code>. Used to map access to network resources such as file systems to users. The IdP attribute must be of integer type. Linux Group ID GID (Optional) If exists in IdP, allows Researcher containers to start with the Linux Group <code>GID</code>. The IdP attribute must be of integer type. Linux Supplementary Groups SUPPLEMENTARYGROUPS (Optional) If exists in IdP, allows Researcher containers to start with the relevant Linux supplementary groups. The IdP attribute must be of a type of list of integers. User first name firstName (Optional) Used as the first name showing in the Run:ai user interface. User last name lastName (Optional) Used as the last name showing in the Run:ai user interface"},{"location":"admin/runai-setup/authentication/sso/#example-attribute-mapping-for-google-suite","title":"Example attribute mapping for Google Suite","text":"<p>If you are using Google Suite as your Identity provider, to map custom attributes follow the Google support article. Use the Whole Number attribute type. For Supplementary Groups use the Multi-value designation. </p>"},{"location":"admin/runai-setup/authentication/sso/#step-1-ui-configuration","title":"Step 1: UI Configuration","text":"<ul> <li>Open the Administration User interface.</li> <li>Go to <code>Settings | General</code>.</li> <li>Turn on <code>Login with SSO</code>. </li> <li>Under <code>Metadata XML Url</code> enter the URL to the XML Metadata file obtained above.</li> <li>Under Administrator email, enter the first administrator user.</li> <li>Press <code>Save</code>.</li> </ul> <p>Once you press <code>Save</code> you will receive a <code>Redirect URI</code> and an <code>Entity ID</code>. Both values must be set on the IdP side.</p> <p>Important</p> <p>Upon pressing <code>Save</code>, all existing users will be rendered non-functional, and the only valid user will be the Administrator email entered above. You can always revert by disabling Login via SSO. </p>"},{"location":"admin/runai-setup/authentication/sso/#test","title":"Test","text":"<p>Test Connectivity to Administration User Interface:</p> <ul> <li>Using an incognito browser tab and open the Run:ai user interface.</li> <li>Select the <code>Login with SSO</code> button. </li> <li>Provide the <code>Organization name</code> obtained above. </li> <li>You will be redirected to the IdP login page. Use the previously entered Administrator email to log in. </li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#troubleshooting","title":"Troubleshooting","text":"<p>Single sign-on log in can be separated into two parts:</p> <ol> <li>Run:ai redirects to the IdP (e.g. Google) for login using a SAML Request.</li> <li>Upon successful login, IdP redirects back to Run:ai with a SAML Response.</li> </ol> <p>You can follow that by following the URL changes from app.run.ai to the IdP provider (e.g. accounts.google.com) and back to app.run.ai:</p> <ul> <li>If there is an issue on the IdP site (e.g. <code>app_is_not_configred</code> error in Google), the problem is likely to be in the SAML Request.</li> <li>If the user is redirected back to Run:ai and something goes wrong, the problem is most likely in the SAML Response.</li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#troubleshooting-saml-request","title":"Troubleshooting SAML Request","text":"<ul> <li>When logging in, have the Chrome network inspector open (Open by <code>Right-Click | Inspect</code> on the page, then open the network tab).</li> <li>After the IdP login screen shows, search in the network tab for an HTTP request showing the SAML Request. Depending on the IdP this would be a request to the IdP domain name. E.g. accounts.google.com/idp?1234.</li> <li>When found, go to the \"Payload\" tab and copy the value of the SAML Request. </li> <li>Paste the value into a SAML decoder. A typical response should look like this:</li> </ul> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;samlp:AuthnRequest xmlns:samlp=\"urn:oasis:names:tc:SAML:2.0:protocol\" xmlns=\"urn:oasis:names:tc:SAML:2.0:assertion\" xmlns:saml=\"urn:oasis:names:tc:SAML:2.0:assertion\" AssertionConsumerServiceURL=\"https://.../auth/realms/runai/broker/saml/endpoint\" Destination=\"https://accounts.google.com/o/saml2/idp?idpid=....\" ForceAuthn=\"false\" ID=\"ID_66da617d-b862-4cca-9ei5-b727a920f3cb\" IssueInstant=\"2022-01-12T12:54:22.907Z\" ProtocolBinding=\"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\" Version=\"2.0\"&gt;\n&lt;saml:Issuer&gt;runai-jtqee5v8ob&lt;/saml:Issuer&gt;\n&lt;samlp:NameIDPolicy AllowCreate=\"true\" Format=\"urn:oasis:names:tc:SAML:2.0:nameid-format:persistent\"/&gt;\n&lt;/samlp:AuthnRequest&gt;\n</code></pre> <p>Check in the above that:</p> <ul> <li>The content of the <code>&lt;saml:Issuer&gt;</code> tag is the same as <code>Entity ID</code> defined above.</li> <li><code>AssertionConsumerServiceURL</code> is the same as the <code>Redirect URI</code>. </li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#troubleshooting-saml-response","title":"Troubleshooting SAML Response","text":"<ul> <li>When logging in, have the Chrome network inspector open (Open by <code>Right-Click | Inspect</code> on the page, then open the network tab).</li> <li>Search for \"endpoint\". </li> <li>When found, go to the \"Payload\" tab and copy the value of the SAML Response.</li> <li>Paste the value into a SAML decoder. A typical response should look like this:</li> </ul> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\n&lt;saml2p:Response\nxmlns:saml2p=\"urn:oasis:names:tc:SAML:2.0:protocol\" Destination=\"https://.../auth/realms/runai/broker/saml/endpoint\" ID=\"_2d085ed4f45a7ab221a49e6c02e30cac\" InResponseTo=\"ID_295f2723-79f5-4410-99b2-5f4acb2d4f8e\" IssueInstant=\"2022-01-12T12:06:31.175Z\" Version=\"2.0\"&gt;\n&lt;saml2:Issuer\nxmlns:saml2=\"urn:oasis:names:tc:SAML:2.0:assertion\"&gt;https://accounts.google.com/o/saml2?idpid=....\n    &lt;/saml2:Issuer&gt;\n&lt;saml2p:Status&gt;\n&lt;saml2p:StatusCode Value=\"urn:oasis:names:tc:SAML:2.0:status:Success\"/&gt;\n&lt;/saml2p:Status&gt;\n&lt;saml2:Assertion\nxmlns:saml2=\"urn:oasis:names:tc:SAML:2.0:assertion\" ID=\"_befe8441fa06594b365c516558dc5636\" IssueInstant=\"2022-01-12T12:06:31.175Z\" Version=\"2.0\"&gt;\n&lt;saml2:Issuer&gt;https://accounts.google.com/o/saml2?idpid=...&lt;/saml2:Issuer&gt;\n&lt;ds:Signature\nxmlns:ds=\"http://www.w3.org/2000/09/xmldsig#\"&gt;\n&lt;ds:SignedInfo&gt;\n&lt;ds:CanonicalizationMethod Algorithm=\"http://www.w3.org/2001/10/xml-exc-c14n#\"/&gt;\n&lt;ds:SignatureMethod Algorithm=\"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\"/&gt;\n&lt;ds:Reference URI=\"#_befe8441fa06594b365c516558dc5636\"&gt;\n&lt;ds:Transforms&gt;\n&lt;ds:Transform Algorithm=\"http://www.w3.org/2000/09/xmldsig#enveloped-signature\"/&gt;\n&lt;ds:Transform Algorithm=\"http://www.w3.org/2001/10/xml-exc-c14n#\"/&gt;\n&lt;/ds:Transforms&gt;\n&lt;ds:DigestMethod Algorithm=\"http://www.w3.org/2001/04/xmlenc#sha256\"/&gt;\n&lt;ds:DigestValue&gt;QxNCjtz9Gomv2qaz8Rb4X8cQJOSGkK+87CrHDkBPidM=&lt;/ds:DigestValue&gt;\n&lt;/ds:Reference&gt;\n&lt;/ds:SignedInfo&gt;\n&lt;ds:SignatureValue&gt;...&lt;/ds:SignatureValue&gt;\n&lt;ds:KeyInfo&gt;\n&lt;ds:X509Data&gt;\n&lt;ds:X509SubjectName&gt;ST=California,C=US,OU=Google For Work,CN=Google,L=Mountain View,O=Google Inc.&lt;/ds:X509SubjectName&gt;\n&lt;ds:X509Certificate&gt;...&lt;/ds:X509Certificate&gt;\n&lt;/ds:X509Data&gt;\n&lt;/ds:KeyInfo&gt;\n&lt;/ds:Signature&gt;\n&lt;saml2:Subject&gt;\n&lt;saml2:NameID Format=\"urn:oasis:names:tc:SAML:2.0:nameid-format:persistent\"&gt;john@example.com&lt;/saml2:NameID&gt;\n&lt;saml2:SubjectConfirmation Method=\"urn:oasis:names:tc:SAML:2.0:cm:bearer\"&gt;\n&lt;saml2:SubjectConfirmationData InResponseTo=\"ID_295f2723-79f5-4410-99b2-5f4acb2d4f8e\" NotOnOrAfter=\"2022-01-12T12:11:31.175Z\" Recipient=\"https://.../auth/realms/runai/broker/saml/endpoint\"/&gt;\n&lt;/saml2:SubjectConfirmation&gt;\n&lt;/saml2:Subject&gt;\n&lt;saml2:Conditions NotBefore=\"2022-01-12T12:01:31.175Z\" NotOnOrAfter=\"2022-01-12T12:11:31.175Z\"&gt;\n&lt;saml2:AudienceRestriction&gt;\n&lt;saml2:Audience&gt;runai-jtqee5v8ob&lt;/saml2:Audience&gt;\n&lt;/saml2:AudienceRestriction&gt;\n&lt;/saml2:Conditions&gt;\n&lt;saml2:AttributeStatement&gt;\n&lt;saml2:Attribute Name=\"email\"&gt;\n&lt;saml2:AttributeValue\nxmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;john@example.com\n                &lt;/saml2:AttributeValue&gt;\n&lt;/saml2:Attribute&gt;\n&lt;saml2:Attribute Name=\"GID\"&gt;\n&lt;saml2:AttributeValue\nxmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;8765\n                &lt;/saml2:AttributeValue&gt;\n&lt;/saml2:Attribute&gt;\n&lt;saml2:Attribute Name=\"SUPPLEMENTARYGROUPS\"&gt;\n&lt;saml2:AttributeValue\nxmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;200\n                &lt;/saml2:AttributeValue&gt;\n&lt;saml2:AttributeValue\nxmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;300\n                &lt;/saml2:AttributeValue&gt;\n&lt;saml2:AttributeValue\nxmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;400\n                &lt;/saml2:AttributeValue&gt;\n&lt;saml2:AttributeValue\nxmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;100\n                &lt;/saml2:AttributeValue&gt;\n&lt;/saml2:Attribute&gt;\n&lt;saml2:Attribute Name=\"UID\"&gt;\n&lt;saml2:AttributeValue\nxmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"xs:anyType\"&gt;4321\n                &lt;/saml2:AttributeValue&gt;\n&lt;/saml2:Attribute&gt;\n&lt;/saml2:AttributeStatement&gt;\n&lt;saml2:AuthnStatement AuthnInstant=\"2022-01-12T12:06:30.000Z\" SessionIndex=\"_befe8441fa06594b365c516558dc5636\"&gt;\n&lt;saml2:AuthnContext&gt;\n&lt;saml2:AuthnContextClassRef&gt;urn:oasis:names:tc:SAML:2.0:ac:classes:unspecified&lt;/saml2:AuthnContextClassRef&gt;\n&lt;/saml2:AuthnContext&gt;\n&lt;/saml2:AuthnStatement&gt;\n&lt;/saml2:Assertion&gt;\n&lt;/saml2p:Response&gt;      </code></pre> <p>Check in the above that:</p> <ul> <li>The content of the <code>&lt;saml2:Audience&gt;</code> tag is the same as <code>Entity ID</code> defined above.</li> <li>The <code>Destination</code> at the top is the same as the <code>Redirect URI</code>.</li> <li>The user email under the <code>&lt;saml2:Subject&gt;</code> tag is the same as the logged-in user. </li> <li>Make sure that under the <code>&lt;saml2:AttributeStatement&gt;</code> tag, there is an Attribute named <code>email</code> (lowercase). This attribute is mandatory. </li> <li>If other, optional attributes (such as UID, GID) are mapped, make sure they exist under <code>&lt;saml2:AttributeStatement&gt;</code> along with their respective values.</li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#step-2-cluster-authentication","title":"Step 2: Cluster Authentication","text":"<p>Researchers should be authenticated when accessing the Run:ai GPU Cluster. To perform that, the Kubernetes cluster and the user's Kubernetes profile must be aware of the IdP. Follow the instructions here. If you have followed these instructions in the past, you must do so again and replace the client-side and server-side configuration values with the new values as provided by on <code>General | Settings | Researcher Authentication</code>.</p>"},{"location":"admin/runai-setup/authentication/sso/#test_1","title":"Test","text":"<p>Test connectivity to Run:ai command-line interface:</p> <ul> <li>In the command-line, run <code>runai login</code>.</li> <li>You will receive a link that you must copy and open in your browser. Post login you will receive a verification code which you must paste into the shell window.</li> <li>Verify successful login.</li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#step-3-uidgid-mapping","title":"Step 3: UID/GID Mapping","text":"<p>Configure the IdP to add UID, GID, and Supplementary groups in the IdP.</p>"},{"location":"admin/runai-setup/authentication/sso/#test_2","title":"Test","text":"<p>Test the mapping of UID/GID to within the container:</p> <p>Submit a job with the flag <code>--run-as-user</code>, for example:</p> <p><pre><code>runai submit -i ubuntu --interactive --run-as-user --attach -- bash\n</code></pre> When a shell opens inside the container, run <code>id</code> and verify that UID, GID, and the supplementary groups are the same as in the user's profile in the organization's directory.</p>"},{"location":"admin/runai-setup/authentication/sso/#step-4-adding-users","title":"Step 4: Adding Users","text":"<p>You can add additional users, by either:</p> <ol> <li>Manually adding roles for each user.</li> <li>Mapping roles to IdP groups. </li> </ol> <p>The latter option is easier to maintain. </p>"},{"location":"admin/runai-setup/authentication/sso/#adding-roles-for-a-user","title":"Adding Roles for a User","text":"<ul> <li>Go to <code>Settings | Users</code>.</li> <li>Select the <code>Users</code> button at the top. </li> <li>Map users as explained here.</li> </ul>"},{"location":"admin/runai-setup/authentication/sso/#mapping-role-groups","title":"Mapping Role Groups","text":"<ul> <li>Go to <code>Settings | Users</code>.</li> <li>Select the <code>Groups</code> button. </li> <li>Assuming you have mapped the IdP <code>Groups</code> attribute as described in the prerequisites section above, add a name of a group that has been created in the directory and create an equivalent Run:ai Group. </li> <li>If the role group contains the <code>Researcher</code> role, you can assign this group to a Run:ai Project. All members of the group will have access to the cluster. </li> </ul> <p>Note</p> <p>This feature also works in OpenShift. If you create a group in Run:ai with the same name as an OpenShift Group, the associated permissions will be applied to all users in the group.</p>"},{"location":"admin/runai-setup/authentication/sso/#logout-url","title":"Logout URL","text":"<p>It is possible to configure the redirect URL when the session ends. To perform this configuration please contact Run:ai customer support. </p>"},{"location":"admin/runai-setup/authentication/sso/#implementation-notes","title":"Implementation Notes","text":"<p>Run:ai SSO does not support single logout. As such, logging out from Run:ai will not log you out from other systems.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-delete/","title":"Deleting a Cluster Installation","text":"<p>To delete a Run:ai Cluster installation while retaining existing running jobs, run the following commands:</p> Version 2.9 or laterVersion 2.8Version 2.7 or earlier <pre><code>helm delete runai-cluster -n runai\n</code></pre> <pre><code>kubectl delete RunaiConfig runai -n runai\nhelm delete runai-cluster -n runai\n</code></pre> <pre><code>kubectl patch RunaiConfig runai -n runai -p '{\"metadata\":{\"finalizers\":[]}}' --type=\"merge\"\nkubectl delete RunaiConfig runai -n runai\nhelm delete runai-cluster runai -n runai\n</code></pre> <p>The commands will not delete existing Jobs submitted by users. </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/","title":"Cluster Install","text":"<p>Below are instructions on how to install a Run:ai cluster. Before installing, please review the installation prerequisites here: Run:ai GPU Cluster Prerequisites. </p> <p>Important</p> <ul> <li>We strongly recommend running the Run:ai pre-install script to verify that all prerequisites are met. </li> <li>Starting version 2.9 you must pre-install  NGINX ingress controller</li> <li>Starting version 2.9 you must pre-install the Prometheus stack.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#install-runai","title":"Install Run:ai","text":"<p>Log in to Run:ai user interface at <code>&lt;company-name&gt;.run.ai</code>. Use credentials provided by Run:ai Customer Support:</p> <ul> <li>If no clusters are currently configured, you will see a Cluster installation wizard</li> <li>If a cluster has already been configured, use the menu on the top left and select \"Clusters\". On the top right, click \"Add New Cluster\". </li> </ul> <p>Using the Wizard:</p> <ol> <li>Choose a target Kubernetes platform (see table above)</li> <li>(SaaS and remote self-hosted cluster only) Provide a domain name for your cluster as described here.</li> <li>(SaaS and remote self-hosted cluster only) Install a trusted certificate to the domain within Kubernetes. </li> <li>Download a Helm values YAML file <code>runai-&lt;cluster-name&gt;.yaml</code></li> <li>(Optional) customize the values file. See Customize Cluster Installation</li> <li>Install Helm</li> <li>For RKE and EKS, perform the steps here</li> <li>Run the <code>helm</code> commands as provided in the wizard. </li> </ol> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-cluster</code>.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#verify-your-installation","title":"Verify your Installation","text":"<ul> <li>Go to <code>&lt;company-name&gt;.run.ai/dashboards/now</code>.</li> <li>Verify that the number of GPUs on the top right reflects your GPU resources on your cluster and the list of machines with GPU resources appears on the bottom line.</li> </ul> <p> Version 2.9 and up </p> <p>Run: <code>kubectl get cm runai-public -n runai -o jsonpath='{.data}' | yq -P</code></p> <p>(assumes the yq is instaled)</p> <p>Example output:</p> <pre><code>cluster-version: 2.9.0\nrunai-public: version: 2.9.0\nrunaiConfigStatus: # (1)\nconditions:\n- type: DependenciesFulfilled\nstatus: \"True\"\nreason: dependencies_fulfilled\nmessage: Dependencies are fulfilled\n- type: Deployed\nstatus: \"True\"\nreason: deployed\nmessage: Resources Deployed\n- type: Available\nstatus: \"True\"\nreason: available\nmessage: System Available\n- type: Reconciled\nstatus: \"True\"\nreason: reconciled\nmessage: Reconciliation completed successfully\noptional:  # (2)\nknative:    # (3)  \ncomponents:\nhpa:\navailable: true\nknative:\navailable: true\nkourier:\navailable: true\nmpi:        # (4) \navailable: true\n</code></pre> <ol> <li>Verifies that all mandatory dependencies are met: NVIDIA GPU Operator, Prometheus and NGINX controller. </li> <li>Checks whether optional product dependencies have been met.</li> <li>See Inference prerequisites.</li> <li>See distributed training prerequisites.</li> </ol> <p>For a more extensive verification of cluster health, see Determining the health of a cluster.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#researcher-authentication","title":"Researcher Authentication","text":"<p>You must now set up Researcher Access Control. </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#optional-set-node-roles","title":"(Optional) Set Node Roles","text":"<p>When installing a production cluster you may want to:</p> <ul> <li>Set one or more Run:ai system nodes. These are nodes dedicated to Run:ai software. </li> <li>Machine learning frequently requires jobs that require CPU but not GPU. You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. </li> <li>Limit Run:ai to specific nodes in the cluster. </li> </ul> <p>To perform these tasks. See Set Node Roles.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#next-steps","title":"Next Steps","text":"<ul> <li>Set up Run:ai Users Working with Users.</li> <li>Set up Projects for Researchers Working with Projects.</li> <li>Set up Researchers to work with the Run:ai Command-line interface (CLI). See  Installing the Run:ai Command-line Interface on how to install the CLI for users.</li> <li>Review advanced setup and maintenance scenarios.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/","title":"Prerequisites","text":"<p>Below are the prerequisites of a cluster installed with Run:ai. </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#prerequisites-in-a-nutshell","title":"Prerequisites in a Nutshell","text":"<p>The following is a checklist of the Run:ai prerequisites:</p> Prerequisite Details Kubernetes Verify certified vendor and correct version. NVIDIA GPU Operator Different Kubernetes flavors have slightly different setup instructions.   Verify correct version. Ingress Controller Install and configure NGINX (some Kubernetes flavors have NGINX pre-installed). Version 2.7 or earlier of Run:ai already installs NGINX as part of the Run:ai cluster installation. Prometheus Install Prometheus. Version 2.8 or earlier of Run:ai already installs Prometheus as part of the Run:ai cluster installation. Trusted domain name You must provide a trusted domain name (Version 2.7: a cluster IP). Accessible only inside the organization Cert manager For RKE and EKS, you must install a certificate manager  and configure Run:ai to use it (Optional) Distributed Training Install Kubeflow MPI if required. (Optional) Inference Some third party software needs to be installed to use the Run:ai inference module. <p>There are also specific hardware, operating system and network access requirements. A pre-install script is available to test if the prerequisites are met before installation. </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#software-requirements","title":"Software Requirements","text":""},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#operating-system","title":"Operating System","text":"<p>Run:ai will work on any Linux operating system that is supported by both Kubernetes and NVIDIA. Having said that, Run:ai performs its internal tests on Ubuntu 20.04 (and CoreOS for OpenShift). The exception is GKE (Google Kubernetes Engine) where Run:ai has only been certified on an Ubuntu image.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#kubernetes","title":"Kubernetes","text":"<p>Run:ai requires Kubernetes. The latest Run:ai version supports Kubernetes versions 1.21 through 1.26 and OpenShift 4.8 to 4.11. For an up-to-date end-of-life statement of Kubernetes see Kubernetes Release History.</p> <p>Run:ai does not support Pod Security Admission. </p> <p>Run:ai has been tested with the following Kubernetes distributions: </p> Target Platform Description Installation Notes Vanilla Kubernetes Using no specific distribution but rather k8s native installation OCP OpenShift Container Platform The Run:ai operator is certified for OpenShift by Red Hat. EKS Amazon Elastic Kubernetes Service AKS Azure Kubernetes Services GKE Google Kubernetes Engine RKE Rancher Kubernetes Engine When installing Run:ai, select On Premise. RKE2 has a defect which requires a specific installation flow. Please contact Run:ai customer support for additional details. Bright NVIDIA Bright Cluster Manager In addition, NVIDIA DGX comes bundled with Run:ai Ezmeral HPE Ezmeral Container Platform See Run:ai at Ezmeral marketplace Tanzu VMWare Kubernetes Tanzu supports containerd rather than docker. See the NVIDIA prerequisites below as well as cluster customization for changes required for containerd <p>Run:ai provides instructions for a simple (non-production-ready) Kubernetes Installation.</p> <p>Notes</p> <ul> <li>Kubernetes recommends the usage of the <code>systemd</code> as the container runtime cgroup driver. Kubernetes 1.22 and above defaults to <code>systemd</code>. </li> <li>Run:ai 2.8 or earlier Supports Kubernetes Pod Security Policy if used. Pod Security Policy is deprecated and will be removed from Kubernetes 1.25. As such, Run:ai has removed support for PSP in Run:ai 2.9</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#nvidia","title":"NVIDIA","text":"<p>Run:ai requires NVIDIA GPU Operator version 1.9 or 22.9. The interim versions (1.10 and 1.11) have a documented issue as per the note below. </p> <p>Important</p> <p>NVIDIA GPU Operator has a bug that affects metrics and scheduling. The bug affects NVIDIA GPU Operator versions 1.10 and 1.11 but does not exist in 1.9 and is resolved in 22.9.0. For more details see NVIDIA bug report. </p> On PremEKSGKERKE <p>Follow the Getting Started guide to install the NVIDIA GPU Operator.</p> <ul> <li>Do not install the NVIDIA device plug-in  (as we want the NVIDIA GPU Operator to install it instead). When using the eksctl tool to create an AWS EKS cluster, use the flag <code>--install-nvidia-plugin=false</code> to disable this install.</li> <li>Follow the Getting Started guide to install the NVIDIA GPU Operator. For GPU nodes, EKS uses an AMI which already contains the NVIDIA drivers. As such, you must use the GPU Operator flags: <code>--set driver.enabled=false</code>. </li> </ul> <p>Create the <code>gpu-operator</code> namespace by running</p> <pre><code>kubectl create ns gpu-operator\n</code></pre> <p>Before installing the GPU Operator you must create the following file:</p> resourcequota.yaml<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\nname: gcp-critical-pods\nnamespace: gpu-operator\nspec:\nscopeSelector:\nmatchExpressions:\n- operator: In\nscopeName: PriorityClass\nvalues:\n- system-node-critical\n- system-cluster-critical\n</code></pre> <p>Then run: <code>kubectl apply -f resourcequota.yaml</code></p> <p>Important</p> <ul> <li>Run:ai on GKE has only been tested with GPU Operator version 1.11.1 and up.</li> <li>The above only works for Run:ai 2.7.16 and above. </li> </ul> <p>Install the NVIDIA GPU Operator as discussed here.</p> <p>Notes</p> <ul> <li>Use the default namespace <code>gpu-operator</code>. Otherwise, you must specify the target namespace using the flag <code>runai-operator.config.nvidiaDcgmExporter.namespace</code> as described in customized cluster installation.</li> <li>NVIDIA drivers may already be installed on the nodes. In such cases, use the NVIDIA GPU Operator flags <code>--set driver.enabled=false</code>. DGX OS is one such example as it comes bundled with NVIDIA Drivers. </li> <li>To work with containerd (e.g. for Tanzu), use the defaultRuntime flag accordingly.</li> <li>To use Dynamic MIG, the GPU Operator must be installed with the flag <code>mig.strategy=mixed</code>. If the GPU Operator is already installed, edit the clusterPolicy by running <code>kubectl patch clusterPolicy cluster-policy -n gpu-operator --type=merge -p '{\"spec\":{\"mig\":{\"strategy\": \"mixed\"}}}</code></li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#ingress-controller","title":"Ingress Controller","text":"<p> Version 2.8 and up.</p> <p>Run:ai requires an ingress controller as a prerequisite. The Run:ai cluster installation configures one or more ingress objects on top of the controller. </p> <p>There are many ways to install and configure an ingress controller and configuration is environment-dependent. A simple solution is to install &amp; configure NGINX:</p> On PremManaged Kubernetes <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm upgrade -i nginx-ingress ingress-nginx/ingress-nginx   \\\n--namespace nginx-ingress --create-namespace \\\n--set controller.kind=DaemonSet \\\n--set controller.service.externalIPs=\"{&lt;INTERNAL-IP&gt;,&lt;EXTERNAL-IP&gt;}\" # (1)\n</code></pre> <ol> <li>External and internal IP of one of the nodes</li> </ol> <p>For managed Kubernetes such as EKS: </p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install nginx-ingress ingress-nginx/ingress-nginx \\\n--namespace nginx-ingress --create-namespace </code></pre> <p>For support of ingress controllers different than NGINX please contact Run:ai customer support. </p> <p>Note</p> <p>In a self-hosted installation, the typical scenario is to install the first Run:ai cluster on the same Kubernetes cluster as the control plane. In this case, there is no need to install an ingress controller as it is pre-installed by the control plane.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#cluster-url","title":"Cluster URL","text":"<p>The Run:ai user interface requires a URL to the Kubernetes cluster. You can use a domain name (FQDN) or an IP Address (deprecated).</p> <p>Note</p> <p>In a self-hosted installation, the typical scenario is to install the first Run:ai cluster on the same Kubernetes cluster as the control plane. In this case, the cluster URL need not be provided as it will be the same as the control-plane URL. </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#domain-name","title":"Domain Name","text":"<p> Version 2.8 and up.</p> <p>You must supply a domain name as well as a trusted certificate for that domain. </p> <p>Use an HTTPS-based domain (e.g. https://my-cluster.com) as the cluster URL. Make sure that the DNS is configured with the cluster IP.</p> <p>In addition, to configure HTTPS for your URL, you must create a TLS secret named <code>runai-cluster-domain-tls-secret</code> in the <code>runai</code> namespace. The secret should contain a trusted certificate for the domain:</p> <pre><code>kubectl create ns runai\nkubectl create secret tls runai-cluster-domain-tls-secret -n runai \\\n--cert /path/to/fullchain.pem  \\ # (1)\n--key /path/to/private.pem # (2)\n</code></pre> <ol> <li>The domain's cert (public key).</li> <li>The domain's private key. </li> </ol> <p>For more information on how to create a TLS secret see: https://kubernetes.io/docs/concepts/configuration/secret/#tls-secrets.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#cluster-ip","title":"Cluster IP","text":"<p>(Deprecated in version 2.8, no longer available in version 2.9)</p> <p>Following are instructions on how to get the IP and set firewall settings. </p> Unmanaged KubernetesUnmanaged Kubernetes on the cloudEKSGKE <ul> <li>Use the node IP of any of the Kubernetes nodes. </li> <li>Set up the firewall such that the IP is available to Researchers running within the organization (but not outside the organization).</li> </ul> <ul> <li>Use the node IPs of any of the Kubernetes nodes. Both internal and external IP in the format external-IP,internal-IP. </li> <li>Set up the firewall such that the external IP is available to Researchers running within the organization (but not outside the organization).</li> </ul> <p>You will need to externalize an IP address via a load balancer. If you do not have an existing load balancer already, install NGINX as follows:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install nginx-ingress ingress-nginx/ingress-nginx\n</code></pre> <p>Find the Cluster IP by running:</p> <pre><code>echo $(kubectl get svc nginx-ingress-ingress-nginx-controller  -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n</code></pre> <p>You will need to externalize an IP address via a load balancer. If you do not have an existing load balancer already, install NGINX as follows:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install nginx-ingress ingress-nginx/ingress-nginx\n</code></pre> <p>Find the Cluster IP by running:</p> <pre><code>echo $(kubectl get svc nginx-ingress-ingress-nginx-controller  -o=jsonpath='{.status.loadBalancer.ingress[0].ip}')\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#prometheus","title":"Prometheus","text":"Version 2.9 or laterVersion 2.8 or below <p>If not already installed on your cluster, install the full <code>kube-prometheus-stack</code> through the Prometheus community Operator. </p> <p>Note</p> <p>Due to a Prometheus bug described here you may need to apply Prometheus CRDs before installing Prometheus.</p> <p>Then install the Prometheus stack by running:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n-n monitoring --create-namespace --set grafana.enabled=false # (1)\n</code></pre> <ol> <li>The Grafana component is not required for Run:ai. </li> </ol> <p>The Run:ai Cluster installation will, by default, install Prometheus, but it can also connect to an existing Prometheus instance installed by the organization. Such a configuration can only be performed together with Run:ai support. In the latter case, it's important to:</p> <ul> <li>Verify that both Prometheus Node Exporter and kube-state-metrics are installed. Both are part of the default Prometheus installation</li> <li>Understand how Prometheus has been installed. Whether directly or with the Prometheus Operator. The distinction is important during the Run:ai Cluster installation.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#cert-manager","title":"Cert Manager","text":"<p>Rancher Kubernetes Engine (RKE) and Amazon Elastic Kubernetes Engine (EKS) require a certificate manager as described here. Example:</p> <pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --set installCRDs=true\n</code></pre> <p>For RKE only, you must then configure Run:ai to use the cert-manager. When creating a cluster on the Run:ai user interface:</p> <ul> <li>Download the \"On Premise\" Kubernetes type. </li> <li>Edit the cluster values file and change <code>useCertManager</code> to <code>true</code> </li> </ul> <pre><code>init-ca:\nenabled: true\nuseCertManager: true\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#distributed-training","title":"Distributed Training","text":"<p>Distributed training is the ability to run workloads on multiple nodes (not just multiple GPUs on the same node). Run:ai provides this capability via Kubeflow MPI. If you need this functionality, you will need to install the Kubeflow MPI Operator. Run:ai supports MPI version 0.3.0 (the latest version at the time of writing). </p> <p>Use the following installation guide. As per instruction, run:</p> <ul> <li>Verify that the <code>mpijob</code> custom resource does not currently exist in the cluster by running <code>kubectl get crds | grep mpijobs</code>. If it does, delete it by running <code>kubectl delete crd mpijobs.kubeflow.org</code></li> <li>run <code>kubectl apply -f https://raw.githubusercontent.com/kubeflow/mpi-operator/master/deploy/v2beta1/mpi-operator.yaml</code></li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#inference","title":"Inference","text":"<p>To use the Run:ai inference module you must pre-install Knative Serving. Follow the instructions here to install. Run:ai is certified on Knative 1.4 to 1.8 with Kubernetes 1.22 or later.  </p> <p>Post-install, you must configure Knative to use the Run:ai scheduler and allow pod affinity, by running: </p> <pre><code>kubectl patch configmap/config-features \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"kubernetes.podspec-schedulername\":\"enabled\",\"kubernetes.podspec-affinity\":\"enabled\"}}'\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#inference-autoscaling","title":"Inference Autoscaling","text":"<p>Run:ai allows to autoscale a deployment according to various metrics:</p> <ol> <li>GPU Utilization (%)</li> <li>CPU Utilization (%)</li> <li>Latency (milliseconds)</li> <li>Throughput (requests/second)</li> <li>Concurrency </li> <li>Any custom metric</li> </ol> <p>Additional installation may be needed for some of the metrics as follows:</p> <ul> <li>Using Throughput or Concurrency does not require any additional installation.</li> <li>Any other metric will require installing the HPA Autoscaler.</li> <li>Using GPU Utilization, Latency or Custom metric will also require the Prometheus adapter. The Prometheus adapter is part of the Run:ai installer and can be added by setting the <code>prometheus-adapter.enabled</code> flag to <code>true</code>. See Customizing the Run:ai installation for further information.</li> </ul> <p>If you wish to use an existing Prometheus adapter installation, you will need to configure it manually with the Run:ai Prometheus rules, specified in the Run:ai chart values under <code>prometheus-adapter.rules</code> field. For further information please contact Run:ai customer support. </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#accessing-inference-from-outside-the-cluster","title":"Accessing Inference from outside the Cluster","text":"<p>Inference workloads will typically be accessed by consumers residing outside the cluster. You will hence want to provide consumers with a URL to access the workload. The URL can be found in the Run:ai user interface under the deployment screen (alternatively, run <code>kubectl get ksvc -n &lt;project-namespace&gt;</code>). </p> <p>However, for the URL to be accessible outside the cluster you must configure your DNS as described here.</p> Altenative Configuration <p>When the above DNS configuration is not possible, you can manually add the <code>Host</code> header to the REST request as follows:</p> <ul> <li>Get an <code>&lt;external-ip&gt;</code> by running <code>kubectl get service -n kourier-system kourier</code>. If you have been using istio during Run:ai installation, run:  <code>kubectl -n istio-system get service istio-ingressgateway</code> instead. </li> <li>Send a request to your workload by using the external ip, and place the workload url as a <code>Host</code> header. For example</li> </ul> <pre><code>curl http://&lt;external-ip&gt;/&lt;container-specific-path&gt;\n    -H 'Host: &lt;host-name&gt;'\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#hardware-requirements","title":"Hardware Requirements","text":"<p>(see picture below)</p> <ul> <li> <p>(Production only) Run:ai System Nodes: To reduce downtime and save CPU cycles on expensive GPU Machines, we recommend that production deployments will contain two or more worker machines, designated for Run:ai Software. The nodes do not have to be dedicated to Run:ai, but for Run:ai purposes we would need:</p> <ul> <li>4 CPUs</li> <li>8GB of RAM</li> <li>50GB of Disk space  </li> </ul> </li> <li> <p>Shared data volume: Run:ai uses Kubernetes to abstract away the machine on which a container is running:</p> <ul> <li>Researcher containers: The Researcher's containers need to be able to access data from any machine in a uniform way, to access training data and code as well as save checkpoints, weights, and other machine-learning-related artifacts. </li> <li>The Run:ai system needs to save data on a storage device that is not dependent on a specific node.  </li> </ul> <p>Typically, this is achieved via Network File Storage (NFS) or Network-attached storage (NAS).</p> </li> <li> <p>Docker Registry: With Run:ai, Workloads are based on Docker images. For container images to run on any machine, these images must be downloaded from a docker registry rather than reside on the local machine (though this also is possible). You can use a public registry such as docker hub or set up a local registry on-prem (preferably on a dedicated machine). Run:ai can assist with setting up the repository.</p> </li> <li> <p>Kubernetes: Production Kubernetes installation requires separate nodes for the Kubernetes master. For more details see your specific Kubernetes distribution documentation. </p> </li> </ul> <p></p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#user-requirements","title":"User requirements","text":"<p>Usage of containers and images: The individual Researcher's work should be based on container images. </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#network-access-requirements","title":"Network Access Requirements","text":"<p>Internal networking: Kubernetes networking is an add-on rather than a core part of Kubernetes. Different add-ons have different network requirements. You should consult the documentation of the specific add-on on which ports to open. It is however important to note that unless special provisions are made, Kubernetes assumes all cluster nodes can interconnect using all ports. </p> <p>Outbound network: Run:ai user interface runs from the cloud. All container nodes must be able to connect to the Run:ai cloud. Inbound connectivity (connecting from the cloud into nodes) is not required. If outbound connectivity is proxied/limited, the following exceptions should be applied: </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#during-installation","title":"During Installation","text":"<p>Run:ai requires an installation over the Kubernetes cluster. The installation access the web to download various images and registries. Some organizations place limitations on what you can pull from the internet. The following list shows the various solution components and their origin: </p> Name Description URLs Ports <p>Run:ai  Repository</p> <p> Run:ai Helm Package Repository </p> <p> runai-charts.storage.googleapis.com </p> <p>443</p> <p>Docker Images Repository</p> <p>Run:ai images</p>  gcr.io/run-ai-prod  <p>443</p> <p> Docker Images Repository </p> <p> Third party Images</p> <p>hub.docker.com </p> <p>quay.io </p> <p>  443   </p> <p> Cert Manager </p> <p> (Run:ai version 2.7 or lower only) Creates a letsencrypt-based certificate for the cluster  </p> <p> 8.8.8.8, 1.1.1.1, dynu.com </p> <p> </p> <p>53</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#post-installation","title":"Post Installation","text":"<p>In addition, once running, Run:ai requires an outbound network connection to the following targets:</p> Name Description URLs Ports <p>Grafana</p> <p>Grafana Metrics Server</p> <p> prometheus-us-central1.grafana.net and runailabs.com </p> <p>443 </p> <p> Run:ai </p> <p> Run:ai   Cloud instance </p> <p> app.run.ai </p> <p> </p> <p>443</p> <p> Cert Manager </p> <p> (Run:ai version 2.7 or lower only) Creates a letsencrypt-based certificate for the cluster  </p> <p> 8.8.8.8, 1.1.1.1, dynu.com </p> <p> </p> <p>53</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#pre-install-script","title":"Pre-install Script","text":"<p>Once you believe that the Run:ai prerequisites are met, we highly recommend installing and running the Run:ai pre-install diagnostics script. The tool:</p> <ul> <li>Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking.</li> <li>Looks at additional components installed and analyze their relevance to a successful Run:ai installation. </li> </ul> <p>To use the script download the latest version of the script and run:</p> <pre><code>chmod +x preinstall-diagnostics-&lt;platform&gt;\n./preinstall-diagnostics-&lt;platform&gt;\n</code></pre> <p>If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file <code>runai-preinstall-diagnostics.txt</code> in the current directory and send it to Run:ai technical support. </p> <p>For more information on the script including additional command-line flags, see here.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/","title":"SaaS Cluster Setup Introduction","text":"<p>This section is a step-by-step guide for setting up a Run:ai cluster. </p> <ul> <li>A Run:ai cluster is installed on top of a Kubernetes cluster.</li> <li>A Run:ai cluster connects to the Run:ai control plane on the cloud. The control plane provides a control point as well as a monitoring and control user interface for Administrators and Researchers.</li> <li>A customer may have multiple Run:ai Clusters, all connecting to a single control plane.</li> </ul> <p>For additional details see the Run:ai system components</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#documents","title":"Documents","text":"<ul> <li>Review Run:ai cluster prerequisites.</li> <li>Step-by-step installation instructions.</li> <li>Look for troubleshooting tips if required.</li> <li>Upgrade cluster and delete cluster instructions. </li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#customization","title":"Customization","text":"<p>For a list of optional customizations see Customize Installation</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#additional-configuration","title":"Additional Configuration","text":"<p>For a list of advanced configuration scenarios such as configuring researcher authentication, Single sign-on limiting the installation to specific nodes, and more, see the Configuration Articles section.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#next-steps","title":"Next Steps","text":"<p>After setting up the cluster, you may want to start setting up Researchers. See: Researcher Setup.</p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/","title":"Upgrading a Cluster Installation","text":""},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#find-out-runai-cluster-version","title":"Find out Run:ai Cluster version","text":"<p>To find the Run:ai cluster version, run:</p> <pre><code>helm list -n runai -f runai-cluster\n</code></pre> <p>and record the chart version in the form of <code>runai-cluster-&lt;version-number&gt;</code></p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade-runai-cluster","title":"Upgrade Run:ai cluster","text":""},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade-to-version-29-from-an-earlier-version","title":"Upgrade to version 2.9 from an earlier version.","text":"<p>The process of upgrading to 2.9 requires uninstalling and then installing again. No data is lost during the process. </p> <p>Note</p> <p>The reason for this process is that Run:ai 2.9 cluster installation no longer installs pre-requisites. As such ownership of dependencies such as Prometheus will be undefined if a <code>helm upgrade</code> is run.</p> <p>The process:</p> <ul> <li>Delete the Run:ai cluster installation according to these instructions (do not delete the Run:ai cluster object from the user interface).</li> <li>Run: <code>kubectl delete svc -n kube-system runai-cluster-kube-prometh-kubelet</code> </li> <li>Install the mandatory Run:ai prerequisites:<ul> <li>If you have previously installed the SaaS version of Run:ai version 2.7 or below, you will need to install both Ingress Controller and Prometheus.</li> <li>If you have previously installed the SaaS version of Run:ai version 2.8 or any Self-hosted version of Run:ai, you will need to install Prometheus only.</li> </ul> </li> <li> <p>Install The Run:ai CRDs: <pre><code>kubectl apply -f https://raw.githubusercontent.com/run-ai/public/main/&lt;version&gt;/runai-crds.yaml\n</code></pre>     Where <code>version</code> is <code>2.9.X</code>. You can find Run:ai version numbers by running <code>helm search repo -l runai-cluster</code>.</p> </li> <li> <p>Install Run:ai cluster as described here</p> </li> </ul>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade-to-28","title":"Upgrade to 2.8","text":"<p>Replace <code>&lt;version&gt;</code> with the new version number in the command below. Then run: </p> <p><pre><code>kubectl apply -f https://raw.githubusercontent.com/run-ai/public/main/&lt;version&gt;/runai-crds.yaml\n</code></pre> The number should have 3 digits (for example <code>1.2.34</code>). You can find Run:ai version numbers by running <code>helm search repo -l runai-cluster</code>.</p> <p>Then run:</p> <pre><code>helm repo update\nhelm get values runai-cluster -n runai &gt; values.yaml\nhelm upgrade runai-cluster runai/runai-cluster -n runai -f values.yaml\n</code></pre> <p>Note</p> <p>To upgrade to a specific version of the Run:ai cluster, add <code>--version &lt;version-number&gt;</code> to the <code>helm upgrade</code> command. You can find the relevant version with <code>helm search repo</code> as described above. </p>"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#verify-successful-installation","title":"Verify Successful Installation","text":"<p>See Verify your installation on how to verify a Run:ai cluster installation</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/","title":"(Optional) Customize Cluster Installation","text":"<p>The Run:ai cluster creation wizard requires the download of a Helm values file <code>runai-&lt;cluster-name&gt;.yaml</code>. The file may be edited to customize the cluster installation.</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#configuration-flags","title":"Configuration Flags","text":"Key Default Description <code>runai-operator.config.project-controller.createNamespaces</code> <code>true</code> Set to <code>false</code>if unwilling to provide Run:ai the ability to create namespaces. When set to false, will requires an additional manual step when creating new Run:ai Projects <code>runai-operator.config.project-controller.clusterWideSecret</code> <code>true</code> Set to <code>false</code> when using PodSecurityPolicy or OpenShift <code>runai-operator.config.mps-server.enabled</code> <code>false</code> Set to <code>true</code> to allow the use of NVIDIA MPS. MPS is useful with Inference workloads <code>runai-operator.config.global.runtime</code> <code>docker</code> Defines the container runtime of the cluster (supports <code>docker</code> and <code>containerd</code>). Set to <code>containerd</code> when using Tanzu <code>runai-operator.config.global.nvidiaDcgmExporter.namespace</code> <code>gpu-operator</code> The namespace where dcgm-exporter (or gpu-operator) was installed <code>runai-operator.config.global.nvidiaDcgmExporter.installedFromGpuOperator</code> <code>true</code> Indicated whether the dcgm-exporter was installed via gpu-operator or not <code>kube-prometheus-stack.enabled</code> <code>true</code> (Version 2.8 or lower)  Set to <code>false</code> when the cluster has an existing Prometheus installation that is not based on the Prometheus operator. This setting requires Run:ai customer support <code>kube-prometheus-stack.prometheusOperator.enabled</code> <code>true</code> (Version 2.8 or lower)  Set to <code>false</code> when the cluster has an existing Prometheus installation based on the Prometheus operator and Run:ai should use the existing one rather than install a new one <code>prometheus-adapter.enabled</code> <code>false</code> (Version 2.8 or lower) Install Prometheus Adapter. Used for Inference workloads using a custom metric for autoscaling. Set to <code>true</code> if Prometheus Adapter is not already installed in the cluster <code>prometheus-adapter.prometheus</code> The address of the default Prometheus Service (Version 2.8 or lower) If you installed your own custom Prometheus Service, set this field accordingly with <code>url</code> and <code>port</code>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#prometheus","title":"Prometheus","text":"Version 2.9 or higherVersion 2.8 or lower <p>Not relevant</p> <p>The Run:ai Cluster installation uses Prometheus. There are 3 alternative configurations:</p> <ol> <li>Run:ai installs Prometheus (default).</li> <li>Run:ai uses an existing Prometheus installation based on the Prometheus operator.</li> <li>Run:ai uses an existing Prometheus installation based on a regular Prometheus installation.</li> </ol> <p>For option 2, disable the flag <code>kube-prometheus-stack.prometheusOperator.enabled</code>. For option 3, please contact Run:ai Customer support. </p> <p>For options 2 and 3, if you enabled <code>prometheus-adapter</code>, please configure it as described in the Prometheus Adapter documentation</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#understanding-custom-access-roles","title":"Understanding Custom Access Roles","text":"<p>To review the access roles created by the Run:ai Cluster installation, see Understanding Access Roles.</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#manual-creation-of-namespaces","title":"Manual Creation of Namespaces","text":"<p>Run:ai Projects are implemented as Kubernetes namespaces. By default, the administrator creates a new Project via the Administration user interface which then triggers the creation of a Kubernetes namespace named <code>runai-&lt;PROJECT-NAME&gt;</code>. There are a couple of use cases that customers will want to disable this feature:</p> <ul> <li>Some organizations prefer to use their internal naming convention for Kubernetes namespaces, rather than Run:ai's default <code>runai-&lt;PROJECT-NAME&gt;</code> convention.</li> <li>Some organizations will not allow Run:ai to automatically create Kubernetes namespaces. </li> </ul> <p>Follow these steps to achieve this:</p> <ol> <li>Disable the namespace creation functionality. See the  <code>runai-operator.config.project-controller.createNamespaces</code> flag above.</li> <li>Create a Project using the Run:ai User Interface. </li> <li>Create the namespace if needed by running: <code>kubectl create ns &lt;NAMESPACE&gt;</code>. The suggested Run:ai default is <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Label the namespace to connect it to the Run:ai Project by running <code>kubectl label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;</code>, where <code>&lt;PROJECT_NAME&gt;</code> is the name of the project you have created in the Run:ai user interface above and <code>&lt;NAMESPACE&gt;</code> is the name you chose for your namespace.</li> </ol>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#rke-specific-setup","title":"RKE-Specific Setup","text":"<p>Rancher Kubernetes Engine (RKE) requires additional steps:</p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#certificate-signing-request-rke1-only","title":"Certificate Signing Request (RKE1 only)","text":"<p>When creating a cluster on the Run:ai user interface:</p> <ul> <li>Download the \"On Premise\" Kubernetes type. </li> <li>Edit the cluster values file and change <code>useCertManager</code> to <code>true</code> </li> </ul> <pre><code>init-ca:\nenabled: true\nuseCertManager: true\n</code></pre> <p>On the cluster, install Cert manager as follows:</p> <pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install \\\ncert-manager jetstack/cert-manager \\\n--namespace cert-manager \\\n--create-namespace \\\n--set installCRDs=true\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#nginx-both-rke1-and-rke2","title":"NGINX (both RKE1 and RKE2)","text":"<p>RKE comes pre-installed with NGINX. Thus, the Run:ai prerequisite for ingress controller is not needed. </p>"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#researcher-authentication","title":"Researcher Authentication","text":"<p>See the RKE and RKE2 tabs in the Researcher Authentication document, on how to set up researcher authentication.  </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/","title":"NVIDIA DGX Bundle","text":""},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#runai-nvidia-dgx-bundle","title":"Run:ai &amp; NVIDIA DGX Bundle","text":"<p>NVIDIA DGX is a line of NVIDIA-produced servers and workstations which specialize in using GPUs to accelerate deep learning applications.</p> <p>NVIDIA DGX comes bundled out of the box with Run:ai. The purpose of this document is to guide you through the process of installing and configuring Run:ai in this scenario</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#nvidia-bright-cluster-manager","title":"NVIDIA Bright Cluster Manager","text":"<p>NVIDIA Bright Cluster Manager allows the deployment of software on NVIDIA DGX servers. During the installation of the DGX you will select <code>Run:ai</code> as well as Run:ai prerequisites from the Bright installer.</p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#prerequisites","title":"Prerequisites","text":""},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#software-prerequisites","title":"Software Prerequisites","text":"<p>Run:ai assumes the following components to be pre-installed:</p> <ul> <li><code>NVIDIA GPU Operator</code> - available for installation via the bright installer</li> <li><code>Prometheus</code> - available for installation via the bright installer</li> <li><code>Ingress controller</code> - NGINX is available for installation via the bright installer. </li> </ul>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#runai-prerequisites","title":"Run:ai prerequisites","text":"<p>The Run:ai cluster installer will require the following:</p> <ul> <li><code>Run:ai tenant name</code> - provided by Run:ai customer support.</li> <li><code>Run:ai install secret</code> - provided by Run:ai customer support.</li> <li><code>Cluster URL</code> - your organization should provide you with a domain name.</li> <li><code>Private and public keys</code> -your organization should provide a trusted certificate for the above domain name. The Run:ai installer will require both private key and full-chain in PEM format. </li> <li>Post-installation - credentials for the Run:ai user interface. Provided by Run:ai customer support.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#installing-runai-installer","title":"Installing Run:ai installer","text":"<p>Select Run:ai via the bright installer. Remember to select all of the above software prerequisites as well. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#using-the-runai-installer","title":"Using the Run:ai installer","text":"<p>Find out the cluster's IP address. Then browse to <code>http://&lt;CLUSTER-IP&gt;:30080/runai-installer</code>. Alternatively use the Bright landing page at <code>http://&lt;CLUSTER-IP&gt;/#runai</code>.  </p> <p>Note</p> <ul> <li>Use <code>http</code> rather than <code>https</code>.</li> <li>Use the IP and not a domain name.</li> </ul> <p>A wizard would open up containing 3 pages: Prerequisites, setup, and installation. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#prerequisites-page","title":"Prerequisites Page","text":"<p>The first, verification page, verifies that all of the above software prerequisites are met. Press the \"Verify\" button. You will not be able to continue unless all prerequisites are met. When all are met, press the <code>Continue</code> button. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#setup-page","title":"Setup Page","text":"<p>The setup page asks to provide all of the Run:ai prerequisites described above. The page will verify the Run:ai input (tenant name and install secret) but will not verify the validity of the cluster URL and certificate. If those are incorrect, the Run:ai installation will show as successful but certain aspects of Run:ai will not work. </p> <p>After filling up the form, press <code>Continue</code>. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#installation-page","title":"Installation page","text":"<p>The Run:ai installation will start. Depending on your download network speed the installation can take from 2 to 10 minutes. When the installation is successful you will see a <code>START USING RUN:AI</code> button. Press the button and enter your credentials to enter the Run:ai user interface. </p> <p>Save the URL for future use. </p>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#post-installation","title":"Post-installation.","text":"<p>Post installation, you will want to:</p> <ul> <li>(Mandatory) Set up Researcher Access Control. Without this, the Job Submit form will not work. Note the Bright section in that document.</li> <li>Set up Run:ai Users Working with Users.</li> <li>Set up Projects for Researchers Working with Projects.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#troubleshooting","title":"Troubleshooting","text":"<p>The cluster installer is a pod in Kubernetes. The pod is responsible for the installation preparation and prerequisite gathering phase. In case of an error during this pre-installation, you need to gather the pod's log. </p> <p>Once the Run:ai cluster installation has started, the behavior is identical to any Run:ai cluster installation flavor. See the troubleshooting page.</p>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/","title":"Native Kubernetes Installation","text":"<p>Kubernetes is composed of master(s) and workers. The instructions and script below are for creating a bare-bones installation of a single master and several workers for testing purposes. For a more complex, production-grade, Kubernetes installation, use tools such as Rancher Kubernetes Engine, or review Kubernetes documentation to learn how to customize the native installation.</p>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#prerequisites","title":"Prerequisites:","text":"<ul> <li>The script below assumes all machines have Ubuntu 18.04 or later. For other Linux-based operating-systems see Kubernetes documentation. </li> <li>The script must be run with ROOT privileges.</li> <li>Inbound ports 6443,443,8080 must be allowed. </li> <li>The script support Kubernetes 1.24 or later.</li> </ul>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#install-kubernetes","title":"Install Kubernetes","text":""},{"location":"admin/runai-setup/cluster-setup/install-k8s/#install-kubernetes-master","title":"Install Kubernetes Master","text":"<ul> <li>Get the script by running:  <pre><code>wget https://raw.githubusercontent.com/run-ai/docs/master/install/kube-install.sh\n</code></pre></li> <li>Run the script with ROOT privileges: <code>sudo ./kube-install.sh</code></li> <li>When prompted, select the option to install Kubernetes master.</li> <li>Select the Kubernetes version you want or press <code>Enter</code> for the default script version. </li> <li>Select the CNI (networking) version or press <code>Enter</code> for the default.</li> </ul> <p>When the script finishes, it will prompt a join command_ to be run on all workers. Save the command for later use.</p> <p>Note</p> <p>The default token expires after 24 hours. If the token has expired, go to the master node and run <code>sudo kubeadm token create --print-join-command</code>. This will produce an up-to-date join command.</p> <p>Test that Kubernetes is up by running: <pre><code>kubectl get nodes\n</code></pre> Verify that the master node is ready</p>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#install-kubernetes-workers","title":"Install Kubernetes Workers","text":"<p>On each designated worker node:</p> <ul> <li>Get the script by running:  <pre><code>wget https://raw.githubusercontent.com/run-ai/docs/master/install/kube-install.sh\n</code></pre></li> <li>Run the script with ROOT privileges: <code>sudo ./kube-install.sh</code></li> <li>When prompted, select the option to install Kubernetes worker.</li> <li>Select the Kubernetes version you want or press <code>Enter</code> for the default script version. The version should be the same as the one selected for the Kubernetes master. </li> </ul> <p>When the script finishes, run the join command saved above. </p> <p>To test that the worker has successfully joined, on the master node run: <pre><code>kubectl get nodes\n</code></pre> Verify that the new worker node is showing and ready (may take a couple of seconds).</p>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#avoiding-accidental-upgrades","title":"Avoiding Accidental Upgrades","text":"<p>To avoid an accidental upgrade of Kubernetes binaries during Linux upgrades, it is recommended to hold the version. Run the following on all nodes:</p> <pre><code>sudo apt-mark hold kubeadm kubelet kubectl\n</code></pre>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#next-steps","title":"Next Steps","text":"<p>The administrative Kubernetes profile can be found in the master node under the <code>.kube</code> folder. </p>"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#reset-nodes","title":"Reset Nodes","text":"<p>The same script also contains an option to completely remove Kubernetes from nodes (master or workers). To use, run: </p> <ul> <li>Get the script by running:  <pre><code>wget https://raw.githubusercontent.com/run-ai/docs/master/install/kube-install.sh\n</code></pre></li> <li>Run the script with ROOT privileges: <code>sudo ./kube-install.sh</code></li> <li>When prompted, select the option to reset/delete kubernetes.</li> <li>Select yes when prompted to reset the cluster and remove Kubernetes packages.</li> </ul>"},{"location":"admin/runai-setup/config/access-roles/","title":"Understand the Kubernetes Cluster Access provided to Run:ai","text":"<p>Run:ai has configuration flags that control specific behavioral aspects of Run:ai. Specifically, those which require additional permissions. Such as automatic namespace/project creation, secret propagation, and more.</p> <p>The purpose of this document is to provide security officers with the ability to review what cluster-wide access Run:ai requires, and verify that it is in line with organizational policy, before installing the Run:ai cluster. </p>"},{"location":"admin/runai-setup/config/access-roles/#review-cluster-access-roles","title":"Review Cluster Access Roles","text":"<p>Run:</p> <pre><code>helm repo add runai https://run-ai-charts.storage.googleapis.com\nhelm repo update\nhelm install runai-cluster runai/runai-cluster -n runai -f runai-&lt;cluster-name&gt;.yaml \\\n        --dry-run &gt; cluster-all.yaml\n</code></pre> <p>The file <code>cluster-all.yaml</code> can be then be reviewed. You can use the internal filenames (provided in comments within the file) to gain more understanding according to the table below:</p> Folder File Purpose <code>clusterroles</code> <code>base.yaml</code> Mandatory Kubernetes Cluster Roles and Cluster Role Bindings <code>clusterroles</code> <code>project-controller-ns-creation.yaml</code> Automatic Project Creation and Maintenance. Provides Run:ai with the ability to create Kubernetes namespaces when the Run:ai administrator creates new Projects. Can be turned on/off via flag <code>clusterroles</code> <code>project-controller-rb-creation.yaml</code> Automatically assign Users to Projects. Can be turned on/off via flag <code>clusterroles</code> <code>project-controller-cluster-wide-secrets.yaml</code> Allow the propagation of Secrets. See Secrets in Jobs. Can be turned on/off via flag <code>clusterroles</code> <code>project-controller-limit-range.yaml</code> Disables the usage of the Kubernetes Limit Range feature. Can be turned on/off via flag <code>ocp</code> <code>scc.yaml</code> OpenShift-specific Security Contexts <code>priorityclasses</code> 4 files Folder contains a list of Priority Classes used by Run:ai"},{"location":"admin/runai-setup/config/allow-external-access-to-containers/","title":"External access to Containers","text":""},{"location":"admin/runai-setup/config/allow-external-access-to-containers/#introduction","title":"Introduction","text":"<p>Researchers working with containers. many times need to remotely access the container. Some examples:</p> <ul> <li>Using a Jupyter notebook that runs within the container</li> <li>Using PyCharm to run python commands remotely.</li> <li>Using TensorBoard to view machine learning visualizations</li> </ul> <p>This requires exposing container ports. When using docker, the way Researchers expose ports is by declaring them when starting the container. Run:ai has similar syntax.</p> <p>Run:ai is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers several options:</p> Method Description Prerequisites Port Forwarding Simple port forwarding allows access to the container via local and/or remote port. None NodePort Exposes the service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service from outside the cluster by requesting <code>&lt;NODE-IP&gt;:&lt;NODE-PORT&gt;</code> regardless of which node the container actually resides in. None LoadBalancer Exposes the service externally using a cloud provider\u2019s load balancer. Only available with cloud providers <p>See https://kubernetes.io/docs/concepts/services-networking/service for further details on these four options.</p>"},{"location":"admin/runai-setup/config/allow-external-access-to-containers/#workspaces-configuration","title":"Workspaces configuration","text":"<p> Version 2.9 and up </p> <p>Version 2.9 introduces Workspaces which allow the Researcher to build AI models interactively. </p> <p>Workspaces allow the Researcher to launch tools such as Visual Studio code, TensorFlow, TensorBoard etc. These tools require access to the container. Access is provided via URLs. </p> <p>Run:ai uses the Cluster URL provided to dynamically create SSL-secured URLs for researchers\u2019 workspaces in the format of <code>https://&lt;CLUSTER_URL&gt;/project-name/workspace-name</code>.</p> <p>While this form of path-based routing conveniently works with applications like Jupyter Notebooks, it may often not be compatible with other applications. These applications assume running at the root file system, so hardcoded file paths and settings within the container may become invalid when running at a path other than the root. For instance, if the container is expecting to find a file at <code>/etc/config.json</code> but is running at <code>/project-name/workspace-name</code>, the file will not be found. This can cause the container to fail or not function as intended.</p> <p>To address this issue, Run:ai provides support for host-based routing. When enabled, Run:ai creates workspace URLs in a subdomain format (<code>https://project-name-workspace-name.&lt;CLUSTER_URL&gt;/</code>), which allows all workspaces to run at the root path and function properly. </p> <p>To enable host-based routing you must perform the following steps:</p> <ol> <li>Create a second DNS entry  <code>*.&lt;CLUSTER_URL&gt;</code>, pointing to the same IP as the original Cluster URL DNS.</li> <li> <p>Obtain a star SSL certificate for this DNS.</p> </li> <li> <p>Add the certificate as a secret:</p> </li> </ol> SaaSSelf hosted <pre><code>kubectl create secret tls runai-cluster-domain-star-tls-secret -n runai \\ \n    --cert /path/to/fullchain.pem --key /path/to/private.pem\n</code></pre> <pre><code>kubectl create secret tls runai-cluster-domain-star-tls-secret -n runai-backend \\\n    --cert /path/to/fullchain.pem --key /path/to/private.pem\n</code></pre> <ol> <li>Create an ingress rule to direct traffic:</li> </ol> SaaSSelf hosted <pre><code>kubectl patch ingress researcher-service-ingress -n runai --type json \\\n    --patch '[{ \"op\": \"add\", \"path\": \"/spec/tls/-\", \"value\": { \"hosts\": [ \"*.&lt;CLUSTER_URL&gt;\" ], \"secretName\": \"runai-cluster-domain-star-tls-secret\" } }]'\n</code></pre> <pre><code>kubectl patch ingress runai-backend-ingress -n runai-backend --type json \\\n    --patch '[{ \"op\": \"add\", \"path\": \"/spec/tls/-\", \"value\": { \"hosts\": [ \"*.&lt;CLUSTER_URL&gt;\" ], \"secretName\": \"runai-cluster-domain-star-tls-secret\" } }]'\n</code></pre> <ol> <li>Edit Runaiconfig to generate the URLs correctly:</li> </ol> <pre><code>kubectl patch RunaiConfig runai -n runai --type=\"merge\" \\\n    -p '{\"spec\":{\"global\":{\"subdomainSupport\": true}}}' \n</code></pre> <p>Once these requirements have been met, all workspaces will automatically be assigned a secured URL with a subdomain, ensuring full functionality for all researcher applications.</p>"},{"location":"admin/runai-setup/config/allow-external-access-to-containers/#see-also","title":"See Also","text":"<ul> <li>To learn how to use port forwarding see the Quickstart document:  Launch an Interactive Build Workload with Connected Ports.</li> <li>See CLI command runai submit.</li> </ul>"},{"location":"admin/runai-setup/config/cli-admin-install/","title":"Install the Run:ai Administrator Command-line Interface","text":"<p>The Run:ai Administrator Command-line Interface (Administrator CLI) allows performing administrative tasks on the Run:ai Cluster.  </p> <p>The instructions below will guide you through the process of installing the Administrator CLI.</p>"},{"location":"admin/runai-setup/config/cli-admin-install/#prerequisites","title":"Prerequisites","text":"<ul> <li>Run:ai Administrator CLI runs on Mac and Linux.   </li> <li>Kubectl (Kubernetes command-line interface) is installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/</li> <li>A Kubernetes configuration file obtained from a computer previously connected to the Kubernetes cluster</li> </ul>"},{"location":"admin/runai-setup/config/cli-admin-install/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<p>The Run:ai Administrator CLI requires a Kubernetes profile with cluster administrative rights. </p>"},{"location":"admin/runai-setup/config/cli-admin-install/#installation","title":"Installation","text":"<p>Download the Run:ai Administrator Command-line Interface by running:</p> MacLinux <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/darwin\nchmod +x runai-adm\nsudo mv runai-adm /usr/local/bin/runai-adm\n</code></pre> <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/linux\nchmod +x runai-adm\nsudo mv runai-adm /usr/local/bin/runai-adm\n</code></pre> <p>To verify the installation run:</p> <pre><code>runai-adm version\n</code></pre>"},{"location":"admin/runai-setup/config/cli-admin-install/#download-a-specific-version","title":"Download a specific version","text":"<p>To download a specific version of <code>runai-adm</code> add the version number to URL. For example:</p> <pre><code>wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/v2.7.22/darwin\n</code></pre>"},{"location":"admin/runai-setup/config/cli-admin-install/#updating-the-runai-administrator-cli","title":"Updating the Run:ai Administrator CLI","text":"<p>To update the CLI to the latest version perform the same install process again. The command <code>runai-adm update</code> is no longer supported.</p>"},{"location":"admin/runai-setup/config/dr/","title":"Planning for Disaster Recovery","text":"<p>The SaaS version of Run:ai moves the bulk of the burden of disaster recovery to Run:ai. Backup of data is hence not an issue in such environments. </p> <p>With the self-hosted version, it is the responsibility of the IT organization to back up data for a possible disaster and to learn how to recover when needed.</p>"},{"location":"admin/runai-setup/config/dr/#backup","title":"Backup","text":""},{"location":"admin/runai-setup/config/dr/#database","title":"Database","text":"<p>Run:ai uses an internal PostgreSQL database. The database is stored on a Kubernetes Persistent Volume (PV). You must provide a backup solution for the database. Typically by backing up the persistent volume holding the database storage.</p>"},{"location":"admin/runai-setup/config/dr/#metrics","title":"Metrics","text":"<p>Run:ai stores metric history using Thanos. Thanos is configured to store data on a persistent volume. The recommendation is to back up the PV.</p>"},{"location":"admin/runai-setup/config/dr/#additional-configuration","title":"Additional Configuration","text":"<p>During the installation of Run:ai you have created two value files:</p> <ul> <li>One for the Run:ai control plane. See Kubernetes or OpenShift,</li> <li>One for the cluster (see Kubernetes or OpenShift). </li> </ul> <p>You will want to save these files or extract a current version of the file by using the upgrade script. </p>"},{"location":"admin/runai-setup/config/dr/#recovery","title":"Recovery","text":"<p>To recover Run:ai</p> <ul> <li>Re-create the Kubernetes/OpenShift cluster.</li> <li>Recover the persistent volumes for metrics and database. </li> <li>Re-install the Run:ai control plane. Use the stored values file. If needed, modify the values file to connect to the restored PostgreSQL PV. Connect Prometheus to the stored metrics PV. </li> <li>Re-install the cluster. Use the stored values file or download a new file from the Administration UI. </li> <li>If the cluster is configured such that Projects do not create a namespace automatically, you will need to re-create namespaces and apply role bindings as discussed in Kubernetes or OpenShift.</li> </ul>"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/","title":"Node affinity with cloud node pools","text":"<p>Run:ai allows for node affinity. Node affinity is the ability to assign a Project to run on specific nodes. To use the node affinity feature, You will need to label the target nodes with the label  <code>run.ai/node-type</code>. Most cloud clusters allow configuring node labels for the node pools in the cluster. This guide shows how to apply this configuration to different cloud providers.</p> <p>To make the node affinity work with node pools on various cloud providers, we need to make sure the node pools are configured with the appropriate Kubernetes label (<code>run.ai/type=&lt;TYPE_VALUE&gt;</code>).</p>"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/#setting-node-labels-while-creating-a-new-cluster","title":"Setting node labels while creating a new cluster","text":"<p>You can configure node-pool labels at cluster creation time</p> GKEAKSEKS <ul> <li>At the first creation screen, you will see a menu on the left side named <code>node-pools</code>.</li> <li>Expand the node pool you want to label.</li> <li>Click on <code>Metadata</code>.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>When creating AKS cluster at the node-pools page click on create new node-pool.</li> <li>Go to the <code>labels</code> section and add key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Create a regular EKS cluster.</li> <li>Click on <code>compute</code>.</li> <li>Click on <code>Add node group</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/#setting-node-labels-for-a-new-node-pool","title":"Setting node labels for a new node pool","text":"GKEAKSEKS <ul> <li>At the node pool creation screen, go to the <code>metadata</code> section.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Go to your AKS page at Azure.</li> <li>On the left menu click the <code>node-pools</code> button.</li> <li>Click on <code>Add Node Pool</code>.</li> <li>In the new Node Pool page go to <code>Optional settings</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <ul> <li>Go to <code>Add node group</code> screen.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/#editing-node-labels-for-an-existing-node-pool","title":"Editing node labels for an existing node pool","text":"GKEAKSEKS <ul> <li>Go to the <code>Google Kubernetes Engine</code> page in the Google Cloud console.</li> <li>Go to <code>Google Kubernetes Engine</code>.</li> <li>In the cluster list, click the name of the cluster you want to modify.</li> <li>Click the <code>Nodes</code> tab</li> <li>Under <code>Node Pools</code>, click the name of the node pool you want to modify, then click <code>Edit</code>.</li> <li>Near the bottom, you will find the Kubernetes <code>label</code> section. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul> <p>To update an existing node pool label you must use the azure cli. Run the following command:</p> <pre><code>az aks nodepool update \\\n    --resource-group [RESOURCE GROUP] \\\n    --cluster-name [CLUSTER NAME] \\\n    --name labelnp \\\n    --labels run.ai/type=[TYPE_VALUE] \\\n    --no-wait\n</code></pre> <ul> <li>Go to the <code>node group</code> page and click on <code>Edit</code>.</li> <li>In the Kubernetes <code>labels</code> section click on <code>Add label</code>. Add the key <code>run.ai/type</code> and the value <code>&lt;TYPE_VALUE&gt;</code>.</li> </ul>"},{"location":"admin/runai-setup/config/node-roles/","title":"Designating Specific Role Nodes","text":"<p>When installing a production cluster you may want to:</p> <ul> <li>Set one or more Run:ai system nodes. These are nodes dedicated to Run:ai software. </li> <li>Machine learning frequently requires jobs that require CPU but not GPU. You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. </li> <li>Limit Run:ai monitoring and scheduling to specific nodes in the cluster. </li> </ul> <p>To perform these tasks you will need the Run:ai Administrator CLI. See Installing the Run:ai Administrator Command-line Interface.</p>"},{"location":"admin/runai-setup/config/node-roles/#dedicated-runai-system-nodes","title":"Dedicated Run:ai System Nodes","text":"<p>Find out the names of the nodes designated for the Run:ai system by running <code>kubectl get nodes</code>. For each such node run:</p> <pre><code>runai-adm set node-role --runai-system-worker &lt;node-name&gt;\n</code></pre> <p>If you re-run <code>kubectl get nodes</code> you will see the node role of these nodes changed to <code>runai-system</code></p> <p>To remove the runai-system node role run:</p> <pre><code>runai-adm remove node-role --runai-system-worker &lt;node-name&gt;\n</code></pre> <p>Warning</p> <p>Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).</p>"},{"location":"admin/runai-setup/config/node-roles/#dedicated-gpu-cpu-nodes","title":"Dedicated GPU &amp; CPU Nodes","text":"<p>Separate nodes into those that:</p> <ul> <li>Run GPU workloads</li> <li>Run CPU workloads</li> <li>Do not run Run:ai at all. these jobs will not be monitored using the Run:ai Administration User interface. </li> </ul> <p>Review nodes names using <code>kubectl get nodes</code>. For each such node run:</p> <pre><code>runai-adm set node-role --gpu-worker &lt;node-name&gt;\n</code></pre> <p>or </p> <pre><code>runai-adm set node-role --cpu-worker &lt;node-name&gt;\n</code></pre> <p>Nodes not marked as GPU worker or CPU worker will not run Run:ai at all.</p> <p>To set all workers not running runai-system as GPU workers run:</p> <pre><code>runai-adm set node-role --all &lt;node-name&gt;\n</code></pre> <p>To remove the CPU or GPU worker node role run:</p> <pre><code>runai-adm remove node-role --cpu-worker &lt;node-name&gt;\n</code></pre> <p>or </p> <pre><code>runai-adm remove node-role --gpu-worker &lt;node-name&gt;\n</code></pre>"},{"location":"admin/runai-setup/config/non-root-containers/","title":"User Identity in Container","text":"<p>The identity of the user in the container determines its access to resources. For example, network file storage solutions typically use this identity to determine the container's access to network volumes. This document explains multiple ways for propagating the user identity into the container.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#the-default-root-access","title":"The Default: Root Access","text":"<p>In docker, as well as in Kubernetes, the default for running containers is running as root. The implication of running as root is that processes running within the container have enough permissions to change anything in the container, and if propagated to network resources - can have permissions outside the container as well. </p> <p>This gives a lot of power to the Researcher but does not sit well with modern security standards of enterprise security. </p> <p>By default, if you run:</p> <p><pre><code>runai submit -i ubuntu --attach --interactive -- bash\n</code></pre> then run <code>id</code>, you will see the root user. </p>"},{"location":"admin/runai-setup/config/non-root-containers/#use-runai-flags-to-limit-root-access","title":"Use Run:ai flags to limit root access","text":"<p>There are two runai submit flags which control user identity at the Researcher level:</p> <ul> <li>The flag <code>--run-as-user</code> starts the container with a specific user. The user is the current Linux user (see below for other behaviors if used in conjunction with Single sign-on). </li> <li>The flag <code>--prevent-privilege-escalation</code> prevents the container from elevating its own privileges into <code>root</code> (e.g. running <code>sudo</code> or changing system files.). </li> </ul> <p>Equivalent flags exist in the Researcher User Interface.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#run-as-current-user","title":"Run as Current User","text":"<p>From a Linux/Mac box, run:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user -- bash\n</code></pre> <p>then run <code>id</code>, you will see the users and groups of the box you have been using to launch the Job.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#prevent-escalation","title":"Prevent Escalation","text":"<p>From a Linux/Mac box, run:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user \\\n  --prevent-privilege-escalation  -- bash\n</code></pre> <p>then verify that you cannot run <code>su</code> to become root within the container. </p>"},{"location":"admin/runai-setup/config/non-root-containers/#setting-a-cluster-wide-default","title":"Setting a Cluster-Wide Default","text":"<p>The two flags are voluntary. They are not enforced by the system. It is however possible to enforce them using Policies. Polices allow an Administrator to force compliance on both the User Interface and Command-line interface. </p>"},{"location":"admin/runai-setup/config/non-root-containers/#passing-user-identity","title":"Passing user identity","text":""},{"location":"admin/runai-setup/config/non-root-containers/#passing-user-identity-from-identity-provider","title":"Passing user identity from Identity Provider","text":"<p>A best practice is to store the user identifier (UID) and the group identifier (GID) in the organization's directory. Run:ai allows you to pass these values to the container and use them as the container identity.</p> <p>To perform this, you must:</p> <ul> <li>Set up single sign-on. Perform the steps for UID/GID integration.</li> <li>Run: <code>runai login</code> and enter your credentials</li> <li>Use the flag --run-as-user</li> </ul> <p>Running <code>id</code> should show the identifier from the directory.</p>"},{"location":"admin/runai-setup/config/non-root-containers/#passing-user-identity-explicitly-via-the-researcher-ui","title":"Passing user identity explicitly via the Researcher UI","text":"<p>Via the Researcher User Interface, it is possible to explicitly provide the user id and group id:</p> <p></p>"},{"location":"admin/runai-setup/config/non-root-containers/#using-openshift-or-gatekeeper-to-provide-cluster-level-controls","title":"Using OpenShift or Gatekeeper to provide Cluster Level Controls","text":"<p>Run:ai supports OpenShift as a Kubernetes platform. In OpenShift the system will provide a random UID to containers. The flags <code>--run-as-user</code> and <code>--prevent-privilege-escalation</code> are disabled on OpenShift. It is possible to achieve a similar effect on Kubernetes systems that are not OpenShift. A leading tool is Gatekeeper. Gatekeeper similarly enforces non-root on containers at the system level. </p>"},{"location":"admin/runai-setup/config/non-root-containers/#creating-a-temporary-home-directory","title":"Creating a Temporary Home Directory","text":"<p>When containers run as a specific user, the user needs to have a pre-created home directory within the image. Otherwise, when running a shell, you will not have a home directory:</p> <pre><code>runai submit -i ubuntu --attach --interactive --run-as-user -- bash\nThe job 'job-0' has been submitted successfully\nYou can run `runai describe job job-0 -p team-a` to check the job status\nWaiting for pod to start running...\nINFO[0007] Job started\nConnecting to pod job-0-0-0\nIf you don't see a command prompt, try pressing enter.\nI have no name!@job-0-0-0:/$ </code></pre> <p>Adding home directories to an image per user is not a viable solution. To overcome this, Run:ai provides an additional flag <code>--create-home-dir</code>. Adding this flag creates a temporary home directory for the user within the container.  </p> <p>Notes</p> <ul> <li>Data saved in this directory will not be saved when the container exits. </li> <li>This flag is set by default to true when the <code>--run-as-user</code> flag is used, and false if not.</li> </ul>"},{"location":"admin/runai-setup/config/overview/","title":"Run:ai Configuration Articles","text":"<p>This section provides a list of installation-related articles dealing with a wide range of subjects:</p> Article Purpose Designating Specific Role Nodes Set one or more designated Run:ai system nodes or limit Run:ai monitoring and scheduling to specific nodes in the cluster. Setup Project-based Researcher Access Control Enable  Run:ai access control is at the Project level. Single sign-on Integrate with the organization's Identity Provider to provide single sign-on for Run:ai Review Kubernetes Access provided to Run:ai In Restrictive Kubernetes environments such as when using OpenShift, understand and control what Kubernetes roles are provided to Run:ai External access to Containers Understand the available options for Researchers to access containers from the outside User Identity in Container The identity of the user in the container determines its access to cluster resources. The document explains multiple way on how to propagate the user identity into the container. Install the Run:ai Administrator Command-line Interface The Administrator command-line is useful in a variety of flows such as cluster upgrade, node setup etc."},{"location":"admin/runai-setup/maintenance/audit-log/","title":"Audit Log","text":""},{"location":"admin/runai-setup/maintenance/audit-log/#introduction","title":"Introduction","text":"<p>The Run:ai control plane provides audit log API and audit log user interface table. Both reflect the same information:</p> <ul> <li>All changes to business objects</li> <li>All logins to the control plane.</li> </ul>"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-audit-log-user-interface","title":"Event History - Audit Log User Interface","text":"<p>The Administrators of the system can view the audit log using the user interface. The audit log screen is under the 'Event History' section:</p> <p></p>"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-audit-log-information-fields","title":"Event History (audit log) information fields","text":"<p>The Administrator can choose what information fields to view within the audit log table, this is done by clicking the 'Columns' button and checking the required fields to be presented:</p> <p></p> <p></p> <p>Here's the list of available information fields in the Event History (audit log) table:</p> Field Type Description User/App user id The identity of the User or Application that executed this operation. Data &amp; Time date The exact timestamp at which the event occured.  Format <code>dd/mm/yyyy</code> for date and <code>hh:mm am/pm</code> for time. Event event type The type of the logged operation. Possible values: <code>Create</code>, <code>Update</code>, <code>Delete</code>, <code>Login</code>. Event ID integer Sequanicialy incrmental number of the logged operation, lower number means older event, higher means newer event. Status string The outcome of the logged operation. Possible values: <code>Succeeded</code>, <code>Failed</code>. Entity type string The type of the logged business object. Possible values: <code>Project</code>, <code>Department</code>, <code>User</code>, <code>Group</code>, <code>Login</code>, <code>Settings</code>, <code>Applications</code>, <code>Node Pool</code>. Entity name string The name of logged business object. Entity ID string The system's internal id of the logged business object. Cluster Name string The name of the cluster that the loged operation relates to. If the operation is not cluster specific - cluster name remains empty. Cluster ID string The system internal identifier of the cluster that the loged operation relates to. If the operation is not cluster specific - cluster id remains empty."},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-date-selector","title":"Event History - Date Selector","text":"<p>The Event History table saves logged operations for the last 90 days. However, the table itself presents up to the last 30 days of information due to the potentially very high number of operations that might be logged during this period. To view older logged operations, or if you wish to refine your search and get more specific results or fewer results, you should use the time selector and change the period you search for. You can also refine your search by using filters as explained below.  </p> <p></p>"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-filters","title":"Event History - Filters","text":"<p>The administrator can choose to filter the table using a list of predefined filters. The filter's value is a free text keyword entered by the administrator and must be fully matched to the requested field's actual value, otherwise, the filter will not find the requested keyword. Multiple filters can be set in parallel.</p> <p></p> <p></p>"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-download-the-audit-log-file","title":"Event History - Download the Audit Log file","text":"<p>The event history table allows you to download the logged information in text form formatted as CSV or JSON files. The scope of the downloaded information is set by the scope of the table filters, i.e. if no filters or date selectors are used, the downloaded file includes the full scope of the information that the table holds - i.e. up to 30 days of logged information. To view older logged information (up to 90 days older, but no more than 30 days at a time), shorter periods, or narrower (filtered) scopes - use the date selector and filters.</p> <p></p>"},{"location":"admin/runai-setup/maintenance/audit-log/#audit-log-api","title":"Audit log API","text":"<p>Since the amount of data is not trivial, the API is based on paging in the sense that it will retrieve a specified number of items for each API call. You can get more data by using subsequent calls. </p>"},{"location":"admin/runai-setup/maintenance/audit-log/#retrieve-audit-log-data-via-api","title":"Retrieve Audit Log data via API","text":"<p>To retrieve the Audit log you need to call an API. You can do this via code or by using the Audit function via a user interface for calling APIs.</p>"},{"location":"admin/runai-setup/maintenance/audit-log/#retrieve-via-code","title":"Retrieve via Code","text":"<p>Create an Application and generate a bearer token by following the API Authentication document.  </p> <p>To get the first 40 records of the audit log starting January 1st, 2022, run:</p> <pre><code>curl -X 'GET' \\\n'https://&lt;COMPANY-URL&gt;/v1/k8s/audit?start=2022-1-1' \\  # (1)\n-H 'accept: application/json' \\\n-H 'Authorization: Bearer &lt;ACCESS-TOKEN&gt;' # (2)\n</code></pre> <ol> <li><code>&lt;COMPANY-URL&gt;</code> is <code>app.run.ai</code> for SaaS installations (not <code>&lt;company&gt;.run.ai</code>) or the Run:ai user interface URL for Self-hosted installations.</li> <li>To obtain a Bearer token see API authentication.</li> </ol> <p>Sample result:</p> <pre><code>[\n{\n\"id\": 3,\n\"tenantId\": 1,\n\"happenedAt\": \"2022-07-07T09:45:32.069Z\",\n\"action\": \"Update\",\n\"version\": \"1.0\",\n\"entityId\": \"1\",\n\"entityType\": \"Project\",\n\"entityName\": \"team-a\",\n\"sourceType\": \"User\",\n\"sourceId\": \"a79500fb-c452-471f-adc0-b65c972bd5c2\",\n\"sourceName\": \"test@run.ai\",\n\"context\": {\n\"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\",\n\"ip_address\": \"10.244.0.0\"\n}\n},\n{\n\"id\": 2,\n\"tenantId\": 1,\n\"happenedAt\": \"2022-07-07T08:27:39.649Z\",\n\"action\": \"Create\",\n\"version\": \"1.0\",\n\"entityId\": \"fdc90aab-b183-4856-8337-14039063b876\",\n\"entityType\": \"App\",\n\"entityName\": \"admin\",\n\"sourceType\": \"User\",\n\"sourceId\": \"a79500fb-c452-471f-adc0-b65c972bd5c2\",\n\"sourceName\": \"test@run.ai\",\n\"context\": {\n\"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\",\n\"ip_address\": \"10.244.0.0\"\n}\n},\n...\n]\n</code></pre>"},{"location":"admin/runai-setup/maintenance/audit-log/#paging","title":"Paging","text":"<p>Use the <code>limit</code> and <code>offset</code> properties to retrieve all audit log entries.</p>"},{"location":"admin/runai-setup/maintenance/audit-log/#additional-filter","title":"Additional filter","text":"<p>You can add additional filters to the query as follows:</p> Field Type Description start date Start date for audit logs retrieval.  Format <code>yyyy-MM-dd</code> for date or <code>yyyy-MM-ddThh:mm:ss</code> for date-time. end date End date for audit logs retrieval.  Format <code>yyyy-MM-dd</code> for date or <code>yyyy-MM-ddThh:mm:ss</code> for date-time. action string The action of the logged operation. Possible values: <code>Create</code>, <code>Update</code>, <code>Delete</code>, <code>Login</code> source_type string The initiator of the action (user or machine to machine key). Possible values: <code>User</code>, <code>Application</code> source_id string The id of the source of the action. For <code>User</code>, this is the internal user id. For an <code>Application</code>, this is the internal id of the Application source_name string The name of the source of the action. For a <code>User</code>, this is the user's email, for an <code>Application</code>, this is the Application name. entity_type string The type of business object. Possible values: <code>Project</code>, <code>Department</code>, <code>User</code>, <code>Group</code>, <code>Login</code>, <code>Settings</code>, <code>Applications</code> entity_id string The id of the business object limit integer Paging: the number of records to fetch at once (default is 40 record) offset integer Paging: The offset from which to start fetching records. success string enter true for successful audit log records and false for failures (default is all records) download string enter true to download the logs into a file <p></p>"},{"location":"admin/runai-setup/maintenance/monitoring/","title":"Cluster Monitoring","text":""},{"location":"admin/runai-setup/maintenance/monitoring/#introduction","title":"Introduction","text":"<p>Organizations typically want to automatically highlight critical issues and escalate issues to IT/DevOps personnel. The standard practice is to install an alert management tool and connect it to critical systems. </p> <p>Run:ai is comprised of two parts:</p> <ul> <li>A control plane part, typically resides in the cloud. The health of the cloud portion of Run:ai can be viewed at status.run.ai. In Self-hosted installations of Run:ai is installed on-prem.</li> <li>One or more GPU Clusters. </li> </ul> <p>The purpose of this document is to configure the Run:ai to emit health alerts and to connect these alerts to alert-management systems within the organization. </p> <p>Alerts are emitted for Run:ai Clusters as well as the Run:ai control plane on Self-hosted installation where the control plane resides on the same Kubernetes cluster as one of the Run:ai clusters. </p>"},{"location":"admin/runai-setup/maintenance/monitoring/#alert-infrastructure","title":"Alert Infrastructure","text":"<p>Run:ai uses Prometheus for externalizing metrics. The Run:ai cluster installation installs Prometheus or can connect to an existing Prometheus instance used in the organization.  Run:ai cluster alerts are based on the Prometheus Alert Manager. The Prometheus Alert Manager is enabled by default.  </p> <p>This document explains, how to:</p> <ul> <li>Configure alert destinations. Triggered alerts will send data to destinations.  </li> <li>Understand the out-of-the-box cluster alerts. </li> <li>Advanced: add additional custom alerts. </li> </ul>"},{"location":"admin/runai-setup/maintenance/monitoring/#configure-alert-destinations","title":"Configure Alert Destinations","text":"<p>Prometheus Alert Manager provides a structured way to connect to alert-management systems. Configuration details are here. There are built-in plugins for popular systems such as PagerDuty and OpsGenie, including a generic webhook. </p> <p>Following is an example showing how to integrate Run:ai to a webhook:</p> <ul> <li>Use https://webhook.site/. Get the <code>Unique URL</code>.</li> <li>When installing the Run:ai cluster, edit the values file to add the following.</li> </ul> <pre><code>kube-prometheus-stack:\n...\nalertmanager:\nenabled: true\nconfig:\nglobal:\nresolve_timeout: 5m\nreceivers:\n- name: \"null\"\n- name: webhook-notifications\nwebhook_configs:\n- url: &lt;WEB-HOOK-URL&gt;\nsend_resolved: true\nroute:\ngroup_by:\n- alertname\ngroup_interval: 5m\ngroup_wait: 30s\nreceiver: 'null'\nrepeat_interval: 10m\nroutes:\n- receiver: webhook-notifications\n</code></pre> <p>(Replace <code>&lt;WEB-HOOK-URL&gt;</code> with the URL above).</p> <ul> <li>On an existing installation, use the upgrade cluster instructions to modify the values file.</li> <li>Verify that you have received alerts at https://webhook.site/.</li> </ul>"},{"location":"admin/runai-setup/maintenance/monitoring/#out-of-the-box-alerts","title":"Out-of-the-box Alerts","text":"<p>A Run:ai cluster comes with several built-in alerts. Each alert tests a specific aspect of the Run:ai functionality. In addition, there is a single, inclusive alert, which aggregates all component-based alerts into a single cluster health test.</p> <p>The aggregated alert is named <code>RunaiCriticalProblem</code>. It is categorized as \"critical\".</p>"},{"location":"admin/runai-setup/maintenance/monitoring/#add-a-custom-alert","title":"Add a custom alert","text":"<p>You can add additional alerts from Run:ai. Alerts are triggered by using the Promtheus query language with any Run:ai metric. To add new alert:</p> <ul> <li>When installing the Run:ai cluster, edit the values file.</li> <li>On an existing installation, use the upgrade cluster instructions to modify the values file.</li> <li>Add an alert according to the structure specified below.</li> </ul> <p>Add more alerts with the following structure:</p> <pre><code>kube-prometheus-stack:\nadditionalPrometheusRulesMap:\ncustom-runai:\ngroups:\n- name: custom-runai-rules\nrules:\n- alert: &lt;ALERT-NAME&gt;\nannotations:\nsummary: &lt;ALERT-SUMMARY-TEXT&gt;\nexpr:  &lt;PROMQL-EXPRESSION&gt;\nfor: &lt;optional: duration s/m/h&gt;\nlabels:\nseverity: &lt;critical/warning&gt;\n</code></pre> <p>You can find an example in the Prometheus documentation here.</p>"},{"location":"admin/runai-setup/maintenance/node-downtime/","title":"Planned and Unplanned Node Downtime","text":""},{"location":"admin/runai-setup/maintenance/node-downtime/#introduction","title":"Introduction","text":"<p>Nodes (Machines) that are part of the cluster are susceptible to occasional downtime. This can be either as part of planned maintenance where we bring down the node for a specified time in an orderly fashion or an unplanned downtime where the machine abruptly stops due to a software or hardware issue.</p> <p>The purpose of this document is to provide a process for retaining the Run:ai service and Researcher workloads during and after the downtime. </p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#self-hosted-installation","title":"Self-hosted installation","text":"<p>The self-hosted installation differs from the Classic (SaaS) installation of Run:ai in that it includes the Run:ai control-plane. The control plane contains data that must be preserved during downtime. As such, you must first follow the disaster recovery planning process. </p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#node-types","title":"Node Types","text":"<p>The document differentiates between Run:ai System Worker Nodes and GPU Worker Nodes:</p> <ul> <li>Worker Nodes - are where Machine Learning workloads run. </li> <li>Run:ai System Nodes - In a production installation Run:ai software runs on one or more Run:ai System Nodes on which the Run:ai software runs. </li> </ul>"},{"location":"admin/runai-setup/maintenance/node-downtime/#worker-nodes","title":"Worker Nodes","text":"<p>Worker Nodes are where machine learning workloads run. Ideally, when a node is down, whether for planned maintenance or due to an abrupt downtime, these workloads should migrate to other available nodes or wait in the queue to be started when possible. </p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#training-vs-interactive","title":"Training vs. Interactive","text":"<p>Run:ai differentiates between Training and Interactive workloads. The key difference at node downtime is that Training workloads will automatically move to a new node while Interactive workloads require a manual process. The manual process is recommended for Training workloads as well, as it hastens the process -- it takes time for Kubernetes to identify that a node is down.</p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#planned-maintenance","title":"Planned Maintenance","text":"<p>Before stopping a Worker node, perform the following: </p> <ul> <li>Stop the Kubernetes scheduler from starting new workloads on the node and drain node from all existing workloads. Workloads will move to other nodes or await on queue for renewed execution:</li> </ul> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute\n</code></pre> <ul> <li> <p>Shut down the node and perform the required maintenance. </p> </li> <li> <p>When done, start the node and then run:</p> </li> </ul> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute-\n</code></pre>"},{"location":"admin/runai-setup/maintenance/node-downtime/#unplanned-downtime","title":"Unplanned Downtime","text":"<ul> <li> <p>If a node has failed and has immediately restarted, all services will automatically start. </p> </li> <li> <p>If a node is to remain down for some time, you will want to drain the node so that workloads will migrate to another node:</p> </li> </ul> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute\n</code></pre> <p>When the node is up again, run: </p> <pre><code>kubectl taint nodes &lt;node-name&gt; runai=drain:NoExecute-\n</code></pre> <ul> <li>If the node is to be permanently shut down, you can remove it completely from Kubernetes. Run:</li> </ul> <pre><code>kubectl delete node &lt;node-name&gt;\n</code></pre> <p>However, if you plan to bring back the node, you will need to rejoin the node into the cluster. See Rejoin.</p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#runai-system-nodes","title":"Run:ai System Nodes","text":"<p>In a production installation, Run:ai software runs on one or more Run:ai system nodes. As a best practice, it's best to have more than one such node so that during planned maintenance or unplanned downtime of a single node, the other node will take over. If a second node does not exist, you will have to designate an arbitrary node on the cluster as a Run:ai system node to complete the process below.</p> <p>Protocols for planned maintenance and unplanned downtime are identical to Worker Nodes. See the section above. </p>"},{"location":"admin/runai-setup/maintenance/node-downtime/#rejoin-a-node-into-the-kubernetes-cluster","title":"Rejoin a Node into the Kubernetes Cluster","text":"<p>To rejoin a node to the cluster follow the following steps:</p> <ul> <li>On the master node, run:</li> </ul> <p><pre><code>kubeadm token create --print-join-command\n</code></pre> * This would output a <code>kubeadm join</code> command. Run the command on the worker node for it to re-join the Kubernetes cluster.  * Verify that the node is joined by running:</p> <pre><code>kubectl get nodes\n</code></pre> <ul> <li>When the machine is up you will need to re-label nodes according to their role</li> </ul>"},{"location":"admin/runai-setup/self-hosted/overview/","title":"Self Hosted Run:ai Installation","text":"<p>The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns.</p> <p>Run:ai self-hosting comes with two variants:</p> Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet  <p>The self-hosted installation is priced differently. For further information please talk to Run:ai sales. </p>"},{"location":"admin/runai-setup/self-hosted/overview/#self-hosting-with-kubernetes-vs-openshift","title":"Self-hosting with Kubernetes vs OpenShift","text":"<p>Kubernetes has many Certified Kubernetes Providers. Run:ai has been installed with a number of those such as Rancher, OpenShift, HPE Ezmeral, and Native Kubernetes. The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections:</p> <ul> <li>OpenShift-based installation. See Run:ai OpenShift installation. The Run:ai operator for OpenShift is certified by Red Hat.</li> <li>Kubernetes-based installation. See Run:ai Kubernetes installation.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/additional-clusters/","title":"Installing additional Clusters","text":"<p>The first Run:ai cluster is typically installed on the same Kubernetes cluster as the Run:ai control plane. Run:ai supports multiple clusters per single control plane. This document is about installing additional clusters on different Kubernetes clusters.</p> <p>The instructions are for Run:ai version 2.8 and up.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/additional-clusters/#installation","title":"Installation","text":"<p>Follow the Run:ai SaaS installation network instructions as describe here.  Specifically:</p> <ol> <li>The Cluster should have a dedicated URL with a trusted certificate.</li> <li>Install NGINX.</li> <li>Create a secret in the Run:ai namespace containing the details of a trusted certificate.  </li> </ol> <p>Create a new cluster and download a values file. Perform the following changes in the file:</p> <ul> <li>Under: <code>runai-operator.config.global</code> set <code>clusterDomain</code> to the domain name of the new cluster.</li> <li>Under <code>runai-operator.config.researcher-service</code> set <code>ingress</code> to <code>true</code>.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/","title":"Install the Run:ai Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/k8s/backend/#create-a-control-plane-configuration","title":"Create a Control Plane Configuration","text":"<p>Create a configuration file to install the Run:ai control plane:</p> ConnectedAirgapped Run:ai 2.7Airgapped Run:ai 2.8 and above <p>Generate a values file by running: <pre><code>runai-adm generate-values \\\n--external-ips &lt;ip&gt; \\ # (1)\n--domain &lt;dns-record&gt; \\ # (2) \n--tls-cert &lt;file-name&gt;  --tls-key &lt;file-name&gt; \\ # (3)  \n--nfs-server &lt;nfs-server-address&gt; --nfs-path &lt;path-in-nfs&gt;  # (4)\n</code></pre></p> <ol> <li>An available, IP Address that is accessible from Run:ai Users' machines. Typically (but not always) the IP of one of the nodes. </li> <li>DNS A record such as <code>runai.&lt;company-name&gt;</code> or similar. The A record should point to the IP address above. </li> <li>TLS certificate and private key for the above domain.</li> <li>NFS server location where Run:ai can create files. For using alternative storage mechanisms see optional values below </li> </ol> <p>Note</p> <p>In cloud environments, the flag <code>--external-ips</code> should contain both the internal and external IPs (comma separated)</p> <p>A file called <code>runai-backend-values.yaml</code> will be created.</p> <p>Generate a values file by running the following under the <code>deploy</code> folder: <pre><code>runai-adm generate-values \\\n--external-ips &lt;ip&gt; \\ # (1)\n--domain &lt;dns-record&gt; \\ # (2) \n--tls-cert &lt;file-name&gt;  --tls-key &lt;file-name&gt; \\ # (3)  \n--nfs-server &lt;nfs-server-address&gt; --nfs-path &lt;path-in-nfs&gt; \\ # (4)\n--airgapped\n</code></pre></p> <ol> <li>An available, IP Address that is accessible from Run:ai Users' machines. Typically (but not always) the IP of one of the nodes. </li> <li>DNS A record such as <code>runai.&lt;company-name&gt;</code> or similar. The A record should point to the IP address above. </li> <li>TLS certificate and private key for the above domain.</li> <li>NFS server location where Run:ai can create files. For using alternative storage mechanisms see optional values below </li> </ol> <p>Ignore the message about a downloaded file.</p> <p>Generate a values file by running the following under the <code>deploy</code> folder: <pre><code>runai-adm generate-values \\\n--external-ips &lt;ip&gt; \\ # (1)\n--domain &lt;dns-record&gt; \\ # (2) \n--tls-cert &lt;file-name&gt;  --tls-key &lt;file-name&gt; \\ # (3)  \n--nfs-server &lt;nfs-server-address&gt; --nfs-path &lt;path-in-nfs&gt; \\ # (4)\n--registry &lt;docker-registry-address&gt; #(5)\n</code></pre></p> <ol> <li>An available, IP Address that is accessible from Run:ai Users' machines. Typically (but not always) the IP of one of the nodes. </li> <li>DNS A record such as <code>runai.&lt;company-name&gt;</code> or similar. The A record should point to the IP address above. </li> <li>TLS certificate and private key for the above domain.</li> <li>NFS server location where Run:ai can create files. For using alternative storage mechanisms see optional values below </li> <li>Docker Registry address in the form of <code>NAME:PORT</code> (do not add <code>https</code>):</li> </ol> <p>Ignore the message about a downloaded file.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#optional-edit-configuration-file","title":"(Optional) Edit Configuration File","text":"<p>There may be cases where you need to change properties in the values file as follows:</p> Key Change Description <code>backend.initTenant.promProxy</code>  and  <code>grafana.datasources.datasources.yaml.datasources.url</code> (Run:ai version 2.8 or lower) When using an existing Prometheus service, replace this URL with the URL of the existing Prometheus service (obtain by running <code>kubectl get svc</code> on the Prometheus namespace) Internal URL to Prometheus server <code>postgresql.persistence</code> PostgreSQL permanent storage via a Persistent Volume You can either use <code>storageClassName</code> to create a PV automatically or set <code>nfs.server</code> and <code>nfs.path</code> to provide the network file storage for the PV. The folder in the path should be pre-created and have full access rights. This key is now covered under the runai-adm flags above <code>nginx-ingress.controller.externalIPs</code> <code>&lt;RUNAI_IP_ADDRESS&gt;</code> IP address allocated for Run:ai. This key is now covered under the runai-adm flags above <code>backend.https</code> Replace <code>key</code> and <code>crt</code> with public and private keys for <code>runai.&lt;company-name&gt;</code>. This key is now covered under the runai-adm flags above <code>thanos.receive.persistence</code> Permanent storage for Run:ai metrics See <code>postgresql.persistence</code> above. Can use the same location. This key is now covered under the runai-adm flags above <code>keycloakx.adminUser</code> User name of the internal identity provider administrator This user is the administrator of Keycloak <code>keycloakx.adminPassword</code> Password of the internal identity provider administrator This password is for the administrator of Keycloak"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#upload-images-airgapped-only","title":"Upload images (Airgapped only)","text":"<p>Upload images to a local Docker Registry. Set the Docker Registry address in the form of <code>NAME:PORT</code> (do not add <code>https</code>):</p> <pre><code>export REGISTRY_URL=&lt;Docker Registry address&gt;\n</code></pre> <p>Run the following script (you must have at least 20GB of free disk space to run): </p> <pre><code>sudo -E ./prepare_installation.sh\n</code></pre> <p>If Docker is configured to run as non-root then <code>sudo</code> is not required.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#install-the-control-plane","title":"Install the Control Plane","text":"<p>Run the helm command below:</p> ConnectedAirgapped <pre><code>helm repo add runai-backend https://backend-charts.storage.googleapis.com\nhelm repo update\nhelm install runai-backend -n runai-backend runai-backend/runai-backend  \\\n    -f runai-backend-values.yaml\n</code></pre> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-backend</code>.</p> <p><pre><code>helm install runai-backend runai-backend-&lt;version&gt;.tgz -n \\\n    runai-backend -f runai-backend-values.yaml \n</code></pre> (replace <code>&lt;version&gt;</code> with the Run:ai control plane version)</p> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#connect-to-runai-user-interface","title":"Connect to Run:ai User Interface","text":"<p>Go to: <code>runai.&lt;company-name&gt;</code>. Log in using the default credentials: User: <code>test@run.ai</code>, Password: <code>Abcd!234</code>. Go to the Users area and change the password. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#optional-enable-forgot-password","title":"(Optional) Enable \"Forgot password\"","text":"<p>To support the \u201cForgot password\u201d functionality, follow the steps below.</p> <ul> <li>Go to <code>runai.&lt;company-name&gt;/auth</code> and Log in. </li> <li>Under <code>Realm settings</code>, select the <code>Login</code> tab and enable the <code>Forgot password</code> feature.</li> <li>Under the <code>Email</code> tab, define an SMTP server, as explained here</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#next-steps","title":"Next Steps","text":"<p>Continue with installing a Run:ai Cluster.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/","title":"Self Hosted installation over Kubernetes - Cluster Setup","text":""},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#prerequisites","title":"Prerequisites","text":"<p>Install prerequisites as per cluster prerequisites document.  </p> Version 2.8 or lower <p>The Run:ai Cluster installation installs Prometheus by default. If your Kubernetes cluster already has Prometheus installed, set the flag <code>kube-prometheus-stack.enabled</code> to <code>false</code>.</p> <p>When using an existing Prometheus installation, you will need to add additional rules to your Prometheus configuration. The rules can be found under <code>deploy/runai-prometheus-rules.yaml</code>.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#customize-installation","title":"Customize Installation","text":"<ul> <li>Perform the cluster installation instructions explained here. </li> <li>(Optional) make the following changes to the configuration file you have downloaded:</li> </ul> Key Default Description <code>runai-operator.config.project-controller.createNamespaces</code> <code>true</code> Set to <code>false</code> if unwilling to provide Run:ai the ability to create namespaces, or would want to create namespaces manually rather than use the Run:ai convention of <code>runai-&lt;PROJECT-NAME&gt;</code>. When set to <code>false</code>, will require an additional manual step when creating new Run:ai Projects. <code>runai-operator.config.project-controller.clusterWideSecret</code> <code>true</code> Set to <code>false</code> if unwilling to provide Run:ai the ability to create Kubernetes Secrets. When not enabled, automatic secret propagation will not be available <code>runai-operator.config.mps-server.enabled</code> <code>false</code> Allow the use of NVIDIA MPS. MPS is useful with Inference workloads. Requires extra cluster permissions  <code>runai-operator.config.runai-container-toolkit.enabled</code> <code>true</code> Controls the usage of Fractions. Requires extra cluster permissions  <code>runai-operator.config.global.runtime</code> <code>docker</code> Defines the container runtime of the cluster (supports <code>docker</code> and <code>containerd</code>). Set to <code>containerd</code> when using Tanzu <code>runai-operator.config.runaiBackend.password</code> Default password already set admin@run.ai password. Need to change only if you have changed the password here <code>runai-operator.config.global.prometheusService.address</code> The address of the default Prometheus Service If you installed your own custom Prometheus Service, add this field with the address <code>kube-prometheus-stack.enabled</code> <code>true</code> (Version 2.8 or lower) Install Prometheus. Set to <code>false</code> if Prometheus is already installed in cluster"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#install-cluster","title":"Install Cluster","text":"<p>Run:</p> ConnectedAirgapped <pre><code>helm repo add runai https://run-ai-charts.storage.googleapis.com\nhelm repo update\n\nhelm install runai-cluster runai/runai-cluster -n runai \\\n    -f runai-&lt;cluster-name&gt;.yaml --create-namespace\n</code></pre> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-cluster</code>.</p> <pre><code>helm install runai-cluster -n runai  \\ \n  runai-cluster-&lt;version&gt;.tgz -f runai-&lt;cluster-name&gt;.yaml --create-namespace\n</code></pre> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation. For more details see Understanding cluster access roles.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/next-steps/","title":"Next Steps","text":"<ul> <li>Create additional I Users.</li> <li>Set up Project-based Researcher Access Control.</li> <li>Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users.</li> <li>Review advanced setup and maintenace scenarios.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/","title":"Self Hosted installation over Kubernetes - Preparations","text":""},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#prerequisites","title":"Prerequisites","text":"<p>See Prerequisites section above.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#prepare-installation-artifacts","title":"Prepare Installation Artifacts","text":""},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#runai-software-files","title":"Run:ai Software Files","text":"<p>SSH into a node with <code>kubectl</code> access to the cluster and <code>Docker</code> installed.</p> ConnectedAirgapped <p>Run the following to enable image download from the Run:ai Container Registry on Google cloud:</p> <pre><code>kubectl create namespace runai-backend\nkubectl apply -f runai-gcr-secret.yaml\n</code></pre> <p>To extract Run:ai files, replace <code>&lt;VERSION&gt;</code> in the command below and run: </p> <pre><code>tar xvf runai-&lt;version&gt;.tar.gz\ncd deploy\n\nkubectl create namespace runai-backend\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#runai-administration-cli","title":"Run:ai Administration CLI","text":"ConnectedAirgapped <p>Install the Run:ai Administrator Command-line Interface by following the steps here.</p> <p>Use the Run:ai Administrator Command-line Interface located in the <code>deploy</code> folder. To allow running the binary, run: </p> <pre><code>chmod +x runai-adm\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#install-helm","title":"Install Helm","text":"<p>If helm v3 does not yet exist on the machine, install it now:</p> ConnectedAirgapped <p>See https://helm.sh/docs/intro/install/ on how to install Helm. Run:ai works with Helm version 3 only (not helm 2).</p> <p>The Helm installation image is under the <code>deploy</code> directory. Run: <pre><code>tar xvf helm-&lt;version&gt;-linux-amd64.tar.gz\nsudo mv linux-amd64/helm /usr/local/bin/\n</code></pre></p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#mark-runai-system-workers","title":"Mark Run:ai System Workers","text":"Version 2.9Version 2.8 or below <p>In previous versions, you set nodes that were dedicated to Run:ai software:</p> <ul> <li>Setting the nodes was mandatory.</li> <li>Run:ai was confined to these nodes. If the selected nodes failed or lacked resources, Run:ai would stop working.  </li> </ul> <p>In version 2.9, </p> <ul> <li>There is no need to set nodes for Run:ai.</li> <li>You can optionally set the Run:ai control plane to run on specific nodes. </li> <li>Run:ai is no longer confined to these notes. Instead, Kubernetes will attempt to schedule Run:ai pods to these nodes. If lacking resources, the Run:ai nodes will move to another, non-labeled node.  </li> </ul> <p>To set system worker nodes run:</p> <pre><code>kubectl label node &lt;NODE-NAME&gt; node-role.kubernetes.io/runai-system=true\n</code></pre> <p>If you want to preserve the previous functionality, please contact Run:ai customer support.</p> <p>Note</p> <p>The new functionality is limited to the Run:ai control plane and has not yet been implemented in the Run:ai cluster. </p> <p>The Run:ai control plane should be installed on a set of dedicated Run:ai system worker nodes rather than GPU worker nodes. To set system worker nodes run:</p> <pre><code>kubectl label node &lt;NODE-NAME&gt; node-role.kubernetes.io/runai-system=true\n</code></pre> <p>To avoid single-point-of-failure issues, we recommend assigning more than one node in production environments. </p> <p>Warning</p> <p>Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).</p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#additional-permissions","title":"Additional Permissions","text":"<p>As part of the installation, you will be required to install the Run:ai Control Plane and Cluster Helm Charts. The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the <code>--dry-run</code> on both helm charts. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#next-steps","title":"Next Steps","text":"<p>Continue with installing the Run:ai Control Plane.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/","title":"Prerequisites","text":"<p>Before proceeding with this document, please review the installation types documentation to understand the difference between air-gapped and connected installations. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#control-plane-and-clusters","title":"Control-plane and clusters","text":"<p>As part of the installation process you will install:</p> <ul> <li>A control-plane managing cluster</li> <li>One or more clusters</li> </ul> <p>Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#hardware-requirements","title":"Hardware Requirements","text":"<p>See Cluster prerequisites hardware requirements.</p> <p>In addition, the control plane installation of Run:ai requires the configuration of Kubernetes Persistent Volumes of a total size of 110GB. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#runai-software","title":"Run:ai Software","text":"ConnectedAirgapped <p>You should receive a file: <code>runai-gcr-secret.yaml</code> from Run:ai Customer Support. The file provides access to the Run:ai Container registry.</p> <p>You should receive a single file <code>runai-&lt;version&gt;.tar</code> from Run:ai customer support</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#runai-software-prerequisites","title":"Run:ai Software Prerequisites","text":""},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#operating-system","title":"Operating System","text":"<p>See Run:ai Cluster prerequisites operating system requirements.</p> <p>The Run:ai control plane operating system prerequisites are identical.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#kubernetes","title":"Kubernetes","text":"<p>See Run:ai Cluster prerequisites Kubernetes requirements.</p> <p>The Run:ai control plane operating system prerequisites are identical.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#nvidia-prerequisites","title":"NVIDIA Prerequisites","text":"<p>See Run:ai Cluster prerequisites NVIDIA requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#prometheus-prerequisites","title":"Prometheus Prerequisites","text":"<p>See Run:ai Cluster prerequisites Prometheus requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Prometheus prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#optional-inference-prerequisites","title":"(Optional) Inference Prerequisites","text":"<p>See Run:ai Cluster prerequisites Inference requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#network-requirements","title":"Network Requirements","text":""},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#ingress-controller","title":"Ingress Controller","text":"Version 2.9Version 2.8 or lower <p>The Run:ai control plane installation assumes an existing installation of NGINX as the ingress controller. You can follow the Run:ai Cluster prerequisites ingress controller installation.</p> <p>The Run:ai controller installs NGINX. Thus, in the typical scenario where the Run:ai control plane is installed together with the first cluster, NGINX need not be installed.</p> <p>If the Run:ai cluster is installed on a separate Kubernetes cluster, follow the Run:ai Cluster prerequisites ingress controller requirements.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#domain-name","title":"Domain name","text":"<p>The Run:ai control plane requires a domain name (FQDN). You must supply a domain name as well as a trusted certificate for that domain. </p> <ul> <li>When installing the first Run:ai cluster on the same Kubernetes cluster as the control plane, the Run:ai cluster URL will be the same as the control-plane URL.</li> <li>When installing the Run:ai cluster on a separate Kubernetes cluster, follow the Run:ai domain name requirements. </li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#installer-machine","title":"Installer Machine","text":"<p>The machine running the installation script (typically the Kubernetes master) must have:</p> <ul> <li>At least 50GB of free space.</li> <li>Docker installed.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#other","title":"Other","text":"<ul> <li>(Airgapped installation only)  Private Docker Registry. Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <code>&lt;REGISTRY_URL&gt;</code>). </li> <li>(Optional) SAML Integration as described under single sign-on. </li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#pre-install-script","title":"Pre-install Script","text":"<p>Once you believe that the Run:ai prerequisites are met, we highly recommend installing and running the Run:ai pre-install diagnostics script. The tool:</p> <ul> <li>Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking.</li> <li>Looks at additional components installed and analyze their relevance to a successful Run:ai installation. </li> </ul> <p>To use the script download the latest version of the script and run:</p> <pre><code>chmod +x preinstall-diagnostics-&lt;platform&gt;\n./preinstall-diagnostics-&lt;platform&gt; --domain &lt;dns-entry&gt;\n</code></pre> <p>If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file <code>runai-preinstall-diagnostics.txt</code> in the current directory and send it to Run:ai technical support. </p> <p>For more information on the script including additional command-line flags, see here.</p>"},{"location":"admin/runai-setup/self-hosted/k8s/project-management/","title":"Self Hosted installation over Kubernetes - Create Projects","text":""},{"location":"admin/runai-setup/self-hosted/k8s/project-management/#introduction","title":"Introduction","text":"<p>The Administrator creates Run:ai Projects via the Run:ai user interface. When enabling Researcher Authentication you also assign users to Projects.</p> <p>Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically:</p> <ol> <li>Creates a namespace by the name of <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Labels the namespace as managed by Run:ai.</li> <li>Provides access to the namespace for Run:ai services.</li> <li>Associates users with the namespace. </li> </ol> <p>This process may need to be altered if,</p> <ul> <li>Researchers already have existing Kubernetes namespaces</li> <li>The organization's Kubernetes namespace naming convention does not allow the <code>runai-</code> prefix. </li> <li>The organization's policy does not allow the automatic creation of namespaces.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/project-management/#process","title":"Process","text":"<p>Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace:</p> <ul> <li>When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag <code>createNamespaces</code> to <code>false</code>.</li> <li>Using the Run:ai User Interface, create a new Project <code>&lt;PROJECT-NAME&gt;</code></li> <li>Assuming an existing namespace <code>&lt;NAMESPACE&gt;</code>, associate it with the Run:ai project by running:</li> </ul> <pre><code>kubectl label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;\n</code></pre> <p>Caution</p> <p>Setting the <code>createNamespaces</code> flag to <code>false</code> moves the responsibility of creating namespaces to match Run:ai Projects to the administrator. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/","title":"Uninstall Run:ai","text":""},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/#uninstall-a-runai-cluster","title":"Uninstall a Run:ai Cluster","text":"<p>To uninstall the cluster see: cluster delete </p>"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/#uninstall-the-runai-control-plane","title":"Uninstall the Run:ai Control Plane","text":"<p>To delete the control plane, run:</p> <pre><code>helm delete runai-backend -n runai-backend\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/","title":"Upgrade Run:ai","text":"<p>Warning</p> <p>The Run:ai data is stored in a Kubernetes persistent volume, depending on how you installed the Run:ai control plane, uninstalling the <code>runai-backend</code> helm chart may delete all your data. </p> <p>The upgrade process described below does not require uninstalling the helm chart. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#preparations","title":"Preparations","text":"ConnectedAirgapped <p>No preparation required.</p> <ul> <li>Ask for a tar file <code>runai-air-gapped-&lt;new-version&gt;.tar</code> from Run:ai customer support. The file contains the new version you want to upgrade to. <code>new-version</code> is the updated version of the Run:ai control plane.</li> <li>Upload the images as described here.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-to-version-29","title":"Upgrade to Version 2.9","text":"<p>Before upgrading the control plane, run: </p> <pre><code>kubectl delete --namespace runai-backend --all \\\ndeployments,statefulset,svc,ing,ServiceAccount\nkubectl delete svc -n kube-system runai-cluster-kube-prometh-kubelet\n</code></pre> <p>Delete all secrets in the <code>runai-backend</code> namespace except the <code>helm</code> secret (the secret of type <code>helm.sh/release.v1</code>).</p> <p>Before version 2.9, the Run:ai installation, by default, included NGINX. It was possible to disable this installation. if NGINX is disabled in your current installation then do not run the following 2 lines. </p> <pre><code>kubectl delete ValidatingWebhookConfiguration runai-backend-nginx-ingress-admission\nkubectl delete ingressclass nginx </code></pre> <p>Next, install NGINX as described here</p> <p>Then upgrade the control plane as described in the next section. </p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-the-control-plane","title":"Upgrade the Control Plane","text":"<p>If you have customized the backend values file in the older version, save it now by running</p> <pre><code>helm get values runai-backend -n runai-backend &gt; old-be-values.yaml\n</code></pre> <p>Generate a new backend values file as described here. Change the new file with the above customization if relevant.</p> <p>Run the helm command below. </p> ConnectedAirgapped <pre><code>helm repo add runai-backend https://backend-charts.storage.googleapis.com\nhelm repo update\nhelm upgrade runai-backend -n runai-backend runai-backend/runai-backend -f runai-backend-values.yaml\n</code></pre> <p><pre><code>helm upgrade runai-backend runai-backend/runai-backend-&lt;version&gt;.tgz -n \\\n    runai-backend  -f runai-backend-values.yaml\n</code></pre> (replace <code>&lt;version&gt;</code> with the control plane version)</p>"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-cluster","title":"Upgrade Cluster","text":"ConnectedAirgapped <p>To upgrade the cluster follow the instructions here.</p> <p><pre><code>kubectl apply -f runai-crds.yaml\nhelm get values runai-cluster -n runai &gt; values.yaml\nhelm upgrade runai-cluster -n runai runai-cluster-&lt;version&gt;.tgz -f values.yaml\n</code></pre> (replace <code>&lt;version&gt;</code> with the cluster version)</p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/","title":"Install the Run:ai Control Plane","text":""},{"location":"admin/runai-setup/self-hosted/ocp/backend/#create-a-control-plane-configuration","title":"Create a Control Plane Configuration","text":"<p>Note</p> <p>The control-plane installation on OpenShift assumes that an identity provider has been configured in OpenShift.</p> <p>Run: <code>oc login</code>. Then create a configuration file to install the Run:ai control plane:</p> ConnectedAirgapped 2.7Airgapped 2.8 and above <p>Generate a values file by running: <pre><code>runai-adm generate-values --openshift \\\n--first-admin &lt;FIRST_ADMIN_USER_OF_RUNAI&gt; # (1)\n</code></pre></p> <ol> <li>Name of the administrator user in the company directory</li> </ol> <p>Generate a values file by running the following under the <code>deploy</code> folder: <pre><code>runai-adm generate-values --openshift  \\\n--first-admin &lt;FIRST_ADMIN_USER_OF_RUNAI&gt; \\ # (1)\n--airgapped\n</code></pre></p> <ol> <li>Name of the administrator user in the company directory</li> </ol> <p>Generate a values file by running the following under the <code>deploy</code> folder: <pre><code>runai-adm generate-values --openshift  \\\n--first-admin &lt;FIRST_ADMIN_USER_OF_RUNAI&gt; \\ # (1)\n--registry &lt;docker-registry-address&gt; # (2)\n</code></pre></p> <ol> <li>Name of the administrator user in the company directory</li> <li>Docker Registry address in the form of <code>NAME:PORT</code> (do not add <code>https</code>):</li> </ol> <p>A file called <code>runai-backend-values.yaml</code> will be created.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#upload-images-airgapped-only","title":"Upload images (Airgapped only)","text":"<p>Upload images to a local Docker Registry. Set the Docker Registry address in the form of <code>NAME:PORT</code> (do not add <code>https</code>):</p> <pre><code>export REGISTRY_URL=&lt;Docker Registry address&gt;\n</code></pre> <p>Run the following script (you must have at least 20GB of free disk space to run): </p> <pre><code>sudo -E ./prepare_installation.sh\n</code></pre> <p>(If docker is configured to run as non-root then <code>sudo</code> is not required).</p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#install-the-control-plane","title":"Install the Control Plane","text":"<p>Run the helm command below:</p> ConnectedAirgapped <pre><code>helm repo add runai-backend https://backend-charts.storage.googleapis.com\nhelm repo update\nhelm install runai-backend -n runai-backend runai-backend/runai-backend \\\n    -f runai-backend-values.yaml \n</code></pre> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-backend</code>.</p> <p><pre><code>helm install runai-backend ./runai-backend-&lt;version&gt;.tgz -n \\\n    runai-backend -f runai-backend-values.yaml \n</code></pre> (replace <code>&lt;version&gt;</code> with the control plane version)</p> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#connect-to-runai-user-interface","title":"Connect to Run:ai User Interface","text":"<p>Run: <code>oc get routes -n runai-backend</code> to find the Run:ai Administration User Interface URL. Log in using the default credentials: User: <code>test@run.ai</code>, Password: <code>Abcd!234</code>. Go to the Users area and change the password. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#next-steps","title":"Next Steps","text":"<p>Continue with installing a Run:ai Cluster.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/","title":"Self-Hosted installation over OpenShift - Cluster Setup","text":""},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#install-nvidia-dependencies","title":"Install NVIDIA Dependencies","text":"<p>Note</p> <p>You must have Cluster Administrator rights to install these dependencies. </p> <p>Before installing Run:ai, you must install NVIDIA software on your OpenShift cluster to enable GPUs.  NVIDIA has provided detailed documentation.  Follow the instructions to install the two operators <code>Node Feature Discovery</code> and <code>NVIDIA GPU Operator</code> from the OpenShift web console. </p> <p>When done, verify that the GPU Operator is installed by running:</p> <pre><code>oc get pods -n nvidia-gpu-operator\n</code></pre> <p>(the GPU Operator namespace may differ in different operator versions).</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#create-openshift-projects","title":"Create OpenShift Projects","text":"<p>Run:ai cluster installation uses several namespaces (or projects in OpenShift terminology). The installation will automatically create the namespaces, but if your organization requires manual creation of namespaces, you must create them before installing:</p> <pre><code>oc new-project runai\noc new-project runai-reservation\noc new-project runai-scale-adjust\n</code></pre> <p>The last namespace (<code>runai-scale-adjust</code>) is only required if the cluster is a cloud cluster and is configured for auto-scaling. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#monitoring-pre-check","title":"Monitoring Pre-check","text":"Version 2.9Version 2.8 or lower <p>Not required</p> <p>Run:ai uses the OpenShift monitoring stack. As such, it requires creating or changing the OpenShift monitoring configuration. Check if a <code>configmap</code> already exists: </p> <pre><code>oc get configmap cluster-monitoring-config -n openshift-monitoring\n</code></pre> <p>If it does,</p> <ol> <li>To the cluster values file, add the flag <code>createOpenshiftMonitoringConfig</code> as described under <code>Cluster Installation</code> below. </li> <li>Post-installation, edit the <code>configmap</code> by running: <code>oc edit configmap cluster-monitoring-config -n openshift-monitoring</code>. Add the following:</li> </ol> <p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: cluster-monitoring-config\nnamespace: openshift-monitoring\ndata:\nconfig.yaml: |\nprometheusK8s:\nscrapeInterval: \"10s\"\nevaluationInterval: \"10s\"\nexternalLabels:\nclusterId: &lt;CLUSTER_ID&gt;\nprometheus: \"\"\nprometheus_replica: \"\"\n</code></pre> For <code>&lt;CLUSTER_ID&gt;</code> use the <code>Cluster UUID</code> field as shown in the Run:ai user interface under the <code>Clusters</code> area.  </p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#cluster-installation","title":"Cluster Installation","text":"<p>Perform the cluster installation instructions explained here. When creating a new cluster, select the OpenShift  target platform.</p> <p>Attention</p> <p>The cluster wizard shows extra commands which are unique to OpenShift. Remember to run them all.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#optional-configuration","title":"Optional configuration","text":"<p>Make the following changes to the configuration file you have downloaded:</p> Key Change Description <code>createOpenshiftMonitoringConfig</code> false see Monitoring Pre-check above. <code>runai-operator.config.project-controller.createNamespaces</code> <code>true</code> Set to <code>false</code> if unwilling to provide Run:ai the ability to create namespaces, or would want to create namespaces manually rather than use the Run:ai convention of <code>runai-&lt;PROJECT-NAME&gt;</code>. When set to <code>false</code>, will require an additional manual step when creating new Run:ai Projects. <code>runai-operator.config.mps-server.enabled</code> Default is <code>false</code> Allow the use of NVIDIA MPS. MPS is useful with Inference workloads. Requires extra permissions <code>runai-operator.config.runai-container-toolkit.enabled</code> Default is <code>true</code> Controls the usage of Fractions. Requires extra permissions <code>runai-operator.config.runaiBackend.password</code> Default password already set admin@run.ai password. Need to change only if you have changed the password here <code>runai-operator.config.global.prometheusService.address</code> The address of the default Prometheus Service If you installed your own custom Prometheus Service, change to its' address <p>Run:</p> ConnectedAirgapped <p>Follow the instructions on the Cluster Wizard</p> <p>Info</p> <p>To install a specific version, add <code>--version &lt;version&gt;</code> to the install command. You can find available versions by running <code>helm search repo -l runai-cluster</code>.</p> <pre><code>oc label ns runai openshift.io/cluster-monitoring=true\noc -n openshift-ingress-operator patch ingresscontroller/default --patch '{\"spec\":{\"routeAdmission\":{\"namespaceOwnership\":\"InterNamespaceAllowed\"}}}' --type=merge\n\nhelm install runai-cluster -n runai  \\ \n  runai-cluster-&lt;version&gt;.tgz -f runai-&lt;cluster-name&gt;.yaml  \n</code></pre> <p>Tip</p> <p>Use the  <code>--dry-run</code> flag to gain an understanding of what is being installed before the actual installation. For more details see understanding cluster access roles.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#connect-runai-to-gpu-operator","title":"Connect Run:ai to GPU Operator","text":"Version 2.9Version 2.8 or lower <p>Not required</p> <p>Locate the name of the GPU operator namespace and run:</p> <pre><code>kubectl patch RunaiConfig runai -n runai -p '{\"spec\": {\"global\": {\"nvidiaDcgmExporter\": {\"namespace\": \"INSERT_NAMESPACE_HERE\"}}}}' --type=\"merge\"\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#optional-prometheus-adapter-for-inference","title":"(Optional) Prometheus Adapter for Inference","text":"<p>The Prometheus adapter is required if you are using Inference workloads and require a custom metric for autoscaling. The following additional steps are required for it to work:</p> <ol> <li>Copy <code>prometheus-adapter-prometheus-config</code> and <code>serving-certs-ca-bundle</code> ConfigMaps from <code>openshift-monitoring</code> namespace to the <code>monitoring</code> namespace <pre><code>kubectl get cm prometheus-adapter-prometheus-config --namespace=openshift-monitoring -o yaml \\\n  | sed 's/namespace: openshift-monitoring/namespace: monitoring/' \\\n  | kubectl create -f -\nkubectl get cm serving-certs-ca-bundle --namespace=openshift-monitoring -o yaml \\\n  | sed 's/namespace: openshift-monitoring/namespace: monitoring/' \\\n  | kubectl create -f -\n</code></pre></li> <li>Allow Prometheus Adapter <code>serviceaccount</code> to create a <code>SecurityContext</code> with RunAsUser 10001: <pre><code>oc adm policy add-scc-to-user anyuid system:serviceaccount:monitoring:runai-cluster-prometheus-adapter\n</code></pre></li> </ol>"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#next-steps","title":"Next Steps","text":"<p>Continue to create Run:ai Projects.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/next-steps/","title":"Next Steps","text":"<ul> <li>Create additional Run:ai Users.</li> <li>Set up Project-based Researcher Access Control.</li> <li>Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users.</li> <li>Review advanced setup and maintenace scenarios.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/","title":"Preparing for a Run:ai OpenShift Installation","text":"<p>The following section provides IT with the information needed to prepare for a Run:ai installation. This includes third-party dependencies which must be met as well as access control that must be granted for Run:ai components. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#create-openshift-projects","title":"Create OpenShift Projects","text":"<p>Run:ai control plane uses a namespace <code>runai-backend</code> (or project in OpenShift terminology). The installation will automatically create the namespace, but if your organization requires manual creation of namespaces, you must create it before installing:</p> <pre><code>oc new-project runai-backend\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#prepare-runai-installation-artifacts","title":"Prepare Run:ai Installation Artifacts","text":""},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#runai-software-files","title":"Run:ai Software Files","text":"<p>SSH into a node with <code>oc</code> access (<code>oc</code> is the OpenShift command line) to the cluster and <code>Docker</code> installed.</p> ConnectedAirgapped <p>Run the following to enable image download from the Run:ai Container Registry on Google cloud:</p> <pre><code>oc apply -f runai-gcr-secret.yaml -n runai-backend\n</code></pre> <p>To extract Run:ai files, replace <code>&lt;VERSION&gt;</code> in the command below and run: </p> <pre><code>tar xvf runai-&lt;version&gt;.tar.gz\ncd deploy\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#runai-administration-cli","title":"Run:ai Administration CLI","text":"ConnectedAirgapped <p>Install the Run:ai Administrator Command-line Interface by following the steps here.</p> <p>Install the Run:ai Administrator Command-line Interface by following the steps here. Use the image under <code>deploy/runai-admin-cli-&lt;version&gt;-linux-amd64.tar.gz</code></p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#install-helm","title":"Install Helm","text":"<p>If helm v3 does not yet exist on the machine, install it now:</p> ConnectedAirgapped <p>See https://helm.sh/docs/intro/install/ on how to install Helm. Run:ai works with Helm version 3 only (not helm 2).</p> <pre><code>tar xvf helm-&lt;version&gt;-linux-amd64.tar.gz\nsudo mv linux-amd64/helm /usr/local/bin/\n</code></pre>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#mark-runai-system-workers","title":"Mark Run:ai System Workers","text":"<p>The Run:ai Control plane should be installed on a set of dedicated Run:ai system worker nodes rather than GPU worker nodes. To set system worker nodes run:</p> <pre><code>oc label node &lt;NODE-NAME&gt; node-role.kubernetes.io/runai-system=true\n</code></pre> <p>To avoid single-point-of-failure issues, we recommend assigning more than one node in production environments. </p> <p>Warning</p> <p>Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).</p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#additional-permissions","title":"Additional Permissions","text":"<p>As part of the installation, you will be required to install the Control plane and Cluster Helm Charts. The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the <code>--dry-run</code> on both helm charts. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#next-steps","title":"Next Steps","text":"<p>Continue with installing the Run:ai Control Plane.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/","title":"Self Hosted installation over OpenShift - Prerequisites","text":"<p>Before proceeding with this document, please review the installation types documentation to understand the difference between air-gapped and connected installations. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#control-plane-and-clusters","title":"Control-plane and clusters","text":"<p>As part of the installation process you will install:</p> <ul> <li>A control-plane managing cluster</li> <li>One or more clusters</li> </ul> <p>Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must. </p> <p>Important</p> <p>In OpenShift environments, adding a cluster connecting to a remote control plane currently requires the assistance of customer support.  </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#hardware-requirements","title":"Hardware Requirements","text":"<p>See Cluster prerequisites hardware requirements.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#runai-software","title":"Run:ai Software","text":"ConnectedAirgapped <p>You should receive a file: <code>runai-gcr-secret.yaml</code> from Run:ai Customer Support. The file provides access to the Run:ai Container registry.</p> <p>You should receive a single file <code>runai-&lt;version&gt;.tar</code> from Run:ai customer support</p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#runai-software-prerequisites","title":"Run:ai Software Prerequisites","text":""},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#operating-system","title":"Operating System","text":"<p>OpenShift has specific operating system requirements that can be found in the RedHat documentation. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#openshift","title":"OpenShift","text":"<p>Run:ai supports OpenShift. Supported versions are 4.8 through 4.11. </p> <ul> <li>OpenShift must be configured with a trusted certificate. Run:ai installation relies on OpenShift to create certificates for subdomains. </li> <li>OpenShift must have a configured identity provider. </li> <li>OpenShift must have Entitlement. Entitlement is the RedHat OpenShift licensing mechanism. Without entitlement, you will not be able to install the NVIDIA drivers used by the GPU Operator. For further information see here, or the equivalent NVIDIA documentation. Entitlement is not required anymore if you are using OpenShift 4.9.9 or above</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#nvidia-prerequisites","title":"NVIDIA Prerequisites","text":"<p>See Run:ai Cluster prerequisites installing NVIDIA dependencies in OpenShift.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites.</p> <p>Information on how to download the GPU Operator for air-gapped installation can be found in the NVIDIA GPU Operator pre-requisites. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#optional-inference-prerequisites","title":"(Optional) Inference Prerequisites","text":"<p>See Run:ai Cluster prerequisites Inference requirements.</p> <p>The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#installer-machine","title":"Installer Machine","text":"<p>The machine running the installation script (typically the Kubernetes master) must have:</p> <ul> <li>At least 50GB of free space.</li> <li>Docker installed.</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#other","title":"Other","text":"<ul> <li>(Airgapped installation only) Private Docker Registry. Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <code>&lt;REGISTRY_URL&gt;</code>). </li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#pre-install-script","title":"Pre-install Script","text":"<p>Once you believe that the Run:ai prerequisites are met, we highly recommend installing and running the Run:ai pre-install diagnostics script. The tool:</p> <ul> <li>Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking.</li> <li>Looks at additional components installed and analyzes their relevancy to a successful Run:ai installation. </li> </ul> <p>To use the script download the latest version of the script and run:</p> <pre><code>chmod +x preinstall-diagnostics-&lt;platform&gt;\n./preinstall-diagnostics-&lt;platform&gt; \n</code></pre> <p>If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file <code>runai-preinstall-diagnostics.txt</code> in the current directory and send it to Run:ai technical support. </p> <p>For more information on the script including additional command-line flags, see here.</p>"},{"location":"admin/runai-setup/self-hosted/ocp/project-management/","title":"Self Hosted installation over OpenShift - Create Projects","text":""},{"location":"admin/runai-setup/self-hosted/ocp/project-management/#introduction","title":"Introduction","text":"<p>The Administrator creates Run:ai Projects via the Run:ai User Interface. When enabling Researcher Authentication you also assign users to Projects.</p> <p>Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically:</p> <ol> <li>Creates a namespace by the name of <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> <li>Labels the namespace as managed by Run:ai.</li> <li>Provides access to the namespace for Run:ai services.</li> <li>Associates users with the namespace. </li> </ol> <p>This process may need to be altered if, </p> <ul> <li>Researchers already have existing Kubernetes namespaces</li> <li>The organization's Kubernetes namespace naming convention does not allow the <code>runai-</code> prefix. </li> <li>The organization's policy does not allow the automatic creation of namespaces</li> </ul>"},{"location":"admin/runai-setup/self-hosted/ocp/project-management/#process","title":"Process","text":"<p>Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace:</p> <ul> <li>When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag <code>createNamespaces</code> to <code>false</code>.</li> <li>Using the Run:ai User Interface, create a new Project <code>&lt;PROJECT-NAME&gt;</code></li> <li>Assuming an existing namespace <code>&lt;NAMESPACE&gt;</code>, associate it with the Run:ai project by running:</li> </ul> <pre><code>oc label ns &lt;NAMESPACE&gt;  runai/queue=&lt;PROJECT_NAME&gt;\n</code></pre> <p>Caution</p> <p>Setting the <code>createNamespaces</code> flag to <code>false</code> moves the responsibility of creating namespaces to match Run:ai Projects to the administrator. </p>"},{"location":"admin/runai-setup/self-hosted/ocp/uninstall/","title":"Uninstall Run:ai","text":"<p>See uninstall section here</p>"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/","title":"Upgrade Run:ai","text":"<p>See the upgrade section here</p>"},{"location":"admin/troubleshooting/cluster-health-check/","title":"Verifying Cluster Health","text":"<p>Following is a set of tests that determine the Run:ai cluster health:</p>"},{"location":"admin/troubleshooting/cluster-health-check/#verify-that-data-is-sent-to-the-cloud","title":"Verify that data is sent to the cloud","text":"<p>Log in to <code>&lt;company-name&gt;.run.ai/dashboards/now</code>.</p> <ul> <li>Verify that all metrics in the overview dashboard are showing. </li> <li>Verify that all metrics are showing in the Nodes view. </li> <li>Go to Projects and create a new Project. Find the new Project using the CLI command: <code>runai list projects</code></li> </ul>"},{"location":"admin/troubleshooting/cluster-health-check/#verify-that-the-runai-services-are-running","title":"Verify that the Run:ai services are running","text":"<p>Run:</p> <p><pre><code>kubectl get pods -n runai\nkubectl get pods -n monitoring\nkibectl get pods -n cert-manager\n</code></pre> Verify that all pods are in <code>Running</code> status and a ready state (1/1 or similar)</p> <p>Run:</p> <pre><code>kubectl get deployments -n runai\n</code></pre> <p>Check that all deployments are in a ready state (1/1)</p> <p>Run:</p> <pre><code>kubectl get daemonset -n runai\n</code></pre> <p>A Daemonset runs on every node. Some of the Run:ai daemon-sets run on all nodes. Others run only on nodes that contain GPUs. Verify that for all daemonsets the desired number is equal to  current and to ready. </p>"},{"location":"admin/troubleshooting/cluster-health-check/#submit-a-job-via-the-command-line-interface","title":"Submit a Job via the command-line interface","text":"<p>Submitting a Job will allow you to verify that the Run:ai scheduling service is in order. </p> <ul> <li>Make sure that the Project you have created has a quota of at least 1 GPU</li> <li>Run:</li> </ul> <pre><code>runai config project &lt;project-name&gt;\nrunai submit -i gcr.io/run-ai-demo/quickstart -g 1\n</code></pre> <ul> <li>Verify that the Job is in a Running state by running: </li> </ul> <pre><code>runai list jobs\n</code></pre> <ul> <li>Verify that the Job is showing in the Jobs area at <code>&lt;company-name&gt;.run.ai/jobs</code>.</li> </ul>"},{"location":"admin/troubleshooting/cluster-health-check/#submit-a-job-via-the-user-interface","title":"Submit a Job via the user interface","text":"<p>Log into the Run:ai user interface, and verify that you have a <code>Researcher</code> or <code>Research Manager</code> role.  Go to the <code>Jobs</code> area. On the top right, press the button to create a Job. Once the form opens -- submit a Job. </p>"},{"location":"admin/troubleshooting/diagnostics/","title":"Diagnostic Tools","text":""},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-the-database-container","title":"Add Verbosity to the Database container","text":"<p>Run:ai Self-hosted installation contains an internal database. To diagnose database issues, you can run the database in debug mode.</p> <p>In the runai-backend-values, search for <code>postgresql</code>. Add: </p> <pre><code>postgresql:\nimage:\ndebug: true\n</code></pre> <p>Re-install the Run:ai control-plane and then review the database logs by running: </p> <pre><code>kubectl logs -n runai-backend runai-postgresql-0\n</code></pre>"},{"location":"admin/troubleshooting/diagnostics/#internal-networking-issues","title":"Internal Networking Issues","text":"<p>Run:ai is based on Kubernetes. Kubernetes runs its own internal subnet with a separate DNS service. If you see in the logs that services have trouble connecting, the problem may reside there.  You can find further information on how to debug Kubernetes DNS here. Specifically, it is useful to start a pod with networking utilities and use it for network resolution:</p> <pre><code>kubectl run -i --tty netutils --image=dersimn/netutils -- bash\n</code></pre>"},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-prometheus","title":"Add Verbosity to Prometheus","text":"<p>Add verbosity to Prometheus by editing RunaiConfig:</p> <pre><code>kubectl edit runaiconfig runai -n runai\n</code></pre> <p>Add a <code>debug</code> log level:</p> <pre><code>prometheus-operator:\nprometheus:\nprometheusSpec:\nlogLevel: debug\n</code></pre> <p>To view logs, run: <pre><code>kubectl logs prometheus-runai-prometheus-operator-prometheus-0 prometheus \\\n      -n monitoring -f --tail 100\n</code></pre></p>"},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-scheduler","title":"Add Verbosity to Scheduler","text":"<p>To view extended logs run:</p> <pre><code>kubectl edit ruaiconfig runai -n runai\n</code></pre> <p>Then under the <code>scheduler</code> section add:</p> <pre><code>runai-scheduler:\n   args:\n     verbosity: 6\n</code></pre> <p>Warning</p> <p>Verbose scheduler logs consume a significant amount of disk space.</p>"},{"location":"admin/troubleshooting/troubleshooting/","title":"Troubleshooting Run:ai","text":""},{"location":"admin/troubleshooting/troubleshooting/#installation","title":"Installation","text":"init-ca pod crashing <p>Symptom:  After installing, when running <code>kubectl get pods -n runai</code> you see a pod named <code>init-ca</code> which has crashed. </p> <p>Root cause: Run:ai installs <code>Cert Manager</code>, but there is an existing cert-manager on the cluster. </p> <p>Review the logs for <code>init-ca</code>. If the logs say: </p> <p><code>Failed to sign certificate: Operation cannot be fulfilled on certificatesigningrequests.certificates.k8s.io \u201crunai-admission-controller.runai\u201d: the object has been modified; please apply your changes to the latest version and try again</code> </p> <p>Resolution: Call Run:ai customer support to understand how to modify the Run:ai installation not to install cert-manager. </p> Upgrade fails with \"Ingress already exists\" <p>Symptom:  The installation fails with error: <code>Error: rendered manifests contain a resource that already exists. Unable to continue with install: IngressClass \"nginx\" in namespace \"\" exists</code></p> <p>Root cause: Run:ai installs <code>NGINX</code>, but there is an existing NGINX on the cluster. </p> <p>Resolution: In the Run:ai cluster YAML file, disable the installation of NGINX by setting:</p> <pre><code>ingress-nginx:\n    enabled: false\n</code></pre>"},{"location":"admin/troubleshooting/troubleshooting/#dashboard-issues","title":"Dashboard Issues","text":"No Metrics are showing on Dashboard <p>Symptom: No metrics are showing on dashboards at <code>https://&lt;company-name&gt;.run.ai/dashboards/now</code></p> <p>Typical root causes:</p> <ul> <li>Firewall-related issues.</li> <li>Internal clock is not synced.</li> <li>Prometheus pods are not running.</li> </ul> <p>Firewall issues</p> <p>Add verbosity to Prometheus as describe here.Verify that there are no errors. If there are connectivity-related errors you may need to:</p> <ul> <li>Check your firewall for outbound connections. See the required permitted URL list in Network requirements.</li> <li>If you need to set up an internet proxy or certificate, please contact Run:ai customer support. </li> </ul> <p>Machine Clocks are not synced</p> <p>Run: <code>date</code> on cluster nodes and verify that date/time is correct.  If not:</p> <ul> <li>Set the Linux time service (NTP).</li> <li>Restart Run:ai services. Depending on the previous time gap between servers, you may need to reinstall the Run:ai cluster</li> </ul> <p>Prometheus pods are not running</p> <p>Run: <code>kubectl get pods -n monitoring -o wide</code></p> <ul> <li>Verify that all pods are running.</li> <li>The default Prometheus installation is not built for high availability. If a node is down, the Prometheus pod may not recover by itself unless manually deleted. Delete the pod to see it start on a different node and consider adding a second replica to Prometheus.</li> </ul> GPU Relates metrics not showing <p>Symptom: GPU-related metrics such as <code>GPU Nodes</code> and <code>Total GPUs</code> are showing zero but other metrics, such as <code>Cluster load</code> are shown.</p> <p>Root cause: An installation issue related to the NVIDIA stack.</p> <p>Resolution: </p> <p>Need to run through the NVIDIA stack and find the issue. The current NVIDIA stack looks as follows:</p> <ol> <li>NVIDIA Drivers (at the OS level, on every node)</li> <li>NVIDIA Docker (extension to Docker, on every node)</li> <li>Kubernetes Node feature discovery (mark node properties)</li> <li>NVIDIA GPU Feature discovery (mark nodes as \u201chaving GPUs\u201d)</li> <li>NVIDIA Device plug-in (Exposes GPUs to Kubernetes)</li> <li>NVIDIA DCGM Exporter (Exposes metrics from GPUs in Kubernetes)</li> </ol> <p>Run:ai requires the installation of the NVIDIA GPU Operator which installs the entire stack above. However, there are two alternative methods for using the operator:</p> <ul> <li>Use the default operator values to install 1 through 6.</li> <li>If  NVIDIA Drivers (#1 above) are already installed on all nodes, use the operator with a flag that disables drivers install. </li> </ul> <p>For more information see Cluster prerequisites.</p> <p>NVIDIA GPU Operator</p> <p>Run: <code>kubectl get pods -n gpu-operator | grep nvidia</code> and verify that all pods are running.</p> <p>Node and GPU feature discovery</p> <p>Kubernetes Node feature discovery identifies and annotates nodes. NVIDIA GPU Feature Discovery identifies and annotates nodes with GPU properties. See that: </p> <ul> <li>All such pods are up.</li> <li>The GPU feature discovery pod is available for every node with a GPU.</li> <li>And finally, when describing nodes, they show an active <code>gpu/nvidia</code> resource.</li> </ul> <p>NVIDIA Drivers</p> <ul> <li>If NVIDIA drivers have been installed on the nodes themselves, ssh into each node and run <code>nvidia-smi</code>. Run <code>sudo systemctl status docker</code> and verify that docker is running. Run <code>nvidia-docker</code> and verify that it is installed and working.  Linux software upgrades may require a node restart.</li> <li>If NVIDIA drivers are installed by the Operator, verify that the NVIDIA driver daemonset has created a pod for each node and that all nodes are running. Review the logs of all such pods. A typical problem may be the driver version which is too advanced for the GPU hardware. You can set the driver version via operator flags. </li> </ul> <p>NVIDIA DCGM Exporter</p> <ul> <li>View the logs of the DCGM exporter pod and verify that no errors are prohibiting the sending of metrics. </li> <li>To validate that the dcgm-exporter exposes metrics, find one of the DCGM Exporter pods and run:</li> </ul> <pre><code>kubectl port-forward &lt;dcgm-exporter-pod-name&gt; 9400:9400\n</code></pre> <p>Then browse to http://localhost:9400/metrics and verify that the metrics have reached the DCGM exporter.</p> <ul> <li>The next step after the DCGM Exporter is <code>Prometheus</code>. To validate that metrics from the DCGM Exporter reach Prometheus, run:</li> </ul> <pre><code>kubectl port-forward svc/runai-cluster-kube-prometh-prometheus -n monitoring 9090:9090\n</code></pre> <p>Then browse to localhost:9090. In the UI, type <code>DCGM_FI_DEV_GPU_UTIL</code> as the metric name, and verify that the metric has reached Prometheus. </p> <p>If the DCGM Exporter is running correctly and exposing metrics, but this metric does not appear in Prometheus, there may be a connectivity issue between these components.</p> Allocation-related metrics not showing <p>Symptom: GPU Allocation-related metrics such as <code>Allocated GPUs</code> are showing zero but other metrics, such as <code>Cluster load</code> are shown.</p> <p>Root cause: The origin of such metrics is the scheduler. </p> <p>Resolution:</p> <ul> <li>Run: <code>kubectl get pods -n runai | grep scheduler</code>. Verify that the pod is running.</li> <li>Review the scheduler logs and look for errors. If such errors exist, contact Run:ai customer support. </li> </ul> All metrics are showing \"No Data\" <p>Symptom: All data on all dashboards is showing the text \"No Data\".</p> <p>Root cause: Internal issue with metrics infrastructure.</p> <p>Resolution: Please contact Run:ai customer support.</p>"},{"location":"admin/troubleshooting/troubleshooting/#authentication-issues","title":"Authentication Issues","text":"After a successful login, you are redirected to the same login page <p>For a self-hosted installation, check Linux clock synchronization as described above. Use the Run:ai pre-install script to test this automatically. </p> Single-sign-on issues <p>For single-sign-on issues, see the troubleshooting section in the single-sign-on configuration document. </p>"},{"location":"admin/troubleshooting/troubleshooting/#user-interface-submit-job-issues","title":"User Interface Submit Job Issues","text":"New Job button is grayed out <p>Symptom: The <code>New Job</code> button on the top right of the Job list is grayed out.</p> <p>Root Cause: This can happen due to multiple configuration issues: </p> <ul> <li>Open Chrome developer tools and refresh the screen.</li> <li>Under <code>Network</code> locate a network call error. Search for the HTTP error code.</li> </ul> <p>Resolution for 401 HTTP Error</p> <ul> <li>The Cluster certificate provided as part of the installation is valid and trusted (not self-signed).</li> <li>Researcher Authentication has not been properly configured. Try running <code>runai login</code> from the Command-line interface. Alternatively, run: <code>kubectl get pods -n kube-system</code>, identify the api-server pod and review its logs. </li> </ul> <p>Resolution for 403 HTTP Error</p> <p>Run: <code>kubectl get pods -n runai</code>, identify the <code>agent</code> pod, see that it's running, and review its logs.</p> New Job button is not showing <p>Symptom: The <code>New Job</code> button on the top right of the Job list does not show.</p> <p>Root Causes: (multiple)</p> <ul> <li>You do not have <code>Researcher</code> or <code>Research Manager</code> permissions.</li> <li>Under <code>Settings | General</code>, verify that <code>Unified UI</code> is on.</li> </ul> Submit form is distorted <p>Symptom: Submit form is showing vertical lines.</p> <p>Root Cause: The control plane does not know the cluster URL.</p> <p>Using the Run:ai user interface, go to the Clusters list. See that there is no cluster URL next to your cluster.</p> <p>Resolution: Cluster must be re-installed. </p> Submit form is not showing after pressing Create Job button <p>Symptom: (SaaS only) Submit form now showing and under Chrome developer tools you see that all network calls with <code>/workload</code> return an error. </p> <p>Root Cause: Multiple network-related issues.</p> <p>Resolution: </p> <ul> <li>Incorrect cluster IP</li> <li> <p>Cluster certificate has not been created</p> <ul> <li>Run: <code>kubectl get certificate -n runai</code>. Verify that all 3 entries are of status <code>Ready</code>.</li> <li>Run: <code>kubectl get pods -n cert-manager</code> and verify that all pods are Running.</li> <li> <p>Run: <code>kubectl get crd | grep cert</code> Verify that there are at least the following 6 entries: <pre><code>certificaterequests.cert-manager.io                   \ncertificates.cert-manager.io                          \nchallenges.acme.cert-manager.io                       \nclusterissuers.cert-manager.io                        \nissuers.cert-manager.io                               \norders.acme.cert-manager.io\n</code></pre></p> </li> <li> <p>Run: <code>kubectl get challenges -A</code> verify that all challenges are in running state. If not, run: <code>kubectl describe challenge &lt;challlenge-name&gt; -n runai</code>. Possible issues are:</p> <ul> <li>Networking issues: most likely a firewall problem.</li> <li><code>Waiting for dns propagation</code>: This is normal. Need to wait.  </li> </ul> </li> </ul> </li> </ul> Submit form does not show the list of Projects <p>Symptom: When connected with Single-sign-on, in the Submit form, the list of Projects is empty.</p> <p>Root Cause:  SSO is on and researcher authentication is not properly configured as such.</p> <p>Resolution: Verify API Server settings as described in Researcher Authentication configuration.</p> Job form is not opening on OpenShift <p>Symptom: When clicking on \"New Job\" the Job forms does not load. Network shows 405</p> <p>Root Cause: An installation step has been missed. </p> <p>Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a <code>patch</code> command at the end of the instruction set. Run it. </p>"},{"location":"admin/troubleshooting/troubleshooting/#networking-issues","title":"Networking Issues","text":"'admission controller' connectivity issue <p>Symptoms:</p> <ul> <li>Pods are failing with 'admission controller' connectivity errors.</li> <li>The command-line <code>runai submit</code> fails with an 'admission controller' connectivity error.</li> <li>Agent or cluster sync pods are crashing in self-hosted installation.</li> </ul> <p>Root cause: Connectivity issues between different nodes in the cluster.</p> <p>Resolution:</p> <ul> <li>Run the preinstall script and search for networking errors.</li> <li>Run: <code>kubectl get pods -n kube-system -o wide</code>. Verify that all networking pods are running. </li> <li>Run: <code>kubectl get nodes</code>. Check that all nodes are ready and connected.</li> <li>Run: <code>kubectl get pods -o wide -A</code> to see which pods are Pending or in Error and which nodes they belong to. </li> <li>See if pods from different nodes have trouble communicating with each other.</li> <li>Advanced, run: <code>kubectl exec &lt;pod-name&gt; -it /bin/sh</code> from a pod in one node and ping a pod from another. </li> </ul> Projects are not syncing <p>Symptom: Create a Project on the Run:ai user interface, then run: <code>runai list projects</code>. The new Project does not appear.</p> <p>Root cause: The Run:ai agent is not syncing properly. This may be due to firewall issues. </p> <p>Resolution</p> <ul> <li>Run: <code>runai pods -n runai | grep agent</code>. See that the agent is in Running state. Select the agent's full name and run: <code>kubectl logs -n runai runai-agent-&lt;id&gt;</code>.</li> <li>Verify that there are no errors. If there are connectivity-related errors you may need to check your firewall for outbound connections. See the required permitted URL list in Network requirements. </li> <li>If you need to set up an internet proxy or certificate, please contact Run:ai customer support. </li> </ul> Jobs are not syncing <p>Symptom: A Job on the cluster (<code>runai list jobs</code>) does not show in the Run:ai user interface Job list. </p> <p>Root cause: The Run:ai cluster-sync pod is not syncing properly.  </p> <p>Resolution: Search the cluster-sync pod for errors.</p>"},{"location":"admin/troubleshooting/troubleshooting/#job-related-issues","title":"Job-related Issues","text":"Jobs fail with ContainerCannotRun status  <p>Symptom: When running <code>runai list jobs</code>, your Job has a status of <code>ContainerCannotRun</code>.</p> <p>Root Cause: The issue may be caused due to an unattended upgrade of the NVIDIA driver.</p> <p>To verify, run: <code>runai describe job &lt;job-name&gt;</code>, and search for an error <code>driver/library version mismatch</code>.</p> <p>Resolution: Reboot the node on which the Job attempted to run.</p> <p>Going forward, we recommend blacklisting NVIDIA driver from unattended-upgrades. You can do that by editing <code>/etc/apt/apt.conf.d/50unattended-upgrades</code>, and adding <code>nvidia-driver-</code> to the <code>Unattended-Upgrade::Package-Blacklist</code> section. It should look something like that:</p> <pre><code>Unattended-Upgrade::Package-Blacklist {\n    // The following matches all packages starting with linux-\n    //  \"linux-\";\n    \"nvidia-driver-\";\n</code></pre>"},{"location":"admin/troubleshooting/troubleshooting/#kubernetes-specific-issues","title":"Kubernetes-specific Issues","text":"Cluster Installation failed on RKE or EKS <p>Symptom: Cluster is not installed. When running <code>kubectl get pods -n runai</code> you see that pod <code>init-ca</code> has not started.</p> <p>Resolution:</p> <p>Perform the required cert-manager steps here.</p>"},{"location":"admin/troubleshooting/troubleshooting/#inference-issues","title":"Inference Issues","text":"New Deployment button is grayed out <p>Symptoms: </p> <ul> <li>The <code>New Deployment</code> button on the top right of the Deployment list is grayed out.</li> <li>Cannot create a deployment via Inference API.</li> </ul> <p>Root Cause: Run:ai Inference prerequisites have not been met.</p> <p>Resolution: Review inference prerequisites and install accordingly.</p> New Deployment button is not showing <p>Symptom: The <code>New Deployment</code> button on the top right of the Deployments list does not show.</p> <p>Root Cause: You do not have <code>ML Engineer</code> permissions.</p> Submitted Deployment remains in Pending state <p>Symptom: A submitted deployment is not running.</p> <p>Root Cause: The patch statement to add the runai-scheduler has not been performed.</p> Some Autoscaling metrics are not working <p>Symptom: Deployments do not autoscale when using metrics other than <code>requests-per-second</code> or <code>concurrency</code>.</p> <p>Root Cause: The horizontal pod autoscaler prerequisite has not been installed. </p> Deployment status is \"Failed\" <p>Symptom: Deployment status is always <code>Failed</code>.</p> <p>Root Cause: (multiple)</p> <ul> <li>Not enough resources in the cluster.</li> <li>Server model command is misconfigured (i.e sleep infinity).</li> <li>Server port is misconfigured. </li> </ul> Deployment does not scale up from zero <p>Symptom: In the Deployment form, when \"Auto-scaling\" is enabled, and \"Minimum Replicas\" is set to zero, the deployment cannot scale up from zero.</p> <p>Root Cause: </p> <ul> <li>Clients are not sending requests.</li> <li>Clients are not using the same port/protocol as the server model.</li> <li>Server model command is misconfigured (i.e sleep infinity).</li> </ul>"},{"location":"admin/troubleshooting/troubleshooting/#command-line-interface-issues","title":"Command-line interface Issues","text":"Unable to install CLI due to certificate errors <p>Symptom: The curl command and download button to download the CLI is not working.</p> <p>Root Cause: The cluster is not accessible from the download location</p> <p>Resolution: </p> <p>Use an alternate method for downloading the CLI. Run:</p> <pre><code>kubectl port-forward -n runai svc/researcher-service 4180\n</code></pre> <p>In another shell, run: <pre><code>wget --content-disposition http://localhost:4180/cli/linux\n</code></pre></p> When running the CLI you get an error: open .../.kube/config.lock: permission denied <p>Symptom: When running any CLI command you get a permission denied error.</p> <p>Root Cause: The user running the CLI does not have read permissions to the <code>.kube</code> directory.</p> <p>Resolution: Change permissions for the directory.</p> When running 'runai logs', the logs are delayed <p>Symptom: Printout from the container is not immediately shown in the log. </p> <p>Root Cause: By default, Python buffers stdout, and stderr, which are not flushed in real-time. This may cause logs to appear sometimes minutes after being buffered.</p> <p>Resolution: Set the env var PYTHONUNBUFFERED to any non-empty string or pass -u to Python. e.g. <code>python -u main.py</code>.</p> CLI does not download properly on OpenShift <p>Symptom: When trying to download the CLI on OpenShift, the <code>wget</code> statement downloads a text file named <code>darwin</code> or <code>linux</code> rather than the binary <code>runai</code>.</p> <p>Root Cause: An installation step has been missed. </p> <p>Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a <code>patch</code> command at the end of the instruction set. Run it. </p>"},{"location":"admin/workloads/inference-overview/","title":"Inference","text":""},{"location":"admin/workloads/inference-overview/#what-is-inference","title":"What is Inference","text":"<p>Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output. </p> <p>With Inference, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time. </p>"},{"location":"admin/workloads/inference-overview/#inference-and-gpus","title":"Inference and GPUs","text":"<p>The Inference process is a subset of the original Training algorithm on a single datum (e.g. one sentence or one image), or a small batch. As such, GPU memory requirements are typically smaller than a full-blown Training process. </p> <p>Given that, Inference lends itself nicely to the usage of Run:ai Fractions. You can, for example, run 4 instances of an Inference server on a single GPU, each employing a fourth of the memory. </p>"},{"location":"admin/workloads/inference-overview/#inference-runai","title":"Inference @Run:ai","text":"<p>Run:ai provides Inference services as an equal part together with the other two Workload types: Train and Build.</p> <ul> <li> <p>Inference is considered a high-priority workload as it is customer-facing. Running an Inference workload (within the Project's quota) will preempt any Run:ai Workload marked as Training.</p> </li> <li> <p>Inference workloads will receive priority over Train and Build workloads during scheduling.</p> </li> <li> <p>Inference is implemented as a Kubernetes Deployment object with a defined number of replicas. The replicas are load-balanced by Kubernetes so adding more replicas will improve the overall throughput of the system.</p> </li> <li> <p>Multiple replicas will appear in Run:ai as a single Inference workload. The workload will appear in all Run:ai dashboards and views as well as the Command-line interface.</p> </li> <li> <p>Inference workloads can be submitted via Run:ai user interface as well as Run:ai API. Internally, spawning an Inference workload also creates a Kubernetes Service. The service is an end-point to which clients can connect. </p> </li> </ul>"},{"location":"admin/workloads/inference-overview/#auto-scaling","title":"Auto Scaling","text":"<p>To withstand SLA, Inference workloads are typically set with auto scaling. Auto-scaling is the ability to add more computing power (Kubernetes pods) when the load increases and shrink allocated resources when the system is idle.</p> <p>There are a number of ways to trigger auto-scaling. Run:ai supports the following:</p> Metric Units Run:ai name GPU Utilization % gpu-utilization CPU Utilization % cpu-utilization Latency milliseconds latency Throughput requests/second throughput Concurrency concurrency Custom metric custom <p>The Minimum and Maximum number of replicas can be configured as part of the autoscaling configuration.</p> <p>Auto Scaling also supports a scale to zero policy with Throughput and Concurrency metrics, meaning that given enough time under the target threshold, the number of replicas will be scaled down to 0. This has the benefit of conserving resources at the risk of a delay from \"cold starting\" the model when traffic resumes. </p>"},{"location":"admin/workloads/inference-overview/#see-also","title":"See Also","text":"<ul> <li>To set up Inference, see Cluster installation prerequisites.</li> <li>For running Inference see Inference quick-start.</li> <li>To run Inference from the user interface see Deployments.</li> <li>To run Inference using API see Workload overview.</li> </ul>"},{"location":"admin/workloads/policies/","title":"Configure Policies","text":""},{"location":"admin/workloads/policies/#what-are-policies","title":"What are Policies?","text":"<p>Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For example:</p> <ol> <li>Restrict researchers from requesting more than 2 GPUs, or less than 1GB of memory for an interactive workload.</li> <li>Set the default memory of each training job to 1GB, or mount a default volume to be used by any submitted Workload.</li> </ol> <p>Policies are stored as Kubernetes custom resources.</p> <p>Policies are specific to Workload type as such there are several kinds of Policies:</p> Workload Type Kubernetes Workload Name Kubernetes Policy Name Interactive <code>InteractiveWorkload</code> <code>InteractivePolicy</code> Training <code>TrainingWorkload</code> <code>TrainingPolicy</code> Inference <code>InferenceWorkload</code> <code>InferencePolicy</code> <p>A Policy can be created per Run:ai Project (Kubernetes namespace). Additionally, a Policy resource can be created in the <code>runai</code> namespace. This special Policy will take effect when there is no project-specific Policy for the relevant workload kind.</p>"},{"location":"admin/workloads/policies/#creating-a-policy","title":"Creating a Policy","text":""},{"location":"admin/workloads/policies/#creating-your-first-policy","title":"Creating your First Policy","text":"<p>To create a sample <code>InteractivePolicy</code>, prepare a file (e.g. <code>policy.yaml</code>) containing the following YAML:</p> gpupolicy.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractivePolicy\nmetadata:\nname: interactive-policy1\nnamespace: runai-team-a # (1)\nspec:\ngpu:\nrules:\nrequired: true\nmin: \"1\"  # (2)\nmax: \"4\"  value: \"1\"\n</code></pre> <ol> <li>Set the Project namespace here.</li> <li>GPU values are quoted as they can contain non-integer values. </li> </ol> <p>The policy places a default and limit on the available values for GPU allocation. To apply this policy, run: </p> <p><pre><code>kubectl apply -f gpupolicy.yaml </code></pre> Now, try the following command: <pre><code>runai submit --gpu 5 --interactive -p team-a\n</code></pre> The following message will appear: <pre><code>gpu: must be no greater than 4\n</code></pre> A similar message will appear in the New Job form of the Run:ai user interface, when attempting to enter the number of GPUs, which is out of range for an Interactive tab.  </p>"},{"location":"admin/workloads/policies/#read-only-values","title":"Read-only values","text":"<p>When you do not want the user to be able to change a value, you can force the corresponding user interface control to become read-only by using the <code>canEdit</code> key. For example, </p> runasuserpolicy.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingPolicy\nmetadata:\nname: train-policy1\nnamespace: runai-team-a # (1) \nspec:\nrunAsUser:\nrules:\nrequired: true  # (2)\ncanEdit: false  # (3)\nvalue: true # (4)\n</code></pre> <ol> <li>Set the Project namespace here.</li> <li>The field is required. </li> <li>The field will be shown as read-only in the user interface. </li> <li>The field value is true.  </li> </ol>"},{"location":"admin/workloads/policies/#complex-values","title":"Complex Values","text":"<p>The example above illustrated rules for parameters of \"primitive\" types, such as GPU allocation, CPU memory, working directory, etc. These parameters contain a single value. </p> <p>Other workload parameters, such as ports or volumes, are \"complex\", in the sense that they may contain multiple values: a workload may contain multiple ports and multiple volumes. </p> <p>Following is an example of a policy containing the value <code>ports</code>, which is complex: The <code>ports</code> flag typically contains two values: The <code>external</code> port that is mapped to an internal <code>container</code> port. One can have multiple port tuples defined for a single Workload:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractivePolicy\nmetadata:\nname: interactive-policy\nnamespace: runai\nspec:\nports:\nrules:\ncanAdd: true\nitemRules:\ncontainer:\nmin: 30000\nmax: 32767\nexternal:\nmax: 32767\nitems:\nadmin-port-a:\nrules:\ncanRemove: false\ncanEdit: false\nvalue:\ncontainer: 30100\nexternal: 8080\nadmin-port-b:\nvalue:\ncontainer: 30101\nexternal: 8081\n</code></pre> <p>A policy for a complex field is composed of three parts:</p> <ul> <li>Rules: Rules apply to the <code>ports</code> parameter as a whole. In this example, the administrator specifies <code>canAdd</code> rule with <code>true</code> value, indicating that a researcher submitting an interactive job can add additional ports to the ports listed by the policy (true is the default for <code>canAdd</code>, so it actually could have been omitted from the policy above). When <code>canAdd</code> is set to <code>false</code>, the researcher will not be able to add any additional port except those already specified by the policy.</li> <li>itemRules: itemRules impose restrictions on the data members of each item, in this case - <code>container</code> and <code>external</code>. In the above example, the administrator has limited the value of <code>container</code> to 30000-32767, and the value of <code>external</code> to a maximum of 32767. </li> <li>Items: Specifies a list of default ports. Each port is an item in the ports list and given a label (e.g. <code>admin-port-b</code>). The administrator can also specify whether a researcher can change/delete ports from the submitted workload. In the above example, <code>admin-port-a</code> is hardwired and cannot be changed or deleted, while <code>admin-port-b</code> can be changed or deleted by the researcher when submitting the Workload.</li> </ul>"},{"location":"admin/workloads/policies/#syntax","title":"Syntax","text":"<p>The complete syntax of the policy YAML can be obtained using the <code>explain</code> command of kubectl. For example:</p> <p><pre><code>kubectl explain trainingpolicy.spec\n</code></pre> Should provide the list of all possible fields in the spec of training policies:</p> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/v2alpha1\nRESOURCE: spec &lt;Object&gt;\nDESCRIPTION:\nThe specifications of this TrainingPolicy\nFIELDS:\nannotations &lt;Object&gt;\nSpecifies annotations to be set in the container running the created\nworkload.\narguments   &lt;Object&gt;\nIf set, the arguments are sent along with the command which overrides the\nimage's entry point of the created workload.\ncommand &lt;Object&gt;\nIf set, overrides the image's entry point with the supplied command.\n...\n</code></pre> <p>You can further drill down to get the syntax for <code>ports</code> by running:</p> <pre><code>kubectl explain trainingpolicy.spec.ports\n</code></pre> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/v2alpha1\nRESOURCE: ports &lt;Object&gt;\nDESCRIPTION:\nSpecify the set of ports exposed from the container running the created\nworkload. Used together with --service-type.\nFIELDS:\nitemRules    &lt;Object&gt;\nitems    &lt;map[string]Object&gt;\nrules    &lt;Object&gt;\nthese rules apply to a value of type map (=non primitive) as a whole\nadditionally there are rules which apply for specific items of the map\n</code></pre> <p>Drill down into the <code>ports.rules</code> object by running:</p> <pre><code>kubectl explain trainingpolicy.spec.ports.rules\n</code></pre> <pre><code>KIND:     TrainingPolicy\nVERSION:  run.ai/\nRESOURCE: rules &lt;Object&gt;\nDESCRIPTION:\nthese rules apply to a value of type map (=non primitive) as a whole\nadditionally there are rules which apply for specific items of the map\nFIELDS:\ncanAdd   &lt;boolean&gt;\nis it allowed for a workload to add items to this map\nrequired &lt;boolean&gt;\nif the map as a whole is required\n</code></pre> <p>Note that each kind of policy has a slightly different set of parameters. For example, an <code>InteractivePolicy</code> has a <code>jupyter</code> parameter that is not available under <code>TrainingPolicy</code>. </p>"},{"location":"admin/workloads/policies/#using-secrets-for-environment-variables","title":"Using Secrets for Environment Variables","text":"<p>It is possible to add values from Kubernetes secrets as the value of environment variables included in the policy. The secret will be extracted from the secret object when the Job is created. For example:</p> <pre><code>  environment:\nitems:\nMYPASSWORD:\nvalue: \"SECRET:my-secret,password\"\n</code></pre> <p>When submitting a workload that is affected by this policy, the created container will have an environment variable called <code>MYPASSWORD</code> whose value is the key <code>password</code> residing in Kubernetes secret <code>my-secret</code> which has been pre-created in the namespace where the workload runs.</p> <p>Note</p> <p>Run:ai provides a secret propagation mechanism from the <code>runai</code> namespace to all project namespaces. For further information see secret propagation</p>"},{"location":"admin/workloads/policies/#modifyingdeleting-policies","title":"Modifying/Deleting Policies","text":"<p>Use the standard kubectl get/apply/delete commands to modify and delete policies.</p> <p>For example, to view the global interactive policy:</p> <p><pre><code>kubectl get interactivepolicies -n runai\n</code></pre> Should return the following: <pre><code>NAME                 AGE\ninteractive-policy   2d3h\n````\nTo delete this policy:\n```bash\nkubectl delete InteractivePolicy interactive-policy -n runai\n</code></pre> To access project-specific policies, replace the <code>-n runai</code> parameter with the namespace of the relevant project.</p>"},{"location":"admin/workloads/policies/#see-also","title":"See Also","text":"<ul> <li>For creating workloads based on policies, see the Run:ai submitting workloads</li> </ul>"},{"location":"admin/workloads/secrets/","title":"Secrets in Workloads","text":""},{"location":"admin/workloads/secrets/#kubernetes-secrets","title":"Kubernetes Secrets","text":"<p>Sometimes you want to use sensitive information within your code. For example passwords, OAuth tokens, or ssh keys. The best practice for saving such information in Kubernetes is via Kubernetes Secrets. Kubernetes Secrets let you store and manage sensitive information. Access to secrets is limited via configuration.</p> <p>A Kubernetes secret may hold multiple key - value pairs.</p>"},{"location":"admin/workloads/secrets/#using-secrets-in-runai-workloads","title":"Using Secrets in Run:ai Workloads","text":"<p>Our goal is to provide Run:ai Workloads with secrets as input in a secure way. Using the Run:ai command line, you will be able to pass a reference to a secret that already exists in Kubernetes. </p>"},{"location":"admin/workloads/secrets/#creating-a-secret","title":"Creating a secret","text":"<p>For details on how to create a Kubernetes secret see: https://kubernetes.io/docs/concepts/configuration/secret/. Here is an example:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: my-secret\nnamespace: runai-&lt;project-name&gt;\ndata:\nusername: am9obgo=\npassword: bXktcGFzc3dvcmQK\n</code></pre> <p>Then run: <pre><code>kubectl apply -f &lt;file-name&gt;\n</code></pre></p> <p>Notes</p> <ul> <li>Secrets are base64 encoded</li> <li>Secrets are stored in the scope of a namespace and will not be accessible from other namespaces. Hence the reference to the Run:ai Project name above. Run:ai provides the ability to propagate secrets throughout all Run:ai Projects. See below.</li> </ul>"},{"location":"admin/workloads/secrets/#attaching-a-secret-to-a-workload-on-submit","title":"Attaching a secret to a Workload on Submit","text":"<p>When you submit a new Workload, you will want to connect the secret to the new Workload. To do that, run:</p> <pre><code>runai submit -e &lt;ENV-VARIABLE&gt;=SECRET:&lt;secret-name&gt;,&lt;secret-key&gt; ....\n</code></pre> <p>For example:</p> <pre><code>runai submit -i ubuntu -e MYUSERNAME=SECRET:my-secret,username\n</code></pre>"},{"location":"admin/workloads/secrets/#secrets-and-projects","title":"Secrets and Projects","text":"<p>As per the note above, secrets are namespace-specific. If your secret relates to all Run:ai Projects, do the following to propagate the secret to all Projects:</p> <ul> <li>Create a secret within the <code>runai</code> namespace.</li> <li>Run the following once to allow Run:ai to propagate the secret to all Run:ai Projects:</li> </ul> <pre><code>runai-adm set secret &lt;secret name&gt; --cluster-wide\n</code></pre> <p>Reminder</p> <p>The Run:ai Administrator CLI can be obtained here.</p> <p>To delete a secret from all Run:ai Projects, run:</p> <pre><code>runai-adm remove secret &lt;secret name&gt; --cluster-wide\n</code></pre>"},{"location":"admin/workloads/secrets/#secrets-and-policies","title":"Secrets and Policies","text":"<p>A Secret can be set at the policy level. For additional information see policies guide</p>"},{"location":"admin/workloads/workload-overview-admin/","title":"Workloads Overview","text":""},{"location":"admin/workloads/workload-overview-admin/#workloads","title":"Workloads","text":"<p>Run:ai schedules Workloads. Run:ai workloads are comprised of:</p> <ul> <li>The Kubernetes object (Job, Deployment, etc) which is used to launch the container, inside which the data science code runs. </li> <li>A set of additional resources that are required to run the Workload. Examples: a service entry point that allows access to the Job, a persistent volume claim to access data on the network, and more. </li> </ul> <p>All of these components are created together and deleted together when the Workload ends. </p> <p>Run:ai currently supports the following Workloads types:</p> Workload Type Kubernetes Name Description Interactive <code>InteractiveWorkload</code> Submit an interactive workload Training <code>TrainingWorkload</code> Submit a training workload Inference <code>InferenceWorkload</code> Submit an inference workload"},{"location":"admin/workloads/workload-overview-admin/#values","title":"Values","text":"<p>A Workload will typically have a list of values (sometimes called flags), such as name, image, and resources. A full list of values is available in the runai-submit Command-line reference.</p>"},{"location":"admin/workloads/workload-overview-admin/#how-to-submit","title":"How to Submit","text":"<p>A Workload can be submitted via various channels:</p> <ul> <li>The Run:ai user interface.</li> <li>The Run:ai command-line interface, via the runai submit command.</li> <li>The Run:ai Cluster API.</li> </ul>"},{"location":"admin/workloads/workload-overview-admin/#workload-policies","title":"Workload Policies","text":"<p>As an administrator, you can set Policies on Workloads.  Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For more information see Workload Policies.</p>"},{"location":"developer/overview-developer/","title":"Overview: Developer Documentation","text":"<p>Developers can access Run:ai through various programmatic interfaces. </p>"},{"location":"developer/overview-developer/#api-architecture","title":"API Architecture","text":"<p>Run:ai is composed of a single, multi-tenant control plane. Each tenant can be connected to one or more GPU clusters. See Run:ai system components for detailed information.</p> <p>Below is a diagram of the Run:ai API Architecture. A developer may:</p> <ol> <li>Access the control plane via the Administrator API.</li> <li>Access any one of the GPU clusters via Cluster API.</li> <li>Access cluster metrics via the Metrics API.  </li> </ol> <p></p>"},{"location":"developer/overview-developer/#administrator-api","title":"Administrator API","text":"<p>Add, delete, modify and list Run:ai meta-data objects such as Projects, Departments, Users, and more. </p> <p>The API is provided as REST and is accessible via the control plane endpoint.  </p> <p>For more information see Administrator REST API. </p>"},{"location":"developer/overview-developer/#cluster-api","title":"Cluster API","text":"<p>Submit and delete Workloads. </p> <p>The API is provided as Kubernetes API.</p> <p>Cluster API is accessible via the GPU cluster itself. As such, multiple clusters may have multiple endpoints.</p> <p>Note</p> <p>The same functionality is also available via the Run:ai Command-line interface. The CLI provides an alternative for automating with shell scripts. </p>"},{"location":"developer/overview-developer/#metrics-api","title":"Metrics API","text":"<p>Retrieve metrics from multiple GPU clusters. </p> <p>See the Metrics API document.</p>"},{"location":"developer/overview-developer/#api-authentication","title":"API Authentication","text":"<p>See API Authentication for information on how to gain authenticated access to Run:ai APIs.</p>"},{"location":"developer/rest-auth/","title":"API Authentication","text":"<p>The following document explains how to authenticate with Run:ai APIs. </p> <p>Run:ai APIs are accessed using bearer tokens. A token can be obtained in several ways:</p> <ul> <li>When logging into the Run:ai user interface, you enter an email and password (or authenticated via single sign-on) which are used to obtain a token.</li> <li>When using the Run:ai command-line, you use a Kubernetes profile and are authenticated by pre-running <code>runai login</code> (or oc login with OpenShift). The command attachs a token to the profile and allows you access to Run:ai functionality.</li> <li>When using Run:ai APIs, you need to create an Application through the Run:ai user interface. The Application is created with specific roles and contains a secret. Using the secret you can obtain a token and use it within subsequent API calls.</li> </ul>"},{"location":"developer/rest-auth/#create-a-client-application","title":"Create a Client Application","text":"<ul> <li>Open the Run:ai Run:ai User Interface.</li> <li>Go to <code>Settings | Application</code> and create a new Application. </li> <li>Set the required roles:<ul> <li>Select <code>Researcher</code> to manipulate Jobs using the Cluster API. To provide access to a specific project, you will also need to go to <code>Application | Projects</code> and provide the Application with access to specific projects. </li> <li>Select <code>Editor</code> to manipulate Projects and Departments using the Administrator REST API. </li> <li>Select <code>Administrator</code> to manipulate Users, Tenant Settings and Clusters using the Administrator REST API.</li> </ul> </li> <li>Copy the <code>&lt;APPLICATION-NAME&gt;</code> and <code>&lt;CLIENT-SECRET&gt;</code> to be used below</li> <li>Go to <code>Settings | General</code>, under <code>Researcher Authentication</code> copy <code>&lt;REALM&gt;</code>.</li> </ul> <p>Important</p> <p>Creating Client Application tokens is only available with SaaS installations where the tenant has been created post-January 2022 or any Self-hosted installation. If you are an administrator but do not see the <code>Settings | Application</code> area, please contact Run:ai customer support.  </p>"},{"location":"developer/rest-auth/#request-an-api-token","title":"Request an API Token","text":"<p>Use the above parameters to get a temporary token to access Run:ai as follows. </p>"},{"location":"developer/rest-auth/#example-command-to-get-an-api-token","title":"Example command to get an API token","text":"<p>Replace <code>&lt;COMPANY-URL&gt;</code> below with  <code>app.run.ai</code> for SaaS installations (not <code>&lt;company&gt;.run.ai</code>) or the Run:ai user interface URL for Self-hosted installations.</p> cURLPython <pre><code>curl -X POST 'https://&lt;COMPANY-URL&gt;/auth/realms/&lt;REALM&gt;/protocol/openid-connect/token' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--data-urlencode 'grant_type=client_credentials' \\\n--data-urlencode 'scope=openid' \\\n--data-urlencode 'response_type=id_token' \\\n--data-urlencode 'client_id=&lt;APPLICATION-NAME&gt;' \\\n--data-urlencode 'client_secret=&lt;CLIENT-SECRET&gt;'\n</code></pre> <pre><code>import http.client\nconn = http.client.HTTPSConnection(\"\")\npayload = \"grant_type=client_credentials&amp;client_id=&lt;APPLICATION-NAME&gt;&amp;client_secret=&lt;CLIENT_SECRET&gt;\"\nheaders = { 'content-type': \"application/x-www-form-urlencoded\" }\nconn.request(\"POST\", \"/&lt;COMPANY-URL&gt;/auth/realms/&lt;REALM&gt;/protocol/openid-connect/token\", payload, headers)\nres = conn.getresponse()\ndata = res.read()\nprint(data.decode(\"utf-8\"))\n</code></pre>"},{"location":"developer/rest-auth/#response","title":"Response","text":"<p>The API response will look as follows: </p> API Response<pre><code>{\n\"access_token\": \"...\", // (1)\n\"expires_in\": 36000,\n....\n\"token_type\": \"bearer\"\n\"id_token\": \"...\"\n}\n</code></pre> <ol> <li>Use the <code>id_token</code> as the Bearer token below.</li> </ol> <p>To call APIs, the application must pass the retrieved <code>access_token</code> as a Bearer token in the Authorization header of your HTTP request.</p> <ul> <li>To retrieve and manipulate Workloads, use the Cluster API. Researcher API works at the cluster level and you will have different endpoints for different clusters. </li> <li>To retrieve and manipulate other metadata objects, use the Administrator REST API. Administrator API works at the control-plane level and you have a single endpoint for all clusters. </li> </ul>"},{"location":"developer/admin-rest-api/overview/","title":"Administrator REST API","text":"<p>The purpose of the Administrator REST API is to provide an easy-to-use programming interface for administrative tasks.</p>"},{"location":"developer/admin-rest-api/overview/#endpoint-url-for-api","title":"Endpoint URL for API","text":"<p>The domain used for Administrator REST APIs is the same domain used to browse for the Run:ai User Interface. Either <code>&lt;company&gt;.run.ai</code>, or <code>app.run.ai</code> for older tenants or a custom URL used for Self-hosted installations.</p>"},{"location":"developer/admin-rest-api/overview/#authentication","title":"Authentication","text":"<ul> <li>Create a Client Application to make API requests. Use the client application and secret, to obtain a time-bound bearer token (<code>&lt;ACCESS-TOKEN&gt;</code>). For details, see Calling REST APIs.</li> <li>Use the token for subsequent API calls. </li> </ul>"},{"location":"developer/admin-rest-api/overview/#example-usage","title":"Example Usage","text":"<p>For example, if you have an Administrator role, you can get a list of clusters by running:</p> cURLPython <pre><code>curl 'https://&lt;COMPANY-URL&gt;/v1/k8s/clusters' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;ACCESS-TOKEN&gt;' \n</code></pre> <pre><code>import http.client\nconn = http.client.HTTPSConnection(\"https://&lt;COMPANY-URL&gt;\")\nheaders = {\n'content-type': \"application/json\",\n'authorization': \"Bearer &lt;ACCESS-TOKEN&gt;\"\n}\nconn.request(\"GET\", \"/v1/k8s/clusters\", headers=headers)\nres = conn.getresponse()\ndata = res.read()\nprint(data.decode(\"utf-8\"))\n</code></pre> <p>(replace <code>&lt;ACCESS-TOKEN&gt;</code> with the bearer token from above).</p> <p>For an additional example, see the following code. It is an example of how to use the Run:ai Administrator REST API to create a User and a Project and set the User to the Project.  </p>"},{"location":"developer/admin-rest-api/overview/#administrator-api-documentation","title":"Administrator API Documentation","text":"<p>The Administrator API provides the developer interfaces for getting and manipulating the Run:ai metadata objects such as Projects, Departments, Clusters, and Users.</p> <p>Detailed API documentation can be found under https://app.run.ai/api/docs. The document uses the Open API specification to describe the API. You can test the API within the document after creating a token.</p> <p>Administrator API Documentation</p>"},{"location":"developer/cluster-api/other-resources/","title":"Support for other Kubernetes Applications","text":""},{"location":"developer/cluster-api/other-resources/#introduction","title":"Introduction","text":"<p>Kubernetes has several built-in resources that encapsulate running Pods. These are called Kubernetes Workloads and should not be confused with Run:ai Workloads. </p> <p>Examples of such resources are a Deployment that manages a stateless application, or a Job that runs tasks to completion. </p> <p>Run:ai natively runs Run:ai Workloads. A Run:ai workload encapsulates all the resources needed to run, creates them, and deletes them together. However, Run:ai, being an open platform allows the scheduling of any Kubernetes Workflow.</p>"},{"location":"developer/cluster-api/other-resources/#how-to","title":"How To","text":"<p>To run Kubernetes Workloads with Run:ai you must add the following to the YAML:</p> <ul> <li>A namespace that is associated with a Run:ai Project.</li> <li>A scheduler name: <code>runai-scheduler</code>.</li> <li>When using Fractions, use a specific syntax for the <code>nvidia/gpu</code> limit.</li> </ul>"},{"location":"developer/cluster-api/other-resources/#example-job","title":"Example: Job","text":"job1.yaml<pre><code>apiVersion: batch/v1\nkind: Job # (1)\nmetadata:\nname: job1\nnamespace: runai-team-a # (2)\nspec:\ntemplate:\nspec:\ncontainers:\n- name: job1-container\nimage: gcr.io/run-ai-demo/quickstart\nresources:\nlimits:\nnvidia.com/gpu: 1 # (4)\nrestartPolicy: Never\nschedulerName: runai-scheduler # (3)\n</code></pre> <ol> <li>This is a Kubernetes Job.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>The job to be scheduled with the Run:ai scheduler. </li> <li>To run with half a GPU replace 1 with \"0.5\" (with apostrophes).</li> </ol> <p>To submit the Job run:</p> <pre><code>kubectl apply -f job1.yaml\n</code></pre> <p>You will be able to see the Job in the Run:ai User interface, including all metrics and lists </p>"},{"location":"developer/cluster-api/other-resources/#example-deployment","title":"Example: Deployment","text":"deployment1.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment # (1)\nmetadata:\nname: inference-1\nnamespace: runai-team-a # (2)\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: inference-1\ntemplate:\nmetadata:\nlabels:\napp: inference-1\nspec:\ncontainers:\n- resources:\nlimits:\nnvidia.com/gpu: 1 # (4)\nimage: runai/example-marian-server\nimagePullPolicy: Always\nname: inference-1\nports:\n- containerPort: 8888\nschedulerName: runai-scheduler # (3)\n---\napiVersion: v1\nkind: Service # (5)\nmetadata:\nlabels:\napp: inference-1\nname: inference-1\nspec:\ntype: ClusterIP\nports:\n- port: 8888\ntargetPort: 8888\nselector:\napp: inference-1\n</code></pre> <ol> <li>This is a Kubernetes Deployment.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>The job to be scheduled with the Run:ai scheduler. </li> <li>To run with half a GPU replace 1 with \"0.5\" (with apostrophes).</li> <li>This example also contains the creation of a service to connect to the deployment. It is not mandatory.   </li> </ol> <p>To submit the Deployment run:</p> <pre><code>kubectl apply -f deployment1.yaml\n</code></pre>"},{"location":"developer/cluster-api/other-resources/#limitations","title":"Limitations","text":"<p>The Run:ai command line interface provides limited support for Kubernetes Workloads.</p>"},{"location":"developer/cluster-api/other-resources/#see-also","title":"See Also","text":"<p>Run:ai has specific integrations with additional third-party tools such as KubeFlow, MLFlow, and more. These integrations use the same instructions as described above. </p>"},{"location":"developer/cluster-api/submit-rest/","title":"Submitting Workloads via HTTP/REST","text":"<p>You can submit Workloads via HTTP calls, using the Kubernetes REST API.</p>"},{"location":"developer/cluster-api/submit-rest/#submit-workload-example","title":"Submit Workload Example","text":"<p>To submit a workload via HTTP, run the following:</p> <pre><code>curl -X POST \\ # (1) \n'https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads' \\ \n--header 'Content-Type: application/yaml' \\\n--header 'Authorization: Bearer &lt;BEARER&gt;' \\  # (2) \n--data-raw 'apiVersion: run.ai/v2alpha1\nkind: TrainingWorkload  # (3)\nmetadata:\n  name: job-1    spec:\n  gpu:\n    value: \"1\"\nimage:\n    value: gcr.io/run-ai-demo/quickstart\n  name:\n    value: job-1  </code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code> or <code>inferenceworkloads</code> according to type.</li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> <li>See Submitting a Workload via YAML for an explanation of the YAML-based workload.</li> </ol> <p>Run: <code>runai list jobs</code> to see the new Workload.</p>"},{"location":"developer/cluster-api/submit-rest/#delete-workload-example","title":"Delete Workload Example","text":"<p>To delete a workload run:</p> <pre><code>curl -X DELETE \\ # (1) \n'https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads/&lt;JOB-NAME&gt;' \\ \n--header 'Content-Type: application/yaml' \\\n--header 'Authorization: Bearer &lt;BEARER&gt;'   # (2)\n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code> or <code>inferenceworkloads</code> according to type.  Replace <code>&lt;JOB-NAME&gt;</code> with the name of the Job. </li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> </ol>"},{"location":"developer/cluster-api/submit-rest/#using-other-programming-languages","title":"Using other Programming Languages","text":"<p>You can use any Kubernetes client library together with the YAML documentation above to submit workloads via other programming languages. For more information see Kubernetes client libraries.</p>"},{"location":"developer/cluster-api/submit-rest/#python-example","title":"Python example","text":"<p>Create the following file and run it via python:</p> create-train.py<pre><code>import json\nimport requests\n# (1)\nurl = \"https://&lt;IP&gt;:6443/apis/run.ai/v2alpha1/namespaces/&lt;PROJECT&gt;/trainingworkloads\"\npayload = json.dumps({\n\"apiVersion\": \"run.ai/v2alpha1\",\n\"kind\": \"TrainingWorkload\",\n\"metadata\": {\n\"name\": \"train1\",\n\"namespace\": \"runai-team-a\"\n},\n\"spec\": {\n\"image\": {\n\"value\": \"gcr.io/run-ai-demo/quickstart\"\n},\n\"name\": {\n\"value\": \"train1\"\n},\n\"gpu\": {\n\"value\": \"1\"\n}\n}\n})\nheaders = {\n'Content-Type': 'application/json',\n'Authorization': 'Bearer &lt;TOKEN&gt;' #(2)\n}\nresponse = requests.request(\"POST\", url, headers=headers, data=payload) # (3)\nprint(json.dumps(json.loads(response.text), indent=4))\n</code></pre> <ol> <li>Replace <code>&lt;IP&gt;</code> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile).  Replace <code>&lt;PROJECT&gt;</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).  Replace <code>trainingworkloads</code> with <code>interactiveworkloads</code> or <code>inferenceworkloads</code> according to type.</li> <li>Add Bearer token. To obtain a Bearer token see API authentication.</li> <li>if you do not have a valid certificate, you can add the flag <code>verify=False</code>.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/","title":"Submitting Workloads via YAML","text":"<p>You can use YAML to submit Workloads directly to Run:ai. Below are examples of how to create training, interactive and inference workloads via YAML.</p>"},{"location":"developer/cluster-api/submit-yaml/#submit-workload-example","title":"Submit Workload Example","text":"<p>Create a file named <code>training1.yaml</code> with the following text:</p> training1.yaml<pre><code>apiVersion: run.ai/v2alpha1\nkind: TrainingWorkload # (1)\nmetadata:\nname: job-1  # (2) \nnamespace: runai-team-a # (3)\nspec:\ngpu:\nvalue: \"1\"\nimage:\nvalue: gcr.io/run-ai-demo/quickstart\nname:\nvalue: job-1 # (4)\n</code></pre> <ol> <li>This is a Training workload.</li> <li>Kubernetes object name. Mandatory, but has no functional significance.</li> <li>Namespace: Replace <code>runai-team-a</code> with the name of the Run:ai namespace for the specific Project (typically <code>runai-&lt;Project-Name&gt;</code>).</li> <li>Job name as appears in Run:ai. Can provide name, or create automatically if name prefix is configured. </li> </ol> <p>Change the namespace and run: <code>kubectl apply -f training1.yaml</code></p> <p>Run: <code>runai list jobs</code> to see the new Workload.</p>"},{"location":"developer/cluster-api/submit-yaml/#delete-workload-example","title":"Delete Workload Example","text":"<p>Run: <code>kubectl delete -f training1.yaml</code> to delete the Workload. </p>"},{"location":"developer/cluster-api/submit-yaml/#creating-a-yaml-syntax-from-a-cli-command","title":"Creating a YAML syntax from a CLI command","text":"<p>An easy way to create a YAML for a workload is to generate it from the <code>runai submit</code> command by using the <code>--dry-run</code> flag. For example, run:</p> <pre><code>runai submit build1 -i ubuntu -g 1 --interactive --dry-run \\\n     -- sleep infinity \n</code></pre> <p>The result will be the following Kubernetes object declaration:</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InteractiveWorkload  # (1)\nmetadata:\ncreationTimestamp: null\nlabels:\nPreviousJob: \"true\"\nname: job-0-2022-05-02t08-50-57\nnamespace: runai-team-a\nspec:\ncommand:\nvalue: sleep infinity\ngpu:\nvalue: \"1\"\nimage:\nvalue: ubuntu\nimagePullPolicy:\nvalue: Always\nname:\nvalue: job-0\n... Additional internal and status properties...\n</code></pre> <ol> <li>This is an Interactive workload.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/#inference-workload-example","title":"Inference Workload Example","text":"<p>Creating an inference workload is similar to the above two examples.</p> <pre><code>apiVersion: run.ai/v2alpha1\nkind: InferenceWorkload\nmetadata:\nname: inference1\nnamespace: runai-team-a\nspec:\nname:\nvalue: inference1\ngpu:\nvalue: \"0.5\"\nimage:\nvalue: \"gcr.io/run-ai-demo/example-triton-server\"\nminScale:\nvalue: 1\nmaxScale:\nvalue: 2\nmetric:\nvalue: concurrency # (1)\ntarget:\nvalue: 80  # (2)\nports:\nitems:\nport1:\nvalue:\ncontainer: 8000\n</code></pre> <ol> <li>Possible metrics can be <code>cpu-utilization</code>, <code>latency</code>, <code>throughput</code>, <code>concurrency</code>, <code>gpu-utilization</code>, <code>custom</code>. Different metrics may require additional installations at the cluster level. </li> <li>Inference requires a port to receive requests.</li> </ol>"},{"location":"developer/cluster-api/submit-yaml/#see-also","title":"See Also","text":"<ul> <li>To understand how to connect to the inference workload, see Inference Quickstart.</li> <li>To learn more about Inference and Run:ai see Inference overview.</li> </ul>"},{"location":"developer/cluster-api/workload-overview-dev/","title":"Workloads Overview","text":""},{"location":"developer/cluster-api/workload-overview-dev/#workloads","title":"Workloads","text":"<p>Run:ai schedules Workloads. Run:ai workloads contain:</p> <ul> <li>The Kubernetes resource (Job, Deployment, etc) that is used to launch the container inside which the data science code runs. </li> <li>A set of additional resources that is required to run the Workload. Examples: a service entry point that allows access to the Job, a persistent volume claim to access data on the network and more. </li> </ul> <p>Run:ai supports the following Workloads types:</p> Workload Type Kubernetes Name Description Interactive <code>InteractiveWorkload</code> Submit an interactive workload Training <code>TrainingWorkload</code> Submit a training workload Inference <code>InferenceWorkload</code> Submit an inference workload"},{"location":"developer/cluster-api/workload-overview-dev/#values","title":"Values","text":"<p>A Workload will typically have a list of values, such as name, image, and resources. A full list of values is available in the runai-submit Command-line reference.  </p> <p>You can also find the exact YAML syntax run:</p> <pre><code>kubectl explain TrainingWorkload.spec\n</code></pre> <p>(and similarly for other Workload types).</p> <p>To get information on a specific value (e.g. <code>node type</code>), you can also run:</p> <pre><code>kubectl explain TrainingWorkload.spec.nodeType\n</code></pre> <p>Result:</p> <pre><code>KIND:     TrainingWorkload\nVERSION:  run.ai/v2alpha1\n\nRESOURCE: nodeType &lt;Object&gt;\n\nDESCRIPTION:\n     Specifies nodes (machines) or a group of nodes on which the workload will\n     run. To use this feature, your Administrator will need to label nodes as\n     explained in the Group Nodes guide at\n     https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag\n     can be used in conjunction with Project-based affinity. In this case, the\n     flag is used to refine the list of allowable node groups set in the\n     Project. For more information consult the Projects guide at\n     https://docs.run.ai/admin/admin-ui-setup/project-setup.\n\nFIELDS:\n   value    &lt;string&gt;\n</code></pre>"},{"location":"developer/cluster-api/workload-overview-dev/#how-to-submit","title":"How to Submit","text":"<p>A Workload can be submitted via various channels:</p> <ul> <li>The Run:ai user interface.</li> <li>The Run:ai command-line interface, via the runai submit command.</li> <li>The Run:ai Cluster API.</li> </ul>"},{"location":"developer/cluster-api/workload-overview-dev/#policies","title":"Policies","text":"<p>An Administrator can set Policies for Workload submission. Policies serve two purposes:</p> <ol> <li>To constrain the values a researcher can specify.</li> <li>To provide default values.</li> </ol> <p>For example, an administrator can,</p> <ul> <li>Set a maximum of 5 GPUs per Workload. </li> <li>Provide a default value of 1 GPU for each container. </li> </ul> <p>Each workload type has a matching kind of workload policy. For example, an <code>InteractiveWorkload</code> has a matching <code>InteractivePolicy</code></p> <p>A Policy of each type can be defined per-project. There is also a global policy that applies to any project that does not have a per-project policy.</p> <p>For further details on policies, see Policies.</p>"},{"location":"developer/cluster-api/reference/inference/","title":"Inference Workload Parameters","text":"<p>Following is a full list of all inference workload parameters. The text below is equivalent to running <code>kubectl explain inferenceworkload.spec</code>. You can also run <code>kubectl explain inferenceworkload.spec.&lt;parameter-name&gt;</code> to see the description of a specific parameter. </p> <pre><code>KIND:     InferenceWorkload\nVERSION:  run.ai/v2alpha1\nRESOURCE: spec &lt;Object&gt;\nDESCRIPTION:\nThe specifications of this workload\nFIELDS:\nannotations  &lt;Object&gt;\nSpecifies annotations to be set in the container that is running the\ncreated workload.\narguments    &lt;Object&gt;\nWhen set,contains the arguments sent along with the command. These override\nthe entry point of the image in the created workload.\nbaseWorkload &lt;string&gt;\nReference to an another workload. When set, this workload inherits its\nvalues from the base workload. Base workload can either reside on the same\nnamespace of this workload (referred to as \"user\" template) or can reside\nin the runai namespace (referred to as a \"global\" template)\ncapabilities &lt;Object&gt;\nThe capabilities field allows adding a set of unix capabilities to the\ncontainer running the workload. Linux capabilities are distinct privileges\ntraditionally associated with superuser which can be independently enabled\nand disabled. For more information see\nhttps://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container\nclass    &lt;Object&gt;\nThe autoscaler class for knative to use\ncommand  &lt;Object&gt;\nIf set, overrides the image's entry point with the supplied command.\ncpu  &lt;Object&gt;\nSpecifies CPU units to allocate for the created workload (0.5, 1, .etc).\nThe workload will receive at least this amount of CPU. Note that the\nworkload will not be scheduled unless the system can guarantee this amount\nof CPUs to the workload.\ncpuLimit &lt;Object&gt;\nSpecifies a limit on the number of CPUs consumed by the workload (0.5, 1,\n.etc). The system guarantees that this workload will not be able to consume\nmore than this amount of CPUs.\ncreateHomeDir    &lt;Object&gt;\nInstructs the system to create a temporary home directory for the user\nwithin the container. Data stored in this directory will not be saved when\nthe container exits. When the runAsUser flag is set to true, this flag will\ndefault to true as well.\nenvironment  &lt;Object&gt;\nSpecifies environment variables to be set in the container running the\ncreated workload.\nexposedUrls  &lt;Object&gt;\nSpecifies a set of exported url (e.g. ingress) from the container running\nthe created workload.\nextendedResources    &lt;Object&gt;\nSpecifies values for extended resources. Extended resources are third-party\ndevices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that\nyou want to allocate to your Job. For more information see:\nhttps://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\ngitSync  &lt;Object&gt;\nSpecifies git repositories to mount into the container running the\nworkload.\ngpu  &lt;Object&gt;\nSpecifies the number on the number of GPUs to allocate for the created\nworkload. The default is no allocated GPUs. The GPU value can be an integer\nor a fraction between 0 and 1.\ngpuLimit &lt;Object&gt;\nSpecifies a limit on the GPUs to allocate for this workload (1G, 20M,\n.etc). Intended to use for Opportunistic jobs (with the smart\nnode-scheduler).\ngpuMemory    &lt;Object&gt;\nSpecifies GPU memory to allocate for the created workload. The workload\nwill receive this amount of memory. Note that the workload will not be\nscheduled unless the system can guarantee this amount of GPU memory to the\nworkload.\nhostIpc  &lt;Object&gt;\nSpecifies that the created workload will use the host's ipc namespace.\nhostNetwork  &lt;Object&gt;\nSpecifies that the created workload will use the host's network stack\ninside its container. For more information see the Docker Run Reference at\nhttps://docs.docker.com/engine/reference/run/\nimage    &lt;Object&gt;\nSpecifies the image to use when creating the container running the\nworkload.\nimagePullPolicy  &lt;Object&gt;\nSpecifies the pull policy of the image when starting a container running\nthe created workload. Options are: always, ifNotPresent, or never. For more\ninformation see: https://kubernetes.io/docs/concepts/containers/images\ningressUrl   &lt;Object&gt;\nThis field is for internal use only.\nisPrivateServiceUrl  &lt;Object&gt;\nConfigure the inference service to be available only on the cluster-local\nnetwork, and not on the public internet\nlabels   &lt;Object&gt;\nSpecifies labels to be set in the container running the created workload.\nlargeShm &lt;Object&gt;\nSpecifies a large /dev/shm device to mount into a container running the\ncreated workload. An shm is a shared file system mounted on RAM.\nmaxScale &lt;Object&gt;\nThe maximum number of replicas to run\nmemory   &lt;Object&gt;\nSpecifies the amount of CPU memory to allocate for this workload (1G, 20M,\n.etc). The workload will receive at least this amount of memory. Note that\nthe workload will not be scheduled unless the system can guarantee this\namount of memory to the workload\nmemoryLimit  &lt;Object&gt;\nSpecifies a limit on the CPU memory to allocate for this workload (1G, 20M,\n.etc). The system guarantees that this workload will not be able to consume\nmore than this amount of memory. The workload will receive an error when\ntrying to allocate more memory than this limit.\nmetric   &lt;Object&gt;\nThe predefined metric to use for autoscaling. Possible values are:\ncpu-utilization, latency, throughput, concurrency, gpu-utilization, custom.\nmetricName   &lt;Object&gt;\nThe exact metric name to use for autoscaling (overrides Metric field)\nmigProfile   &lt;Object&gt;\nSpecifies the memory profile to be used for workload running on NVIDIA\nMulti-Instance GPU (MIG) technology.\nminScale &lt;Object&gt;\nThe minimum number of replicas to run\nmountPropagation &lt;Object&gt;\nAllows for sharing volumes mounted by a container to other containers in\nthe same pod, or even to other pods on the same node. The volume mount will\nreceive all subsequent mounts that are mounted to this volume or any of its\nsubdirectories.\nname &lt;Object&gt;\nThe specific name of the created resource. Either name of namePrefix should\nbe provided, but not both.\nnamePrefix   &lt;Object&gt;\nA prefix used for assigning a name to the created resource. Either name of\nnamePrefix should be provided, but not both.\nnodePool &lt;Object&gt;\nSpecifies a group of nodes (machines) on which the workload will run. To use this feature, your Administrator will need to label nodes and create a node-pool, as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group.\nAdministrator can assign quota for each node-pool in any of the Projects and Departments that the system contains. This flag\ncan optionally be used in conjunction with NodeType and Project-based\naffinity. In this case, the combination of both flags is used to refine\nthe list of allowable nodes from a node-pool which the workload can use to run. For more information see the Projects setup guide at\nhttps://docs.run.ai/admin/admin-ui-setup/project-setup.\nnodeType &lt;Object&gt;\nSpecifies nodes (machines) or a group of nodes on which the workload will\nrun. To use this feature, your Administrator will need to label nodes as\nexplained in the Group Nodes guide at\nhttps://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag\ncan be used in conjunction with Project-based affinity. In this case, the\nflag is used to refine the list of allowable node groups set in the\nProject. For more information see the Projects setup guide at\nhttps://docs.run.ai/admin/admin-ui-setup/project-setup.\nports    &lt;Object&gt;\nSpecifies a set of ports exposed from the container running the created\nworkload. Used together with --service-type.\npvcs &lt;Object&gt;\nSpecifies persistent volume claims to mount into a container running the\ncreated workload.\nrunAsGid &lt;Object&gt;\nSpecifies the Unix group id with which the container should run. Will be\nused only if runAsUser is set to true.\nrunAsUid &lt;Object&gt;\nSpecifies the Unix user id with which the container running the created\nworkload should run. Will be used only if runAsUser is set to true.\nrunAsUser    &lt;Object&gt;\nLimits the container running the created workload to run in the context of\na specific non-root user. The user id is provided by the runAsUid field.\nThis would manifest itself in access to operating system resources, in the\nownership of new folders created under shared directories, etc.\nAlternatively, if your cluster is connected to Run:ai via SAML, you can map\nthe container to use the Linux UID/GID which is stored in the\norganization's directory. For more information see the User Identity guide\nat https://docs.run.ai/admin/runai-setup/config/non-root-containers/\ns3   &lt;Object&gt;\nSpecifies S3 buckets to mount into the container running the workload\nserviceType  &lt;Object&gt;\nSpecifies the default service exposure method for ports. The default shall\nbe used for ports which do not specify service type. Options are:\nLoadBalancer, NodePort or ClusterIP. For more information see the External\nAccess to Containers guide on\nhttps://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/\nstdin    &lt;Object&gt;\nInstructs the system to keep stdin open for the container(s) running the\ncreated workload, even if nothing is attached.\nsupplementalGroups   &lt;Object&gt;\n';' separated list of supplemental group IDs. Will be added to the security\ncontext of the container running the created workload.\ntarget   &lt;Object&gt;\nThe target value for the autoscaling metric\ntolerations  &lt;Object&gt;\nToleration rules which apply to the pods running the workload. Toleration\nrules guide (but do not require) the system to which node each pod can be\nscheduled to or evicted from, based on matching between those rules and the\nset of taints defined for each Kubernetes node.\ntty  &lt;Object&gt;\nInstructs the system to allocate a pseudo-TTY for the created workload.\nusage    &lt;string&gt;\nThe intended usage of this workload. possible values are \"Template\": this\nworkload is used as the base for other workloads. \"Submit\": this workload\nis used for submitting a job and/or other Kubernetes resources.\nusername &lt;Object&gt;\nDisplay-only field describing the user who owns the workload. The data is\nnot used for authentication or authorization purposes.\nvolumes  &lt;Object&gt;\nSpecifies volumes to mount into a container running the created workload.\nworkingDir   &lt;Object&gt;\nSpecifies a directory that will be used as the current directory when the\ncontainer running the created workload starts.\n</code></pre>"},{"location":"developer/cluster-api/reference/interactive/","title":"Interactive Workload Parameters","text":"<p>Following is a full list of all interactive workload parameters. The text below is equivalent to running <code>kubectl explain interactiveworkload.spec</code>. You can also run <code>kubectl explain interactiveworkload.spec.&lt;parameter-name&gt;</code> to see the description of a specific parameter. </p> <pre><code>KIND:     InteractiveWorkload\nVERSION:  run.ai/v2alpha1\nRESOURCE: spec &lt;Object&gt;\nDESCRIPTION:\nThe specifications of this InteractiveWorkload\nFIELDS:\nallowPrivilegeEscalation &lt;Object&gt;\nAllow the container running the workload and all launched processes to gain\nadditional privileges after the workload starts. For more information see\nthe \"User Identity in Container\" guide at\nhttps://docs.run.ai/admin/runai-setup/config/non-root-containers/\nannotations  &lt;Object&gt;\nSpecifies annotations to be set in the container that is running the\ncreated workload.\narguments    &lt;Object&gt;\nWhen set,contains the arguments sent along with the command. These override\nthe entry point of the image in the created workload.\nbaseWorkload &lt;string&gt;\nReference to an another workload. When set, this workload inherits its\nvalues from the base workload. Base workload can either reside on the same\nnamespace of this workload (referred to as \"user\" template) or can reside\nin the runai namespace (referred to as a \"global\" template)\ncapabilities &lt;Object&gt;\nThe capabilities field allows adding a set of unix capabilities to the\ncontainer running the workload. Linux capabilities are distinct privileges\ntraditionally associated with superuser which can be independently enabled\nand disabled. For more information see\nhttps://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container\ncommand  &lt;Object&gt;\nIf set, overrides the image's entry point with the supplied command.\ncpu  &lt;Object&gt;\nSpecifies CPU units to allocate for the created workload (0.5, 1, .etc).\nThe workload will receive at least this amount of CPU. Note that the\nworkload will not be scheduled unless the system can guarantee this amount\nof CPUs to the workload.\ncpuLimit &lt;Object&gt;\nSpecifies a limit on the number of CPUs consumed by the workload (0.5, 1,\n.etc). The system guarantees that this workload will not be able to consume\nmore than this amount of CPUs.\ncreateHomeDir    &lt;Object&gt;\nInstructs the system to create a temporary home directory for the user\nwithin the container. Data stored in this directory will not be saved when\nthe container exits. When the runAsUser flag is set to true, this flag will\ndefault to true as well.\nenvironment  &lt;Object&gt;\nSpecifies environment variables to be set in the container running the\ncreated workload.\nexposedUrls  &lt;Object&gt;\nSpecifies a set of exported url (e.g. ingress) from the container running\nthe created workload.\nextendedResources    &lt;Object&gt;\nSpecifies values for extended resources. Extended resources are third-party\ndevices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that\nyou want to allocate to your Job. For more information see:\nhttps://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\ngitSync  &lt;Object&gt;\nSpecifies git repositories to mount into the container running the\nworkload.\ngpu  &lt;Object&gt;\nSpecifies the number on the number of GPUs to allocate for the created\nworkload. The default is no allocated GPUs. The GPU value can be an integer\nor a fraction between 0 and 1.\ngpuLimit &lt;Object&gt;\nSpecifies a limit on the GPUs to allocate for this workload (1G, 20M,\n.etc). Intended to use for Opportunistic jobs (with the smart\nnode-scheduler).\ngpuMemory    &lt;Object&gt;\nSpecifies GPU memory to allocate for the created workload. The workload\nwill receive this amount of memory. Note that the workload will not be\nscheduled unless the system can guarantee this amount of GPU memory to the\nworkload.\nhostIpc  &lt;Object&gt;\nSpecifies that the created workload will use the host's ipc namespace.\nhostNetwork  &lt;Object&gt;\nSpecifies that the created workload will use the host's network stack\ninside its container. For more information see the Docker Run Reference at\nhttps://docs.docker.com/engine/reference/run/\nimage    &lt;Object&gt;\nSpecifies the image to use when creating the container running the\nworkload.\nimagePullPolicy  &lt;Object&gt;\nSpecifies the pull policy of the image when starting a container running\nthe created workload. Options are: always, ifNotPresent, or never. For more\ninformation see: https://kubernetes.io/docs/concepts/containers/images\ningressUrl   &lt;Object&gt;\nThis field is for internal use only.\njupyter  &lt;Object&gt;\nIndication if an interactive workload should run jupyter notebook\nlabels   &lt;Object&gt;\nSpecifies labels to be set in the container running the created workload.\nlargeShm &lt;Object&gt;\nSpecifies a large /dev/shm device to mount into a container running the\ncreated workload. An shm is a shared file system mounted on RAM.\nmemory   &lt;Object&gt;\nSpecifies the amount of CPU memory to allocate for this workload (1G, 20M,\n.etc). The workload will receive at least this amount of memory. Note that\nthe workload will not be scheduled unless the system can guarantee this\namount of memory to the workload\nmemoryLimit  &lt;Object&gt;\nSpecifies a limit on the CPU memory to allocate for this workload (1G, 20M,\n.etc). The system guarantees that this workload will not be able to consume\nmore than this amount of memory. The workload will receive an error when\ntrying to allocate more memory than this limit.\nmigProfile   &lt;Object&gt;\nSpecifies the memory profile to be used for workload running on NVIDIA\nMulti-Instance GPU (MIG) technology.\nmountPropagation &lt;Object&gt;\nAllows for sharing volumes mounted by a container to other containers in\nthe same pod, or even to other pods on the same node. The volume mount will\nreceive all subsequent mounts that are mounted to this volume or any of its\nsubdirectories.\nmpi  &lt;Object&gt;\nThis workload produces mpijob\nname &lt;Object&gt;\nThe specific name of the created resource. Either name of namePrefix should\nbe provided, but not both.\nnamePrefix   &lt;Object&gt;\nA prefix used for assigning a name to the created resource. Either name of\nnamePrefix should be provided, but not both.\nnodePool &lt;Object&gt;\nSpecifies a group of nodes (machines) on which the workload will run. To use this feature, your Administrator will need to label nodes and create a node-pool, as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group.\nAdministrator can assign quota for each node-pool in any of the Projects and Departments that the system contains. This flag\ncan optionally be used in conjunction with NodeType and Project-based\naffinity. In this case, the combination of both flags is used to refine\nthe list of allowable nodes from a node-pool which the workload can use to run. For more information see the Projects setup guide at\nhttps://docs.run.ai/admin/admin-ui-setup/project-setup.\nnodeType &lt;Object&gt;\nSpecifies nodes (machines) or a group of nodes on which the workload will\nrun. To use this feature, your Administrator will need to label nodes as\nexplained in the Group Nodes guide at\nhttps://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag\ncan be used in conjunction with Project-based affinity. In this case, the\nflag is used to refine the list of allowable node groups set in the\nProject. For more information see the Projects setup guide at\nhttps://docs.run.ai/admin/admin-ui-setup/project-setup.\nnotebookToken    &lt;Object&gt;\nports    &lt;Object&gt;\nSpecifies a set of ports exposed from the container running the created\nworkload. Used together with --service-type.\npreemptible  &lt;Object&gt;\nSpecifies that the created workload will be preemptible. Interactive\npreemptible workloads can be scheduled above the guaranteed quota but may\nbe reclaimed at any time.\nprocesses    &lt;Object&gt;\nNumber of distributed training processes that will be allocated for the\ncreated mpijob.\npvcs &lt;Object&gt;\nSpecifies persistent volume claims to mount into a container running the\ncreated workload.\nrunAsGid &lt;Object&gt;\nSpecifies the Unix group id with which the container should run. Will be\nused only if runAsUser is set to true.\nrunAsUid &lt;Object&gt;\nSpecifies the Unix user id with which the container running the created\nworkload should run. Will be used only if runAsUser is set to true.\nrunAsUser    &lt;Object&gt;\nLimits the container running the created workload to run in the context of\na specific non-root user. The user id is provided by the runAsUid field.\nThis would manifest itself in access to operating system resources, in the\nownership of new folders created under shared directories, etc.\nAlternatively, if your cluster is connected to Run:ai via SAML, you can map\nthe container to use the Linux UID/GID which is stored in the\norganization's directory. For more information see the User Identity guide\nat https://docs.run.ai/admin/runai-setup/config/non-root-containers/\ns3   &lt;Object&gt;\nSpecifies S3 buckets to mount into the container running the workload\nserviceType  &lt;Object&gt;\nSpecifies the default service exposure method for ports. The default shall\nbe used for ports which do not specify service type. Options are:\nLoadBalancer, NodePort or ClusterIP. For more information see the External\nAccess to Containers guide on\nhttps://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/\nslotsPerWorker   &lt;Object&gt;\nNumber of slots to allocate per worker in the created mpijob.\nstdin    &lt;Object&gt;\nInstructs the system to keep stdin open for the container(s) running the\ncreated workload, even if nothing is attached.\nsupplementalGroups   &lt;Object&gt;\n';' separated list of supplemental group IDs. Will be added to the security\ncontext of the container running the created workload.\ntensorboard  &lt;Object&gt;\nIndicates that this interactive workload should also run a TensorBoard\ndashboard\ntensorboardLogdir    &lt;Object&gt;\nThe TensorBoard Logs directory\ntolerations  &lt;Object&gt;\nToleration rules which apply to the pods running the workload. Toleration\nrules guide (but do not require) the system to which node each pod can be\nscheduled to or evicted from, based on matching between those rules and the\nset of taints defined for each Kubernetes node.\ntty  &lt;Object&gt;\nInstructs the system to allocate a pseudo-TTY for the created workload.\nusage    &lt;string&gt;\nThe intended usage of this workload. possible values are \"Template\": this\nworkload is used as the base for other workloads. \"Submit\": this workload\nis used for submitting a job and/or other Kubernetes resources.\nusername &lt;Object&gt;\nDisplay-only field describing the user who owns the workload. The data is\nnot used for authentication or authorization purposes.\nvolumes  &lt;Object&gt;\nSpecifies volumes to mount into a container running the created workload.\nworkingDir   &lt;Object&gt;\nSpecifies a directory that will be used as the current directory when the\ncontainer running the created workload starts.\n</code></pre>"},{"location":"developer/cluster-api/reference/training/","title":"Training Workload Parameters","text":"<p>Following is a full list of all training workload parameters. The text below is equivalent to running <code>kubectl explain trainingworkload.spec</code>. You can also run <code>kubectl explain trainingworkload.spec.&lt;parameter-name&gt;</code> to see the description of a specific parameter. </p> <pre><code>KIND:     TrainingWorkload\nVERSION:  run.ai/v2alpha1\nRESOURCE: spec &lt;Object&gt;\nDESCRIPTION:\nThe specifications of this TrainingWorkload\nFIELDS:\nallowPrivilegeEscalation &lt;Object&gt;\nAllow the container running the workload and all launched processes to gain\nadditional privileges after the workload starts. For more information see\nthe \"User Identity in Container\" guide at\nhttps://docs.run.ai/admin/runai-setup/config/non-root-containers/\nannotations  &lt;Object&gt;\nSpecifies annotations to be set in the container that is running the\ncreated workload.\narguments    &lt;Object&gt;\nWhen set,contains the arguments sent along with the command. These override\nthe entry point of the image in the created workload.\nbackoffLimit &lt;Object&gt;\nSpecifies the number of retries before marking a workload as failed.\nDefaults to 6\nbaseWorkload &lt;string&gt;\nReference to an another workload. When set, this workload inherits its\nvalues from the base workload. Base workload can either reside on the same\nnamespace of this workload (referred to as \"user\" template) or can reside\nin the runai namespace (referred to as a \"global\" template)\ncapabilities &lt;Object&gt;\nThe capabilities field allows adding a set of unix capabilities to the\ncontainer running the workload. Linux capabilities are distinct privileges\ntraditionally associated with superuser which can be independently enabled\nand disabled. For more information see\nhttps://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container\ncommand  &lt;Object&gt;\nIf set, overrides the image's entry point with the supplied command.\ncompletions  &lt;Object&gt;\nUsed with Hyperparameter Optimization. Specifies the number of successful\npods the job should reach to be completed. The Job will be marked as\nsuccessful once the specified amount of pods has succeeded. The default\nvalue for 'completions' is 1. The 'parallelism' flag should be smaller or\nequal to 'completions' For more information see:\nhttps://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/\ncpu  &lt;Object&gt;\nSpecifies CPU units to allocate for the created workload (0.5, 1, .etc).\nThe workload will receive at least this amount of CPU. Note that the\nworkload will not be scheduled unless the system can guarantee this amount\nof CPUs to the workload.\ncpuLimit &lt;Object&gt;\nSpecifies a limit on the number of CPUs consumed by the workload (0.5, 1,\n.etc). The system guarantees that this workload will not be able to consume\nmore than this amount of CPUs.\ncreateHomeDir    &lt;Object&gt;\nInstructs the system to create a temporary home directory for the user\nwithin the container. Data stored in this directory will not be saved when\nthe container exits. When the runAsUser flag is set to true, this flag will\ndefault to true as well.\nenvironment  &lt;Object&gt;\nSpecifies environment variables to be set in the container running the\ncreated workload.\nexposedUrls  &lt;Object&gt;\nSpecifies a set of exported url (e.g. ingress) from the container running\nthe created workload.\nextendedResources    &lt;Object&gt;\nSpecifies values for extended resources. Extended resources are third-party\ndevices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that\nyou want to allocate to your Job. For more information see:\nhttps://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\ngitSync  &lt;Object&gt;\nSpecifies git repositories to mount into the container running the\nworkload.\ngpu  &lt;Object&gt;\nSpecifies the number on the number of GPUs to allocate for the created\nworkload. The default is no allocated GPUs. The GPU value can be an integer\nor a fraction between 0 and 1.\ngpuLimit &lt;Object&gt;\nSpecifies a limit on the GPUs to allocate for this workload (1G, 20M,\n.etc). Intended to use for Opportunistic jobs (with the smart\nnode-scheduler).\ngpuMemory    &lt;Object&gt;\nSpecifies GPU memory to allocate for the created workload. The workload\nwill receive this amount of memory. Note that the workload will not be\nscheduled unless the system can guarantee this amount of GPU memory to the\nworkload.\nhostIpc  &lt;Object&gt;\nSpecifies that the created workload will use the host's ipc namespace.\nhostNetwork  &lt;Object&gt;\nSpecifies that the created workload will use the host's network stack\ninside its container. For more information see the Docker Run Reference at\nhttps://docs.docker.com/engine/reference/run/\nimage    &lt;Object&gt;\nSpecifies the image to use when creating the container running the\nworkload.\nimagePullPolicy  &lt;Object&gt;\nSpecifies the pull policy of the image when starting a container running\nthe created workload. Options are: always, ifNotPresent, or never. For more\ninformation see: https://kubernetes.io/docs/concepts/containers/images\ningressUrl   &lt;Object&gt;\nThis field is for internal use only.\nlabels   &lt;Object&gt;\nSpecifies labels to be set in the container running the created workload.\nlargeShm &lt;Object&gt;\nSpecifies a large /dev/shm device to mount into a container running the\ncreated workload. An shm is a shared file system mounted on RAM.\nmemory   &lt;Object&gt;\nSpecifies the amount of CPU memory to allocate for this workload (1G, 20M,\n.etc). The workload will receive at least this amount of memory. Note that\nthe workload will not be scheduled unless the system can guarantee this\namount of memory to the workload\nmemoryLimit  &lt;Object&gt;\nSpecifies a limit on the CPU memory to allocate for this workload (1G, 20M,\n.etc). The system guarantees that this workload will not be able to consume\nmore than this amount of memory. The workload will receive an error when\ntrying to allocate more memory than this limit.\nmigProfile   &lt;Object&gt;\nSpecifies the memory profile to be used for workload running on NVIDIA\nMulti-Instance GPU (MIG) technology.\nmountPropagation &lt;Object&gt;\nAllows for sharing volumes mounted by a container to other containers in\nthe same pod, or even to other pods on the same node. The volume mount will\nreceive all subsequent mounts that are mounted to this volume or any of its\nsubdirectories.\nmpi  &lt;Object&gt;\nThis workload produces mpijob\nname &lt;Object&gt;\nThe specific name of the created resource. Either name of namePrefix should\nbe provided, but not both.\nnamePrefix   &lt;Object&gt;\nA prefix used for assigning a name to the created resource. Either name of\nnamePrefix should be provided, but not both.\nnodePool &lt;Object&gt;\nSpecifies a group of nodes (machines) on which the workload will run. To use this feature, your Administrator will need to label nodes and create a node-pool, as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group.\nAdministrator can assign quota for each node-pool in any of the Projects and Departments that the system contains. This flag\ncan optionally be used in conjunction with NodeType and Project-based\naffinity. In this case, the combination of both flags is used to refine\nthe list of allowable nodes from a node-pool which the workload can use to run. For more information see the Projects setup guide at\nhttps://docs.run.ai/admin/admin-ui-setup/project-setup.\nnodeType &lt;Object&gt;\nSpecifies nodes (machines) or a group of nodes on which the workload will\nrun. To use this feature, your Administrator will need to label nodes as\nexplained in the Group Nodes guide at\nhttps://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag\ncan be used in conjunction with Project-based affinity. In this case, the\nflag is used to refine the list of allowable node groups set in the\nProject. For more information see the Projects setup guide at\nhttps://docs.run.ai/admin/admin-ui-setup/project-setup.\nparallelism  &lt;Object&gt;\nSpecifies the maximum desired number of pods the workload should run at any\ngiven time. The actual number of pods running in a steady state will be\nless than this number when ((.spec.completions - .status.successful) &lt;\n.spec.parallelism), i.e. when the work left to do is less than max\nparallelism. For more information, see:\nhttps://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/\nports    &lt;Object&gt;\nSpecifies a set of ports exposed from the container running the created\nworkload. Used together with --service-type.\nprocesses    &lt;Object&gt;\nNumber of distributed training processes that will be allocated for the\ncreated mpijob.\npvcs &lt;Object&gt;\nSpecifies persistent volume claims to mount into a container running the\ncreated workload.\nrunAsGid &lt;Object&gt;\nSpecifies the Unix group id with which the container should run. Will be\nused only if runAsUser is set to true.\nrunAsUid &lt;Object&gt;\nSpecifies the Unix user id with which the container running the created\nworkload should run. Will be used only if runAsUser is set to true.\nrunAsUser    &lt;Object&gt;\nLimits the container running the created workload to run in the context of\na specific non-root user. The user id is provided by the runAsUid field.\nThis would manifest itself in access to operating system resources, in the\nownership of new folders created under shared directories, etc.\nAlternatively, if your cluster is connected to Run:ai via SAML, you can map\nthe container to use the Linux UID/GID which is stored in the\norganization's directory. For more information see the User Identity guide\nat https://docs.run.ai/admin/runai-setup/config/non-root-containers/\ns3   &lt;Object&gt;\nSpecifies S3 buckets to mount into the container running the workload\nserviceType  &lt;Object&gt;\nSpecifies the default service exposure method for ports. The default shall\nbe used for ports which do not specify service type. Options are:\nLoadBalancer, NodePort or ClusterIP. For more information see the External\nAccess to Containers guide on\nhttps://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/\nslotsPerWorker   &lt;Object&gt;\nNumber of slots to allocate per worker in the created mpijob.\nstdin    &lt;Object&gt;\nInstructs the system to keep stdin open for the container(s) running the\ncreated workload, even if nothing is attached.\nsupplementalGroups   &lt;Object&gt;\n';' separated list of supplemental group IDs. Will be added to the security\ncontext of the container running the created workload.\ntolerations  &lt;Object&gt;\nToleration rules which apply to the pods running the workload. Toleration\nrules guide (but do not require) the system to which node each pod can be\nscheduled to or evicted from, based on matching between those rules and the\nset of taints defined for each Kubernetes node.\nttlAfterFinish   &lt;Object&gt;\nSpecifies the duration after which it is possible for a finished workload\nto be automatically deleted. When the workload is being deleted, its\nlifecycle guarantees (e.g. finalizers) will be honored. If this field is\nunset, the workload won't be automatically deleted. If this field is set to\nzero, the workload becomes eligible to be deleted immediately after it\nfinishes. This field is alpha-level and is only honored by servers that\nenable the TTLAfterFinished feature.\ntty  &lt;Object&gt;\nInstructs the system to allocate a pseudo-TTY for the created workload.\nusage    &lt;string&gt;\nThe intended usage of this workload. possible values are \"Template\": this\nworkload is used as the base for other workloads. \"Submit\": this workload\nis used for submitting a job and/or other Kubernetes resources.\nusername &lt;Object&gt;\nDisplay-only field describing the user who owns the workload. The data is\nnot used for authentication or authorization purposes.\nvolumes  &lt;Object&gt;\nSpecifies volumes to mount into a container running the created workload.\nworkingDir   &lt;Object&gt;\nSpecifies a directory that will be used as the current directory when the\ncontainer running the created workload starts.\n</code></pre>"},{"location":"developer/deprecated/inference/overview/","title":"Overview","text":"<p>Warning</p> <p>Inference API is deprecated. See Cluster API for its replacement. To read more about Inference see the new Inference Overview.</p>"},{"location":"developer/deprecated/inference/overview/#what-is-inference","title":"What is Inference","text":"<p>Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output. </p> <p>With Inference, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time. </p>"},{"location":"developer/deprecated/inference/overview/#inference-and-gpus","title":"Inference and GPUs","text":"<p>The Inference process is a subset of the original Training algorithm on a single datum (e.g. one sentence or one image), or a small batch. As such, GPU memory requirements are typically smaller than a full-blown Training process. </p> <p>Given that, Inference lends itself nicely to the usage of Run:ai Fractions. You can, for example, run 4 instances of an Inference server on a single GPU, each employing a fourth of the memory. </p>"},{"location":"developer/deprecated/inference/overview/#inference-runai","title":"Inference @Run:ai","text":"<p>Run:ai provides Inference services as an equal part together with the other two Workload types: Train and Build.</p> <ul> <li> <p>Inference is considered a high-priority workload as it is customer-facing. Running an Inference workload (within the Project's quota) will preempt any Run:ai Workload marked as Training.</p> </li> <li> <p>Inference is implemented as a Kubernetes Deployment with a defined number of replicas. The replicas are load-balanced by Kubernetes so that adding more replicas will improve the overall throughput of the system.</p> </li> <li> <p>Multiple replicas will appear in Run:ai as a single Inference workload. The workload will appear in all Run:ai dashboards and views as well as the Command-line interface.</p> </li> <li> <p>Inference workloads can be submitted via Run:ai Command-line interface as well as Kubernetes API/YAML. Internally, spawning an Inference workload also creates a Kubernetes Service. The service is an end-point to which clients can connect. </p> </li> </ul>"},{"location":"developer/deprecated/inference/overview/#see-also","title":"See Also","text":"<ul> <li>To setup Inference, see Inference Setup</li> <li>For running Inference see Inference quick-start</li> </ul>"},{"location":"developer/deprecated/inference/setup/","title":"Inference Setup","text":"<p>Warning</p> <p>Inference API is deprecated. See Cluster API for its replacement.</p> <p>Inference Jobs are an integral part of Run:ai and do not require setting up per se. However, Running multiple production-grade processes on a single GPU is best performed with an NVIDIA technology called Multi-Process Service or MPS</p> <p>By default, MPS is not enabled on GPU nodes.</p>"},{"location":"developer/deprecated/inference/setup/#enable-mps","title":"Enable MPS","text":"<p>To enable the MPS server on all nodes, you must edit the cluster installation values file:</p> <ul> <li>When installing the Run:ai cluster, edit the values file.</li> <li>On an existing installation, use the upgrade cluster instructions to modify the values file.</li> </ul> <p>Use:</p> <pre><code>runai-operator:\nconfig:\nmps-server:\nenabled: true\n</code></pre> <p>Wait for the MPS server to start running:</p> <pre><code> kubectl get pods -n runai\n</code></pre> <p>When the MPS server pod has started to run, restart the <code>nvidia-device-plugin</code> pods:</p> <pre><code>kubectl rollout restart ds/nvidia-device-plugin-daemonset -n gpu-operator\n</code></pre> <p>To enable the MPS server on selected nodes, please contact Run:ai customer support.</p>"},{"location":"developer/deprecated/inference/setup/#verify-mps-is-enabled","title":"Verify MPS is Enabled","text":"<p>Run:</p> <pre><code>kubectl get pods -n runai --selector=app=runai-mps-server -o wide\n</code></pre> <ul> <li> <p>Verify that all mps-server pods are in <code>Running</code> state. </p> </li> <li> <p>Submit a workload with MPS enabled using the --mps flag.  Then run:</p> </li> </ul> <pre><code>runai list\n</code></pre> <ul> <li>Identify the node on which the workload is running. In the <code>get pods</code> command above find the pod running on the same node and then run: </li> </ul> <pre><code>kubectl logs -n runai runai-mps-server-&lt;name&gt; -f\n</code></pre> <p>You should see activity in the log </p>"},{"location":"developer/deprecated/inference/submit-via-cli/","title":"Submit an inference Workload","text":"<p>Warning</p> <p>Inference API is deprecated. See Cluster API for its replacement.</p> <p>The easiest way to submit a new Inference workload is using the Run:ai Command-line interface. For additional information see the Inference Quickstart documentation.</p>"},{"location":"developer/deprecated/k8s-api/launch-job-via-kubernetes-api/","title":"Submit a Run:ai Job via Kubernetes API","text":"<p>Warning</p> <p>Researcher Kubernetes API is deprecated. See Cluster API for its replacement.</p> <p>This article is a complementary article to the article on launching jobs via YAML. It shows how to use a programming language and Kubernetes API to submit jobs. </p> <p>The article uses Python, though Kubernetes API is available in several other programming languages. </p>"},{"location":"developer/deprecated/k8s-api/launch-job-via-kubernetes-api/#submit-a-runai-job","title":"Submit a Run:ai Job","text":"<pre><code>from __future__ import print_function\nimport kubernetes\nfrom kubernetes import client, config\nfrom pprint import pprint\nimport json\nconfig.load_kube_config()\nwith client.ApiClient() as api_client:\nnamespace = 'runai-team-a'  # Run:ai project name is prefixed by runai-\njobname = 'my-job'\nusername = 'john'  # used in un-authenticated systems only\ngpus = 1\nbody = client.V1Job(api_version=\"run.ai/v1\", kind=\"RunaiJob\")\nbody.metadata = client.V1ObjectMeta(namespace=namespace, name=jobname)\ntemplate = client.V1PodTemplate()\ntemplate.template = client.V1PodTemplateSpec()\ntemplate.template.metadata = client.V1ObjectMeta(labels = {'user' : username})\nresource = client.V1ResourceRequirements(limits= {'nvidia.com/gpu' : gpus})\ncontainer = client.V1Container(\nname=jobname, image='gcr.io/run-ai-demo/quickstart', resources=resource)\ntemplate.template.spec = client.V1PodSpec(\ncontainers=[container], restart_policy='Never', scheduler_name='runai-scheduler')\nbody.spec = client.V1JobSpec(template=template.template)\npprint(body)\ntry:\napi_instance = client.CustomObjectsApi(api_client)\napi_response = api_instance.create_namespaced_custom_object(\n\"run.ai\", \"v1\", namespace, \"runaijobs\", body)\npprint(api_response)\nexcept client.rest.ApiException as e:\nprint(\"Exception when calling AppsV1Api-&gt;create_namespaced_job: %s\\n\" % e)\n</code></pre>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/","title":"Submit a Run:ai Job via YAML","text":"<p>Warning</p> <p>Researcher Kubernetes API is deprecated. See Cluster API for its replacement.</p> <p>You can use YAML files to submit jobs directly to Kubernetes. A frequent scenario for using the Kubernetes YAML syntax to submit Jobs is integrations. Researchers may already be working with an existing system that submits Jobs, and want to continue working with the same system. </p>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#terminology","title":"Terminology","text":"<p>We differentiate between three types of Workloads:</p> <ul> <li>Train workloads. Train workloads are characterized by a deep learning session that has a start and an end. A Training session can take anywhere from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. Training workloads typically utilize large percentages of GPU computing power and memory.</li> <li>Build workloads. Build workloads are interactive. They are used by data scientists to write machine learning code and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. </li> <li>Inference workloads. Inference workloads are used for serving models in production. For details on how to submit Inference workloads via YAML see here.</li> </ul> <p>The internal Kubernetes implementation of a Run:ai Job is a CRD (Customer Resource) named <code>RunaiJob</code> which is similar to a Kubernetes Job. </p> <p>Run:ai extends the Kubernetes Scheduler. A Kubernetes Scheduler is the software that determines which workload to start on which node. Run:ai provides a custom scheduler named <code>runai-scheduler</code>.</p> <p>The Run:ai scheduler schedules computing resources by associating Workloads with  Run:ai Projects:</p> <ul> <li>A Project is assigned with a GPU quota through the Run:ai Run:ai User Interface. </li> <li>A workload must be associated with a Project name and will receive resources according to the defined quota for the Project and the currently running Workloads</li> </ul> <p>Internally, Run:ai Projects are implemented as Kubernetes namespaces. The scripts below assume that the code is being run after the relevant namespace has been set. </p>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#submit-workloads","title":"Submit Workloads","text":"<ul> <li><code>&lt;JOB-NAME&gt;</code>. The name of the Job. </li> <li><code>&lt;IMAGE-NAME&gt;</code>. The name of the docker image to use. Example: <code>gcr.io/run-ai-demo/quickstart</code>.</li> <li><code>&lt;USER-NAME&gt;</code>. The name of the user submitting the Job. The name is used for display purposes only when Run:ai is installed in an unauthenticated mode.</li> <li><code>&lt;REQUESTED-GPUs&gt;</code>. An integer number of GPUs you request to be allocated for the Job. Examples: 1, 2.</li> <li><code>&lt;NAMESAPCE&gt;</code>. The name of the Project's namespace. This is usually <code>runai-&lt;PROJECT-NAME&gt;</code>.</li> </ul>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#regular-jobs","title":"Regular Jobs","text":"<p>Copy the following into a file and change the parameters:</p> <pre><code>apiVersion: run.ai/v1\nkind: RunaiJob (* see note below)\nmetadata:\nname: &lt;JOB-NAME&gt;\nnamespace: &lt;NAMESPACE&gt;\nlabels:\npriorityClassName: \"build\" (* see note below)\nspec:\ntemplate:\nmetadata:\nlabels:\nuser: &lt;USER-NAME&gt;\nspec:\ncontainers:\n- name: &lt;JOB-NAME&gt;\nimage: &lt;IMAGE-NAME&gt;\nresources:\nlimits:\nnvidia.com/gpu: &lt;REQUESTED-GPUs&gt;\nrestartPolicy: Never\nschedulerName: runai-scheduler\n</code></pre> <p>To submit the job, run:</p> <pre><code>kubectl apply -f &lt;FILE-NAME&gt;\n</code></pre> <p>Note</p> <ul> <li>You can use either a regular <code>Job</code> or <code>RunaiJob</code>. The latter is a Run:ai object which solves various Kubernetes Bugs and provides a better naming for multiple pods in Hyper-Parameter Optimization scenarios</li> <li>Using <code>build</code> in the <code>priorityClassName</code> field is equivalent to running a job via the CLI with a '--interactive' flag. To run a Train job, delete this line.</li> <li>The runai submit CLI command includes many more flags. These flags can be correlated with Kubernetes API functions and added to the YAML above. </li> </ul>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#using-fractional-gpus","title":"Using Fractional GPUs","text":"<p>To submit a Job with fractions of a GPU, replace <code>&lt;REQUESTED-GPUs&gt;</code> with a fraction in quotes. e.g. </p> <pre><code>limits:\nnvidia.com/gpu: \"0.5\"\n</code></pre> <p>where \"0.5\" is the requested GPU fraction.</p>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#mapping-additional-flags","title":"Mapping Additional Flags","text":"<p>Run:ai Command-Line <code>runai submit</code> has a significant number of flags. The easiest way to find out the mapping from a flag to the correct YAML attribute is to use the <code>--dry-run</code> flag.</p> <p>For example, to find the location of the <code>--large-shm</code> flag, run:</p> <pre><code>&gt; runai submit -i ubuntu --large-shm --dry-run\nTemplate YAML file can be found at:\n/var/folders/xb/rnf9b1bx2jg45c7jprv71d9m0000gn/T/job.yaml185826190\n</code></pre>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#delete-workloads","title":"Delete Workloads","text":"<p>To delete a Run:ai workload, delete the Job:</p> <pre><code>kubectl delete runaijob &lt;JOB-NAME&gt;\n</code></pre>"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#see-also","title":"See Also","text":"<ul> <li>See how to use the above YAML syntax with Kubernetes API</li> <li>Use the Researcher REST API to submit, list and delete Jobs.</li> </ul>"},{"location":"developer/deprecated/k8s-api/overview/","title":"Overview: Launch a Job via Kubernetes API","text":"<p>Warning</p> <p>Researcher Kubernetes API is deprecated. See Cluster API for its replacement.</p> <p>You can create, submit, list or delete jobs using the Command-line interface or the Run:ai User Interface. </p> <p>To do the same programmatically you can use the Run:ai Researcher REST API. </p> <p>You can also communicate directly with the underlying Kubernetes infrastructure by:</p> <ul> <li>Using YAML files or,</li> <li>By using a variety of programming languages to send requests to Kubernetes. See Submit a Run:ai Job via Kubernetes API for a python sample.</li> </ul>"},{"location":"developer/deprecated/researcher-rest-api/overview/","title":"Researcher REST API","text":"<p>Warning</p> <p>Researcher Kubernetes API is deprecated. See Cluster API for its replacement.</p> <p>The purpose of the Researcher REST API is to provide an easy-to-use programming interface for submitting, listing, and deleting Jobs. </p> <p>There are other APIs that provide the same functionality. Specifically:</p> <ul> <li>If your code is script-based, you may consider using the Run:ai command-line interface.</li> <li>You can communicate directly with the underlying Kubernetes infrastructure by sending YAML files or by using a variety of programming languages to send requests to Kubernetes. See Submit a Run:ai Job via Kubernetes API.</li> </ul> <p>The Researcher REST API is cluster-specific in the sense that if you have multiple GPU clusters, you will have a separate URL per cluster. This <code>&lt;CLUSTER-ENDPOINT&gt;</code> can be found in the Run:ai User Interface, under <code>Clusters</code>. Each cluster will have a separate URL.</p>"},{"location":"developer/deprecated/researcher-rest-api/overview/#authentication","title":"Authentication","text":"<ul> <li>By default, researcher APIs are unauthenticated. To protect researcher API, you must configure researcher authentication.</li> <li>Once configured, you must create a Client Application to make API requests. Use the client application and secret, to obtain a time-bound bearer token (<code>&lt;ACCESS-TOKEN&gt;</code>). For details, see Calling REST APIs.</li> <li>Use the token for subsequent API calls. </li> </ul>"},{"location":"developer/deprecated/researcher-rest-api/overview/#example","title":"Example","text":"<p>Get all the jobs for a project named <code>team-a</code>: </p> <pre><code>curl  'https://&lt;CLUSTER-ENDPOINT&gt;/researcher/api/v1/jobs/team-a' \\\n-H 'accept: application/json' \\\n--header 'Authorization: Bearer &lt;ACCESS-TOKEN&gt;' </code></pre>"},{"location":"developer/deprecated/researcher-rest-api/overview/#researcher-api-scope","title":"Researcher API Scope","text":"<p>The Researcher API provides the following functionality:</p> <ul> <li>Submit a new Job</li> <li>List jobs for specific Projects.</li> <li>Delete an existing Job</li> <li>Get a list of Projects for which you have access to</li> </ul>"},{"location":"developer/deprecated/researcher-rest-api/overview/#researcher-api-documentation","title":"Researcher API Documentation","text":"<p>To review API documentation:</p> <ul> <li>Open the Run:ai user interface</li> <li>Go to <code>Clusters</code></li> <li>Locate your cluster and browse to <code>https://&lt;cluster-url&gt;/researcher/api/docs</code>.</li> <li>When using the <code>Authenticate</code> button, add <code>Bearer &lt;ACCESS TOKEN&gt;</code> (simply adding the access token will not work).</li> </ul> <p>The document uses the Open API specification to describe the API. You can test the API within the document after creating and saving a token.</p>"},{"location":"developer/metrics/metrics/","title":"Metrics","text":""},{"location":"developer/metrics/metrics/#what-are-metrics","title":"What are Metrics","text":"<p>Metrics are numeric measurements recorded over time that are emitted from the Run:ai cluster. Typical metrics involve utilization, allocation, time measurements and so on. Metrics are used in Run:ai dashboards as well as in the Run:ai administration user interface. </p> <p>The purpose of this document is to detail the structure and purpose of metrics emitted by Run:ai to enable customers to create custom dashboards or integrate metric data into other monitoring systems. </p> <p>Run:ai uses Prometheus for collecting and querying metrics.</p>"},{"location":"developer/metrics/metrics/#published-runai-metrics","title":"Published Run:ai Metrics","text":"<p>Following is the list of published Run:ai metrics:</p> Metric name Labels Measurement Description runai_active_job_cpu_requested_cores {clusterId,  job_name, job_uuid} CPU Cores Job's requested CPU cores runai_active_job_memory_requested_bytes {clusterId,  job_name, job_uuid} Bytes Job's requested CPU memory runai_cluster_cpu_utilization \u00a0{clusterId} 0 to 1 CPU utilization of the entire cluster runai_cluster_memory_used_bytes \u00a0{clusterId} Bytes Used CPU memory of the entire cluster runai_cluster_memory_utilization \u00a0{clusterId} 0 to 1 CPU memory utilization of the entire cluster runai_gpu_is_allocated \u00a0{gpu, clusterId, node} 0/1 Is a GPU hosting a pod runai_gpu_is_running_fractional_job \u00a0{gpu, clusterId, node} 0/1 Is GPU hosting Fractional GPU jobs runai_gpu_last_active_time \u00a0{gpu, clusterId, node} Unix time Last time GPU was not idle runai_gpu_utilization_non_fractional_jobs \u00a0{job_uuid, job_name, clusterId, gpu, node} 0 to 100 Utilization per GPU for jobs running on a full GPU runai_gpu_utilization_with_pod_info \u00a0{pod_namespace, pod_name, clusterId, gpu, node} 0 to 100 GPU utilization per GPU runai_job_allocated_gpus \u00a0{job_type, job_uuid, job_name, clusterId, project} Double GPUs allocated to Jobs runai_job_gpu_utilization \u00a0{job_uuid, clusterId, job_name, project} 0 to 100 Average GPU utilization per job runai_job_image \u00a0{image, job_uuid, job_name, clusterId} N/A Image name per job runai_job_requested_gpu_memory \u00a0{job_type, job_uuid, job_name, clusterId, project} Bytes Requested GPU memory per job (0 if not specified by the user) runai_job_requested_gpus \u00a0{job_type, job_uuid, job_name, clusterId, project} Double Number of requested GPU per job runai_job_status_with_info \u00a0{user, job_type, status, job_name, clusterId, node, project} N/A Additional information on jobs runai_job_total_runtime \u00a0{clusterId, job_uuid} Seconds Total run time per job runai_job_total_wait_time \u00a0{clusterId, job_uuid} Seconds Total wait time per job runai_job_used_gpu_memory_bytes \u00a0{clusterId, job_uuid} Bytes Used GPU memory per job runai_job_used_gpu_memory_bytes_with_gpu_node \u00a0{job_uuid, job_name, clusterId, gpu, node} Bytes Used GPU memory per job, per GPU on which the job is running runai_node_cpu_requested_cores \u00a0{clusterId, node} Double Sum of the requested CPU cores of all jobs running in a node runai_node_cpu_utilization \u00a0{clusterId, node} 0 to 1 CPU utilization per node runai_node_gpu_used_memory_bytes \u00a0{clusterId, node} Bytes Used GPU memory per node runai_node_memory_utilization \u00a0{clusterId, node} 0 to 1 CPU memory utilization per node runai_node_requested_memory_bytes \u00a0{clusterId, node} Bytes Sum of the requested CPU memory of all jobs running in a node runai_node_total_memory_bytes \u00a0{clusterId, node} Bytes Total GPU memory per node runai_node_used_memory_bytes \u00a0{clusterId, node} Bytes Used CPU memory per node runai_project_guaranteed_gpus \u00a0{clusterId, project} Double Guaranteed GPU quota per project runai_project_info \u00a0{memory_quota, cpu_quota, gpu_guaranteed_quota, clusterId, project, department_name} N/A Information on CPU, CPU memory, GPU quota per project runai_running_job_cpu_limit_cores {clusterId, job_name , job_uuid} Double Jobs CPU limit (in number of cores). See link runai_running_job_cpu_requested_cores \u00a0{clusterId, job_name, job_uuid} Double Jobs requested CPU cores. See link runai_running_job_cpu_used_cores \u00a0{job_uuid, clusterId, job_name, project} Double Jobs CPU usage (in number of cores) runai_running_job_memory_limit_bytes \u00a0{clusterId, job_name, job_uuid} Bytes Jobs CPU memory limit. See link runai_running_job_memory_requested_bytes \u00a0{clusterId, job_name, job_uuid} Bytes Jobs requested CPU memory. See link runai_running_job_memory_used_bytes \u00a0{job_uuid, clusterId, job_name, project} Bytes Jobs used CPU memory runai_mig_mode_gpu_count \u00a0{clusterId, node} Double Number of GPUs on MIG nodes runai_job_swap_memory_used_bytes {clusterId, job_uuid, job_name, project, node} Bytes Used Swap CPU memory for the job runai_deployment_request_rate {clusterId, namespace_name, deployment_name} Number Rate of received HTTP requests per second runai_deployment_request_latencies {clusterId, namespace_name, deployment_name, le} Number Histogram of response time (bins are in milliseconds) <p>Additional metrics for version 2.9 and above</p> Metric name Labels Measurement Description runai_gpu_utilization_per_gpu {clusterId, gpu, node} % GPU Utilization per GPU runai_gpu_utilization_per_node {clusterId, node} % GPU Utilization per Node runai_gpu_memory_used_mebibytes_per_gpu {clusterId, gpu, node} MiB Used GPU memory per GPU runai_gpu_memory_used_mebibytes_per_node {clusterId, node} MiB Used GPU memory per Node runai_gpu_memory_total_mebibytes_per_gpu {clusterId, gpu, node} MiB Total GPU memory per GPU runai_gpu_memory_total_mebibytes_per_node {clusterId, node} MiB Toal GPU memory per Node runai_gpu_count_per_node {clusterId, node} Number Number of GPUs per Node runai_allocated_gpu_count_per_workload {clusterId, job_name, job_uuid, job_type, user} Double Number of allocated GPUs per Workload runai_allocated_gpu_count_per_project {clusterId, project} Double Number of allocated GPUs per Project runai_gpu_memory_used_mebibytes_per_pod_per_gpu {clusterId, pod_name, pod_uuid, pod_namespace, node, gpu} MiB Used GPU Memory per Pod per Gpu runai_gpu_memory_used_mebibytes_per_workload {clusterId, job_name, job_uuid, job_type, user} MiB Used GPU Memory Per Workload runai_gpu_utilization_per_pod_per_gpu {clusterId, pod_name, pod_uuid, pod_namespace, node, gpu} % GPU Utilization per Pod per GPU runai_gpu_utilization_per_workload {clusterId, job_name, job_uuid, job_type, user} % GPU Utilization per Workload runai_gpu_utilization_per_project {clusterId, project} % GPU Utilization per Project runai_last_gpu_utilization_time_per_workload {clusterId, job_name, job_uuid, job_type, user} Seconds (Unix Timestamp) Job's requested CPU memory runai_gpu_idle_time_per_workload {clusterId, job_name, job_uuid, job_type, user} Seconds Job's requested CPU memory runai_allocated_gpu_count_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Bytes Job's requested CPU memory runai_allocated_gpu_count_per_node {clusterId, node} Bytes Job's requested CPU memory runai_allocated_millicpus_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Bytes Job's requested CPU memory runai_allocated_memory_per_pod {clusterId, pod_name, pod_uuid, pod_namespace, node} Bytes Job's requested CPU memory <p>Following is a list of labels appearing in Run:ai metrics:</p> Label Description clusterId Cluster Identifier department_name Name of Run:ai Department cpu_quota CPU limit per project gpu GPU index gpu_guaranteed_quota Guaranteed GPU quota per project image Name of Docker image namespace_name Namespace deployment_name Deployment name job_name Job name job_type Job type: training, interactive or inference job_uuid Job identifier pod_name Pod name. A Job can contain many pods. pod_namespace Pod namespace memory_quota CPU memory limit per project node Node name project Name of Run:ai Project status Job status: Running, Pending, etc. For more information on Job statuses see document user User identifier"},{"location":"developer/metrics/metrics/#other-metrics","title":"Other Metrics","text":"<p>Run:ai exports other metrics emitted by NVIDIA and Kubernetes packages, as follows:</p> Metric name Description dcgm_gpu_utilization GPU utilization kube_node_status_allocatable Resources (CPU, memory, GPU etc) are allocatble (available for scheduling) kube_node_status_capacity The capacity for different resources of a node kube_node_status_condition The condition of a cluster node kube_pod_container_resource_requests The number of requested resources by a container kube_pod_container_resource_requests_cpu_cores The number of CPU cores requested by container kube_pod_container_resource_requests_memory_bytes Bytes of memory requested by a container kube_pod_info Information about pod kube_pod_status_phase The current phase of the pod <p>For additional information, see Kubernetes kube-state-metrics and NVIDIA dcgm exporter.</p>"},{"location":"developer/metrics/metrics/#how-to-query-metrics","title":"How to Query Metrics","text":"SaaSSelf Hosted <p>Run:ai customer support should provide <code>&lt;BASE-METRICS-URL&gt;</code>, <code>&lt;DATASOURCE-ID&gt;</code> and <code>&lt;GRAFANA-API-KEY&gt;</code>. </p> <ul> <li>Browse to  <code>&lt;RUNAI-URL&gt;/grafana</code> (<code>&lt;BASE-METRICS-URL&gt;</code>) and log in as administrator</li> <li>Under Keys, generate a viewer key (<code>&lt;GRAFANA-API-KEY&gt;</code>)</li> <li>Under Data sources, locate a numeric data source ID ( <code>&lt;DATASOURCE-ID&gt;</code>)</li> </ul> <p>Use the Run:ai metrics documentation above together with Prometheus API syntax to access data. Example: </p> <pre><code>curl \"https://&lt;BASE-METRICS-URL&gt;/api/datasources/proxy/&lt;DATASOURCE-ID&gt;/api/v1/query?query=runai_job_total_runtime\" \\\n--header 'Accept: application/json' \\\n--header 'Authorization: Bearer &lt;GRAFANA-API_KEY&gt;'\n</code></pre>"},{"location":"home/components/","title":"Run:ai System Components","text":""},{"location":"home/components/#components","title":"Components","text":"<ul> <li> <p>Run:ai is installed over a Kubernetes Cluster</p> </li> <li> <p>Researchers submit Machine Learning workloads via the Run:ai Command-Line Interface (CLI), or directly by sending YAML files to Kubernetes. </p> </li> <li> <p>Administrators monitor and set priorities via the Run:ai User Interface</p> </li> </ul> <p></p>"},{"location":"home/components/#the-runai-cluster","title":"The Run:ai Cluster","text":"<p>The Run:ai Cluster contains:</p> <ul> <li>The Run:ai Scheduler which extends the Kubernetes scheduler. It uses business rules to schedule workloads sent by Researchers. </li> <li>Fractional GPU management. Responsible for the Run:ai technology which allows Researchers to allocate parts of a GPU rather than a whole GPU  </li> <li>The Run:ai agent. Responsible for sending Monitoring data to the Run:ai Cloud.</li> <li>Clusters require outbound network connectivity to the Run:ai Cloud.  </li> </ul>"},{"location":"home/components/#kubernetes-related-details","title":"Kubernetes-Related Details","text":"<ul> <li>The Run:ai cluster is installed as a Kubernetes Operator</li> <li>Run:ai is installed in its own Kubernetes namespace named runai</li> <li>Workloads are run in the context of Projects. Each Project is a Kubernetes namespace with its own settings and access control. </li> </ul>"},{"location":"home/components/#the-runai-control-plane","title":"The Run:ai Control Plane","text":"<p>The Run:ai control plane is the basis of the Run:ai User Interface. </p> <ul> <li>The Run:ai cloud aggregates monitoring information from multiple tenants (customers).</li> <li>Each customer may manage multiple Run:ai clusters. </li> </ul> <p></p> <p>The Run:ai control plane resides on the cloud but can also be locally installed. To understand the various installation options see the installation types document.</p>"},{"location":"home/data-privacy-details/","title":"Data Privacy","text":"<p>Run:ai SaaS Cluster installation uses the Run:ai cloud as its control plane. The cluster sends information to the cloud for the purpose of control as well as dashboards. The document below is a run-down of the data that is being sent to the Run:ai cloud.</p> <p>Note</p> <p>If the data detailed below is not in line with your organization's policy, you can choose to install the Run:ai self-hosted version. The self-hosted installation includes the Run:ai control-plane and will not communicate with the cloud. The self-hosted installation has different pricing. </p>"},{"location":"home/data-privacy-details/#data","title":"Data","text":"<p>Following is a list of platform data items that are sent to the Run:ai cloud.</p> Asset Data Details Job Metrics Job names, CPU, GPU, and Memory metrics, parameters sent using the <code>runai submit</code> command Node Metrics Node names and IPs, CPU, GPU, and Memory metrics Cluster Metrics Cluster names, CPU, GPU, and Memory metrics Projects &amp; Departments Names, quota information Users User Run:ai roles, emails and passwords (when single-sign on not used) <p>Run:ai does not send deep-learning artifacts to the cloud. As such any Code, images, container logs, training data, models, checkpoints and the like, stay behind corporate firewalls. </p>"},{"location":"home/data-privacy-details/#see-also","title":"See Also","text":"<p>The Run:ai privacy policy. </p>"},{"location":"home/whats-new-2-8/","title":"Run:ai Version 2.8","text":""},{"location":"home/whats-new-2-8/#release-date","title":"Release Date","text":"<p>November 2022 </p>"},{"location":"home/whats-new-2-8/#release-content","title":"Release Content","text":""},{"location":"home/whats-new-2-8/#node-pools","title":"Node Pools","text":"<p>Node Pools is a new method for managing GPU and CPU resources by grouping the resources into distinct pools. With node pools:</p> <ul> <li>The administrator allocates Project and Department resources from these pools to be used by Workloads. </li> <li>The administrator controls which workloads can use which resources, allowing an optimized utilization of resources according to customer's specific mode of operation. </li> </ul>"},{"location":"home/whats-new-2-8/#user-interface-enhancements","title":"User Interface Enhancements","text":"<ul> <li>The Departments screen has been revamped and new functionality added, including a new and clean look and feel, and improved search and filtering capabilities.</li> <li>The Jobs screen has been split into 2 tabs for ease of use:<ul> <li>Current:  (the default tab) consists of all the jobs that currently exist in the cluster. </li> <li>History:  consists of all the jobs that have been deleted from the cluster. Deleting Jobs also deletes their Log (no change).</li> </ul> </li> </ul>"},{"location":"home/whats-new-2-8/#installation-improvements","title":"Installation improvements","text":"<p>The Run:ai user interface requires a URL address to the Kubernetes cluster. The requirement is relevant for SaaS installation only. </p> <p>In previous versions of Run:ai the administrator should provide an IP address and Run:ai would automatically create a DNS entry for it and a matching trusted certificate. </p> <p>In version 2.8,  the default is for the Run:ai administrator to provide a DNS and a trusted certificate. </p> <p>The older option still exists but is being deprecated due to complexity.</p>"},{"location":"home/whats-new-2-8/#inference","title":"Inference","text":"<p>The Deployment details page now contains the URL for the Inference service </p>"},{"location":"home/whats-new-2-8/#hyperparameter-optimization-hpo","title":"Hyperparameter Optimization (HPO)","text":"<p>HPO Jobs are now presented as a single line in the Job List rather than a separate line per experiment. </p>"},{"location":"home/whats-new-2-8/#known-issues","title":"Known Issues","text":"Internal ID Description Workaround RUN-6236 The Run:ai access control system prevents setting a role of researcher together with ML engineer or researcher manager at the same time. However, using the UI you can select these two roles by clicking the text near the check None RUN-6218 When installing Run:ai on OpenShift a second time, oauth client secret is incorrect/not updated. As a result, login is not possible Can be performed via manual configuration. Please contact Run:ai support. RUN-6216 In the multi cluster overview, the allocated GPU in the table of each cluster is wrong. The correct number is in the overview dashboard. None RUN-6190 When deleting a cluster, there are leftover pods that are not deleted. No side effects on functionality. Delete the pods manually. RUN-5855 (SaaS version only) The new control plane, versioned 2.8 does not allow the creation of a new deployment on a cluster whose version is lower than 2.8. Upgrade your cluster to 2.8 RUN-5780 It is possible to change runai/node-pool label of a running pod. This is a wrong usage of the system and may cause unexpected behavior. None. RUN-5527 Idle allocated GPU metric is not displayed for MIG workloads in OpenShift. None RUN-5519 When selecting a Job, the GPU memory utilization metrics is not displayed on the right-hand side. This is an NVIDIA DCGM known bug (see:  https://github.com/NVIDIA/dcgm-exporter/issues/103 ) which has been fixed in a later version but was not yet included in the latest NVIDIA GPU Operator Install the suggested version as described by NVIDIA. RUN-5478 Dashboard panels of GPU Allocation/project and Allocated jobs per project metrics:  In rare cases, some metrics reflect the wrong number of GPUs None RUN-5444 Dynamic MIG feature does not work with A-100 with 80GB of memory. None RUN-5424 When a workload is selected in the job list, the GPU tab in the right panel, shows the details of the whole GPUs in the node, instead of the details of the GPUs used by the workload. None RUN-5226 In rare occasions, when there is more than 1 NVIDIA MIG workload, nvidia-smi command to one of the workloads will result with no devices. None RUN-6359 In rare cases, when using fractions and the kubelet service on the scheduled node is down (Kubernetes not running on node)the pending workload will never run, even when the IT problem is solved. Delete the job and re-submit the workload. RUN-6399 Requested GPUs are sometimes displayed in the Job list as 0 for distributed workloads. None. This is a display-only issue RUN-6400 On EKS (Amazon Kubernetes Server), when using runai CLI, every command response starts with an error. No functionality harm. None. The CLI functions as expected."},{"location":"home/whats-new-2-8/#fixed-issues","title":"Fixed Issues","text":"Internal ID Description RUN-5676 When Interactive Jupyter notebook workloads that contain passwords are cloned, the password is exposed in the displayed CLI command. RUN-5457 When using the Home environment variable in conjunction with the ran-as-user option in the CLI, the Home environment variable is overwritten with the user's home directory. RUN-5370 It is possible to submit two jobs with the same node-port. RUN-5314 When you apply an inference deployment via a file, the allocated GPUs are displayed as 0 in the deployments list. RUN-5284 When workloads are deleted while the cluster synchronization is down, there might be a non-existent Job shown in the user interface. The Job cannot be deleted. RUN-5160 In some situations, when a Job is deleted, there may be leftover Kubernetes configMaps in the system RUN-5154 In some cases, an error \"failed to load data\" can be seen in the graphs showing on the Job sidebar. RUN-5145 The default Kubernetes \"priority Class\" for deployments is the same as the priority class for interactive jobs. RUN-5039 In some scenarios, Dashboards may show \"found duplicate series for the match group\" error RUN-4941 The scheduler is wrongly trying to schedule jobs on a node, where there are allocated GPU jobs at an \"ImagePullBackoff\" state. This causes an error of \"UnexpectedAdmissionError\" RUN-4574 The role \"Researcher Manager\" is not displayed in the access control list of projects. RUN-4554 Users are trying to login with single-sign-on get a \"review profile\" page. RUN-4464 Single HPO (hyperparameter optimization) workload is displayed in the Job list user interfgace as multiple jobs (one for every pod)."},{"location":"home/whats-new-2-9/","title":"Run:ai Version 2.9","text":""},{"location":"home/whats-new-2-9/#release-date","title":"Release Date","text":"<p>February 2023</p>"},{"location":"home/whats-new-2-9/#release-content","title":"Release Content","text":""},{"location":"home/whats-new-2-9/#authentication","title":"Authentication","text":"<p>OpenShift groups</p> <p>Ability to manage access control through IDP groups declaration - groups are managed from the OpenShift platform and integrated into Run:ai platform, as opposed to group management in vanilla k8s with SSO. OpenShift doesn\u2019t need any additional configuration as this comes built-in with regular installation or the upgrade option.</p> <p>UID/GID for SSO users</p> <p>When running a workload through the UI the Run:ai platform now automatically injects the UID and GID into the created container. This has changed from previous versions where the user would enter data in these fields manually. This is designed for environments where UIDs and GIDs are managed in an SSO server, and Run:ai is configured with SSO.   </p> <p>SSO: block access to Run:ai</p> <p>When configuring SSO in the Run:ai platform all users are assigned a new default role. It means an SSO user will not have any access to the Run:ai platform unless a manager explicitly assigns additional roles via the user or group management areas.</p> <p>Run CPU over-quota workloads</p> <p>Added support for CPU workloads to support over-quota - CPU resources fairness was added to the Run:ai scheduler in addition to the GPU fairness that is already supported. The updated fairness algorithm takes into account all resource types (GPU, CPU compute and CPU memory) and is supported regardless of node pool configuration. </p>"},{"location":"home/whats-new-2-9/#runai-workspaces","title":"Run:ai Workspaces","text":"<p>A Run:ai workspace is a simplified, efficient tool for researchers to conduct their experiments, build AI models, access standard MLOps tools, and collaborate with their peers.</p> <p>Run:ai workspaces abstract complex concepts related to running containerized workloads in a Kubernetes environment, such as networking, storage, and secrets, and are built from predefined abstracted setups, that ease and streamline the researcher AI models development. A workspace consists of container images, data sets, resource requests, and all the required tools for the research. They are quickly created with the workspace wizard. For more information see Workspaces.</p>"},{"location":"home/whats-new-2-9/#new-supported-tools-for-researchers","title":"New supported tools for researchers","text":"<p>As part of the introduction of Run:ai workspaces a few new development and research tools were added. The new supported tools are: RStudio, Visual Studio Code, Matlab and Weights and Biases (see full details). This is an addition to adding already supported tools, such as JupyterNotebook and TensorBoard to Run:ai workspaces.</p> <p>Weight and Biases</p> <p>Weights and Biases is a commercial tool that provides experiment tracking, model visualization, and collaboration for machine learning projects. It helps researchers and developers keep track of their experiments, visualize their results, and compare different models to make informed decisions. This integration provides data researchers with connectivity between the running Workspace in Run:ai and the relevant project for experiment tracking. For more information, see Weights and Biases</p> <p>Node pools enhancements</p> <p>Added additional support to multi-node pools. This new capability allows the researcher to specify a prioritized list of node pools for the Run:ai scheduler to use. Researchers now gain the flexibility to use multiple resource types and maximize the utilization of the system\u2019s GPU and CPU resources. Administrators now have the option to set a default Project (namespace) level with a prioritized list of node pools that a workload will use if the researcher did not set its own priorities.</p> <p>New nodes and node pools Screens</p> <p>Run:ai has revised the nodes table, adding new information fields and graphs. It is now easier to assess how resources are allocated and utilized. Run:ai has also added a new \u2018node pools\u2019 table where Administrators can add a new node pool, update, and delete an existing node pool. In addition, the node pools table presents a large number of metrics and details about each of the node pools. A set of graphs reflect the node pools\u2019 resource status over time according to different criteria.</p> <p>Consumption Dashboard</p> <p>Added a \u201cConsumption\u201d dashboard. When enabled by the \u201cShow Consumption Dashboard\u201d alpha flag under \u201cSettings\u201d, this dashboard allows the admin to review consumption patterns for GPUs, CPUs and RAM over time. You can segregate consumption by over or in-quota allocation in the project or department level.</p> <p>Event History (Audit Log UI)</p> <p>Added the option for Administrators to view the system\u2019s Audit Log via the Run:ai user interface. Configuration changes and other administrative operations (login/logout etc) are saved in an Audit Log facility. Administrators can browse through the Admin Log (Event History), download as a JSON or CSV, filter specific date periods, set multiple criteria filters, and decide which information fields to view.</p> <p>Idle jobs timeout policy</p> <p>Added an option \u2018Editor\u2019 so that Administrators can terminate idle workloads by setting the criteria of \u2018idle time\u2019 per project so that the editor can identify and terminate idle Training and Interactive (build) workloads. This is used for maximizing and maintaining system sanitation.</p>"},{"location":"home/whats-new-2-9/#installation-enhancements","title":"Installation Enhancements","text":""},{"location":"home/whats-new-2-9/#cluster-upgrade","title":"Cluster Upgrade","text":"<p>Cluster upgrade to 2.9 requires uninstalling and then installing. No data is lost during the process. For more information see cluster upgrade.</p> <p>Using an IP address for a cluster URL is no longer available in this version. You must use a domain name.</p>"},{"location":"home/whats-new-2-9/#cluster-prerequisites","title":"Cluster Prerequisites","text":"<ul> <li> <p>Prometheus is no longer installed together with Run:ai. You must install the Prometheus stack before installing Run:ai. This is designed for organizations that already have Prometheus installed in the cluster. The Run:ai installation configures the existing Prometheus with a custom set of rules designed to extract metrics from the cluster.</p> </li> <li> <p>NGINX is no longer installed together with Run:ai. You must install an Ingress controller before installing Run:ai. This is designed for organizations that already have an ingress controller installed. The Run:ai installation creates NGINX rules to work with the controller.</p> </li> <li> <p>List of Run:ai installation Prerequisites can be found here.</p> </li> <li> <p>The Run:ai installation now performs a series of checks to verify the installation's validity. When the installation is complete, verify by reviewing the following in the log file:</p> <ul> <li>Are all mandatory prerequisites met?</li> <li>Are optional prerequisites met?</li> <li>Does the cluster have connectivity to the Run:ai control plane?</li> <li>Does Run:ai support the underlying Kubernetes version?</li> </ul> </li> </ul>"},{"location":"home/whats-new-2-9/#control-plane-upgrade","title":"Control Plane Upgrade","text":"<p>A special process is required to upgrade the control-plane to version 2.9. </p>"},{"location":"home/whats-new-2-9/#control-plane-prerequisites","title":"Control plane Prerequisites","text":"<ul> <li> <p>Run:ai control plane installation no longer installs NGINX. You must pre-install an ingress controller.</p> </li> <li> <p>The default persistent storage is now a default storage class preconfigured in Kubernetes rather than the older NFS assumptions. NFS flags in <code>runai-adm</code> generate-values still exist for backward compatibility.</p> </li> </ul>"},{"location":"home/whats-new-2-9/#other","title":"Other","text":"<p>Cluster Wizard has been simplified for environments with multiple clusters   in a self-hosted configuration. Clusters are now easier to configure. Choose a cluster location: </p> <ul> <li>Same as Control Plane. </li> <li>Remote to Control Plane. </li> </ul>"},{"location":"home/whats-new-2-9/#new-supported-software","title":"New Supported Software","text":"<ul> <li>Run:ai now supports Kubernetes 1.25 and 1.26.</li> <li>Run:ai now supports OpenShift 4.11</li> <li>Run:ai now supports Dynamic MIG with NVIDIA H100 hardware</li> <li>The Run:ai command-line interface now supports Microsoft Windows. See Install the Run:ai Command-line Interface.</li> </ul>"},{"location":"home/whats-new-2-9/#known-issues","title":"Known Issues","text":"Internal ID Description Workaround RUN-7874 When a project is not connected to a namespace - new job returns \"malformed URL\" None RUN-7617 Cannot delete Node affinity from project after it was created Remove it using the API."},{"location":"home/whats-new-2-9/#fixed-issues","title":"Fixed Issues","text":"Internal ID Description RUN-7776 user does not exist in the UI due to pagination limitation RUN-6995 Group Mapping from SSO Group to Researcher Manager Role no working RUN-6460 S3 Fail (read/write in Jupyter notebook) RUN-6445 Project can be created with deleted node pool RUN-6400 EKS - Every command response in runai CLI starts with an error. No functionality harm RUN-6399 Requested GPU is always 0 for MPI jobs, making also other metrics wrong RUN-6359 Job gets UnexpectedAdmissionError race condition with Kubelet RUN-6272 runai pod which owner is not RunaiJob - Do not allow deletion, suspension, cloning RUN-6218 When installing Run:ai on OpenShift a second time, oauth client secret is incorrect/not updated RUN-6216 Multi cluster: allocated GPU is wrong as a result of metric not counting jobs in error RUN-6029 CLI Submit git sync severe bug RUN-6027 [Security Issue] Job submitted with github sync -- Password is displayed in the UI RUN-5822 Environment Variables in the UI do not honor the \"canRemove:false\" attribute in Policy RUN-5676 Security issue with \"Clone Job\" functionality RUN-5527 Metrics (MIG - OCP): GPU Idle Allocated GPUs show No Data RUN-5478 # of GPUs is higher than existing GPUs in the cluster RUN-5444 MIG doesn't work on A100 - 80GB RUN-5424 Deployment GPUs tab shows all the GPUs on the node instead of the ones in use by the deployment RUN-5370 Can submit job with the same node port + imagePullpolicy RUN-5226 MIG job can't see device after submitting a different mig job RUN-4869 S3 jobs run forever with NotReady state RUN-4244 Run:ai Alertmanager shows false positive errors on Agent"},{"location":"home/whats-new-2020/","title":"Whats New 2020","text":""},{"location":"home/whats-new-2020/#december-28th-2020","title":"December 28th, 2020","text":"<p>It is now possible to allocate a specific amount of GPU memory rather than use the fraction syntax. Use <code>--gpu-memory=5G</code>.</p>"},{"location":"home/whats-new-2020/#december-15th-2020","title":"December 15th, 2020","text":"<p>Project and Departments can now be set to not allocate resources beyond the assigned GPUs. This is useful for budget-conscious Projects/Departments. </p>"},{"location":"home/whats-new-2020/#december-1st-2020","title":"December 1st, 2020","text":"<p>New integration documents:</p> <ul> <li>Apache Airflow</li> <li>TensorBoard</li> </ul>"},{"location":"home/whats-new-2020/#november-25th-2020","title":"November 25th, 2020","text":"<p>Syntax changes in CLI:</p> <ul> <li><code>runai &lt;object&gt; list</code>  has been replaced by <code>runai list &lt;object&gt;</code>.</li> <li><code>runai get</code> has been replaced by <code>runai describe job</code>.</li> <li><code>runai &lt;object&gt; set</code> has been replaced by <code>runai config &lt;object&gt;</code>.</li> </ul> <p>The older style will still work with a deprecation notice.</p> <p><code>runai top node</code> has been revamped.</p>"},{"location":"home/whats-new-2020/#november-12th-2020","title":"November 12th, 2020","text":"<p>An Admin can now create templates for the Command-line interface. Both a default template and specific templates, that can be used with the --template flag. The new templates allow for mandatory values, defaults, and run-time environment variable resolution.</p> <p>It is now also possible to pass Secrets to Job. see here</p>"},{"location":"home/whats-new-2020/#november-2nd-2020","title":"November 2nd, 2020","text":"<p>Several changes and additions to the Command-line interface:</p> <ul> <li>Passing a command and arguments is now done docker-style by adding <code>--</code> at the end of the command</li> <li>You no longer need to provide a Job name. If you don't, a Job name will be generated automatically. You can also control the job-name prefix using an additional flag. </li> <li>New <code>--image-pull-policy</code> flag, allowing Researcher support for updating images without tagging.</li> </ul> <p>For further information see runai submit</p>"},{"location":"home/whats-new-2020/#september-6th-2020","title":"September 6th, 2020","text":"<p>We released a module that helps the Researcher perform Hyperparameter optimization (HPO). HPO is about running many smaller experiments with varying parameters to help determine the optimal parameter set Hyperparameter Optimization Quickstart</p>"},{"location":"home/whats-new-2020/#september-3rd-2020","title":"September 3rd, 2020","text":"<p>GPU Fractions now run in training and not only interactive. GPU Fractions training Job can be preempted, bin-packed and consolidated like any integer Job. See Run:ai Scheduler Fraction for more.</p>"},{"location":"home/whats-new-2020/#august-10th-2020","title":"August 10th, 2020","text":"<p>Run:ai Now supports Distributed Training and Gang Scheduling. For further information, see the Launch Distributed Training Workloads quickstart.</p>"},{"location":"home/whats-new-2020/#august-4th-2020","title":"August 4th, 2020","text":"<p>There is now an optional second level of Project hierarchy called Departments. For further information on how to configure and use Departments, see Working with Departments </p>"},{"location":"home/whats-new-2020/#july-28th-2020","title":"July 28th, 2020","text":"<p>You can now enforce a cluster-wise setting that mandates all containers running using the Run:ai CLI to run as non root. For further information, see Enforce non-root Containers</p>"},{"location":"home/whats-new-2020/#july-21th-2020","title":"July 21th, 2020","text":"<p>It is now possible to mount a Persistent Storage Claim using the Run:ai CLI. See the <code>--pvc</code> flag in the runai submit CLI flag</p>"},{"location":"home/whats-new-2020/#june-13th-2020","title":"June 13th, 2020","text":""},{"location":"home/whats-new-2020/#new-settings-for-the-allocation-of-cpu-and-memory","title":"New Settings for the Allocation of CPU and Memory","text":"<p>It is now possible to set limits for CPU and memory as well as to establish defaults based on the ratio of GPU to CPU and GPU to memory. </p> <p>For further information see: Allocation of CPU and Memory</p>"},{"location":"home/whats-new-2020/#june-3rd-2020","title":"June 3rd, 2020","text":""},{"location":"home/whats-new-2020/#node-group-affinity","title":"Node Group Affinity","text":"<p>Projects now support Node Affinity. This feature allows the Administrator to assign specific Projects to run only on specific nodes (machines). Example use cases:</p> <ul> <li>The Project team needs specialized hardware (e.g. with enough memory)</li> <li>The Project team is the owner of specific hardware which was acquired with a specialized budget</li> <li>We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes</li> </ul> <p>For further information see: Working with Projects</p>"},{"location":"home/whats-new-2020/#limit-duration-of-interactive-jobs","title":"Limit Duration of Interactive Jobs","text":"<p>Researchers frequently forget to close Interactive Job. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive Job and close them automatically. </p> <p>For further information on how to set up duration limits see: Working with Projects</p>"},{"location":"home/whats-new-2020/#may-24th-2020","title":"May 24th, 2020","text":""},{"location":"home/whats-new-2020/#kubernetes-operators","title":"Kubernetes Operators","text":"<p>Cluster installation now works with Kubernetes Operators. Operators make it easy to install, update, and delete a Run:ai cluster. </p> <p>For further information see: Upgrading a Run:ai Cluster Installation and Deleting a a Run:ai Cluster Installation</p>"},{"location":"home/whats-new-2020/#march-3rd-2020","title":"March 3rd, 2020","text":""},{"location":"home/whats-new-2020/#admin-overview-dashboard","title":"Admin Overview Dashboard","text":"<p>A new admin overview dashboard that shows a more holistic view of multiple clusters. Applicable for customers with more than one cluster.</p>"},{"location":"home/whats-new-2021/","title":"Whats New 2021","text":""},{"location":"home/whats-new-2021/#december-8th-2021","title":"December 8th 2021","text":"<p>To comply with organizational policies and enhance the Run:ai platform security, Run:ai now supports Single Sign-On (SSO). This functionality is currently in beta and is available for new customer tenants only. For further details on SSO see Single Sign-On.</p> <p>To optimize resource management and utilization of Nvidia GPUs based on Ampere architecture, such as A100, Run:ai now supports dynamic creation and allocation of MIG partitions. This functionality is currently in beta. For further details on the dynamic allocation of MIG partitions see Dynamic MIG.</p> <p>Run:ai now supports AI workloads running in containerized clusters based on the VMWare Tanzu orchestrator. For further details on supported orchestrators see the prerequisites document.</p> <p>Supportability enhancements:</p> <ul> <li>A new \"Status History\" tab has been added to the job details view. The new tab shows the details of each status change of each job and allows researchers to analyze how to improve experiments as well as equip administrators with a tool to analyze running and historical jobs. In addition, the details of the reason a job is in the current status are available when hovering over the job status on the jobs table.</li> <li>To improve the ability to monitor the Run:ai environment, Run:ai components now expose alerts indicating whether the component is running. For further details on cluster monitoring see Cluster Monitoring</li> </ul> <p>User Experience (UX) enhancements:</p> <ul> <li>Run:ai cluster version is now available in the clusters list.</li> <li>Researchers can now submit and integrate with Git directly from the user interface.</li> </ul>"},{"location":"home/whats-new-2021/#october-29th-2021","title":"October 29th 2021","text":"<p>The Run:ai cluster now enforces the access definitions of the user and lists only jobs under permitted projects. For example, <code>runai list jobs</code>  will only show jobs from projects to which the researcher has access to.</p> <p>The Run:ai CLI <code>runai list projects</code> option now displays the quota definitions of each project.</p> <p>The Run:ai CLI port forwarding option now supports any IP address.</p> <p>The Run:ai CLI binary download is now signed with a checksum, to allow customers to validate the origin of the CLI and align with security best practices and standards.</p> <p>The Run:ai Researcher User Interface now supports setting GPU Memory as well as volumes in NFS servers.</p> <p>The Run:ai metrics used in the Dashboards are now officially documented and can be accessed via APIs as documented here.</p> <p>Run:ai now officially supports integration with Seldon Core. For more details read here.</p> <p>Run:ai now support VMWare Tanzu Kubernetes.</p>"},{"location":"home/whats-new-2021/#august-30th-2021","title":"August 30th 2021","text":"<p>Run:ai now supports a self-hosted installation. With the self-hosted installation the Run:ai control-plane which typically resides on the cloud, is deployed at the customer's data center. For further details on  supported installation types see Installation Types.</p> <p>Note</p> <p>The Run:ai self-hosted installation requires a dedicated license, and has different pricing than the SaaS installation. For more details contact your Run:ai account manager.</p> <p>NFS volumes can now be mounted directly to containers run by Run:ai while submitting jobs via Run:ai. See the <code>--nfs-server</code> flag of runai submit.</p> <p>To ease the manageability of user templates, Run:ai now supports global user templates. Global user templates are user templates that are managed by the Run:ai administrator and are available for all the projects within a specific cluster. The purpose of global user templates is to help define and enforce cross-organization resource policies.</p> <p>To simplify researchers' job submission via the Run:ai Researcher User Interface (UI), the UI now supports autocomplete, which is based on pre-defined values, as configured by the Administrator using the administrative templates.</p> <p>Run:ai extended the usage of Cluster name, as defined by the Administrator while configuring clusters at Run:ai. The Cluster name is now populated to the Run:ai dashboards as well as the Researcher UI.</p> <p>The original command line, which was used for running a Job, is now shown under the Job details under the General tab.</p>"},{"location":"home/whats-new-2021/#august-4th-2021","title":"August 4th 2021","text":"<p>Researcher User Interface (UI) enhancements:</p> <ul> <li>Revised user interface and user experience</li> <li>Researchers can create templates for the ease of jobs submission. Templates can be saved and used at the project level</li> <li>Researchers can be easily re-submit jobs from the Submit page or directly from the jobs list on the Jobs page</li> <li>Administrators can create administrative templates which set cluster-wide defaults, constraints, and defaults for the submission of Jobs. </li> <li>Different teams can collaborate and share templates by exporting and importing templates in the Submit screen</li> </ul> <p>Researcher Command Line Interface (CLI) enhancements:</p> <ul> <li>Jobs can be manually suspended and resumed using the new commands: <code>runai suspend</code> &amp; <code>runai resume</code></li> <li>A new command was added: <code>runai top job</code></li> </ul> <p>Kubeflow integration is now supported. The new integration allows building ML pipelines in Kubeflow Pipelines as well as Kubeflow Notebooks and run the workloads via the Run:ai platform. For further details see Integrate Run:ai with Kubeflow.</p> <p>Mlflow integration is now supported. For further details see Integrate Run:ai with MLflow.</p> <p>Run:ai Projects are implemented as Kubernetes namespaces. Run:ai now supports customizable namespace names. For further details see Manual Creation of Namespaces.</p>"},{"location":"home/whats-new-2021/#may-10th-2021","title":"May 10th 2021","text":"<p>Usability improvements of Run:ai Command-line interface (CLI). The CLI now supports autocomplete for all options and parameters.</p> <p>Usability improvements of the Administration user interface navigation menu now allow for easier navigation.</p> <p>Run:ai can be installed when Kubernetes has Pod Security Policy (PSP) enabled.</p>"},{"location":"home/whats-new-2021/#april-20th-2021","title":"April 20th 2021","text":"<p>Job List and Node list now show the GPU type (e.g. v-100).</p>"},{"location":"home/whats-new-2021/#april-18th-2021","title":"April 18th, 2021","text":"<p>Inference workloads are now supported. For further details see Inference Overview.</p> <p>JupyterHub integration is now supported. For further details see JupyterHub Integration</p> <p>NVIDIA MIG is now supported. You can use the NVIDIA MIG technology to partition A-100 GPUs. Each partition will be considered as a single GPU by the Run:ai system and all the Run:ai functionality is supported in the partition level, including GPU Fractions.</p>"},{"location":"home/whats-new-2021/#april-1st-2021","title":"April 1st, 2021","text":"<p>Run:ai now supports Kubernetes 1.20</p>"},{"location":"home/whats-new-2021/#march-24th-2021","title":"March 24th 2021","text":"<p>Job List and Node list now show CPU utilization and CPU memory utilization.</p>"},{"location":"home/whats-new-2021/#february-14th-2021","title":"February 14th, 2021","text":"<p>The Job list now shows per-Job graphs for GPU utilization, GPU memory. </p> <p>The Node list now shows per-Node graphs for GPU utilization, GPU memory. </p>"},{"location":"home/whats-new-2021/#january-22nd-2021","title":"January 22nd, 2021","text":"<p>New Analytics dashboard with emphasis on CPU, CPU Memory, GPU, and GPU Memory. Allows better diagnostics of resource misuse. </p>"},{"location":"home/whats-new-2021/#january-15th-2021","title":"January 15th, 2021","text":"<p>A new developer documentation area has been created. In it:</p> <ul> <li>New documentation for Researcher REST API.</li> <li>New documentation for Administration Rest API.</li> <li>Kubernetes-based API for job creation.</li> </ul>"},{"location":"home/whats-new-2021/#january-9th-2021","title":"January 9th 2021","text":"<p>A new Researcher user interface is now available.</p>"},{"location":"home/whats-new-2021/#january-2nd-2021","title":"January 2nd, 2021","text":"<p>Run:ai Clusters now support Azure Managed Kubernetes Service (AKS)</p>"},{"location":"home/whats-new-2022/","title":"Whats New 2022","text":""},{"location":"home/whats-new-2022/#july-2022-runai-version-27","title":"July 2022 Run:ai Version 2.7","text":"<ul> <li>New Audit Log API is now available. The last login indication is now showing at the bottom left of the screen for single-sign-on users as well as regular users. </li> <li>Built-in Tensorboard support in the Run:ai user interface.</li> <li>You can now submit a Job and allocate Extended Kubernetes Resources. Extended resources are third-party devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that you want to allocate to your Job. The third-party vendor has extended Kubernetes using a Device Plugin. Run:ai now allows the allocation of these resources via the Run:ai user interface Job form as well as the Run:ai Workload API. </li> <li>You can now submit a job with additional Linux Capabilities. Linux capabilities allow the researcher to give the Job additional permissions without actually giving the Job root access to the node. Run:ai allows adding such capabilities to the Job via the Run:ai user interface Job form as well as the Run:ai Workload API.  </li> </ul>"},{"location":"home/whats-new-2022/#june-2022-runai-version-26-cloud-update-only","title":"June 2022 Run:ai Version 2.6 (Cloud update only)","text":"<ul> <li>The login screen now provides the capability to recover a password. </li> <li>With single-sign-on, you can now (optionally) map the user's first and last name from the organizational directory. See single-sign-on prerequisites</li> <li>A new user role of ML Engineer. The role allows the user to view and manage inference deployments and cluster resources. </li> <li>Clearer documentation on how to perform port-forwarding when accessing the Run:ai cluster from Windows.</li> <li>Using the Run:ai user interface it is now possible to clone an existing Job. The clone operation will open a Job form and allow you to change parameters before re-submitting. </li> </ul>"},{"location":"home/whats-new-2022/#may-2022-runai-version-25","title":"May 2022 Run:ai Version 2.5","text":"<ul> <li>Command-line interface installation The command-line interface utility is no longer a separate install. Instead is now installed by logging into the control plane and downloading the utility which matches the cluster's version. </li> </ul> <p>Warning</p> <p>The command-line interface utility for version 2.3 is not compatible with a cluster version of 2.5 or later. If you upgrade the cluster, you must also upgrade the command-line interface. </p> <ul> <li>Inference. Run:ai inference offering has been overhauled with the ability to submit deployments via the user interface and a new and consistent API. For more information see Inference overview. To enable the new inference module call by Run:ai customer support.</li> <li>CPU and CPU memory quotas can now be configured for projects and departments. These are hard quotas which means that the total amount of the requested resource for all workloads associated with a project/department cannot exceed the set limit. To enable this feature please call Run:ai customer support.</li> <li>Workloads. We have revamped the way Run:ai submits Jobs. Run:ai now submits Workloads. The change includes:<ul> <li>New Cluster API. The older API has been deprecated and remains for backward compatibility. The API creates all the resources required for the run, including volumes, services, and the like. It also deletes all resources when the workload itself is deleted. </li> <li>Administrative templates have been replaced with Policies. Policies apply across all ways to submit jobs: command-line, API, and user interface. </li> </ul> </li> <li><code>runai delete</code> has been changed in favor of <code>runai delete job</code> </li> <li>Self-hosted installation: The default OpenShift installation is now set to work with a configured Openshift IdP. See creation of backend values for more information. In addition, the default for OpenShift is now HTTPS.</li> <li>To send logs to Run:ai customer support there is a utility to package all logs into one tar file. Version 2.5 brings a new method that automatically sends all new logs to Run:ai support servers for a set amount of time. See collecting logs for more information.</li> <li>It is now possible to mount an S3 bucket into a Run:ai Job. The option is only available via the command-line interface. For more information see runai submit.</li> <li>User interface improvements: The top navigation bar of the Run:ai user interface has been improved and now allows users to easily access everything related to the account, as well as multiple helpful links to the product documentation, CLI and APIs. </li> <li>Researcher Authentication configuration is now mandatory. </li> </ul>"},{"location":"home/whats-new-2022/#newly-supported-versions","title":"Newly Supported Versions","text":"<ul> <li>Run:ai now supports Kubernetes 1.24</li> <li>Run:ai now supports OpenShift 4.10</li> <li>Distributed training now supports MPI version 0.3. Support for older versions of MPI has been removed. </li> </ul>"},{"location":"home/whats-new-2022/#april-2022-runai-version-24-controlled-release-only","title":"April 2022 Run:ai Version 2.4 (Controlled Release only)","text":""},{"location":"home/whats-new-2022/#important-upgrade-note","title":"Important Upgrade Note","text":"<p>This version contains a significant change in the way that Run:ai uses and installs NVIDIA pre-requisites. Prior to this version, Run:ai has installed its own variants of two NVIDIA components: NVIDIA device plugin and NVIDIA DCGM Exporter. </p> <p>As these two variants are no longer needed, Run:ai now uses the standard NVIDIA installation which makes the Run:ai installation experience simpler. It does however require non-trivial changes when upgrading from older versions of Run:ai. </p> <p>Going forward, we also mandate the usage of the NVIDIA GPU Operator version 1.9. The Operator easies the installation of all NVIDIA software. Drivers and Kubernetes components alike. </p> <p>For further information see the Run:ai NVIDIA prerequisites as well as the Run:ai cluster upgrade.</p>"},{"location":"home/whats-new-2022/#dynamic-mig-support","title":"Dynamic MIG Support","text":"<p>Run:ai now supports the dynamic allocation of NVIDIA MIG slices. For further information see the document on fractions as well as the dynamic MIG quickstart.</p> <p>Other features:</p> <ul> <li>Run:ai now support fractions on GKE. GKE has a different software stack for NVIDIA. To install Run:ai on GKE please contact Run:ai customer support. </li> </ul>"},{"location":"home/whats-new-2022/#march-2022-runai-version-23","title":"March 2022 Run:ai Version 2.3","text":""},{"location":"home/whats-new-2022/#important-upgrade-note_1","title":"Important Upgrade Note","text":"<p>To upgrade to version 2.3 cluster from earlier versions, you must uninstall version 2.2 or earlier and only then install version 2.3. For detailed information see cluster upgrade.</p>"},{"location":"home/whats-new-2022/#unified-user-interface","title":"Unified User Interface","text":"<p>The Researcher user interface and the Administrator user interface have been unified into a single unified Run:ai user interface. The new user interface is served from <code>https://&lt;company-name&gt;.run.ai</code>. The user interface capabilities are subject to the role of the individual user. </p> <ul> <li>See instructions on how to set up the unified user interface. </li> <li>See user interface Jobs area for a description of how to submit, view and delete Jobs from the unified user interface. </li> </ul> <p>Other features:</p> <ul> <li>Additional information about scheduler decisions can now be found as part of the Job's status. View the Job status by running runai describe job or selecting a Job in the user interface and clicking <code>Status History</code>.</li> <li>Run:ai now support Charmed Kubernetes. </li> <li>Run:ai now supports orchestration of containerized virtual machines via KubeVirt. For more information see KubeVirt support.</li> <li>Run:ai now supports OpenShift 4.9, Kubernetes 1.22, and 1.23.</li> </ul>"},{"location":"home/whats-new-2022/#february-2022-runai-version-22-cloud-update-only","title":"February 2022 Run:ai Version 2.2 (Cloud update only)","text":"<ul> <li>When enabling Single-Sign, you can now use role groups. With groups, you no longer need to provide roles to individuals. Rather, you can create a group in the organization's directory and assign its members with specific Run:ai Roles such as Administrator, Researcher, and the like. For more information see single-sign-on.</li> <li>REST API has changed. The new API relies on <code>Applications</code>. See Calling REST APIs for more information. </li> <li>Added a new user role <code>Research Manager</code>. The role automatically assigns the user as a Researcher to all projects, including future projects. </li> </ul>"},{"location":"home/whats-new-2022/#january-2022-runai-version-20","title":"January 2022 Run:ai Version 2.0","text":"<p>We have now stabilized on a single version numbering system for all Run:ai artifacts: </p> <ul> <li>Run:ai Control plane.</li> <li>Run:ai Cluster.</li> <li>Run:ai Command-line interface.</li> <li>Run:ai Administrator Command-line interface.</li> </ul> <p>Future versions will be numbered using 2 digits (2.0, 2.1, 2.2, etc.). The numbering for the different artifacts will vary at the third digit as we provide patches to customers. As such, in the future, the control plane can be tagged as 2.1.0 while the cluster tagged as 2.1.1.</p>"},{"location":"home/whats-new-2022/#release-contents","title":"Release Contents","text":"<ul> <li>To allow for better control over resource allocation, the Run:ai platform now provides the ability to define different over-quota priorities for projects. For full details see Controlling over-quota behavior.</li> <li>To help review and track resource consumption per department, the Department object was added to multiple dashboard metrics.</li> </ul> <p>Supportability enhancements:</p> <ul> <li>A new tool was added, to allow IT administrators to validate cluster and control-plane installation prerequisites. For full details see cluster installation prerequisites, Kubernetes self-hosted prerequisites or OpenShift self-hosted prerequisites.</li> <li>To better analyze scheduling issues, the node name was added to multiple scheduler log events.</li> </ul>"}]}