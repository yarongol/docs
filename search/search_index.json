{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Run:ai Documentation Library \u00b6 Welcome to the Run:ai documentation area. For an introduction about what is the Run:ai Platform see Run:ai platform on the run.ai website. The Run:ai documentation is targeting three personas: Run:ai Administrator - Is responsible for the setup and the day-to-day administration of the product. Administrator documentation can be found here . Researcher - Using Run:ai to submit Jobs. Researcher documentation can be found here . Developer - Using various APIs to manipulate Jobs and integrate with other systems. Developer documentation can be found here . How to get support \u00b6 To get support use the following channels: On the navigation bar of the Run:ai user interface at <company-name>.run.ai , use the 'Support' button. Or submit a ticket by clicking the button below: Submit a Ticket Community \u00b6 Run:ai provides its customers with access to the Run:ai Customer Community portal in order to submit tickets, track ticket progress and update support cases. Customer Community Portal Reach out to customer support for credentials. Run:ai Cloud Status Page \u00b6 Run:ai cloud availability is monitored at status.run.ai . Collect Logs to Send to Support \u00b6 As an IT Administrator, you can collect Run:ai logs to send to support: Install the Run:ai Administrator command-line interface . Use one of the two options: One time collection: Run runai-adm collect-logs . The command will generate a compressed file containing all of the existing Run:ai log files. Continuous send Run runai-adm -d <HOURS_DURATION> . The command will send Run:ai logs directly to Run:ai support for the duration stated. Data sent will not include current logs. Only logs created going forward will be sent. Note Both options include logs of Run:ai components. They do not include logs of researcher containers that may contain private information. Example Code \u00b6 Code for the Docker images referred to on this site is available at https://github.com/run-ai/docs/tree/master/quickstart . The following images are used throughout the documentation: Image Description Source gcr.io/run-ai-demo/quickstart Basic training image. Multi-GPU support https://github.com/run-ai/docs/tree/master/quickstart/main gcr.io/run-ai-demo/quickstart-distributed Distributed training using MPI and Horovod https://github.com/run-ai/docs/tree/master/quickstart/distributed zembutsu/docker-sample-nginx Build (interactive) with Connected Ports https://github.com/zembutsu/docker-sample-nginx gcr.io/run-ai-demo/quickstart-hpo Hyperparameter Optimization https://github.com/run-ai/docs/tree/master/quickstart/hpo gcr.io/run-ai-demo/quickstart-x-forwarding Use X11 forwarding from Docker image https://github.com/run-ai/docs/tree/master/quickstart/x-forwarding gcr.io/run-ai-demo/pycharm-demo Image used for tool integration (PyCharm and VSCode) https://github.com/run-ai/docs/tree/master/quickstart/python%2Bssh gcr.io/run-ai-demo/example-triton-client and gcr.io/run-ai-demo/example-triton-server Basic Inference https://github.com/run-ai/models/tree/main/models/triton","title":"Overview"},{"location":"#runai-documentation-library","text":"Welcome to the Run:ai documentation area. For an introduction about what is the Run:ai Platform see Run:ai platform on the run.ai website. The Run:ai documentation is targeting three personas: Run:ai Administrator - Is responsible for the setup and the day-to-day administration of the product. Administrator documentation can be found here . Researcher - Using Run:ai to submit Jobs. Researcher documentation can be found here . Developer - Using various APIs to manipulate Jobs and integrate with other systems. Developer documentation can be found here .","title":"Run:ai Documentation Library"},{"location":"#how-to-get-support","text":"To get support use the following channels: On the navigation bar of the Run:ai user interface at <company-name>.run.ai , use the 'Support' button. Or submit a ticket by clicking the button below: Submit a Ticket","title":"How to get support"},{"location":"#community","text":"Run:ai provides its customers with access to the Run:ai Customer Community portal in order to submit tickets, track ticket progress and update support cases. Customer Community Portal Reach out to customer support for credentials.","title":"Community"},{"location":"#runai-cloud-status-page","text":"Run:ai cloud availability is monitored at status.run.ai .","title":"Run:ai Cloud Status Page"},{"location":"#collect-logs-to-send-to-support","text":"As an IT Administrator, you can collect Run:ai logs to send to support: Install the Run:ai Administrator command-line interface . Use one of the two options: One time collection: Run runai-adm collect-logs . The command will generate a compressed file containing all of the existing Run:ai log files. Continuous send Run runai-adm -d <HOURS_DURATION> . The command will send Run:ai logs directly to Run:ai support for the duration stated. Data sent will not include current logs. Only logs created going forward will be sent. Note Both options include logs of Run:ai components. They do not include logs of researcher containers that may contain private information.","title":"Collect Logs to Send to Support"},{"location":"#example-code","text":"Code for the Docker images referred to on this site is available at https://github.com/run-ai/docs/tree/master/quickstart . The following images are used throughout the documentation: Image Description Source gcr.io/run-ai-demo/quickstart Basic training image. Multi-GPU support https://github.com/run-ai/docs/tree/master/quickstart/main gcr.io/run-ai-demo/quickstart-distributed Distributed training using MPI and Horovod https://github.com/run-ai/docs/tree/master/quickstart/distributed zembutsu/docker-sample-nginx Build (interactive) with Connected Ports https://github.com/zembutsu/docker-sample-nginx gcr.io/run-ai-demo/quickstart-hpo Hyperparameter Optimization https://github.com/run-ai/docs/tree/master/quickstart/hpo gcr.io/run-ai-demo/quickstart-x-forwarding Use X11 forwarding from Docker image https://github.com/run-ai/docs/tree/master/quickstart/x-forwarding gcr.io/run-ai-demo/pycharm-demo Image used for tool integration (PyCharm and VSCode) https://github.com/run-ai/docs/tree/master/quickstart/python%2Bssh gcr.io/run-ai-demo/example-triton-client and gcr.io/run-ai-demo/example-triton-server Basic Inference https://github.com/run-ai/models/tree/main/models/triton","title":"Example Code"},{"location":"Researcher/overview-researcher/","text":"Overview: Researcher Documentation \u00b6 Researchers use Run:ai to submit jobs. As part of the Researcher documentation you will find: Quickstart Guides which provide step-by-step guides to Run:ai technology. Command line interface reference documentation. Best Practices for Deep Learning with Run:ai. Information about the Run:ai Scheduler . The Run:ai Python Researcher Library which you can optionally use in your container to get additional reporting and further resource optimization. Using Run:ai with various developer tools .","title":"Overview"},{"location":"Researcher/overview-researcher/#overview-researcher-documentation","text":"Researchers use Run:ai to submit jobs. As part of the Researcher documentation you will find: Quickstart Guides which provide step-by-step guides to Run:ai technology. Command line interface reference documentation. Best Practices for Deep Learning with Run:ai. Information about the Run:ai Scheduler . The Run:ai Python Researcher Library which you can optionally use in your container to get additional reporting and further resource optimization. Using Run:ai with various developer tools .","title":"Overview: Researcher Documentation"},{"location":"Researcher/use-cases/","text":"Use Cases \u00b6 This is a collection of various client-requested use cases. Each use case is accompanied by a short live-demo video, along with all the files used. Note For the most up-to-date information, check out the official Run:ai use-cases GitHub page. MLflow with Run:ai : experiment management is important for Data Scientists. This is a demo of how to set up and use MLflow with Run:ai. Introduction to Docker : Run:ai runs using Docker images. This is a brief introduction to Docker, image creation, and how to use them in the context of Run:ai. Please also check out the Persistent Environments use case if you wish to keep the creation of Docker images to a minimum. Tensorboard with Jupyter (ResNet demo) : Many Data Scientists like to use Tensorboard to keep an eye on the their current training experiments. They also like to have it side-by-side with Jupyter. In this demo, we will show how to integrate Tensorboard and Jupyter Lab within the context of Run:ai. Persistent Environments (with Conda/Mamba & Jupyter) : Some Data Scientists find creating Docker images for every single one of their environments a bit of a hindrance. They would often prefer the ability to create and alter environments on the fly and to have those environments remain, even after an image has finished running in a job. This demo shows users how they can create and persist Conda/Mamba environments using an NFS. Weights & Biases with Run:ai : W&B (Weights & Biases) is one of the best tools for experiment tracking and management. W&B is an official Run:ai partner. In this tutorial, we will demo how to use W&B alongside Run:ai","title":"Use Cases"},{"location":"Researcher/use-cases/#use-cases","text":"This is a collection of various client-requested use cases. Each use case is accompanied by a short live-demo video, along with all the files used. Note For the most up-to-date information, check out the official Run:ai use-cases GitHub page. MLflow with Run:ai : experiment management is important for Data Scientists. This is a demo of how to set up and use MLflow with Run:ai. Introduction to Docker : Run:ai runs using Docker images. This is a brief introduction to Docker, image creation, and how to use them in the context of Run:ai. Please also check out the Persistent Environments use case if you wish to keep the creation of Docker images to a minimum. Tensorboard with Jupyter (ResNet demo) : Many Data Scientists like to use Tensorboard to keep an eye on the their current training experiments. They also like to have it side-by-side with Jupyter. In this demo, we will show how to integrate Tensorboard and Jupyter Lab within the context of Run:ai. Persistent Environments (with Conda/Mamba & Jupyter) : Some Data Scientists find creating Docker images for every single one of their environments a bit of a hindrance. They would often prefer the ability to create and alter environments on the fly and to have those environments remain, even after an image has finished running in a job. This demo shows users how they can create and persist Conda/Mamba environments using an NFS. Weights & Biases with Run:ai : W&B (Weights & Biases) is one of the best tools for experiment tracking and management. W&B is an official Run:ai partner. In this tutorial, we will demo how to use W&B alongside Run:ai","title":"Use Cases"},{"location":"Researcher/Walkthroughs/quickstart-inference/","text":"Quickstart: Launch an Inference Workload \u00b6 Introduction \u00b6 Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output. With Inference, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time. Prerequisites \u00b6 To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster . There are additional prerequisites for running inference. See cluster installation prerequisites for more information. Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface You must have ML Engineer access rights. See Adding, Updating and Deleting Users for more information. Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 2 GPUs to the Project. Run an Inference Workload \u00b6 In the Run:ai user interface go to Deployments . If you do not see the Deployments section you may not have the required access control, or the inference module is disabled. Select New Deployment on the top right. Select team-a as a project and add an arbitrary name. Use the image gcr.io/run-ai-demo/example-triton-server . Under Resources add 0.5 GPUs. Under Auto Scaling select a minimum of 1, a maximum of 2. Use the concurrency autoscaling threshold method. Add a threshold of 3. Add a Container port of 8000 . This would start an inference workload for team-a with an allocation of a single GPU. Follow up on the Job's progress using the Deployment list in the user interface or by running runai list jobs Query the Inference Server \u00b6 The specific inference server we just created is accepting queries over port 8000. You can use the Run:ai Triton demo client to send requests to the server: Find an IP address by running kubectl get svc -n runai-team-a . Use the inference1-00001-private Cluster IP. Replace <IP> below and run: runai submit inference-client -i gcr.io/run-ai-demo/example-triton-client \\ -- perf_analyzer -m inception_graphdef -p 3600000 -u <IP> To see the result, run the following: runai logs inference-client View status on the Run:ai User Interface \u00b6 Open the Run:ai user interface. Under Deployments you can view the new Workload. When clicking the workload, note the utilization graphs go up. Stop Workload \u00b6 Use the user interface to delete the workload. See also \u00b6 You can also create Inference deployments via API. For more information see Submitting Workloads via YAML . See Deployment user interface.","title":"Inference"},{"location":"Researcher/Walkthroughs/quickstart-inference/#quickstart-launch-an-inference-workload","text":"","title":"Quickstart: Launch an Inference Workload"},{"location":"Researcher/Walkthroughs/quickstart-inference/#introduction","text":"Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output. With Inference, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time.","title":"Introduction"},{"location":"Researcher/Walkthroughs/quickstart-inference/#prerequisites","text":"To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster . There are additional prerequisites for running inference. See cluster installation prerequisites for more information. Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface You must have ML Engineer access rights. See Adding, Updating and Deleting Users for more information.","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/quickstart-inference/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/quickstart-inference/#setup","text":"Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 2 GPUs to the Project.","title":"Setup"},{"location":"Researcher/Walkthroughs/quickstart-inference/#run-an-inference-workload","text":"In the Run:ai user interface go to Deployments . If you do not see the Deployments section you may not have the required access control, or the inference module is disabled. Select New Deployment on the top right. Select team-a as a project and add an arbitrary name. Use the image gcr.io/run-ai-demo/example-triton-server . Under Resources add 0.5 GPUs. Under Auto Scaling select a minimum of 1, a maximum of 2. Use the concurrency autoscaling threshold method. Add a threshold of 3. Add a Container port of 8000 . This would start an inference workload for team-a with an allocation of a single GPU. Follow up on the Job's progress using the Deployment list in the user interface or by running runai list jobs","title":"Run an Inference Workload"},{"location":"Researcher/Walkthroughs/quickstart-inference/#query-the-inference-server","text":"The specific inference server we just created is accepting queries over port 8000. You can use the Run:ai Triton demo client to send requests to the server: Find an IP address by running kubectl get svc -n runai-team-a . Use the inference1-00001-private Cluster IP. Replace <IP> below and run: runai submit inference-client -i gcr.io/run-ai-demo/example-triton-client \\ -- perf_analyzer -m inception_graphdef -p 3600000 -u <IP> To see the result, run the following: runai logs inference-client","title":"Query the Inference Server"},{"location":"Researcher/Walkthroughs/quickstart-inference/#view-status-on-the-runai-user-interface","text":"Open the Run:ai user interface. Under Deployments you can view the new Workload. When clicking the workload, note the utilization graphs go up.","title":"View status on the Run:ai User Interface"},{"location":"Researcher/Walkthroughs/quickstart-inference/#stop-workload","text":"Use the user interface to delete the workload.","title":"Stop Workload"},{"location":"Researcher/Walkthroughs/quickstart-inference/#see-also","text":"You can also create Inference deployments via API. For more information see Submitting Workloads via YAML . See Deployment user interface.","title":"See also"},{"location":"Researcher/Walkthroughs/quickstart-mig/","text":"Quickstart: Launch Workloads with NVIDIA Dynamic MIG \u00b6 Introduction \u00b6 A single GPU has a significant amount of memory. Ranging from a couple of gigabytes in older generations and up to 80GB per GPU in the later models of the latest NVIDIA GPU technology. A single GPU also has a vast amount of computing power. This amount of memory and computing power is important for processing large amounts of data, such as in training deep learning models. However, there are quite a few applications that do not need this power. Examples can be inference workloads and the model-creation phase. It would thus be convenient if we could divide up a GPU between various workloads, thus achieving better GPU utilization. Run:ai provides two alternatives for splitting GPUs: Fractions and Dynamic MIG allocation . The focus of this article is Dynamic MIG allocation. A detailed explanation of the two Run:ai offerings can be found here . Prerequisites \u00b6 To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface A machine with a single available NVIDIA A100 GPU. This can be achieved by allocating filler workloads to the other GPUs on the node, or by using Google Cloud which allows for the creation of a virtual node with a single A100 GPU. Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:ai user interface. Allocate 2 GPUs to the Project. Mark the node as a dynamic MIG node as described here . Run an Inference Workload - Single Replica \u00b6 At the GPU node level, run: nvidia-smi : +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.91.03 Driver Version: 460.91.03 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 A100-SXM4-40GB Off | 00000000:00:04.0 Off | On | | N/A 32C P0 42W / 400W | 0MiB / 40536MiB | N/A Default | | | | Enabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | MIG devices: | +------------------+----------------------+-----------+-----------------------+ | GPU GI CI MIG | Memory-Usage | Vol| Shared | | ID ID Dev | BAR1-Usage | SM Unc| CE ENC DEC OFA JPG| | | | ECC| | |==================+======================+===========+=======================| | No MIG devices found | +-----------------------------------------------------------------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ In the highlighted text above, note that: MIG is enabled (if Enabled has a star next to it, you need to reboot your machine). The GPU is not yet divided into devices . At the command-line run: runai config project team-a runai submit mig1 -i gcr.io/run-ai-demo/quickstart-cuda --gpu-memory 10GB runai submit mig2 -i gcr.io/run-ai-demo/quickstart-cuda --mig-profile 2g.10gb runai submit mig3 -i gcr.io/run-ai-demo/quickstart-cuda --mig-profile 2g.10gb We used two different methods to create MIG partitions: Stating the amount of GPU memory we require Requiring a partition of explicit size using NVIDIA terminology. Both methods achieve the same effect. They result in three MIG partitions of 10GB each. You can verify that by running nvidia-smi , at the GPU node level: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.91.03 Driver Version: 460.91.03 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 A100-SXM4-40GB Off | 00000000:00:04.0 Off | On | | N/A 47C P0 194W / 400W | 27254MiB / 40536MiB | N/A Default | | | | Enabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | MIG devices: | +------------------+----------------------+-----------+-----------------------+ | GPU GI CI MIG | Memory-Usage | Vol| Shared | | ID ID Dev | BAR1-Usage | SM Unc| CE ENC DEC OFA JPG| | | | ECC| | |==================+======================+===========+=======================| | 0 3 0 0 | 9118MiB / 9984MiB | 28 0 | 2 0 1 0 0 | | | 4MiB / 16383MiB | | | +------------------+----------------------+-----------+-----------------------+ | 0 4 0 1 | 9118MiB / 9984MiB | 28 0 | 2 0 1 0 0 | | | 4MiB / 16383MiB | | | +------------------+----------------------+-----------+-----------------------+ | 0 5 0 2 | 9016MiB / 9984MiB | 28 0 | 2 0 1 0 0 | | | 2MiB / 16383MiB | | | +------------------+----------------------+-----------+-----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 3 0 142213 C ./quickstart 9111MiB | | 0 4 0 146799 C ./quickstart 9111MiB | | 0 5 0 132219 C ./quickstart 9009MiB | +-----------------------------------------------------------------------------+ Highlighted above is a list of 3 MIG devices, each 10GB large. Total of 30GB (out of the 40GB on the GPU) You can also run the same command inside one of the containers: runai exec mig1 nvidia-smi . This will show a single device (the only one that the container sees from its point of view). Run: runai list to see the 3 jobs in Running state. We now want to allocate an interactive job with 20GB. Interactive jobs take precedence over the default training jobs: runai submit mig1-int -i gcr.io/run-ai-demo/quickstart-cuda \\ --interactive --gpu-memory 20G or similarly, runai submit mig1-int -i gcr.io/run-ai-demo/quickstart-cuda \\ --interactive --mig-profile 3g.20gb Using runai list and nvidia-smi on the host machine, you can see that: One training job is preempted, and its device is deleted. The new, interactive job starts running.","title":"Dynamic MIG"},{"location":"Researcher/Walkthroughs/quickstart-mig/#quickstart-launch-workloads-with-nvidia-dynamic-mig","text":"","title":"Quickstart: Launch Workloads with NVIDIA Dynamic MIG"},{"location":"Researcher/Walkthroughs/quickstart-mig/#introduction","text":"A single GPU has a significant amount of memory. Ranging from a couple of gigabytes in older generations and up to 80GB per GPU in the later models of the latest NVIDIA GPU technology. A single GPU also has a vast amount of computing power. This amount of memory and computing power is important for processing large amounts of data, such as in training deep learning models. However, there are quite a few applications that do not need this power. Examples can be inference workloads and the model-creation phase. It would thus be convenient if we could divide up a GPU between various workloads, thus achieving better GPU utilization. Run:ai provides two alternatives for splitting GPUs: Fractions and Dynamic MIG allocation . The focus of this article is Dynamic MIG allocation. A detailed explanation of the two Run:ai offerings can be found here .","title":"Introduction"},{"location":"Researcher/Walkthroughs/quickstart-mig/#prerequisites","text":"To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface A machine with a single available NVIDIA A100 GPU. This can be achieved by allocating filler workloads to the other GPUs on the node, or by using Google Cloud which allows for the creation of a virtual node with a single A100 GPU.","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/quickstart-mig/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/quickstart-mig/#setup","text":"Login to the Projects area of the Run:ai user interface. Allocate 2 GPUs to the Project. Mark the node as a dynamic MIG node as described here .","title":"Setup"},{"location":"Researcher/Walkthroughs/quickstart-mig/#run-an-inference-workload-single-replica","text":"At the GPU node level, run: nvidia-smi : +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.91.03 Driver Version: 460.91.03 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 A100-SXM4-40GB Off | 00000000:00:04.0 Off | On | | N/A 32C P0 42W / 400W | 0MiB / 40536MiB | N/A Default | | | | Enabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | MIG devices: | +------------------+----------------------+-----------+-----------------------+ | GPU GI CI MIG | Memory-Usage | Vol| Shared | | ID ID Dev | BAR1-Usage | SM Unc| CE ENC DEC OFA JPG| | | | ECC| | |==================+======================+===========+=======================| | No MIG devices found | +-----------------------------------------------------------------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ In the highlighted text above, note that: MIG is enabled (if Enabled has a star next to it, you need to reboot your machine). The GPU is not yet divided into devices . At the command-line run: runai config project team-a runai submit mig1 -i gcr.io/run-ai-demo/quickstart-cuda --gpu-memory 10GB runai submit mig2 -i gcr.io/run-ai-demo/quickstart-cuda --mig-profile 2g.10gb runai submit mig3 -i gcr.io/run-ai-demo/quickstart-cuda --mig-profile 2g.10gb We used two different methods to create MIG partitions: Stating the amount of GPU memory we require Requiring a partition of explicit size using NVIDIA terminology. Both methods achieve the same effect. They result in three MIG partitions of 10GB each. You can verify that by running nvidia-smi , at the GPU node level: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.91.03 Driver Version: 460.91.03 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 A100-SXM4-40GB Off | 00000000:00:04.0 Off | On | | N/A 47C P0 194W / 400W | 27254MiB / 40536MiB | N/A Default | | | | Enabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | MIG devices: | +------------------+----------------------+-----------+-----------------------+ | GPU GI CI MIG | Memory-Usage | Vol| Shared | | ID ID Dev | BAR1-Usage | SM Unc| CE ENC DEC OFA JPG| | | | ECC| | |==================+======================+===========+=======================| | 0 3 0 0 | 9118MiB / 9984MiB | 28 0 | 2 0 1 0 0 | | | 4MiB / 16383MiB | | | +------------------+----------------------+-----------+-----------------------+ | 0 4 0 1 | 9118MiB / 9984MiB | 28 0 | 2 0 1 0 0 | | | 4MiB / 16383MiB | | | +------------------+----------------------+-----------+-----------------------+ | 0 5 0 2 | 9016MiB / 9984MiB | 28 0 | 2 0 1 0 0 | | | 2MiB / 16383MiB | | | +------------------+----------------------+-----------+-----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 3 0 142213 C ./quickstart 9111MiB | | 0 4 0 146799 C ./quickstart 9111MiB | | 0 5 0 132219 C ./quickstart 9009MiB | +-----------------------------------------------------------------------------+ Highlighted above is a list of 3 MIG devices, each 10GB large. Total of 30GB (out of the 40GB on the GPU) You can also run the same command inside one of the containers: runai exec mig1 nvidia-smi . This will show a single device (the only one that the container sees from its point of view). Run: runai list to see the 3 jobs in Running state. We now want to allocate an interactive job with 20GB. Interactive jobs take precedence over the default training jobs: runai submit mig1-int -i gcr.io/run-ai-demo/quickstart-cuda \\ --interactive --gpu-memory 20G or similarly, runai submit mig1-int -i gcr.io/run-ai-demo/quickstart-cuda \\ --interactive --mig-profile 3g.20gb Using runai list and nvidia-smi on the host machine, you can see that: One training job is preempted, and its device is deleted. The new, interactive job starts running.","title":"Run an Inference Workload - Single Replica"},{"location":"Researcher/Walkthroughs/quickstart-overview/","text":"Below are a set of Quickstart documents. The purpose of these documents is to get you acquainted with an aspect of Run:ai in the simplest possible form. Follow the Quickstart documents below to learn more: Unattended training sessions Interactive build sessions Interactive build sessions with externalized services Using GPU Fractions Distributed Training Hyperparameter Optimization Over-Quota, Basic Fairness & Bin Packing Fairness Inference Dynamic MIG Most quickstarts rely on an image called gcr.io/run-ai-demo/quickstart . The image is based on TensorFlow Release 20-08 . This TensorFlow image has minimal requirements for CUDA and NVIDIA Compute Capability . If your GPUs do not meet these requirements, use gcr.io/run-ai-demo/quickstart:legacy instead.","title":"Run:ai Quickstart Guides"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/","text":"Quickstart: Launch Interactive Build Workloads with Connected Ports \u00b6 Introduction \u00b6 This Quickstart is an extension of the Quickstart document: Start and Use Interactive Build Workloads When starting a container with the Run:ai Command-Line Interface (CLI), it is sometimes needed to expose internal ports to the user. Examples are: accessing a Jupyter notebook, using the container from a development environment such as PyCharm. Exposing a Container Port \u00b6 There are four ways to expose ports in Kubernetes: Port Forwarding , NodePort , and LoadBalancer . The first two will always work. The others require a special setup by your administrator. The four methods are explained here . The document below provides an example based on Port Forwarding. Port Forwarding, Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:ai user interface. Add a Project named team-a . Run Workload \u00b6 At the command-line run: runai config project team-a runai submit nginx-test -i zembutsu/docker-sample-nginx --interactive \\ --service-type portforward --port 8080 :80 The Job is based on a sample NGINX webserver docker image zembutsu/docker-sample-nginx . Once accessed via a browser, the page shows the container name. Note the interactive flag which means the Job will not have a start or end. It is the Researcher's responsibility to close the Job. In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8080 to localhost as long as the runai submit command is not stopped It is possible to forward traffic from multiple IP addresses by using the \"--address\" parameter. Check the CLI reference for further details. The result will be: The job 'nginx-test-0' has been submitted successfully You can run ` runai describe job nginx-test-0 -p team-a ` to check the job status Waiting for pod to start running... INFO [ 0023 ] Job started Open access point ( s ) to service from localhost:8080 Forwarding from 127 .0.0.1:8080 -> 80 Forwarding from [ ::1 ] :8080 -> 80 Access the Webserver \u00b6 Open the following in the browser at http://localhost:8080 . You should see a web page with the name of the container. Stop Workload \u00b6 Press Ctrl-C in the shell to stop port forwarding. Then delete the Job by running runai delete job nginx-test See Also \u00b6 Develop on Run:ai using Visual Studio Code Develop on Run:ai using PyCharm Use a Jupyter notbook with Run:ai.","title":"Build with Connected Ports"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#quickstart-launch-interactive-build-workloads-with-connected-ports","text":"","title":"Quickstart: Launch Interactive Build Workloads with Connected Ports"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#introduction","text":"This Quickstart is an extension of the Quickstart document: Start and Use Interactive Build Workloads When starting a container with the Run:ai Command-Line Interface (CLI), it is sometimes needed to expose internal ports to the user. Examples are: accessing a Jupyter notebook, using the container from a development environment such as PyCharm.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#exposing-a-container-port","text":"There are four ways to expose ports in Kubernetes: Port Forwarding , NodePort , and LoadBalancer . The first two will always work. The others require a special setup by your administrator. The four methods are explained here . The document below provides an example based on Port Forwarding.","title":"Exposing a Container Port"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#port-forwarding-step-by-step-walkthrough","text":"","title":"Port Forwarding, Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#setup","text":"Login to the Projects area of the Run:ai user interface. Add a Project named team-a .","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#run-workload","text":"At the command-line run: runai config project team-a runai submit nginx-test -i zembutsu/docker-sample-nginx --interactive \\ --service-type portforward --port 8080 :80 The Job is based on a sample NGINX webserver docker image zembutsu/docker-sample-nginx . Once accessed via a browser, the page shows the container name. Note the interactive flag which means the Job will not have a start or end. It is the Researcher's responsibility to close the Job. In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8080 to localhost as long as the runai submit command is not stopped It is possible to forward traffic from multiple IP addresses by using the \"--address\" parameter. Check the CLI reference for further details. The result will be: The job 'nginx-test-0' has been submitted successfully You can run ` runai describe job nginx-test-0 -p team-a ` to check the job status Waiting for pod to start running... INFO [ 0023 ] Job started Open access point ( s ) to service from localhost:8080 Forwarding from 127 .0.0.1:8080 -> 80 Forwarding from [ ::1 ] :8080 -> 80","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#access-the-webserver","text":"Open the following in the browser at http://localhost:8080 . You should see a web page with the name of the container.","title":"Access the Webserver"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#stop-workload","text":"Press Ctrl-C in the shell to stop port forwarding. Then delete the Job by running runai delete job nginx-test","title":"Stop Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#see-also","text":"Develop on Run:ai using Visual Studio Code Develop on Run:ai using PyCharm Use a Jupyter notbook with Run:ai.","title":"See Also"},{"location":"Researcher/Walkthroughs/walkthrough-build/","text":"Quickstart: Launch Interactive Build Workloads \u00b6 Introduction \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Quickstart you will learn how to: Use the Run:ai command-line interface (CLI) to start a deep learning Build workload Open an ssh session to the Build workload Stop the Build workload It is also possible to open ports to specific services within the container. See \"Next Steps\" at the end of this article. Prerequisites \u00b6 To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface Step by Step Quickstart \u00b6 Setup \u00b6 Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 2 GPUs to the Project. Run Workload \u00b6 At the command-line run: runai config project team-a runai submit build1 -i ubuntu -g 1 --interactive -- sleep infinity The job is based on a sample docker image ubuntu We named the job build1 . Note the interactive flag which means the job will not have a start or end. It is the Researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. The command provided is sleep infinity . You must provide a command or the container will start and then exit immediately. Alternatively, replace these flags with --attach to attach immediately to a session. Follow up on the job's status by running: runai list jobs The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running A full list of Job statuses can be found here To get additional status on your job run: runai describe job build1 Get a Shell to the container \u00b6 Run: runai bash build1 This should provide a direct shell into the computer View status on the Run:ai User Interface \u00b6 Open the Run:ai user interface. Under \"Jobs\" you can view the new Workload: Stop Workload \u00b6 Run the following: runai delete job build1 This would stop the training workload. You can verify this by running runai list jobs again. Next Steps \u00b6 Expose internal ports to your interactive build workload: Quickstart Launch an Interactive Build Workload with Connected Ports . Follow the Quickstart document: Launch Unattended Training Workloads .","title":"Build"},{"location":"Researcher/Walkthroughs/walkthrough-build/#quickstart-launch-interactive-build-workloads","text":"","title":"Quickstart: Launch Interactive Build Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-build/#introduction","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Quickstart you will learn how to: Use the Run:ai command-line interface (CLI) to start a deep learning Build workload Open an ssh session to the Build workload Stop the Build workload It is also possible to open ports to specific services within the container. See \"Next Steps\" at the end of this article.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-build/#prerequisites","text":"To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-build/#step-by-step-quickstart","text":"","title":"Step by Step Quickstart"},{"location":"Researcher/Walkthroughs/walkthrough-build/#setup","text":"Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 2 GPUs to the Project.","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-build/#run-workload","text":"At the command-line run: runai config project team-a runai submit build1 -i ubuntu -g 1 --interactive -- sleep infinity The job is based on a sample docker image ubuntu We named the job build1 . Note the interactive flag which means the job will not have a start or end. It is the Researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. The command provided is sleep infinity . You must provide a command or the container will start and then exit immediately. Alternatively, replace these flags with --attach to attach immediately to a session. Follow up on the job's status by running: runai list jobs The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running A full list of Job statuses can be found here To get additional status on your job run: runai describe job build1","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build/#get-a-shell-to-the-container","text":"Run: runai bash build1 This should provide a direct shell into the computer","title":"Get a Shell to the container"},{"location":"Researcher/Walkthroughs/walkthrough-build/#view-status-on-the-runai-user-interface","text":"Open the Run:ai user interface. Under \"Jobs\" you can view the new Workload:","title":"View status on the Run:ai User Interface"},{"location":"Researcher/Walkthroughs/walkthrough-build/#stop-workload","text":"Run the following: runai delete job build1 This would stop the training workload. You can verify this by running runai list jobs again.","title":"Stop Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build/#next-steps","text":"Expose internal ports to your interactive build workload: Quickstart Launch an Interactive Build Workload with Connected Ports . Follow the Quickstart document: Launch Unattended Training Workloads .","title":"Next Steps"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/","text":"Quickstart: Launch Distributed Training Workloads \u00b6 Introduction \u00b6 Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker node . Worker nodes work in parallel to speed up model training. Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container . Getting Distributed Training to work is more complex than multi-GPU training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Several Deep Learning frameworks support Distributed Training. Horovod is a good example. Run:ai provides the ability to run, manage, and view Distributed Training workloads. The following is a Quickstart document for such a scenario. Prerequisites \u00b6 To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster During the installation, you have installed the Kubeflow MPI Operator as specified here Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 2 GPUs to the Project. Run Training Distributed Workload \u00b6 At the command-line run: runai config project team-a runai submit-mpi dist --processes = 2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 -e RUNAI_SLEEP_SECS = 60 We named the Job dist The Job is assigned to team-a There will be two worker processes (--processes=2), each allocated with a single GPU (-g 1) The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 . The image contains a startup script that runs a deep learning Horovod-based workload. Follow up on the Job's status by running: runai list jobs The result: The Run:ai scheduler ensures that all processes can run together. You can see the list of workers as well as the main \"launcher\" process by running: runai describe job dist You will see two worker processes (pods) their status and on which node they run: To see the merged logs of all pods run: runai logs dist Finally, you can delete the distributed training workload by running: runai delete job dist Run an Interactive Distributed Workload \u00b6 It is also possible to run a distributed training Job as \"interactive\". This is useful if you want to test your distributed training Job before committing on a long, unattended training session. To run such a session use: runai submit-mpi dist-int --processes = 2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 --interactive \\ -- sh -c \"sleep infinity\" When the workers are running run: runai bash dist-int This will provide shell access to the launcher process. From there, you can run your distributed workload. For Horovod version smaller than 0.17.0 run: horovodrun -np $RUNAI_MPI_NUM_WORKERS \\ python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model = resnet20 --num_batches = 1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size = 64 --variable_update = horovod For Horovod version 0.17.0 or later, add the -hostfile flag as follows: horovodrun -np $RUNAI_MPI_NUM_WORKERS -hostfile /etc/mpi/hostfile \\ python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model = resnet20 --num_batches = 1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size = 64 --variable_update = horovod The environment variable RUNAI_MPI_NUM_WORKERS is passed by Run:ai and contains the number of worker processes provided to the runai submit-mpi command (in the above example the value is 2). See Also \u00b6 The source code of the image used in this Quickstart document is in Github For a full list of the submit-mpi options see runai submit-mpi","title":"Distributed Training Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#quickstart-launch-distributed-training-workloads","text":"","title":"Quickstart: Launch Distributed Training Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#introduction","text":"Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker node . Worker nodes work in parallel to speed up model training. Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container . Getting Distributed Training to work is more complex than multi-GPU training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Several Deep Learning frameworks support Distributed Training. Horovod is a good example. Run:ai provides the ability to run, manage, and view Distributed Training workloads. The following is a Quickstart document for such a scenario.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#prerequisites","text":"To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster During the installation, you have installed the Kubeflow MPI Operator as specified here Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#setup","text":"Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 2 GPUs to the Project.","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-training-distributed-workload","text":"At the command-line run: runai config project team-a runai submit-mpi dist --processes = 2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 -e RUNAI_SLEEP_SECS = 60 We named the Job dist The Job is assigned to team-a There will be two worker processes (--processes=2), each allocated with a single GPU (-g 1) The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 . The image contains a startup script that runs a deep learning Horovod-based workload. Follow up on the Job's status by running: runai list jobs The result: The Run:ai scheduler ensures that all processes can run together. You can see the list of workers as well as the main \"launcher\" process by running: runai describe job dist You will see two worker processes (pods) their status and on which node they run: To see the merged logs of all pods run: runai logs dist Finally, you can delete the distributed training workload by running: runai delete job dist","title":"Run Training Distributed Workload"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-an-interactive-distributed-workload","text":"It is also possible to run a distributed training Job as \"interactive\". This is useful if you want to test your distributed training Job before committing on a long, unattended training session. To run such a session use: runai submit-mpi dist-int --processes = 2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 --interactive \\ -- sh -c \"sleep infinity\" When the workers are running run: runai bash dist-int This will provide shell access to the launcher process. From there, you can run your distributed workload. For Horovod version smaller than 0.17.0 run: horovodrun -np $RUNAI_MPI_NUM_WORKERS \\ python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model = resnet20 --num_batches = 1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size = 64 --variable_update = horovod For Horovod version 0.17.0 or later, add the -hostfile flag as follows: horovodrun -np $RUNAI_MPI_NUM_WORKERS -hostfile /etc/mpi/hostfile \\ python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model = resnet20 --num_batches = 1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size = 64 --variable_update = horovod The environment variable RUNAI_MPI_NUM_WORKERS is passed by Run:ai and contains the number of worker processes provided to the runai submit-mpi command (in the above example the value is 2).","title":"Run an Interactive Distributed Workload"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#see-also","text":"The source code of the image used in this Quickstart document is in Github For a full list of the submit-mpi options see runai submit-mpi","title":"See Also"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/","text":"Quickstart: Launch Workloads with GPU Fractions \u00b6 Introduction \u00b6 Run:ai provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs. Run:ai\u2019s fractional GPU system effectively creates logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves. A typical use-case could see 2-8 Jobs running on the same GPU, meaning you could do eight times the work with the same hardware. Prerequisites \u00b6 To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 1 GPU to the Project. Run Workload \u00b6 At the command-line run: runai config project team-a runai submit frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 --interactive runai submit frac03 -i gcr.io/run-ai-demo/quickstart -g 0.3 The Jobs are based on a sample docker image gcr.io/run-ai-demo/quickstart the image contains a startup script that runs a deep learning TensorFlow-based workload. We named the Jobs frac05 and frac03 respectively. Note that fractions may or may not use the --interactive flag. Setting the flag means that the Job will not automatically finish. Rather, it is the Researcher's responsibility to delete the Job. Fractions support both Interactive and non-interactive Jobs. The Jobs are assigned to team-a with an allocation of a single GPU. Follow up on the Job's status by running: runai list jobs The result: Note that both Jobs were allocated to the same node. When both Jobs are running, bash into one of them: runai bash frac05 Now, inside the container, run: nvidia-smi The result: Notes: The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs. The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception Use Exact GPU Memory \u00b6 Instead of requesting a fraction of the GPU, you can ask for specific GPU memory requirements. For example: runai submit -i gcr.io/run-ai-demo/quickstart --gpu-memory 5G Which will provide 5GB of GPU memory.","title":"GPU Fractions"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#quickstart-launch-workloads-with-gpu-fractions","text":"","title":"Quickstart: Launch Workloads with GPU Fractions"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#introduction","text":"Run:ai provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs. Run:ai\u2019s fractional GPU system effectively creates logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves. A typical use-case could see 2-8 Jobs running on the same GPU, meaning you could do eight times the work with the same hardware.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#prerequisites","text":"To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#setup","text":"Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 1 GPU to the Project.","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#run-workload","text":"At the command-line run: runai config project team-a runai submit frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 --interactive runai submit frac03 -i gcr.io/run-ai-demo/quickstart -g 0.3 The Jobs are based on a sample docker image gcr.io/run-ai-demo/quickstart the image contains a startup script that runs a deep learning TensorFlow-based workload. We named the Jobs frac05 and frac03 respectively. Note that fractions may or may not use the --interactive flag. Setting the flag means that the Job will not automatically finish. Rather, it is the Researcher's responsibility to delete the Job. Fractions support both Interactive and non-interactive Jobs. The Jobs are assigned to team-a with an allocation of a single GPU. Follow up on the Job's status by running: runai list jobs The result: Note that both Jobs were allocated to the same node. When both Jobs are running, bash into one of them: runai bash frac05 Now, inside the container, run: nvidia-smi The result: Notes: The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs. The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#use-exact-gpu-memory","text":"Instead of requesting a fraction of the GPU, you can ask for specific GPU memory requirements. For example: runai submit -i gcr.io/run-ai-demo/quickstart --gpu-memory 5G Which will provide 5GB of GPU memory.","title":"Use Exact GPU Memory"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/","text":"Quickstart: Hyperparameter Optimization \u00b6 Introduction \u00b6 Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter can be a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine results to decide what works best. There are several strategies for searching the hyperparameter space. Most notable are Random search and Grid search . The former, as its name implies, selects parameters at random while the latter does an exhaustive search from a list of pre-selected values. Run:ai provides the ability to run, manage, and view HPO runs. The following is a Quickstart of such a scenario. Prerequisites \u00b6 To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 2 GPUs to the Project. On shared storage create a library to store HPO results. E.g. /nfs/john/hpo . Pods \u00b6 With HPO, we introduce the concept of Pods . Pods are units of work within a Job. Typically, each Job has a single Pod. However, with HPO as well as with Distributed Training there are multiple Pods per Job. Pods are independent All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, and memory. HPO Sample Code \u00b6 The Quickstart code uses the Run:ai HPO pythong library github.com/run-ai/docs . And needs to be installed within the image. Below are some highlights of the code: # import Run:ai HPO library import runai.hpo # select Random search or grid search strategy = runai . hpo . Strategy . GridSearch # initialize the Run:ai HPO library. Send the NFS directory used for sync runai . hpo . init ( \"/nfs\" ) # pick a configuration for this HPO experiment # we pass the options of all hyperparameters we want to test # `config` will hold a single value for each parameter config = runai . hpo . pick ( grid = dict ( batch_size = [ 32 , 64 , 128 ], lr = [ 1 , 0.1 , 0.01 , 0.001 ]), strategy = strategy ) .... # Use the selected configuration within your code optimizer = keras . optimizers . SGD ( lr = config [ 'lr' ]) Run an HPO Workload \u00b6 At the command-line run: runai config project team-a runai submit hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/nfs We named the Job hpo1 The Job is assigned to team-a The Job will be complete when 12 pods will run ( --completions 12 ), each allocated with a single GPU ( -g 1 ) At most, there will be 3 pods running concurrently ( --parallelism 3 ) The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart-hpo . The image contains a startup script that selects a set of hyperparameters and then uses them, as described above. The command maps a shared volume /nfs/john/hpo to a directory in the container /nfs . The running pods will use the directory to sync hyperparameters and save results. Follow up on the Job's status by running: runai list jobs The result: Follow up on the Job's pods by running: runai describe job hpo1 You will see 3 running pods currently executing: Once the 3 pods are done, they will be replaced by new ones from the 12 completions . This process will continue until all 12 have run. You can also submit Jobs on another Project until only 2 GPUs remain. This will preempt 1 pod and will henceforth limit the HPO Job to run on 2 pods only. Preempted pods will be picked up and ran later. You can see logs of specific pods by running : runai logs hpo1 --pod <POD-NAME> where <<POD-NAME>> is a pod name as appears above in the runai describe job hpo1 output The logs will contain a couple of lines worth noting: Picked HPO experiment #4 ... Using HPO directory /hpo Using configuration: {'batch_size': 32, 'lr': 0.001} Examine the Results \u00b6 The Run:ai HPO library saves the experiment variations and the experiment results to a single file, making it easier to pick the best HPO run. The file can be found in the shared folder. Below is a snapshot of the file for two experiments with two epochs each: creationTime : 24/08/2020 08:50:06 experiments : - config : batch_size : 32 lr : 1 id : 1 modificationTime : 24/08/2020 08:50:06 reports : - epoch : 0 metrics : acc : 0.09814 loss : 2.310984723968506 val_acc : 0.1 val_loss : 2.3098626373291014 reportTime : 24/08/2020 08:52:11 - epoch : 1 metrics : acc : 0.09914 loss : 2.30994320602417 val_acc : 0.1 val_loss : 2.3110838134765626 reportTime : 24/08/2020 08:54:10 - config : batch_size : 32 lr : 0.1 id : 2 modificationTime : 24/08/2020 08:50:36 reports : - epoch : 0 metrics : acc : 0.11012 loss : 2.2979678358459474 val_acc : 0.1667 val_loss : 2.268467852783203 reportTime : 24/08/2020 08:52:44 - epoch : 1 metrics : acc : 0.2047 loss : 2.0894255745697023 val_acc : 0.2833 val_loss : 1.8615504817962647 reportTime : 24/08/2020 08:54:45 Finally, you can delete the HPO Job by running: runai delete job hpo1 See Also \u00b6 For further information on the Run:ai HPO support library see: The Run:ai HPO Support Library Sample code in Github","title":"Hyperparameter Optimization"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#quickstart-hyperparameter-optimization","text":"","title":"Quickstart: Hyperparameter Optimization"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#introduction","text":"Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter can be a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine results to decide what works best. There are several strategies for searching the hyperparameter space. Most notable are Random search and Grid search . The former, as its name implies, selects parameters at random while the latter does an exhaustive search from a list of pre-selected values. Run:ai provides the ability to run, manage, and view HPO runs. The following is a Quickstart of such a scenario.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#prerequisites","text":"To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#setup","text":"Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 2 GPUs to the Project. On shared storage create a library to store HPO results. E.g. /nfs/john/hpo .","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#pods","text":"With HPO, we introduce the concept of Pods . Pods are units of work within a Job. Typically, each Job has a single Pod. However, with HPO as well as with Distributed Training there are multiple Pods per Job. Pods are independent All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, and memory.","title":"Pods"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#hpo-sample-code","text":"The Quickstart code uses the Run:ai HPO pythong library github.com/run-ai/docs . And needs to be installed within the image. Below are some highlights of the code: # import Run:ai HPO library import runai.hpo # select Random search or grid search strategy = runai . hpo . Strategy . GridSearch # initialize the Run:ai HPO library. Send the NFS directory used for sync runai . hpo . init ( \"/nfs\" ) # pick a configuration for this HPO experiment # we pass the options of all hyperparameters we want to test # `config` will hold a single value for each parameter config = runai . hpo . pick ( grid = dict ( batch_size = [ 32 , 64 , 128 ], lr = [ 1 , 0.1 , 0.01 , 0.001 ]), strategy = strategy ) .... # Use the selected configuration within your code optimizer = keras . optimizers . SGD ( lr = config [ 'lr' ])","title":"HPO Sample Code"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#run-an-hpo-workload","text":"At the command-line run: runai config project team-a runai submit hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/nfs We named the Job hpo1 The Job is assigned to team-a The Job will be complete when 12 pods will run ( --completions 12 ), each allocated with a single GPU ( -g 1 ) At most, there will be 3 pods running concurrently ( --parallelism 3 ) The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart-hpo . The image contains a startup script that selects a set of hyperparameters and then uses them, as described above. The command maps a shared volume /nfs/john/hpo to a directory in the container /nfs . The running pods will use the directory to sync hyperparameters and save results. Follow up on the Job's status by running: runai list jobs The result: Follow up on the Job's pods by running: runai describe job hpo1 You will see 3 running pods currently executing: Once the 3 pods are done, they will be replaced by new ones from the 12 completions . This process will continue until all 12 have run. You can also submit Jobs on another Project until only 2 GPUs remain. This will preempt 1 pod and will henceforth limit the HPO Job to run on 2 pods only. Preempted pods will be picked up and ran later. You can see logs of specific pods by running : runai logs hpo1 --pod <POD-NAME> where <<POD-NAME>> is a pod name as appears above in the runai describe job hpo1 output The logs will contain a couple of lines worth noting: Picked HPO experiment #4 ... Using HPO directory /hpo Using configuration: {'batch_size': 32, 'lr': 0.001}","title":"Run an HPO Workload"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#examine-the-results","text":"The Run:ai HPO library saves the experiment variations and the experiment results to a single file, making it easier to pick the best HPO run. The file can be found in the shared folder. Below is a snapshot of the file for two experiments with two epochs each: creationTime : 24/08/2020 08:50:06 experiments : - config : batch_size : 32 lr : 1 id : 1 modificationTime : 24/08/2020 08:50:06 reports : - epoch : 0 metrics : acc : 0.09814 loss : 2.310984723968506 val_acc : 0.1 val_loss : 2.3098626373291014 reportTime : 24/08/2020 08:52:11 - epoch : 1 metrics : acc : 0.09914 loss : 2.30994320602417 val_acc : 0.1 val_loss : 2.3110838134765626 reportTime : 24/08/2020 08:54:10 - config : batch_size : 32 lr : 0.1 id : 2 modificationTime : 24/08/2020 08:50:36 reports : - epoch : 0 metrics : acc : 0.11012 loss : 2.2979678358459474 val_acc : 0.1667 val_loss : 2.268467852783203 reportTime : 24/08/2020 08:52:44 - epoch : 1 metrics : acc : 0.2047 loss : 2.0894255745697023 val_acc : 0.2833 val_loss : 1.8615504817962647 reportTime : 24/08/2020 08:54:45 Finally, you can delete the HPO Job by running: runai delete job hpo1","title":"Examine the Results"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#see-also","text":"For further information on the Run:ai HPO support library see: The Run:ai HPO Support Library Sample code in Github","title":"See Also"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/","text":"Quickstart: Over-Quota and Bin Packing \u00b6 Goals \u00b6 The goal of this Quickstart is to explain the concepts of over-quota and bin-packing (consolidation) and how they help in maximizing cluster utilization: Show the simplicity of resource provisioning, and how resources are abstracted from users. Show how the system eliminates compute bottlenecks by allowing teams/users to go over their resource quota if there are free GPUs in the cluster. Setup and configuration: \u00b6 4 GPUs on 2 machines with 2 GPUs each 2 Projects: team-a and team-b with 2 allocated GPUs each Run:ai canonical image gcr.io/run-ai-demo/quickstart Part I: Over-quota \u00b6 Run the following commands: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a has 3 GPUs allocated. Which is over its quota by 1 GPU. The system allows this over-quota as long as there are available resources The system is at full capacity with all GPUs utilized. Part 2: Basic Fairness via Preemption \u00b6 Run the following command: runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a can no longer remain in over-quota. Thus, one Job, must be preempted : moved out to allow team-b to grow. Run:ai scheduler chooses to preempt Job a1 . It is important that unattended Jobs will save checkpoints . This will ensure that whenever Job a1 resume, it will do so from where it left off. Part 3: Bin Packing \u00b6 Run the following command: runai delete job a2 -p team-a a1 is now going to start running again. Run: runai list jobs -A You have two Jobs that are running on the first node and one Job that is running alone the second node. Choose one of the two Jobs from the full node and delete it: runai delete job <job-name> -p <project> The status now is: Now, run a 2 GPU Job: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a The status now is: Discussion Note that Job a1 has been preempted and then restarted on the second node, in order to clear space for the new a2 Job. This is bin-packing or consolidation","title":"Over-Quota, Basic Fairness & Bin-Packing"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#quickstart-over-quota-and-bin-packing","text":"","title":"Quickstart: Over-Quota and Bin Packing"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#goals","text":"The goal of this Quickstart is to explain the concepts of over-quota and bin-packing (consolidation) and how they help in maximizing cluster utilization: Show the simplicity of resource provisioning, and how resources are abstracted from users. Show how the system eliminates compute bottlenecks by allowing teams/users to go over their resource quota if there are free GPUs in the cluster.","title":"Goals"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#setup-and-configuration","text":"4 GPUs on 2 machines with 2 GPUs each 2 Projects: team-a and team-b with 2 allocated GPUs each Run:ai canonical image gcr.io/run-ai-demo/quickstart","title":"Setup and configuration:"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-i-over-quota","text":"Run the following commands: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a has 3 GPUs allocated. Which is over its quota by 1 GPU. The system allows this over-quota as long as there are available resources The system is at full capacity with all GPUs utilized.","title":"Part I: Over-quota"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-2-basic-fairness-via-preemption","text":"Run the following command: runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a can no longer remain in over-quota. Thus, one Job, must be preempted : moved out to allow team-b to grow. Run:ai scheduler chooses to preempt Job a1 . It is important that unattended Jobs will save checkpoints . This will ensure that whenever Job a1 resume, it will do so from where it left off.","title":"Part 2: Basic Fairness via Preemption"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-3-bin-packing","text":"Run the following command: runai delete job a2 -p team-a a1 is now going to start running again. Run: runai list jobs -A You have two Jobs that are running on the first node and one Job that is running alone the second node. Choose one of the two Jobs from the full node and delete it: runai delete job <job-name> -p <project> The status now is: Now, run a 2 GPU Job: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a The status now is: Discussion Note that Job a1 has been preempted and then restarted on the second node, in order to clear space for the new a2 Job. This is bin-packing or consolidation","title":"Part 3: Bin Packing"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/","text":"Quickstart: Queue Fairness \u00b6 Goal \u00b6 The goal of this Quickstart is to explain fairness . The over-quota Quickstart shows basic fairness where allocated GPUs per Project are adhered to such that if a Project is in over-quota, its Job will be preempted once another Project requires its resources. This Quickstart is about queue fairness . It shows that Jobs will be scheduled fairly regardless of the time they have been submitted. As such, if a person in Project A has submitted 50 Jobs and soon after that, a person in Project B has submitted 25 Jobs, the Jobs in the queue will be processed fairly. Setup and configuration: \u00b6 4 GPUs on 2 machines with 2 GPUs each. 2 Projects: team-a and team-b with 1 allocated GPU each. Run:ai canonical image gcr.io/run-ai-demo/quickstart Part I: Immediate Displacement of Over-Quota \u00b6 Run the following commands: runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a System status after run: Discussion team-a, even though it has a single GPU as quota, is now using all 4 GPUs. Run the following commands: runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion Two team-b Jobs have immediately displaced team-a. team-a and team-b each have a quota of 1 GPU, thus the remaining over-quota (2 GPUs) is distributed equally between the Projects. Part 2: Queue Fairness \u00b6 Now lets start deleting Jobs. Alternatively, you can wait for Jobs to complete. runai delete job b2 -p team-b Discussion As the quotas are equal (1 for each Project, the remaining pending Jobs will get scheduled one by one alternating between Projects, regardless of the time in which they were submitted.","title":"Queue Fairness"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#quickstart-queue-fairness","text":"","title":"Quickstart: Queue Fairness"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#goal","text":"The goal of this Quickstart is to explain fairness . The over-quota Quickstart shows basic fairness where allocated GPUs per Project are adhered to such that if a Project is in over-quota, its Job will be preempted once another Project requires its resources. This Quickstart is about queue fairness . It shows that Jobs will be scheduled fairly regardless of the time they have been submitted. As such, if a person in Project A has submitted 50 Jobs and soon after that, a person in Project B has submitted 25 Jobs, the Jobs in the queue will be processed fairly.","title":"Goal"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#setup-and-configuration","text":"4 GPUs on 2 machines with 2 GPUs each. 2 Projects: team-a and team-b with 1 allocated GPU each. Run:ai canonical image gcr.io/run-ai-demo/quickstart","title":"Setup and configuration:"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-i-immediate-displacement-of-over-quota","text":"Run the following commands: runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a System status after run: Discussion team-a, even though it has a single GPU as quota, is now using all 4 GPUs. Run the following commands: runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion Two team-b Jobs have immediately displaced team-a. team-a and team-b each have a quota of 1 GPU, thus the remaining over-quota (2 GPUs) is distributed equally between the Projects.","title":"Part I: Immediate Displacement of Over-Quota"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-2-queue-fairness","text":"Now lets start deleting Jobs. Alternatively, you can wait for Jobs to complete. runai delete job b2 -p team-b Discussion As the quotas are equal (1 for each Project, the remaining pending Jobs will get scheduled one by one alternating between Projects, regardless of the time in which they were submitted.","title":"Part 2: Queue Fairness"},{"location":"Researcher/Walkthroughs/walkthrough-train/","text":"Quickstart: Launch Unattended Training Workloads \u00b6 Introduction \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Quickstart you will learn how to: Use the Run:ai command-line interface (CLI) to start a deep learning training workload. View training status and resource consumption using the Run:ai user interface and the Run:ai CLI. View training logs. Stop the training. Prerequisites \u00b6 To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 2 GPUs to the Project. Run Workload \u00b6 At the command-line run: runai config project team-a runai submit train1 -i gcr.io/run-ai-demo/quickstart -g 1 This would start an unattended training Job for team-a with an allocation of a single GPU. The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart . We named the Job train1 Follow up on the Job's progress by running: runai list jobs The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the Job is waiting to be scheduled Running - the Job is running Succeeded - the Job has ended A full list of Job statuses can be found here To get additional status on your Job run: runai describe job train1 View Logs \u00b6 Run the following: runai logs train1 You should see a log of a running deep learning session: View status on the Run:ai User Interface \u00b6 Open the Run:ai user interface. Under \"Jobs\" you can view the new Workload: The image we used for training includes the Run:ai Training library. Among other features, this library allows the reporting of metrics from within the deep learning Job. Metrics such as progress, accuracy, loss, and epoch and step numbers. Progress can be seen in the status column above. To see other metrics, press the settings wheel on the top right and select additional deep learning metrics from the list Under Nodes you can see node utilization: Stop Workload \u00b6 Run the following: runai delete job train1 This would stop the training workload. You can verify this by running runai list jobs again. Next Steps \u00b6 Follow the Quickstart document: Launch Interactive Workloads Use your container to run an unattended training workload","title":"Training"},{"location":"Researcher/Walkthroughs/walkthrough-train/#quickstart-launch-unattended-training-workloads","text":"","title":"Quickstart: Launch Unattended Training Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-train/#introduction","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Quickstart you will learn how to: Use the Run:ai command-line interface (CLI) to start a deep learning training workload. View training status and resource consumption using the Run:ai user interface and the Run:ai CLI. View training logs. Stop the training.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-train/#prerequisites","text":"To complete this Quickstart you must have: Run:ai software installed on your Kubernetes cluster. See: Installing Run:ai on a Kubernetes Cluster Run:ai CLI installed on your machine. See: Installing the Run:ai Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-train/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-train/#setup","text":"Login to the Projects area of the Run:ai user interface. Add a Project named \"team-a\". Allocate 2 GPUs to the Project.","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-train/#run-workload","text":"At the command-line run: runai config project team-a runai submit train1 -i gcr.io/run-ai-demo/quickstart -g 1 This would start an unattended training Job for team-a with an allocation of a single GPU. The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart . We named the Job train1 Follow up on the Job's progress by running: runai list jobs The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the Job is waiting to be scheduled Running - the Job is running Succeeded - the Job has ended A full list of Job statuses can be found here To get additional status on your Job run: runai describe job train1","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-logs","text":"Run the following: runai logs train1 You should see a log of a running deep learning session:","title":"View Logs"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-status-on-the-runai-user-interface","text":"Open the Run:ai user interface. Under \"Jobs\" you can view the new Workload: The image we used for training includes the Run:ai Training library. Among other features, this library allows the reporting of metrics from within the deep learning Job. Metrics such as progress, accuracy, loss, and epoch and step numbers. Progress can be seen in the status column above. To see other metrics, press the settings wheel on the top right and select additional deep learning metrics from the list Under Nodes you can see node utilization:","title":"View status on the Run:ai User Interface"},{"location":"Researcher/Walkthroughs/walkthrough-train/#stop-workload","text":"Run the following: runai delete job train1 This would stop the training workload. You can verify this by running runai list jobs again.","title":"Stop Workload"},{"location":"Researcher/Walkthroughs/walkthrough-train/#next-steps","text":"Follow the Quickstart document: Launch Interactive Workloads Use your container to run an unattended training workload","title":"Next Steps"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/","text":"Best Practice: From Bare Metal to Docker Images \u00b6 Introduction \u00b6 Some Researchers do data science on bare metal . The term bare-metal relates to connecting to a server and working directly on its operating system and disks. This is the fastest way to start working, but it introduces problems when the data science organization scales: More Researchers mean that the machine resources need to be efficiently shared Researchers need to collaborate and share data, code, and results To overcome that, people working on bare-metal typically write scripts to gather data, code as well as code dependencies. This soon becomes an overwhelming task. Why Use Docker Images? \u00b6 Docker images and containerization in general provide a level of abstraction which, by large, frees developers and Researchers from the mundane tasks of setting up an environment . The image is an operating system by itself and thus the 'environment' is by large, a part of the image. When a docker image is instantiated, it creates a container . A container is the running manifestation of a docker image. Moving a Data Science Environment to Docker \u00b6 A data science environment typically includes: Training data Machine Learning (ML) code and inputs Libraries: Code dependencies that must be installed before the ML code can be run Training data \u00b6 Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system. The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the Researcher is currently using, allowing the Researcher to easily migrate between machines. Organizations without a shared file system typically write scripts to copy data from machine to machine. Machine Learning Code and Inputs \u00b6 As a rule, code needs to be saved and versioned in a code repository . There are two alternative practices: The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code. When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. Both practices are valid. Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system. Code Dependencies \u00b6 Any code has code dependencies. These libraries must be installed for the code to run. As the code is changing, so do the dependencies. ML Code is typically python and python dependencies are typically declared together in a single requirements.txt file which is saved together with the code. The best practice is to have your docker startup script (see below) run this file using pip install -r requirements.txt . This allows the flexibility of adding and removing code dependencies dynamically. ML Lifecycle: Build and Train \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm, or similar and accesses GPU resources directly. Build workloads are typically meant for debugging and development sessions. Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results. A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. Getting your docker ready is also a matter of which type of workload you are currently running. Build Workloads \u00b6 With \"build\" you are actually coding and debugging small experiments. You are interactive . In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/ catalog/containers/nvidia: tensorflow ) and use it directly. Start a docker container by running: docker run -it .... \"the well known image\" -v /where/my/code/resides bash You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh. You can also access the container remotely from tools such as PyCharm, Jupyter Notebook, and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service). Training Workloads \u00b6 For training workloads, you can use a well-known image (e.g. the TensorFlow image from the link above) but more often than not, you want to create your own docker image. The best practice is to use the well-known image (e.g. TensorFlow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile . A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.: Base image is nvidia-tensorflow Install popular software (Optional) Run a script The script can be part of the image or can be provided as part of the command line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. The best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training Job. For further information on how to set up and parameterize a training workload via docker or Run:ai see Converting your Workload to use Unattended Training Execution .","title":"Bare-Metal to Docker Images"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#best-practice-from-bare-metal-to-docker-images","text":"","title":"Best Practice: From Bare Metal to Docker Images"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#introduction","text":"Some Researchers do data science on bare metal . The term bare-metal relates to connecting to a server and working directly on its operating system and disks. This is the fastest way to start working, but it introduces problems when the data science organization scales: More Researchers mean that the machine resources need to be efficiently shared Researchers need to collaborate and share data, code, and results To overcome that, people working on bare-metal typically write scripts to gather data, code as well as code dependencies. This soon becomes an overwhelming task.","title":"Introduction"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#why-use-docker-images","text":"Docker images and containerization in general provide a level of abstraction which, by large, frees developers and Researchers from the mundane tasks of setting up an environment . The image is an operating system by itself and thus the 'environment' is by large, a part of the image. When a docker image is instantiated, it creates a container . A container is the running manifestation of a docker image.","title":"Why Use Docker Images?"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#moving-a-data-science-environment-to-docker","text":"A data science environment typically includes: Training data Machine Learning (ML) code and inputs Libraries: Code dependencies that must be installed before the ML code can be run","title":"Moving a Data Science Environment to Docker"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#training-data","text":"Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system. The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the Researcher is currently using, allowing the Researcher to easily migrate between machines. Organizations without a shared file system typically write scripts to copy data from machine to machine.","title":"Training data"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#machine-learning-code-and-inputs","text":"As a rule, code needs to be saved and versioned in a code repository . There are two alternative practices: The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code. When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. Both practices are valid. Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system.","title":"Machine Learning Code and Inputs"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#code-dependencies","text":"Any code has code dependencies. These libraries must be installed for the code to run. As the code is changing, so do the dependencies. ML Code is typically python and python dependencies are typically declared together in a single requirements.txt file which is saved together with the code. The best practice is to have your docker startup script (see below) run this file using pip install -r requirements.txt . This allows the flexibility of adding and removing code dependencies dynamically.","title":"Code Dependencies"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#ml-lifecycle-build-and-train","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm, or similar and accesses GPU resources directly. Build workloads are typically meant for debugging and development sessions. Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results. A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. Getting your docker ready is also a matter of which type of workload you are currently running.","title":"ML Lifecycle: Build and Train"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#build-workloads","text":"With \"build\" you are actually coding and debugging small experiments. You are interactive . In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/ catalog/containers/nvidia: tensorflow ) and use it directly. Start a docker container by running: docker run -it .... \"the well known image\" -v /where/my/code/resides bash You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh. You can also access the container remotely from tools such as PyCharm, Jupyter Notebook, and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service).","title":"Build Workloads"},{"location":"Researcher/best-practices/bare-metal-to-docker-images/#training-workloads","text":"For training workloads, you can use a well-known image (e.g. the TensorFlow image from the link above) but more often than not, you want to create your own docker image. The best practice is to use the well-known image (e.g. TensorFlow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile . A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.: Base image is nvidia-tensorflow Install popular software (Optional) Run a script The script can be part of the image or can be provided as part of the command line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. The best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training Job. For further information on how to set up and parameterize a training workload via docker or Run:ai see Converting your Workload to use Unattended Training Execution .","title":"Training Workloads"},{"location":"Researcher/best-practices/convert-to-unattended/","text":"Best Practice: Convert your Workload to Run Unattended \u00b6 Motivation \u00b6 Run:ai allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this kind of flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires Researchers to switch workloads from running interactively, to running unattended, thus allowing Run:ai to pause/resume the run. Unattended workloads are a good fit for long-duration runs, or sets of smaller hyperparameter optimization runs. Best Practices \u00b6 Docker Image \u00b6 A docker container is based on a docker image. Some Researchers use generic images such as ones provided by Nvidia, for example: NVIDIA NGC TensorFlow . Others, use generic images as the base image to a more customized image using Dockerfiles . Realizing that Researchers are not always proficient with building docker files, as a best practice, you will want to: Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image. Leave some degree of flexibility, which allows the Researcher to add/remove python dependencies without re-creating images. Code Location \u00b6 You will want to minimize the cycle of code change-and-run. There are a couple of best practices which you can choose from: Code resides on the network file storage. This way you can change the code and immediately run the Job. The Job picks up the new files from the network. Use the runai submit flag --git-sync . The flag allows the Researcher to provide details of a Git repository. The repository will be automatically cloned into a specified directory when the container starts. The code can be embedded within the image. In this case, you will want to create an automatic CI/CD process, which packages the code into a modified image. The document below assumes option #1. Create a Startup Script \u00b6 Gather the commands you ran inside the interactive Job into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john ). An example of a common startup script start.sh : pip install -r requirements.txt ... python training.py The first line of this script is there to make sure that all required python libraries are installed before the training script executes, it also allows the Researcher to add/remove libraries without needing changes to the image itself. Support Variance Between Different Runs \u00b6 Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods: Your script can read arguments passed to the script: python training.py --number-of-epochs=30 In which case, change your start.sh script to: pip install -r requirements.txt ... python training.py $@ Your script can read from environment variables during script execution. In case you use environment variables, the variables will be passed to the training script automatically. No special action is required in this case. Checkpoints \u00b6 Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs). TensorFlow, Pytorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node For more information on best practices for saving checkpoints, see Saving Deep Learning Checkpoints . Running the Job \u00b6 Using runai submit , drop the flag --interactive . For submitting a Job using the script created above, please use -- [COMMAND] flag to specify a command, use the -- syntax to pass arguments, and pass environment variables using the flag --environment . Example with Environment variables: runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3 -v /nfs/john:/mydir -g 1 --working-dir /mydir/ -e 'EPOCHS=30' -e 'LEARNING_RATE=0.02' -- ./startup.sh Example with Command-line arguments: runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3 -v /nfs/john:/mydir -g 1 --working-dir /mydir/ -- ./startup.sh batch-size=64 number-of-epochs=3 Please refer to Command-Line Interface, runai submit for a list of all arguments accepted by the Run:ai CLI. Use CLI Policies \u00b6 Different run configurations may vary significantly and can be tedious to be written each time on the command-line. To make life easier, our CLI offers a way to set administrator policies for these configurations and use pre-configured configuration when submitting a Workload. Please refer to Configure Command-Line Interface Policies . Attached Files \u00b6 The 3 relevant files mentioned in this document can be downloaded from Github See Also \u00b6 See the unattended training Quickstart: Launch Unattended Training Workloads","title":"Convert a Workload to Run Unattended"},{"location":"Researcher/best-practices/convert-to-unattended/#best-practice-convert-your-workload-to-run-unattended","text":"","title":"Best Practice: Convert your Workload to Run Unattended"},{"location":"Researcher/best-practices/convert-to-unattended/#motivation","text":"Run:ai allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this kind of flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires Researchers to switch workloads from running interactively, to running unattended, thus allowing Run:ai to pause/resume the run. Unattended workloads are a good fit for long-duration runs, or sets of smaller hyperparameter optimization runs.","title":"Motivation"},{"location":"Researcher/best-practices/convert-to-unattended/#best-practices","text":"","title":"Best Practices"},{"location":"Researcher/best-practices/convert-to-unattended/#docker-image","text":"A docker container is based on a docker image. Some Researchers use generic images such as ones provided by Nvidia, for example: NVIDIA NGC TensorFlow . Others, use generic images as the base image to a more customized image using Dockerfiles . Realizing that Researchers are not always proficient with building docker files, as a best practice, you will want to: Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image. Leave some degree of flexibility, which allows the Researcher to add/remove python dependencies without re-creating images.","title":"Docker Image"},{"location":"Researcher/best-practices/convert-to-unattended/#code-location","text":"You will want to minimize the cycle of code change-and-run. There are a couple of best practices which you can choose from: Code resides on the network file storage. This way you can change the code and immediately run the Job. The Job picks up the new files from the network. Use the runai submit flag --git-sync . The flag allows the Researcher to provide details of a Git repository. The repository will be automatically cloned into a specified directory when the container starts. The code can be embedded within the image. In this case, you will want to create an automatic CI/CD process, which packages the code into a modified image. The document below assumes option #1.","title":"Code Location"},{"location":"Researcher/best-practices/convert-to-unattended/#create-a-startup-script","text":"Gather the commands you ran inside the interactive Job into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john ). An example of a common startup script start.sh : pip install -r requirements.txt ... python training.py The first line of this script is there to make sure that all required python libraries are installed before the training script executes, it also allows the Researcher to add/remove libraries without needing changes to the image itself.","title":"Create a Startup Script"},{"location":"Researcher/best-practices/convert-to-unattended/#support-variance-between-different-runs","text":"Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods: Your script can read arguments passed to the script: python training.py --number-of-epochs=30 In which case, change your start.sh script to: pip install -r requirements.txt ... python training.py $@ Your script can read from environment variables during script execution. In case you use environment variables, the variables will be passed to the training script automatically. No special action is required in this case.","title":"Support Variance Between Different Runs"},{"location":"Researcher/best-practices/convert-to-unattended/#checkpoints","text":"Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs). TensorFlow, Pytorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node For more information on best practices for saving checkpoints, see Saving Deep Learning Checkpoints .","title":"Checkpoints"},{"location":"Researcher/best-practices/convert-to-unattended/#running-the-job","text":"Using runai submit , drop the flag --interactive . For submitting a Job using the script created above, please use -- [COMMAND] flag to specify a command, use the -- syntax to pass arguments, and pass environment variables using the flag --environment . Example with Environment variables: runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3 -v /nfs/john:/mydir -g 1 --working-dir /mydir/ -e 'EPOCHS=30' -e 'LEARNING_RATE=0.02' -- ./startup.sh Example with Command-line arguments: runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3 -v /nfs/john:/mydir -g 1 --working-dir /mydir/ -- ./startup.sh batch-size=64 number-of-epochs=3 Please refer to Command-Line Interface, runai submit for a list of all arguments accepted by the Run:ai CLI.","title":"Running the Job"},{"location":"Researcher/best-practices/convert-to-unattended/#use-cli-policies","text":"Different run configurations may vary significantly and can be tedious to be written each time on the command-line. To make life easier, our CLI offers a way to set administrator policies for these configurations and use pre-configured configuration when submitting a Workload. Please refer to Configure Command-Line Interface Policies .","title":"Use CLI Policies"},{"location":"Researcher/best-practices/convert-to-unattended/#attached-files","text":"The 3 relevant files mentioned in this document can be downloaded from Github","title":"Attached Files"},{"location":"Researcher/best-practices/convert-to-unattended/#see-also","text":"See the unattended training Quickstart: Launch Unattended Training Workloads","title":"See Also"},{"location":"Researcher/best-practices/env-variables/","text":"Environment Variables inside a Run:ai Workload \u00b6 Identifying a Job \u00b6 There may be use cases where your container may need to uniquely identify the Job it is currently running in. A typical use case is for saving Job artifacts under a unique name. Run:ai provides pre-defined environment variables you can use. These variables are guaranteed to be unique even if the Job is preempted or evicted and then runs again. Run:ai provides the following environment variables: JOB_NAME - the name of the Job. JOB_UUID - a unique identifier for the Job. Note that the Job can be deleted and then recreated with the same name. A Job UUID will be different even if the Job names are the same. Identifying a Pod \u00b6 With Hyperparameter Optimization , experiments are run as Pods within the Job. Run:ai provides the following environment variables to identify the Pod. POD_INDEX - An index number (0, 1, 2, 3....) for a specific Pod within the Job. This is useful for Hyperparameter Optimization to allow easy mapping to individual experiments. The Pod index will remain the same if restarted (due to a failure or preemption). Therefore, it can be used by the Researcher to identify experiments. POD_UUID - a unique identifier for the Pod. if the Pod is restarted, the Pod UUID will change. GPU Allocation \u00b6 Run:ai provides an environment variable, visible inside the container, to help identify the number of GPUs allocated for the container. Use RUNAI_NUM_OF_GPUS Node Name \u00b6 There may be use cases where your container may need to identify the node it is currently running on. Run:ai provides an environment variable, visible inside the container, to help identify the name of the node on which the pod was scheduled. Use NODE_NAME Usage Example in Python \u00b6 import os jobName = os . environ [ 'JOB_NAME' ] jobUUID = os . environ [ 'JOB_UUID' ]","title":"Environment Variables"},{"location":"Researcher/best-practices/env-variables/#environment-variables-inside-a-runai-workload","text":"","title":"Environment Variables inside a Run:ai Workload"},{"location":"Researcher/best-practices/env-variables/#identifying-a-job","text":"There may be use cases where your container may need to uniquely identify the Job it is currently running in. A typical use case is for saving Job artifacts under a unique name. Run:ai provides pre-defined environment variables you can use. These variables are guaranteed to be unique even if the Job is preempted or evicted and then runs again. Run:ai provides the following environment variables: JOB_NAME - the name of the Job. JOB_UUID - a unique identifier for the Job. Note that the Job can be deleted and then recreated with the same name. A Job UUID will be different even if the Job names are the same.","title":"Identifying a Job"},{"location":"Researcher/best-practices/env-variables/#identifying-a-pod","text":"With Hyperparameter Optimization , experiments are run as Pods within the Job. Run:ai provides the following environment variables to identify the Pod. POD_INDEX - An index number (0, 1, 2, 3....) for a specific Pod within the Job. This is useful for Hyperparameter Optimization to allow easy mapping to individual experiments. The Pod index will remain the same if restarted (due to a failure or preemption). Therefore, it can be used by the Researcher to identify experiments. POD_UUID - a unique identifier for the Pod. if the Pod is restarted, the Pod UUID will change.","title":"Identifying a Pod"},{"location":"Researcher/best-practices/env-variables/#gpu-allocation","text":"Run:ai provides an environment variable, visible inside the container, to help identify the number of GPUs allocated for the container. Use RUNAI_NUM_OF_GPUS","title":"GPU Allocation"},{"location":"Researcher/best-practices/env-variables/#node-name","text":"There may be use cases where your container may need to identify the node it is currently running on. Run:ai provides an environment variable, visible inside the container, to help identify the name of the node on which the pod was scheduled. Use NODE_NAME","title":"Node Name"},{"location":"Researcher/best-practices/env-variables/#usage-example-in-python","text":"import os jobName = os . environ [ 'JOB_NAME' ] jobUUID = os . environ [ 'JOB_UUID' ]","title":"Usage Example in Python"},{"location":"Researcher/best-practices/save-dl-checkpoints/","text":"Best Practice: Save Deep-Learning Checkpoints \u00b6 Introduction \u00b6 Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs). How to Save Checkpoints \u00b6 TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). This document uses Keras as an example. The code itself can be found here Where to Save Checkpoints \u00b6 It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node. Example: runai submit train-with-checkpoints -i tensorflow/tensorflow:1.14.0-gpu-py3 \\ -v /mnt/nfs_share/john:/mydir -g 1 --working-dir /mydir --command -- ./startup.sh The command saves the checkpoints in an NFS checkpoints folder /mnt/nfs_share/john When to Save Checkpoints \u00b6 Save Periodically \u00b6 It is a best practice to save checkpoints at intervals. For example, every epoch as the Keras code below shows: checkpoints_file = \"weights.best.hdf5\" checkpoint = ModelCheckpoint ( checkpoints_file , monitor = 'val_acc' , verbose = 1 , save_best_only = True , mode = 'max' ) Save on Exit Signal \u00b6 If periodic checkpoints are not enough, you can use a signal-hook provided by Run:ai (via Kubernetes). The hook is python code that is called before your Job is suspended and allows you to save your checkpoints as well as other state data you may wish to store. import signal import time def graceful_exit_handler ( signum , frame ): # save your checkpoints to shared storage # exit with status \"1\" is important for the Job to return later. exit ( 1 ) signal . signal ( signal . SIGTERM , graceful_exit_handler ) By default, you will have 30 seconds to save your checkpoints. Important For the signal to be captured, it must be propagated from the startup script to the python child process. See code here Resuming using Saved Checkpoints \u00b6 A Run:ai unattended workload that is resumed, will run the same startup script as on the first run. It is the responsibility of the script developer to add code that: Checks if saved checkpoints exist (see above) If saved checkpoints exist, load them and start the run using these checkpoints import os checkpoints_file = \"weights.best.hdf5\" if os . path . isfile ( checkpoints_file ): print ( \"loading checkpoint file: \" + checkpoints_file ) model . load_weights ( checkpoints_file )","title":"Save Deep Learning Checkpoints"},{"location":"Researcher/best-practices/save-dl-checkpoints/#best-practice-save-deep-learning-checkpoints","text":"","title":"Best Practice: Save Deep-Learning Checkpoints"},{"location":"Researcher/best-practices/save-dl-checkpoints/#introduction","text":"Run:ai can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:ai will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs).","title":"Introduction"},{"location":"Researcher/best-practices/save-dl-checkpoints/#how-to-save-checkpoints","text":"TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). This document uses Keras as an example. The code itself can be found here","title":"How to Save Checkpoints"},{"location":"Researcher/best-practices/save-dl-checkpoints/#where-to-save-checkpoints","text":"It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node. Example: runai submit train-with-checkpoints -i tensorflow/tensorflow:1.14.0-gpu-py3 \\ -v /mnt/nfs_share/john:/mydir -g 1 --working-dir /mydir --command -- ./startup.sh The command saves the checkpoints in an NFS checkpoints folder /mnt/nfs_share/john","title":"Where to Save Checkpoints"},{"location":"Researcher/best-practices/save-dl-checkpoints/#when-to-save-checkpoints","text":"","title":"When to Save Checkpoints"},{"location":"Researcher/best-practices/save-dl-checkpoints/#save-periodically","text":"It is a best practice to save checkpoints at intervals. For example, every epoch as the Keras code below shows: checkpoints_file = \"weights.best.hdf5\" checkpoint = ModelCheckpoint ( checkpoints_file , monitor = 'val_acc' , verbose = 1 , save_best_only = True , mode = 'max' )","title":"Save Periodically"},{"location":"Researcher/best-practices/save-dl-checkpoints/#save-on-exit-signal","text":"If periodic checkpoints are not enough, you can use a signal-hook provided by Run:ai (via Kubernetes). The hook is python code that is called before your Job is suspended and allows you to save your checkpoints as well as other state data you may wish to store. import signal import time def graceful_exit_handler ( signum , frame ): # save your checkpoints to shared storage # exit with status \"1\" is important for the Job to return later. exit ( 1 ) signal . signal ( signal . SIGTERM , graceful_exit_handler ) By default, you will have 30 seconds to save your checkpoints. Important For the signal to be captured, it must be propagated from the startup script to the python child process. See code here","title":"Save on Exit Signal"},{"location":"Researcher/best-practices/save-dl-checkpoints/#resuming-using-saved-checkpoints","text":"A Run:ai unattended workload that is resumed, will run the same startup script as on the first run. It is the responsibility of the script developer to add code that: Checks if saved checkpoints exist (see above) If saved checkpoints exist, load them and start the run using these checkpoints import os checkpoints_file = \"weights.best.hdf5\" if os . path . isfile ( checkpoints_file ): print ( \"loading checkpoint file: \" + checkpoints_file ) model . load_weights ( checkpoints_file )","title":"Resuming using Saved Checkpoints"},{"location":"Researcher/cli-reference/Introduction/","text":"The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc. To install and configure the Run:ai CLI see Researcher Setup - Start Here","title":"Introduction"},{"location":"Researcher/cli-reference/runai-attach/","text":"Description \u00b6 Attach to a running Job. The command attaches to the standard input, output, and error streams of a running Job. If the Job has multiple pods the job will attach to the first pod unless otherwise set. Synopsis \u00b6 runai attach <job-name> [--no-stdin ] [--no-tty] [--pod string] . [--loglevel value] [--help | -h] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. --no-stdin \u00b6 Do not attach STDIN. --no-tty \u00b6 Do not allocate a pseudo-TTY --pod string \u00b6 Attach to a specific pod within the Job. To find the list of pods run runai describe job <job-name> and then use the pod name with the --pod flag. Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h \u00b6 Show help text. Output \u00b6 None","title":"runai attach"},{"location":"Researcher/cli-reference/runai-attach/#description","text":"Attach to a running Job. The command attaches to the standard input, output, and error streams of a running Job. If the Job has multiple pods the job will attach to the first pod unless otherwise set.","title":"Description"},{"location":"Researcher/cli-reference/runai-attach/#synopsis","text":"runai attach <job-name> [--no-stdin ] [--no-tty] [--pod string] . [--loglevel value] [--help | -h]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-attach/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory.","title":"Options"},{"location":"Researcher/cli-reference/runai-attach/#-no-stdin","text":"Do not attach STDIN.","title":"--no-stdin"},{"location":"Researcher/cli-reference/runai-attach/#-no-tty","text":"Do not allocate a pseudo-TTY","title":"--no-tty"},{"location":"Researcher/cli-reference/runai-attach/#-pod-string","text":"Attach to a specific pod within the Job. To find the list of pods run runai describe job <job-name> and then use the pod name with the --pod flag.","title":"--pod string"},{"location":"Researcher/cli-reference/runai-attach/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-attach/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-attach/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-attach/#output","text":"None","title":"Output"},{"location":"Researcher/cli-reference/runai-bash/","text":"Description \u00b6 Get a bash session inside a running Job This command is a shortcut to runai exec ( runai exec -it job-name bash ). See runai exec for full documentation of the exec command. Synopsis \u00b6 runai bash <job-name> [ --pod string ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. --pod string \u00b6 Specify a pod of a running Job. To get a list of the pods of a specific Job, run runai describe job <job-name> command Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\") --project | -p (string) \u00b6 Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h \u00b6 Show help text Output \u00b6 The command will access the container that should be currently running in the current cluster and attempt to create a command-line shell based on bash. The command will return an error if the container does not exist or has not been in a running state yet. See also \u00b6 Build Workloads. See Quickstart document: Launch Interactive Build Workloads .","title":"runai bash"},{"location":"Researcher/cli-reference/runai-bash/#description","text":"Get a bash session inside a running Job This command is a shortcut to runai exec ( runai exec -it job-name bash ). See runai exec for full documentation of the exec command.","title":"Description"},{"location":"Researcher/cli-reference/runai-bash/#synopsis","text":"runai bash <job-name> [ --pod string ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-bash/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory.","title":"Options"},{"location":"Researcher/cli-reference/runai-bash/#-pod-string","text":"Specify a pod of a running Job. To get a list of the pods of a specific Job, run runai describe job <job-name> command","title":"--pod string"},{"location":"Researcher/cli-reference/runai-bash/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-bash/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\")","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-bash/#-project-p-string","text":"Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> .","title":"--project | -p (string)"},{"location":"Researcher/cli-reference/runai-bash/#-help-h","text":"Show help text","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-bash/#output","text":"The command will access the container that should be currently running in the current cluster and attempt to create a command-line shell based on bash. The command will return an error if the container does not exist or has not been in a running state yet.","title":"Output"},{"location":"Researcher/cli-reference/runai-bash/#see-also","text":"Build Workloads. See Quickstart document: Launch Interactive Build Workloads .","title":"See also"},{"location":"Researcher/cli-reference/runai-config/","text":"Description \u00b6 Set a default Project or Cluster Synopsis \u00b6 runai config project <project-name> [ --loglevel value ] [ --help | -h ] runai config cluster <cluster-name> [ --loglevel value ] [ --help | -h ] Options \u00b6 <project-name> - The name of the Project you want to set as default. Mandatory. <cluster-name> - The name of the cluster you want to set as the current cluster. Mandatory. Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h \u00b6 Show help text. Output \u00b6 None","title":"runai config"},{"location":"Researcher/cli-reference/runai-config/#description","text":"Set a default Project or Cluster","title":"Description"},{"location":"Researcher/cli-reference/runai-config/#synopsis","text":"runai config project <project-name> [ --loglevel value ] [ --help | -h ] runai config cluster <cluster-name> [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-config/#options","text":"<project-name> - The name of the Project you want to set as default. Mandatory. <cluster-name> - The name of the cluster you want to set as the current cluster. Mandatory.","title":"Options"},{"location":"Researcher/cli-reference/runai-config/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-config/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-config/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-config/#output","text":"None","title":"Output"},{"location":"Researcher/cli-reference/runai-delete/","text":"Description \u00b6 Delete a Workload and its associated Pods. Note that once you delete a Workload, its entire data will be gone: You will no longer be able to enter it via bash. You will no longer be able to access logs. Any data saved on the container and not stored in a shared location will be lost. Synopsis \u00b6 runai delete job <job-name> [ --all | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Workload to run the command with. Mandatory. --all | -A \u00b6 Delete all Workloads. Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) \u00b6 Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h \u00b6 Show help text. Output \u00b6 The Workload will be deleted and not available via the command runai list jobs . The Workloads will show as deleted from the Run:ai user interface Job list. See Also \u00b6 Build Workloads. See Quickstart document: Launch Interactive Build Workloads . Training Workloads. See Quickstart document: Launch Unattended Training Workloads .","title":"runai delete"},{"location":"Researcher/cli-reference/runai-delete/#description","text":"Delete a Workload and its associated Pods. Note that once you delete a Workload, its entire data will be gone: You will no longer be able to enter it via bash. You will no longer be able to access logs. Any data saved on the container and not stored in a shared location will be lost.","title":"Description"},{"location":"Researcher/cli-reference/runai-delete/#synopsis","text":"runai delete job <job-name> [ --all | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-delete/#options","text":"<job-name> - The name of the Workload to run the command with. Mandatory.","title":"Options"},{"location":"Researcher/cli-reference/runai-delete/#-all-a","text":"Delete all Workloads.","title":"--all | -A"},{"location":"Researcher/cli-reference/runai-delete/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-delete/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-delete/#-project-p-string","text":"Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> .","title":"--project | -p (string)"},{"location":"Researcher/cli-reference/runai-delete/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-delete/#output","text":"The Workload will be deleted and not available via the command runai list jobs . The Workloads will show as deleted from the Run:ai user interface Job list.","title":"Output"},{"location":"Researcher/cli-reference/runai-delete/#see-also","text":"Build Workloads. See Quickstart document: Launch Interactive Build Workloads . Training Workloads. See Quickstart document: Launch Unattended Training Workloads .","title":"See Also"},{"location":"Researcher/cli-reference/runai-describe/","text":"Description \u00b6 Display details of a Workload or Node. Synopsis \u00b6 runai describe job <job-name> [ --output value | -o value ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] [ --output string | -o string ] runai describe node [ node-name ] [ --loglevel value ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Workload to run the command with. Mandatory. <node-name> - The name of the Node to run the command with. If a Node name is not specified, a description of all Nodes is shown. -o | --output Output format. One of: json|yaml|wide. Default is 'wide' Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) \u00b6 Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project, use: runai config project <project-name> . --help | -h \u00b6 Show help text Output \u00b6 The runai describe job command will show Workload properties and status as well as lifecycle events and the list of related resources and pods. The runai describe node command will show Node properties.","title":"runai describe"},{"location":"Researcher/cli-reference/runai-describe/#description","text":"Display details of a Workload or Node.","title":"Description"},{"location":"Researcher/cli-reference/runai-describe/#synopsis","text":"runai describe job <job-name> [ --output value | -o value ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] [ --output string | -o string ] runai describe node [ node-name ] [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-describe/#options","text":"<job-name> - The name of the Workload to run the command with. Mandatory. <node-name> - The name of the Node to run the command with. If a Node name is not specified, a description of all Nodes is shown. -o | --output Output format. One of: json|yaml|wide. Default is 'wide'","title":"Options"},{"location":"Researcher/cli-reference/runai-describe/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-describe/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-describe/#-project-p-string","text":"Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project, use: runai config project <project-name> .","title":"--project | -p (string)"},{"location":"Researcher/cli-reference/runai-describe/#-help-h","text":"Show help text","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-describe/#output","text":"The runai describe job command will show Workload properties and status as well as lifecycle events and the list of related resources and pods. The runai describe node command will show Node properties.","title":"Output"},{"location":"Researcher/cli-reference/runai-exec/","text":"Description \u00b6 Execute a command inside a running Job Note: to execute a bash command, you can also use the shorthand runai bash Synopsis \u00b6 runai exec <job-name> <command> [ --stdin | -i ] [ --tty | -t ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. <command> the command itself (e.g. bash ). --stdin | -i \u00b6 Keep STDIN open even if not attached. --tty | -t \u00b6 Allocate a pseudo-TTY. Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) \u00b6 Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h \u00b6 Show help text. Output \u00b6 The command will run in the context of the container. See Also \u00b6","title":"runai exec"},{"location":"Researcher/cli-reference/runai-exec/#description","text":"Execute a command inside a running Job Note: to execute a bash command, you can also use the shorthand runai bash","title":"Description"},{"location":"Researcher/cli-reference/runai-exec/#synopsis","text":"runai exec <job-name> <command> [ --stdin | -i ] [ --tty | -t ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-exec/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory. <command> the command itself (e.g. bash ).","title":"Options"},{"location":"Researcher/cli-reference/runai-exec/#-stdin-i","text":"Keep STDIN open even if not attached.","title":"--stdin | -i"},{"location":"Researcher/cli-reference/runai-exec/#-tty-t","text":"Allocate a pseudo-TTY.","title":"--tty | -t"},{"location":"Researcher/cli-reference/runai-exec/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-exec/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-exec/#-project-p-string","text":"Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> .","title":"--project | -p (string)"},{"location":"Researcher/cli-reference/runai-exec/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-exec/#output","text":"The command will run in the context of the container.","title":"Output"},{"location":"Researcher/cli-reference/runai-exec/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-list/","text":"Description \u00b6 Show lists of Workloads, Projects, Clusters or Nodes. Synopsis \u00b6 runai list jobs [ --all-projects | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] runai list projects [ --loglevel value ] [ --help | -h ] runai list clusters [ --loglevel value ] [ --help | -h ] runai list nodes [ node-name ] [ --loglevel value ] [ --help | -h ] Options \u00b6 node-name - Name of a specific node to list (optional). --all-projects | -A \u00b6 Show Workloads from all Projects. Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) \u00b6 Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h \u00b6 Show help text. Output \u00b6 A list of Workloads, Nodes, Projects, or Clusters. To filter 'runai list nodes' for a specific Node, add the Node name. See Also \u00b6 To show details for a specific Workload or Node see runai describe .","title":"runai list"},{"location":"Researcher/cli-reference/runai-list/#description","text":"Show lists of Workloads, Projects, Clusters or Nodes.","title":"Description"},{"location":"Researcher/cli-reference/runai-list/#synopsis","text":"runai list jobs [ --all-projects | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] runai list projects [ --loglevel value ] [ --help | -h ] runai list clusters [ --loglevel value ] [ --help | -h ] runai list nodes [ node-name ] [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-list/#options","text":"node-name - Name of a specific node to list (optional).","title":"Options"},{"location":"Researcher/cli-reference/runai-list/#-all-projects-a","text":"Show Workloads from all Projects.","title":"--all-projects | -A"},{"location":"Researcher/cli-reference/runai-list/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-list/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-list/#-project-p-string","text":"Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> .","title":"--project | -p (string)"},{"location":"Researcher/cli-reference/runai-list/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-list/#output","text":"A list of Workloads, Nodes, Projects, or Clusters. To filter 'runai list nodes' for a specific Node, add the Node name.","title":"Output"},{"location":"Researcher/cli-reference/runai-list/#see-also","text":"To show details for a specific Workload or Node see runai describe .","title":"See Also"},{"location":"Researcher/cli-reference/runai-login/","text":"Description \u00b6 Login to Run:ai When Researcher Authentication is enabled, you will need to login to Run:ai using your username and password before accessing resources Synopsis \u00b6 runai login [ --loglevel value ] [ --help | -h ] Options \u00b6 Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h \u00b6 Show help text. Output \u00b6 You will be prompted for a user name and password See Also \u00b6 runai logout .","title":"runai login"},{"location":"Researcher/cli-reference/runai-login/#description","text":"Login to Run:ai When Researcher Authentication is enabled, you will need to login to Run:ai using your username and password before accessing resources","title":"Description"},{"location":"Researcher/cli-reference/runai-login/#synopsis","text":"runai login [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-login/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-login/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-login/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-login/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-login/#output","text":"You will be prompted for a user name and password","title":"Output"},{"location":"Researcher/cli-reference/runai-login/#see-also","text":"runai logout .","title":"See Also"},{"location":"Researcher/cli-reference/runai-logout/","text":"Description \u00b6 Log out from Run:ai Synopsis \u00b6 runai logout [ --loglevel value ] [ --help | -h ] Options \u00b6 Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h \u00b6 Show help text. Output \u00b6 You will be logged out from Run:ai See Also \u00b6 runai login .","title":"runai logout"},{"location":"Researcher/cli-reference/runai-logout/#description","text":"Log out from Run:ai","title":"Description"},{"location":"Researcher/cli-reference/runai-logout/#synopsis","text":"runai logout [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-logout/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-logout/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-logout/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-logout/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-logout/#output","text":"You will be logged out from Run:ai","title":"Output"},{"location":"Researcher/cli-reference/runai-logout/#see-also","text":"runai login .","title":"See Also"},{"location":"Researcher/cli-reference/runai-logs/","text":"Description \u00b6 Show the logs of a Job. Synopsis \u00b6 runai logs <job-name> [ --follow | -f ] [ --pod string | -p string ] [ --since duration ] [ --since-time date-time ] [ --tail int | -t int ] [ --timestamps ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. --follow | -f \u00b6 Stream the logs. --pod | -p \u00b6 Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running runai describe job <job-name> . --instance (string) | -i (string) \u00b6 Show logs for a specific instance in cases where a Job contains multiple pods. --since (duration) \u00b6 Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together. --since-time (date-time) \u00b6 Return logs after specified date. Date format should be RFC3339 , example: 2020-01-26T15:00:00Z . --tail (int) | -t (int) \u00b6 # of lines of recent log file to display. --timestamps \u00b6 Include timestamps on each line in the log output. Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) \u00b6 Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai config project <project-name> . --help | -h \u00b6 Show help text. Output \u00b6 The command will show the logs of the first process in the container. For training Jobs, this would be the command run at startup. For interactive Jobs, the command may not show anything. See Also \u00b6 Training Workloads. See Quickstart document: Launch Unattended Training Workloads .","title":"runai logs"},{"location":"Researcher/cli-reference/runai-logs/#description","text":"Show the logs of a Job.","title":"Description"},{"location":"Researcher/cli-reference/runai-logs/#synopsis","text":"runai logs <job-name> [ --follow | -f ] [ --pod string | -p string ] [ --since duration ] [ --since-time date-time ] [ --tail int | -t int ] [ --timestamps ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-logs/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory.","title":"Options"},{"location":"Researcher/cli-reference/runai-logs/#-follow-f","text":"Stream the logs.","title":"--follow | -f"},{"location":"Researcher/cli-reference/runai-logs/#-pod-p","text":"Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running runai describe job <job-name> .","title":"--pod | -p"},{"location":"Researcher/cli-reference/runai-logs/#-instance-string-i-string","text":"Show logs for a specific instance in cases where a Job contains multiple pods.","title":"--instance (string) | -i (string)"},{"location":"Researcher/cli-reference/runai-logs/#-since-duration","text":"Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together.","title":"--since (duration)"},{"location":"Researcher/cli-reference/runai-logs/#-since-time-date-time","text":"Return logs after specified date. Date format should be RFC3339 , example: 2020-01-26T15:00:00Z .","title":"--since-time (date-time)"},{"location":"Researcher/cli-reference/runai-logs/#-tail-int-t-int","text":"# of lines of recent log file to display.","title":"--tail (int) | -t (int)"},{"location":"Researcher/cli-reference/runai-logs/#-timestamps","text":"Include timestamps on each line in the log output.","title":"--timestamps"},{"location":"Researcher/cli-reference/runai-logs/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-logs/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-logs/#-project-p-string","text":"Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai config project <project-name> .","title":"--project | -p (string)"},{"location":"Researcher/cli-reference/runai-logs/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-logs/#output","text":"The command will show the logs of the first process in the container. For training Jobs, this would be the command run at startup. For interactive Jobs, the command may not show anything.","title":"Output"},{"location":"Researcher/cli-reference/runai-logs/#see-also","text":"Training Workloads. See Quickstart document: Launch Unattended Training Workloads .","title":"See Also"},{"location":"Researcher/cli-reference/runai-resume/","text":"Description \u00b6 Resume a suspended Job Resuming a previously suspended Job will return it to the queue for scheduling. The Job may or may not start immediately, depending on available resources. Suspend and resume do not work with mpi Jobs. Synopsis \u00b6 runai resume <job-name> [ --all | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. --all | -A \u00b6 Resume all suspended Jobs in the current Project. Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) \u00b6 Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h \u00b6 Show help text. Output \u00b6 The Job will be resumed. When running runai list jobs the Job status will no longer by Suspended . See Also \u00b6 Suspending Jobs: Suspend .","title":"runai resume"},{"location":"Researcher/cli-reference/runai-resume/#description","text":"Resume a suspended Job Resuming a previously suspended Job will return it to the queue for scheduling. The Job may or may not start immediately, depending on available resources. Suspend and resume do not work with mpi Jobs.","title":"Description"},{"location":"Researcher/cli-reference/runai-resume/#synopsis","text":"runai resume <job-name> [ --all | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-resume/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory.","title":"Options"},{"location":"Researcher/cli-reference/runai-resume/#-all-a","text":"Resume all suspended Jobs in the current Project.","title":"--all | -A"},{"location":"Researcher/cli-reference/runai-resume/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-resume/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-resume/#-project-p-string","text":"Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> .","title":"--project | -p (string)"},{"location":"Researcher/cli-reference/runai-resume/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-resume/#output","text":"The Job will be resumed. When running runai list jobs the Job status will no longer by Suspended .","title":"Output"},{"location":"Researcher/cli-reference/runai-resume/#see-also","text":"Suspending Jobs: Suspend .","title":"See Also"},{"location":"Researcher/cli-reference/runai-submit-mpi/","text":"Description \u00b6 Submit a Distributed Training (MPI) Run:ai Job for execution. Note To use distributed training you need to have installed the Kubeflow MPI Operator as specified here Synopsis \u00b6 runai submit-mpi [ --attach ] [ --backoff-limit int ] [ --command ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --environment stringArray | -e stringArray ] [ --git-sync string ] [ --gpu double | -g double ] [ --gpu-memory string ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --interactive ] [ --job-name-prefix string ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --mount-propagation ] [ --name string ] [ --node-pool string ] [ --node-type string ] [ --prevent-privilege-escalation ] [ --processes int ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --s3 string ] [ --stdin ] [ --toleration string ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --nfs-server string ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] -- [ COMMAND ] [ ARGS... ] [ options ] Syntax notes: Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice. Examples \u00b6 start an unattended mpi training Job of name dist1, based on Project team-a using a quickstart-distributed image: runai submit-mpi --name dist1 --processes=2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 -e RUNAI_SLEEP_SECS=60 (see: distributed training Quickstart ). Options \u00b6 Aliases and Shortcuts \u00b6 --name \u00b6 The name of the Job. --interactive \u00b6 Mark this Job as Interactive. Interactive Jobs are not terminated automatically by the system. --job-name-prefix string \u00b6 The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional --job-name-prefix flag creates Job names with the provided prefix. Container Related \u00b6 --attach \u00b6 Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true. --command \u00b6 Overrides the image's entry point with the command supplied after '--'. When not using the --command flag, the entry point will not be overrided and the string after -- will be appended as arguments to the entry point command. Example: --command -- run.sh 1 54 will start the docker and run run.sh 1 54 -- script.py 10000 will augment script.py 10000 to the entry point command (e.g. python ) -e stringArray | --environment stringArray \u00b6 Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ). --git-sync string \u00b6 Clone a git repository into the container running the Job. The parameter should follow the syntax: source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE . Note that source and target fields are mandatory. --image string | -i string \u00b6 Image to use when creating the container for this Job --image-pull-policy string \u00b6 Pulling policy of the image When starting a container. Options are: Always (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded. IfNotPresent : the image is pulled only if it is not already present locally. Never : the image is assumed to exist locally. No attempt is made to pull the image. For more information see Kubernetes documentation . --local-image (deprecated) \u00b6 Deprecated. Please use image-pull-policy=never instead. Use a local image for this Job. A local image is an image that exists on all local servers of the Kubernetes Cluster. --stdin \u00b6 Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY --working-dir string \u00b6 Starts the container with the specified directory as the current directory. Resource Allocation \u00b6 --cpu double \u00b6 CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job. --cpu-limit double \u00b6 Limitations on the number of CPUs consumed by the Job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of CPUs. --gpu double | -g double \u00b6 Number of GPUs to allocate for the Job. The default is no allocated GPUs. the GPU value can be an integer or a fraction between 0 and 1. --gpu-memory \u00b6 GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job. --large-shm \u00b6 Mount a large /dev/shm device. An shm is a shared file system mounted on RAM. --memory string \u00b6 CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job. --memory-limit string \u00b6 CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit. Storage \u00b6 --pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro] \u00b6 --pvc Pvc_Name:Container_Mount_Path:[ro] \u00b6 Mount a persistent volume claim into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name that can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]' \u00b6 Volumes to mount into the container. Examples: -v /raid/public/john/data:/root/data:ro Mount /root/data to local path /raid/public/john/data for read-only access. -v /public/data:/root/data::nfs.example.com Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access. --nfs-server string \u00b6 Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details). --mount-propagation \u00b6 The flag allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. When the flag is set, Run:ai will set mount propagation to the value of HostToContainer as documented here . With HostToContainer the volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories. --git-sync string \u00b6 Clone a git repository into the container running the job. The parameter should follow the syntax: source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE Note that source and target fields are mandatory. --s3 string \u00b6 Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax: bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH All the fields, except url=URL, are mandatory. Default for url is url=https://s3.amazon.com Network \u00b6 --host-ipc \u00b6 Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack. For further information see docker run reference documentation. --host-network \u00b6 Use the host's network stack inside the container. For further information see docker run reference documentation. Job Lifecycle \u00b6 --backoff-limit int \u00b6 The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the --interactive flag is not specified). --processes int \u00b6 Number of distributed training processes. The default is 1. Access Control \u00b6 --create-home-dir \u00b6 Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers . --prevent-privilege-escalation \u00b6 Prevent the Job\u2019s container and all launched processes from gaining additional privileges after the Job starts. Default is false . For more information see non root containers . --run-as-user \u00b6 Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers . Scheduling \u00b6 --node-type string \u00b6 Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see: Working with Projects . --node-pools string \u00b6 Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool . You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects . --toleration string \u00b6 Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node. This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide. The format of the string: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS] Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\") --project | -p (string) \u00b6 Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h \u00b6 Show help text. Output \u00b6 The command will attempt to submit an mpi Job. You can follow up on the Job by running runai list jobs or runai describe job <job-name> . See Also \u00b6 See Quickstart document Running Distributed Training .","title":"runai submit-mpi"},{"location":"Researcher/cli-reference/runai-submit-mpi/#description","text":"Submit a Distributed Training (MPI) Run:ai Job for execution. Note To use distributed training you need to have installed the Kubeflow MPI Operator as specified here","title":"Description"},{"location":"Researcher/cli-reference/runai-submit-mpi/#synopsis","text":"runai submit-mpi [ --attach ] [ --backoff-limit int ] [ --command ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --environment stringArray | -e stringArray ] [ --git-sync string ] [ --gpu double | -g double ] [ --gpu-memory string ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --interactive ] [ --job-name-prefix string ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --mount-propagation ] [ --name string ] [ --node-pool string ] [ --node-type string ] [ --prevent-privilege-escalation ] [ --processes int ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --s3 string ] [ --stdin ] [ --toleration string ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --nfs-server string ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] -- [ COMMAND ] [ ARGS... ] [ options ] Syntax notes: Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-submit-mpi/#examples","text":"start an unattended mpi training Job of name dist1, based on Project team-a using a quickstart-distributed image: runai submit-mpi --name dist1 --processes=2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed:v0.3.0 -e RUNAI_SLEEP_SECS=60 (see: distributed training Quickstart ).","title":"Examples"},{"location":"Researcher/cli-reference/runai-submit-mpi/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-submit-mpi/#aliases-and-shortcuts","text":"","title":"Aliases and Shortcuts"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-name","text":"The name of the Job.","title":"--name"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-interactive","text":"Mark this Job as Interactive. Interactive Jobs are not terminated automatically by the system.","title":"--interactive"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-job-name-prefix-string","text":"The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional --job-name-prefix flag creates Job names with the provided prefix.","title":"--job-name-prefix string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#container-related","text":"","title":"Container Related"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-attach","text":"Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true.","title":"--attach"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-command","text":"Overrides the image's entry point with the command supplied after '--'. When not using the --command flag, the entry point will not be overrided and the string after -- will be appended as arguments to the entry point command. Example: --command -- run.sh 1 54 will start the docker and run run.sh 1 54 -- script.py 10000 will augment script.py 10000 to the entry point command (e.g. python )","title":"--command"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-e-stringarray-environment-stringarray","text":"Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ).","title":"-e stringArray | --environment stringArray"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-git-sync-string","text":"Clone a git repository into the container running the Job. The parameter should follow the syntax: source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE . Note that source and target fields are mandatory.","title":"--git-sync string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-image-string-i-string","text":"Image to use when creating the container for this Job","title":"--image string | -i string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-image-pull-policy-string","text":"Pulling policy of the image When starting a container. Options are: Always (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded. IfNotPresent : the image is pulled only if it is not already present locally. Never : the image is assumed to exist locally. No attempt is made to pull the image. For more information see Kubernetes documentation .","title":"--image-pull-policy string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-local-image-deprecated","text":"Deprecated. Please use image-pull-policy=never instead. Use a local image for this Job. A local image is an image that exists on all local servers of the Kubernetes Cluster.","title":"--local-image (deprecated)"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-stdin","text":"Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY","title":"--stdin"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-working-dir-string","text":"Starts the container with the specified directory as the current directory.","title":"--working-dir string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#resource-allocation","text":"","title":"Resource Allocation"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-cpu-double","text":"CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.","title":"--cpu double"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-cpu-limit-double","text":"Limitations on the number of CPUs consumed by the Job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of CPUs.","title":"--cpu-limit double"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-gpu-double-g-double","text":"Number of GPUs to allocate for the Job. The default is no allocated GPUs. the GPU value can be an integer or a fraction between 0 and 1.","title":"--gpu double | -g double"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-gpu-memory","text":"GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.","title":"--gpu-memory"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-large-shm","text":"Mount a large /dev/shm device. An shm is a shared file system mounted on RAM.","title":"--large-shm"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-memory-string","text":"CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.","title":"--memory string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-memory-limit-string","text":"CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.","title":"--memory-limit string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#storage","text":"","title":"Storage"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-pvc-storage_class_namesizecontainer_mount_pathro","text":"","title":"--pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro]"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-pvc-pvc_namecontainer_mount_pathro","text":"Mount a persistent volume claim into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name that can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only","title":"--pvc Pvc_Name:Container_Mount_Path:[ro]"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-volume-sourcecontainer_mount_pathronfs-host","text":"Volumes to mount into the container. Examples: -v /raid/public/john/data:/root/data:ro Mount /root/data to local path /raid/public/john/data for read-only access. -v /public/data:/root/data::nfs.example.com Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.","title":"--volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-nfs-server-string","text":"Use this flag to specify a default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).","title":"--nfs-server string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-mount-propagation","text":"The flag allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. When the flag is set, Run:ai will set mount propagation to the value of HostToContainer as documented here . With HostToContainer the volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories.","title":"--mount-propagation"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-git-sync-string_1","text":"Clone a git repository into the container running the job. The parameter should follow the syntax: source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE Note that source and target fields are mandatory.","title":"--git-sync string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-s3-string","text":"Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax: bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH All the fields, except url=URL, are mandatory. Default for url is url=https://s3.amazon.com","title":"--s3 string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#network","text":"","title":"Network"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-host-ipc","text":"Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores, and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack. For further information see docker run reference documentation.","title":"--host-ipc"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-host-network","text":"Use the host's network stack inside the container. For further information see docker run reference documentation.","title":"--host-network"},{"location":"Researcher/cli-reference/runai-submit-mpi/#job-lifecycle","text":"","title":"Job Lifecycle"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-backoff-limit-int","text":"The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the --interactive flag is not specified).","title":"--backoff-limit int"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-processes-int","text":"Number of distributed training processes. The default is 1.","title":"--processes int"},{"location":"Researcher/cli-reference/runai-submit-mpi/#access-control","text":"","title":"Access Control"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-create-home-dir","text":"Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers .","title":"--create-home-dir"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-prevent-privilege-escalation","text":"Prevent the Job\u2019s container and all launched processes from gaining additional privileges after the Job starts. Default is false . For more information see non root containers .","title":"--prevent-privilege-escalation"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-run-as-user","text":"Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers .","title":"--run-as-user"},{"location":"Researcher/cli-reference/runai-submit-mpi/#scheduling","text":"","title":"Scheduling"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-node-type-string","text":"Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see: Working with Projects .","title":"--node-type string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-node-pools-string","text":"Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool . You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects .","title":"--node-pools string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-toleration-string","text":"Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node. This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide. The format of the string: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]","title":"--toleration string"},{"location":"Researcher/cli-reference/runai-submit-mpi/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\")","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-project-p-string","text":"Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> .","title":"--project | -p (string)"},{"location":"Researcher/cli-reference/runai-submit-mpi/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-submit-mpi/#output","text":"The command will attempt to submit an mpi Job. You can follow up on the Job by running runai list jobs or runai describe job <job-name> .","title":"Output"},{"location":"Researcher/cli-reference/runai-submit-mpi/#see-also","text":"See Quickstart document Running Distributed Training .","title":"See Also"},{"location":"Researcher/cli-reference/runai-submit/","text":"Description \u00b6 Submit a Run:ai Job for execution. Synopsis \u00b6 runai submit [ --attach ] [ --backoff-limit int ] [ --completions int ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --environment stringArray | -e stringArray ] [ --git-sync string ] [ --gpu double | -g double ] [ --gpu-memory string ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --imagePullPolicy string ] [ --interactive ] [ --jupyter ] [ --job-name-prefix string ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --mount-propagation ] [ --mps ] [ --name string ] [ --node-pool string ] [ --node-type string ] [ --parallelism int ] [ --port stringArray ] [ --preemptible ] [ --prevent-privilege-escalation ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --s3 string ] [ --service-type string | -s string ] [ --stdin ] [ --toleration string ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --nfs-server string ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] -- [ COMMAND ] [ ARGS... ] [ options ] Syntax notes: Flags of type stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice. Examples \u00b6 All examples assume a Run:ai Project has been set using runai config project <project-name> . Start an interactive Job: runai submit -i ubuntu --interactive --attach -g 1 Or runai submit --name build1 -i ubuntu -g 1 --interactive -- sleep infinity (see: build Quickstart ). Externalize ports: runai submit --name build-remote -i rastasheep/ubuntu-sshd:14.04 --interactive \\ --service-type=nodeport --port 30022:22 -- /usr/sbin/sshd -D (see: build with ports Quickstart ). Start a Training Job runai submit --name train1 -i gcr.io/run-ai-demo/quickstart -g 1 (see: training Quickstart ). Use GPU Fractions runai submit --name frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 (see: GPU fractions Quickstart ). Hyperparameter Optimization runai submit --name hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/hpo (see: hyperparameter optimization Quickstart ). Submit a Job without a name (automatically generates a name) runai submit -i gcr.io/run-ai-demo/quickstart -g 1 Submit a Job without a name with a pre-defined prefix and an incremental index suffix runai submit --job-name-prefix -i gcr.io/run-ai-demo/quickstart -g 1 Options \u00b6 Aliases and Shortcuts \u00b6 --name \u00b6 The name of the Job. --interactive \u00b6 Mark this Job as Interactive. Interactive Jobs are not terminated automatically by the system. --jupyter \u00b6 Shortcut for running a Jupyter notebook container. Uses a pre-created image and a default notebook configuration. Example: runai submit --name jup1 --jupyter -g 0.5 --service-type=nodeport will start an interactive session named jup1 and use an nodeport load balancer to connect to it. The output of the command is an access token for the notebook. Run runai list jobs to find the URL for the notebook. --job-name-prefix string \u00b6 The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional --job-name-prefix flag creates Job names with the provided prefix Container Related \u00b6 --attach \u00b6 Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true. --command \u00b6 Overrides the image's entry point with the command supplied after '--'. When not using the --command flag, the entry point will not be overrided and the string after -- will be appended as arguments to the entry point command. Example: --command -- run.sh 1 54 will start the docker and run run.sh 1 54 -- script.py 10000 will augment script.py 10000 to the entry point command (e.g. python ) -e stringArray | --environment stringArray \u00b6 Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ). --image string | -i string \u00b6 Image to use when creating the container for this Job --image-pull-policy string \u00b6 Pulling policy of the image When starting a container. Options are: Always (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded. IfNotPresent : the image is pulled only if it is not already present locally. Never : the image is assumed to exist locally. No attempt is made to pull the image. For more information see Kubernetes documentation . --local-image (deprecated) \u00b6 Deprecated. Please use image-pull-policy=never instead. Use a local image for this Job. A local image is an image that exists on all local servers of the Kubernetes Cluster. --mps \u00b6 Use NVIDIA MPS. NIVDIA MPS is useful with Inference workloads for optimizing multiple processes running on a single GPU. The --mps flag only works in conjunction with GPU fractions (--gpu <num> where <num> is not an integer or using --gpu-memory). --stdin \u00b6 Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY. --working-dir string \u00b6 Starts the container with the specified directory as the current directory. Resource Allocation \u00b6 --cpu double \u00b6 CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job. --cpu-limit double \u00b6 Limitations on the number of CPUs consumed by the Job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of CPUs. --gpu double | -g double \u00b6 Number of GPUs to allocate for the Job. The default is no allocated GPUs. The GPU value can be an integer or a fraction between 0 and 1. --gpu-memory \u00b6 GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job. --large-shm \u00b6 Mount a large /dev/shm device. An shm is a shared file system mounted on RAM. --memory string \u00b6 CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job. --memory-limit string \u00b6 CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit. Storage \u00b6 --pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro] \u00b6 --pvc Pvc_Name:Container_Mount_Path:[ro] \u00b6 Mount a persistent volume claim of Network Attached Storage into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name that can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only --volume 'Source:Container_Mount_Path:[ro]:[nfs-host]' \u00b6 Volumes to mount into the container. Examples: -v /raid/public/john/data:/root/data:ro Mount /root/data to local path /raid/public/john/data for read-only access. -v /public/data:/root/data::nfs.example.com Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access. --nfs-server string \u00b6 Use this flag to specify the default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details). --mount-propagation \u00b6 The flag allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. When the flag is set, Run:ai will set mount propagation to the value of HostToContainer as documented here . With HostToContainer the volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories. --git-sync string \u00b6 Clone a git repository into the container running the job. The parameter should follow the syntax: source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE Note that source and target fields are mandatory. --s3 string \u00b6 Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax: bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH All the fields, except url=URL, are mandatory. Default for url is url=https://s3.amazon.com Network \u00b6 --host-ipc \u00b6 Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack. For further information see docker run reference . --host-network \u00b6 Use the host's network stack inside the container. For further information see docker run reference . --port stringArray \u00b6 Expose ports from the Job container. Used together with --service-type . Example: --port 8080:80 --service-type portforward --service-type string | -s string \u00b6 Service exposure method for interactive Job. Options are: portforward , loadbalancer , nodeport . Use the command runai list to obtain the endpoint to use the service when the Job is running. Different service methods have different endpoint structures. --address string \u00b6 Comma separated list of IP addresses to listen to when running interactive job with portforward service type (default is localhost). Example: --interactive --service-type portforward --address \"localhost,192.168.1.2\" --port 8888:5555 Any traffic on port 8888 of both localhost and 192.168.1.2 will be forwarded to port 5555 of the running job. Job Lifecycle \u00b6 --backoff-limit int \u00b6 The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the --interactive flag is not specified). --completions int \u00b6 The number of successful pods required for this Job to be completed. Used for Hyperparameter optimization . Use together with --parallelism . --parallelism int \u00b6 The number of pods this Job tries to run in parallel at any time. Used for Hyperparameter optimization . Use together with --completions . Access Control \u00b6 --create-home-dir \u00b6 Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers . --prevent-privilege-escalation \u00b6 Prevent the Job\u2019s container and all launched processes from gaining additional privileges after the Job starts. Default is false . For more information see non root containers . --run-as-user \u00b6 Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML , you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers . Scheduling \u00b6 --node-type string \u00b6 Allows defining specific nodes (machines) or a group of nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see: Working with Projects . --node-pools string \u00b6 Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool . You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects . --preemptible \u00b6 Mark an interactive Job as preemptible. Preemptible Jobs can be scheduled above the guaranteed quota but may be reclaimed at any time. --toleration string \u00b6 Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node. This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide. The format of the string: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS] Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) \u00b6 Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h \u00b6 Show help text. Output \u00b6 The command will attempt to submit a Job. You can follow up on the Job by running runai list jobs or runai describe job <job-name> . Note that the submit call may use a policy to provide defaults to any of the above flags. See Also \u00b6 See any of the Quickstart documents here: . See policy configuration for a description on how policies work.","title":"runai submit"},{"location":"Researcher/cli-reference/runai-submit/#description","text":"Submit a Run:ai Job for execution.","title":"Description"},{"location":"Researcher/cli-reference/runai-submit/#synopsis","text":"runai submit [ --attach ] [ --backoff-limit int ] [ --completions int ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --environment stringArray | -e stringArray ] [ --git-sync string ] [ --gpu double | -g double ] [ --gpu-memory string ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --imagePullPolicy string ] [ --interactive ] [ --jupyter ] [ --job-name-prefix string ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --mount-propagation ] [ --mps ] [ --name string ] [ --node-pool string ] [ --node-type string ] [ --parallelism int ] [ --port stringArray ] [ --preemptible ] [ --prevent-privilege-escalation ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --s3 string ] [ --service-type string | -s string ] [ --stdin ] [ --toleration string ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --nfs-server string ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] -- [ COMMAND ] [ ARGS... ] [ options ] Syntax notes: Flags of type stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-submit/#examples","text":"All examples assume a Run:ai Project has been set using runai config project <project-name> . Start an interactive Job: runai submit -i ubuntu --interactive --attach -g 1 Or runai submit --name build1 -i ubuntu -g 1 --interactive -- sleep infinity (see: build Quickstart ). Externalize ports: runai submit --name build-remote -i rastasheep/ubuntu-sshd:14.04 --interactive \\ --service-type=nodeport --port 30022:22 -- /usr/sbin/sshd -D (see: build with ports Quickstart ). Start a Training Job runai submit --name train1 -i gcr.io/run-ai-demo/quickstart -g 1 (see: training Quickstart ). Use GPU Fractions runai submit --name frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 (see: GPU fractions Quickstart ). Hyperparameter Optimization runai submit --name hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/hpo (see: hyperparameter optimization Quickstart ). Submit a Job without a name (automatically generates a name) runai submit -i gcr.io/run-ai-demo/quickstart -g 1 Submit a Job without a name with a pre-defined prefix and an incremental index suffix runai submit --job-name-prefix -i gcr.io/run-ai-demo/quickstart -g 1","title":"Examples"},{"location":"Researcher/cli-reference/runai-submit/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-submit/#aliases-and-shortcuts","text":"","title":"Aliases and Shortcuts"},{"location":"Researcher/cli-reference/runai-submit/#-name","text":"The name of the Job.","title":"--name"},{"location":"Researcher/cli-reference/runai-submit/#-interactive","text":"Mark this Job as Interactive. Interactive Jobs are not terminated automatically by the system.","title":"--interactive"},{"location":"Researcher/cli-reference/runai-submit/#-jupyter","text":"Shortcut for running a Jupyter notebook container. Uses a pre-created image and a default notebook configuration. Example: runai submit --name jup1 --jupyter -g 0.5 --service-type=nodeport will start an interactive session named jup1 and use an nodeport load balancer to connect to it. The output of the command is an access token for the notebook. Run runai list jobs to find the URL for the notebook.","title":"--jupyter"},{"location":"Researcher/cli-reference/runai-submit/#-job-name-prefix-string","text":"The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:ai will generate a Job name. The optional --job-name-prefix flag creates Job names with the provided prefix","title":"--job-name-prefix string"},{"location":"Researcher/cli-reference/runai-submit/#container-related","text":"","title":"Container Related"},{"location":"Researcher/cli-reference/runai-submit/#-attach","text":"Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true.","title":"--attach"},{"location":"Researcher/cli-reference/runai-submit/#-command","text":"Overrides the image's entry point with the command supplied after '--'. When not using the --command flag, the entry point will not be overrided and the string after -- will be appended as arguments to the entry point command. Example: --command -- run.sh 1 54 will start the docker and run run.sh 1 54 -- script.py 10000 will augment script.py 10000 to the entry point command (e.g. python )","title":"--command"},{"location":"Researcher/cli-reference/runai-submit/#-e-stringarray-environment-stringarray","text":"Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ).","title":"-e stringArray | --environment stringArray"},{"location":"Researcher/cli-reference/runai-submit/#-image-string-i-string","text":"Image to use when creating the container for this Job","title":"--image string | -i string"},{"location":"Researcher/cli-reference/runai-submit/#-image-pull-policy-string","text":"Pulling policy of the image When starting a container. Options are: Always (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded. IfNotPresent : the image is pulled only if it is not already present locally. Never : the image is assumed to exist locally. No attempt is made to pull the image. For more information see Kubernetes documentation .","title":"--image-pull-policy string"},{"location":"Researcher/cli-reference/runai-submit/#-local-image-deprecated","text":"Deprecated. Please use image-pull-policy=never instead. Use a local image for this Job. A local image is an image that exists on all local servers of the Kubernetes Cluster.","title":"--local-image (deprecated)"},{"location":"Researcher/cli-reference/runai-submit/#-mps","text":"Use NVIDIA MPS. NIVDIA MPS is useful with Inference workloads for optimizing multiple processes running on a single GPU. The --mps flag only works in conjunction with GPU fractions (--gpu <num> where <num> is not an integer or using --gpu-memory).","title":"--mps"},{"location":"Researcher/cli-reference/runai-submit/#-stdin","text":"Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY.","title":"--stdin"},{"location":"Researcher/cli-reference/runai-submit/#-working-dir-string","text":"Starts the container with the specified directory as the current directory.","title":"--working-dir string"},{"location":"Researcher/cli-reference/runai-submit/#resource-allocation","text":"","title":"Resource Allocation"},{"location":"Researcher/cli-reference/runai-submit/#-cpu-double","text":"CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job.","title":"--cpu double"},{"location":"Researcher/cli-reference/runai-submit/#-cpu-limit-double","text":"Limitations on the number of CPUs consumed by the Job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of CPUs.","title":"--cpu-limit double"},{"location":"Researcher/cli-reference/runai-submit/#-gpu-double-g-double","text":"Number of GPUs to allocate for the Job. The default is no allocated GPUs. The GPU value can be an integer or a fraction between 0 and 1.","title":"--gpu double | -g double"},{"location":"Researcher/cli-reference/runai-submit/#-gpu-memory","text":"GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job.","title":"--gpu-memory"},{"location":"Researcher/cli-reference/runai-submit/#-large-shm","text":"Mount a large /dev/shm device. An shm is a shared file system mounted on RAM.","title":"--large-shm"},{"location":"Researcher/cli-reference/runai-submit/#-memory-string","text":"CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job.","title":"--memory string"},{"location":"Researcher/cli-reference/runai-submit/#-memory-limit-string","text":"CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.","title":"--memory-limit string"},{"location":"Researcher/cli-reference/runai-submit/#storage","text":"","title":"Storage"},{"location":"Researcher/cli-reference/runai-submit/#-pvc-storage_class_namesizecontainer_mount_pathro","text":"","title":"--pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro]"},{"location":"Researcher/cli-reference/runai-submit/#-pvc-pvc_namecontainer_mount_pathro","text":"Mount a persistent volume claim of Network Attached Storage into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name that can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only","title":"--pvc Pvc_Name:Container_Mount_Path:[ro]"},{"location":"Researcher/cli-reference/runai-submit/#-volume-sourcecontainer_mount_pathronfs-host","text":"Volumes to mount into the container. Examples: -v /raid/public/john/data:/root/data:ro Mount /root/data to local path /raid/public/john/data for read-only access. -v /public/data:/root/data::nfs.example.com Mount /root/data to NFS path /public/data on NFS server nfs.example.com for read-write access.","title":"--volume 'Source:Container_Mount_Path:[ro]:[nfs-host]'"},{"location":"Researcher/cli-reference/runai-submit/#-nfs-server-string","text":"Use this flag to specify the default NFS host for --volume flag. Alternatively, you can specify NFS host for each volume individually (see --volume for details).","title":"--nfs-server string"},{"location":"Researcher/cli-reference/runai-submit/#-mount-propagation","text":"The flag allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. When the flag is set, Run:ai will set mount propagation to the value of HostToContainer as documented here . With HostToContainer the volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories.","title":"--mount-propagation"},{"location":"Researcher/cli-reference/runai-submit/#-git-sync-string","text":"Clone a git repository into the container running the job. The parameter should follow the syntax: source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE Note that source and target fields are mandatory.","title":"--git-sync string"},{"location":"Researcher/cli-reference/runai-submit/#-s3-string","text":"Mount an S3 compatible storage into the container running the job. The parameter should follow the syntax: bucket=BUCKET,key=KEY,secret=SECRET,url=URL,target=TARGET_PATH All the fields, except url=URL, are mandatory. Default for url is url=https://s3.amazon.com","title":"--s3 string"},{"location":"Researcher/cli-reference/runai-submit/#network","text":"","title":"Network"},{"location":"Researcher/cli-reference/runai-submit/#-host-ipc","text":"Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or the network stack. For further information see docker run reference .","title":"--host-ipc"},{"location":"Researcher/cli-reference/runai-submit/#-host-network","text":"Use the host's network stack inside the container. For further information see docker run reference .","title":"--host-network"},{"location":"Researcher/cli-reference/runai-submit/#-port-stringarray","text":"Expose ports from the Job container. Used together with --service-type . Example: --port 8080:80 --service-type portforward","title":"--port stringArray"},{"location":"Researcher/cli-reference/runai-submit/#-service-type-string-s-string","text":"Service exposure method for interactive Job. Options are: portforward , loadbalancer , nodeport . Use the command runai list to obtain the endpoint to use the service when the Job is running. Different service methods have different endpoint structures.","title":"--service-type string | -s string"},{"location":"Researcher/cli-reference/runai-submit/#-address-string","text":"Comma separated list of IP addresses to listen to when running interactive job with portforward service type (default is localhost). Example: --interactive --service-type portforward --address \"localhost,192.168.1.2\" --port 8888:5555 Any traffic on port 8888 of both localhost and 192.168.1.2 will be forwarded to port 5555 of the running job.","title":"--address string"},{"location":"Researcher/cli-reference/runai-submit/#job-lifecycle","text":"","title":"Job Lifecycle"},{"location":"Researcher/cli-reference/runai-submit/#-backoff-limit-int","text":"The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the --interactive flag is not specified).","title":"--backoff-limit int"},{"location":"Researcher/cli-reference/runai-submit/#-completions-int","text":"The number of successful pods required for this Job to be completed. Used for Hyperparameter optimization . Use together with --parallelism .","title":"--completions int"},{"location":"Researcher/cli-reference/runai-submit/#-parallelism-int","text":"The number of pods this Job tries to run in parallel at any time. Used for Hyperparameter optimization . Use together with --completions .","title":"--parallelism int"},{"location":"Researcher/cli-reference/runai-submit/#access-control","text":"","title":"Access Control"},{"location":"Researcher/cli-reference/runai-submit/#-create-home-dir","text":"Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. For more information see non root containers .","title":"--create-home-dir"},{"location":"Researcher/cli-reference/runai-submit/#-prevent-privilege-escalation","text":"Prevent the Job\u2019s container and all launched processes from gaining additional privileges after the Job starts. Default is false . For more information see non root containers .","title":"--prevent-privilege-escalation"},{"location":"Researcher/cli-reference/runai-submit/#-run-as-user","text":"Run in the context of the current user running the Run:ai command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML , you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see non root containers .","title":"--run-as-user"},{"location":"Researcher/cli-reference/runai-submit/#scheduling","text":"","title":"Scheduling"},{"location":"Researcher/cli-reference/runai-submit/#-node-type-string","text":"Allows defining specific nodes (machines) or a group of nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see: Working with Projects .","title":"--node-type string"},{"location":"Researcher/cli-reference/runai-submit/#-node-pools-string","text":"Instructs the scheduler to run this workload using specific set of nodes which are part of a Node Pool . You can specify one or more node pools to form a prioritized list of node pools that the scheduler will use to find one node pool that can provide the workload's specification. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group or use existing node labels, then create a node-pool and assign the label to the node-pool. This flag can be used in conjunction with node-type and Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set from a node-pool. For more information see: Working with Projects .","title":"--node-pools string"},{"location":"Researcher/cli-reference/runai-submit/#-preemptible","text":"Mark an interactive Job as preemptible. Preemptible Jobs can be scheduled above the guaranteed quota but may be reclaimed at any time.","title":"--preemptible"},{"location":"Researcher/cli-reference/runai-submit/#-toleration-string","text":"Specify one or more toleration criteria, to ensure that the workload is not scheduled onto an inappropriate node. This is done by matching the workload tolerations to the taints defined for each node. For further details see Kubernetes Taints and Tolerations Guide. The format of the string: operator=Equal|Exists,key=KEY,[value=VALUE],[effect=NoSchedule|NoExecute|PreferNoSchedule],[seconds=SECONDS]","title":"--toleration string"},{"location":"Researcher/cli-reference/runai-submit/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-submit/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-submit/#-project-p-string","text":"Specify the Project to which the command applies. Run:ai Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> .","title":"--project | -p (string)"},{"location":"Researcher/cli-reference/runai-submit/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-submit/#output","text":"The command will attempt to submit a Job. You can follow up on the Job by running runai list jobs or runai describe job <job-name> . Note that the submit call may use a policy to provide defaults to any of the above flags.","title":"Output"},{"location":"Researcher/cli-reference/runai-submit/#see-also","text":"See any of the Quickstart documents here: . See policy configuration for a description on how policies work.","title":"See Also"},{"location":"Researcher/cli-reference/runai-suspend/","text":"Description \u00b6 Suspend a Job Suspending a Running Job will stop the Job and will not allow it to be scheduled until it is resumed using runai resume . This means that, You will no longer be able to enter it via runai bash . The Job logs will be deleted. Any data saved on the container and not stored in a shared location will be lost. Technically, the command deletes the Kubernetes pods associated with the Job and marks the Job as suspended until it is manually released. Suspend and resume do not work with MPI and Inference Synopsis \u00b6 runai suspend <job-name> [ --all | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. --all | -A \u00b6 Suspend all Jobs in the current Project. Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) \u00b6 Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h \u00b6 Show help text. Output \u00b6 The Job will be suspended. When running runai list jobs the Job will be marked as Suspended . See Also \u00b6 Resuming Jobs: Resume .","title":"runai suspend"},{"location":"Researcher/cli-reference/runai-suspend/#description","text":"Suspend a Job Suspending a Running Job will stop the Job and will not allow it to be scheduled until it is resumed using runai resume . This means that, You will no longer be able to enter it via runai bash . The Job logs will be deleted. Any data saved on the container and not stored in a shared location will be lost. Technically, the command deletes the Kubernetes pods associated with the Job and marks the Job as suspended until it is manually released. Suspend and resume do not work with MPI and Inference","title":"Description"},{"location":"Researcher/cli-reference/runai-suspend/#synopsis","text":"runai suspend <job-name> [ --all | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-suspend/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory.","title":"Options"},{"location":"Researcher/cli-reference/runai-suspend/#-all-a","text":"Suspend all Jobs in the current Project.","title":"--all | -A"},{"location":"Researcher/cli-reference/runai-suspend/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-suspend/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-suspend/#-project-p-string","text":"Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> .","title":"--project | -p (string)"},{"location":"Researcher/cli-reference/runai-suspend/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-suspend/#output","text":"The Job will be suspended. When running runai list jobs the Job will be marked as Suspended .","title":"Output"},{"location":"Researcher/cli-reference/runai-suspend/#see-also","text":"Resuming Jobs: Resume .","title":"See Also"},{"location":"Researcher/cli-reference/runai-top-job/","text":"Description \u00b6 Show list of Jobs, their resource requirements and utilization. Synopsis \u00b6 runai top job [--help | -h] [--details | -d] Options \u00b6 Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h \u00b6 Show help text. --details | -d \u00b6 Show additional details. Output \u00b6 Shows a list of Jobs their resource requirements and utilization. See Also \u00b6","title":"runai top job"},{"location":"Researcher/cli-reference/runai-top-job/#description","text":"Show list of Jobs, their resource requirements and utilization.","title":"Description"},{"location":"Researcher/cli-reference/runai-top-job/#synopsis","text":"runai top job [--help | -h] [--details | -d]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-top-job/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-top-job/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-top-job/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-top-job/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-top-job/#-details-d","text":"Show additional details.","title":"--details | -d"},{"location":"Researcher/cli-reference/runai-top-job/#output","text":"Shows a list of Jobs their resource requirements and utilization.","title":"Output"},{"location":"Researcher/cli-reference/runai-top-job/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-top-node/","text":"Description \u00b6 Show list of Nodes (machines), their capacity and utilization. Synopsis \u00b6 runai top node [--help | -h] [--details | -d] Options \u00b6 Global Flags \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h \u00b6 Show help text. --details | -d \u00b6 Show additional details. Output \u00b6 Shows a list of Nodes their capacity and utilization. See Also \u00b6","title":"runai top node"},{"location":"Researcher/cli-reference/runai-top-node/#description","text":"Show list of Nodes (machines), their capacity and utilization.","title":"Description"},{"location":"Researcher/cli-reference/runai-top-node/#synopsis","text":"runai top node [--help | -h] [--details | -d]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-top-node/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-top-node/#global-flags","text":"","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-top-node/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-top-node/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-top-node/#-details-d","text":"Show additional details.","title":"--details | -d"},{"location":"Researcher/cli-reference/runai-top-node/#output","text":"Shows a list of Nodes their capacity and utilization.","title":"Output"},{"location":"Researcher/cli-reference/runai-top-node/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-update/","text":"Description \u00b6 Find and install the latest version of the runai command-line utility. The command must be run with sudo permissions. sudo runai update Synopsis \u00b6 runai update [ --loglevel value ] [ --help | -h ] Options \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h \u00b6 Show help text. Output \u00b6 Update of the Run:ai command-line interface. See Also \u00b6","title":"runai update"},{"location":"Researcher/cli-reference/runai-update/#description","text":"Find and install the latest version of the runai command-line utility. The command must be run with sudo permissions. sudo runai update","title":"Description"},{"location":"Researcher/cli-reference/runai-update/#synopsis","text":"runai update [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-update/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-update/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-update/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-update/#output","text":"Update of the Run:ai command-line interface.","title":"Output"},{"location":"Researcher/cli-reference/runai-update/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-version/","text":"Description \u00b6 Show the version of this utility. Synopsis \u00b6 runai version [ --loglevel value ] [ --help | -h ] Options \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h \u00b6 Show help text. Output \u00b6 The version of the Run:ai command-line interface. See Also \u00b6","title":"runai version"},{"location":"Researcher/cli-reference/runai-version/#description","text":"Show the version of this utility.","title":"Description"},{"location":"Researcher/cli-reference/runai-version/#synopsis","text":"runai version [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-version/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-version/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-version/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-version/#output","text":"The version of the Run:ai command-line interface.","title":"Output"},{"location":"Researcher/cli-reference/runai-version/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-whoami/","text":"Description \u00b6 Show the user name currently logged in Synopsis \u00b6 runai whoami [ --loglevel value ] [ --help | -h ] Options \u00b6 --loglevel (string) \u00b6 Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h \u00b6 Show help text. Output \u00b6 The name of the User currently logged in with the Run:ai command-line interface. See Also \u00b6","title":"runai whoami"},{"location":"Researcher/cli-reference/runai-whoami/#description","text":"Show the user name currently logged in","title":"Description"},{"location":"Researcher/cli-reference/runai-whoami/#synopsis","text":"runai whoami [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-whoami/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-whoami/#-loglevel-string","text":"Set the logging level. One of: debug | info | warn | error (default \"info\").","title":"--loglevel (string)"},{"location":"Researcher/cli-reference/runai-whoami/#-help-h","text":"Show help text.","title":"--help | -h"},{"location":"Researcher/cli-reference/runai-whoami/#output","text":"The name of the User currently logged in with the Run:ai command-line interface.","title":"Output"},{"location":"Researcher/cli-reference/runai-whoami/#see-also","text":"","title":"See Also"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/","text":"Introduction \u00b6 When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But two additional resources are no less important: CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run. Memory. Has a direct influence on the quantities of data a training run can process in batches. GPU servers tend to come installed with a significant amount of memory and CPUs. Requesting CPU & Memory \u00b6 When submitting a Job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G The system guarantees that if the Job is scheduled, you will be able to receive this amount of CPU and memory. For further details on these flags see: runai submit CPU over allocation \u00b6 The number of CPUs your Job will receive is guaranteed to be the number defined using the --cpu flag. In practice, however, you may receive more CPUs than you have asked for: If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined. However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the --cpu flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 cpus, the workloads will receive 10 and 30 CPUs respectively. If the flag --cpu is not specified, it will be taken from the cluster default (see the section below) Memory over allocation \u00b6 The amount of Memory your Job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above. It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your Job may receive an out-of-memory exception and terminate. CPU and Memory limits \u00b6 You can limit your Job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\ --memory 1G --memory-limit 4G The limit behavior is different for CPUs and memory. Your Job will never be allocated with more than the amount stated in the --cpu-limit flag If your Job tries to allocate more than the amount stated in the --memory-limit flag it will receive an out-of-memory exception. The limit (for both CPU and memory) overrides the cluster default described in the section below For further details on these flags see: runai submit Flag Defaults \u00b6 Defaults for --cpu flag \u00b6 If your Job has not specified --cpu , the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs. If, for example, the default has been defined as 1:6 and your Job has specified --gpu 2 and has not specified --cpu , then the implied --cpu flag value is 12 CPUs. The system comes with a cluster-wide default of 1:1. To change the ratio see below. If you didn't request any GPUs for your job and has not specified --cpu , the default is defined as a ratio of CPU limit to CPUs. If, for example, the default has been defined as 1:0.2 and your Job has specified --cpu-limit 10 and has not specified --cpu , then the implied --cpu flag value is 2 CPUs. The system comes with a cluster-wide default of 1:0.1. To change the ratio see below. Defaults for --memory flag \u00b6 If your Job has not specified --memory , the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs. The system comes with a cluster-wide default of 100MiB of allocated CPU memory per GPU. To change the ratio see below. If you didn't request any GPUs for your job and has not specified --memory , the default is defined as a ratio of CPU Memory limit to CPU Memory Request. The system comes with a cluster-wide default of 1:0.1. To change the ratio see below. Defaults for --cpu-limit flag \u00b6 If your Job has not specified --cpu-limit , then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to CPUs. See below on how to change the ratio. Defaults for --memory-limit flag \u00b6 If your Job has not specified --memory-limit , then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to Memory. See below on how to change the ratio. Changing the ratios \u00b6 To change the cluster wide-ratio use the following process. The example shows: a CPU request with a default ratio of 2:1 CPUs to GPUs. a CPU Memory request with a default ratio of 200MB per GPU. a CPU limit with a default ratio of 4:1 CPU to GPU. a Memory limit with a default ratio of 2GB per GPU. a CPU request with a default ratio of 0.1 CPUs per 1 CPU limit. a CPU Memory request with a default ratio of 0.1:1 request per CPU Memory limit. You must edit the cluster installation values file: When installing the Run:ai cluster, edit the values file . On an existing installation, use the upgrade cluster instructions to modify the values file. You must specify at least the first 4 values as follows: runai-operator : config : limitRange : cpuDefaultRequestGpuFactor : 2 memoryDefaultRequestGpuFactor : 200Mi cpuDefaultLimitGpuFactor : 4 memoryDefaultLimitGpuFactor : 2Gi cpuDefaultRequestCpuLimitFactorNoGpu : 0.1 memoryDefaultRequestMemoryLimitFactorNoGpu : 0.1 Validating CPU & Memory Allocations \u00b6 To review CPU & Memory allocations you need to look into Kubernetes. A Run:ai Job creates a Kubernetes pod . The pod declares its resource requests and limits. To see the memory and CPU consumption in Kubernetes: Get the pod name for the Job by running: runai describe job <JOB_NAME> the pod will appear under the PODS category. Run: kubectl describe pod <POD_NAME> The information will appear under Requests and Limits . For example: Limits : nvidia.com/gpu : 2 Requests : cpu : 1 memory : 104857600 nvidia.com/gpu : 2","title":"Allocation of CPU and Memory"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#introduction","text":"When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But two additional resources are no less important: CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run. Memory. Has a direct influence on the quantities of data a training run can process in batches. GPU servers tend to come installed with a significant amount of memory and CPUs.","title":"Introduction"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#requesting-cpu-memory","text":"When submitting a Job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G The system guarantees that if the Job is scheduled, you will be able to receive this amount of CPU and memory. For further details on these flags see: runai submit","title":"Requesting CPU &amp; Memory"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#cpu-over-allocation","text":"The number of CPUs your Job will receive is guaranteed to be the number defined using the --cpu flag. In practice, however, you may receive more CPUs than you have asked for: If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined. However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the --cpu flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 cpus, the workloads will receive 10 and 30 CPUs respectively. If the flag --cpu is not specified, it will be taken from the cluster default (see the section below)","title":"CPU over allocation"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#memory-over-allocation","text":"The amount of Memory your Job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above. It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your Job may receive an out-of-memory exception and terminate.","title":"Memory over allocation"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#cpu-and-memory-limits","text":"You can limit your Job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\ --memory 1G --memory-limit 4G The limit behavior is different for CPUs and memory. Your Job will never be allocated with more than the amount stated in the --cpu-limit flag If your Job tries to allocate more than the amount stated in the --memory-limit flag it will receive an out-of-memory exception. The limit (for both CPU and memory) overrides the cluster default described in the section below For further details on these flags see: runai submit","title":"CPU and Memory limits"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#flag-defaults","text":"","title":"Flag Defaults"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-cpu-flag","text":"If your Job has not specified --cpu , the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs. If, for example, the default has been defined as 1:6 and your Job has specified --gpu 2 and has not specified --cpu , then the implied --cpu flag value is 12 CPUs. The system comes with a cluster-wide default of 1:1. To change the ratio see below. If you didn't request any GPUs for your job and has not specified --cpu , the default is defined as a ratio of CPU limit to CPUs. If, for example, the default has been defined as 1:0.2 and your Job has specified --cpu-limit 10 and has not specified --cpu , then the implied --cpu flag value is 2 CPUs. The system comes with a cluster-wide default of 1:0.1. To change the ratio see below.","title":"Defaults for --cpu flag"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-memory-flag","text":"If your Job has not specified --memory , the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs. The system comes with a cluster-wide default of 100MiB of allocated CPU memory per GPU. To change the ratio see below. If you didn't request any GPUs for your job and has not specified --memory , the default is defined as a ratio of CPU Memory limit to CPU Memory Request. The system comes with a cluster-wide default of 1:0.1. To change the ratio see below.","title":"Defaults for --memory flag"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-cpu-limit-flag","text":"If your Job has not specified --cpu-limit , then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to CPUs. See below on how to change the ratio.","title":"Defaults for --cpu-limit flag"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#defaults-for-memory-limit-flag","text":"If your Job has not specified --memory-limit , then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to Memory. See below on how to change the ratio.","title":"Defaults for --memory-limit flag"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#changing-the-ratios","text":"To change the cluster wide-ratio use the following process. The example shows: a CPU request with a default ratio of 2:1 CPUs to GPUs. a CPU Memory request with a default ratio of 200MB per GPU. a CPU limit with a default ratio of 4:1 CPU to GPU. a Memory limit with a default ratio of 2GB per GPU. a CPU request with a default ratio of 0.1 CPUs per 1 CPU limit. a CPU Memory request with a default ratio of 0.1:1 request per CPU Memory limit. You must edit the cluster installation values file: When installing the Run:ai cluster, edit the values file . On an existing installation, use the upgrade cluster instructions to modify the values file. You must specify at least the first 4 values as follows: runai-operator : config : limitRange : cpuDefaultRequestGpuFactor : 2 memoryDefaultRequestGpuFactor : 200Mi cpuDefaultLimitGpuFactor : 4 memoryDefaultLimitGpuFactor : 2Gi cpuDefaultRequestCpuLimitFactorNoGpu : 0.1 memoryDefaultRequestMemoryLimitFactorNoGpu : 0.1","title":"Changing the ratios"},{"location":"Researcher/scheduling/allocation-of-cpu-and-memory/#validating-cpu-memory-allocations","text":"To review CPU & Memory allocations you need to look into Kubernetes. A Run:ai Job creates a Kubernetes pod . The pod declares its resource requests and limits. To see the memory and CPU consumption in Kubernetes: Get the pod name for the Job by running: runai describe job <JOB_NAME> the pod will appear under the PODS category. Run: kubectl describe pod <POD_NAME> The information will appear under Requests and Limits . For example: Limits : nvidia.com/gpu : 2 Requests : cpu : 1 memory : 104857600 nvidia.com/gpu : 2","title":"Validating CPU &amp; Memory Allocations"},{"location":"Researcher/scheduling/fractions/","text":"Allocation of GPU Fractions \u00b6 Introduction \u00b6 A single GPU has a significant amount of memory. Ranging from a couple of gigabytes in older generations and up to 80GB per GPU in the later models of the latest NVIDIA GPU technology. A single GPU also has a vast amount of computing power. This amount of memory and computing power is important for processing large amounts of data, such as in training deep learning models. However, there are quite a few applications that do not need this power. Examples can be inference workloads and the model-creation phase. It would thus be convenient if we could divide up a GPU between various workloads, thus achieving better GPU utilization. This article describes two complementary technologies that allow the division of GPUs and how to use them with Run:ai. Run:ai Fractions. Dynamic allocation using NVIDIA Multi-instance GPU (MIG) Run:ai Fractions \u00b6 Run:ai provides the capability to allocate a container with a specific amount of GPU RAM. As a researcher, if you know that your code needs 4GB of RAM. You can submit a job using the flag --gpu-memory 4G to specify the exact portion of the GPU memory that you need. Run:ai will allocate your container that specific amount of GPU RAM. Attempting to reach beyond your allotted RAM will result in an out-of-memory exception. You can also use the flag --gpu 0.2 to get 20% of the GPU memory on the GPU assigned for you. For more details on Run:ai fractions see the fractions quickstart . Limitation With the fraction technology all running workloads, which utilize the GPU, share the compute in parallel and on average get an even share of the compute. For example, assuming two containers, one with 0.25 GPU workload and the other with 0.75 GPU workload - both will get (on average) an equal part of the computation power. If one of the workloads does not utilize the GPU, the other workload will get the entire GPU's compute power. Info For interoperability with other Kubernetes schedulers, Run:ai creates special reservation pods. Once a workload has been allocated a fraction of a GPU, Run:ai will create a pod in a dedicated runai-reservation namespace with the full GPU as a resource. This would cause other schedulers to understand that the GPU is reserved. Dynamic MIG \u00b6 NVIDIA MIG allows GPUs based on the NVIDIA Ampere architecture (such as NVIDIA A100) to be partitioned into separate GPU Instances: When divided, the portion acts as a fully independent GPU. The division is static, in the sense that you have to call NVIDIA API or the nvidia-smi command to create or remove the MIG partition. The division is both of compute and memory. The division has fixed sizes. Up to 7 units of compute and memory in fixed sizes. The various MIG profiles can be found in the NVIDIA documentation . A typical profile can be MIG 2g.10gb which provides 2/7 of the compute power and 10GB of RAM Reconfiguration of MIG profiles on the GPU requires administrator permissions and the draining of all running workloads. Run:ai provides a way to dynamically create a MIG partition: Using the same experience as the Fractions technology above, if you know that your code needs 4GB of RAM. You can use the flag --gpu-memory 4G to specify the portion of the GPU memory that you need. Run:ai will call the NVIDIA MIG API to generate the smallest possible MIG profile for your request, and allocate it to your container. MIG is configured on the fly according to workload demand, without needing to drain workloads or to involve an IT administrator. Run:ai will automatically deallocate the partition when the workload finishes. This happens in a lazy fashion in the sense that the partition will not be removed until the scheduler decides that it is needed elsewhere. Run:ai provides an additional flag to dynamically create the specific MIG partition in NVIDIA terminology. As such, you can specify --mig-profile 2g.10gb . In a single GPU cluster you have some MIG nodes that are dynamically allocated and some that are not. For more details on Run:ai fractions see the dynamic MIG quickstart . Setting up Dynamic MIG \u00b6 As described above, MIG is only available in the latest NVIDIA architecture. When working with Kubernetes, NVIDIA defines a concept called MIG Strategy . With Run:ai you must set the MIG strategy to mixed . See NVIDIA prerequisites on how to set this flag. The administrator needs to specifically enable dynamic MIG on the node by running: runai-adm set node-role --dynamic-mig-enabled <node-name> (use runai-adm remove to unset) Make sure that MIG is enabled on the node level (see dynamic MIG quickstart for details) and set: kubectl label node <node-name> node-role.kubernetes.io/runai-mig-enabled=true (use kubectl to unset) Limitations Once a node has been marked as dynamic MIG enabled, it can only be used via the Run:ai scheduler. Run:ai currently supports H100 or A100 nodes with 40GB/80GB RAM. GPU utilization, shown on the Run:ai dashboards, may not be accurate while MIG jobs are running. Mixing Fractions and Dynamic MIG \u00b6 Given a specific node, the IT administrator can decide whether to use one technology or the other. When the Researcher asks for a specific amount of GPU memory, Run:ai will either provide it on an annotated node by dynamically allocating a MIG partition, or use a different node using the fractions technology. See Also \u00b6 Fractions quickstart . Dynamic MIG quickstart","title":"Allocation of GPU Fractions"},{"location":"Researcher/scheduling/fractions/#allocation-of-gpu-fractions","text":"","title":"Allocation of GPU Fractions"},{"location":"Researcher/scheduling/fractions/#introduction","text":"A single GPU has a significant amount of memory. Ranging from a couple of gigabytes in older generations and up to 80GB per GPU in the later models of the latest NVIDIA GPU technology. A single GPU also has a vast amount of computing power. This amount of memory and computing power is important for processing large amounts of data, such as in training deep learning models. However, there are quite a few applications that do not need this power. Examples can be inference workloads and the model-creation phase. It would thus be convenient if we could divide up a GPU between various workloads, thus achieving better GPU utilization. This article describes two complementary technologies that allow the division of GPUs and how to use them with Run:ai. Run:ai Fractions. Dynamic allocation using NVIDIA Multi-instance GPU (MIG)","title":"Introduction"},{"location":"Researcher/scheduling/fractions/#runai-fractions","text":"Run:ai provides the capability to allocate a container with a specific amount of GPU RAM. As a researcher, if you know that your code needs 4GB of RAM. You can submit a job using the flag --gpu-memory 4G to specify the exact portion of the GPU memory that you need. Run:ai will allocate your container that specific amount of GPU RAM. Attempting to reach beyond your allotted RAM will result in an out-of-memory exception. You can also use the flag --gpu 0.2 to get 20% of the GPU memory on the GPU assigned for you. For more details on Run:ai fractions see the fractions quickstart . Limitation With the fraction technology all running workloads, which utilize the GPU, share the compute in parallel and on average get an even share of the compute. For example, assuming two containers, one with 0.25 GPU workload and the other with 0.75 GPU workload - both will get (on average) an equal part of the computation power. If one of the workloads does not utilize the GPU, the other workload will get the entire GPU's compute power. Info For interoperability with other Kubernetes schedulers, Run:ai creates special reservation pods. Once a workload has been allocated a fraction of a GPU, Run:ai will create a pod in a dedicated runai-reservation namespace with the full GPU as a resource. This would cause other schedulers to understand that the GPU is reserved.","title":"Run:ai Fractions"},{"location":"Researcher/scheduling/fractions/#dynamic-mig","text":"NVIDIA MIG allows GPUs based on the NVIDIA Ampere architecture (such as NVIDIA A100) to be partitioned into separate GPU Instances: When divided, the portion acts as a fully independent GPU. The division is static, in the sense that you have to call NVIDIA API or the nvidia-smi command to create or remove the MIG partition. The division is both of compute and memory. The division has fixed sizes. Up to 7 units of compute and memory in fixed sizes. The various MIG profiles can be found in the NVIDIA documentation . A typical profile can be MIG 2g.10gb which provides 2/7 of the compute power and 10GB of RAM Reconfiguration of MIG profiles on the GPU requires administrator permissions and the draining of all running workloads. Run:ai provides a way to dynamically create a MIG partition: Using the same experience as the Fractions technology above, if you know that your code needs 4GB of RAM. You can use the flag --gpu-memory 4G to specify the portion of the GPU memory that you need. Run:ai will call the NVIDIA MIG API to generate the smallest possible MIG profile for your request, and allocate it to your container. MIG is configured on the fly according to workload demand, without needing to drain workloads or to involve an IT administrator. Run:ai will automatically deallocate the partition when the workload finishes. This happens in a lazy fashion in the sense that the partition will not be removed until the scheduler decides that it is needed elsewhere. Run:ai provides an additional flag to dynamically create the specific MIG partition in NVIDIA terminology. As such, you can specify --mig-profile 2g.10gb . In a single GPU cluster you have some MIG nodes that are dynamically allocated and some that are not. For more details on Run:ai fractions see the dynamic MIG quickstart .","title":"Dynamic MIG"},{"location":"Researcher/scheduling/fractions/#setting-up-dynamic-mig","text":"As described above, MIG is only available in the latest NVIDIA architecture. When working with Kubernetes, NVIDIA defines a concept called MIG Strategy . With Run:ai you must set the MIG strategy to mixed . See NVIDIA prerequisites on how to set this flag. The administrator needs to specifically enable dynamic MIG on the node by running: runai-adm set node-role --dynamic-mig-enabled <node-name> (use runai-adm remove to unset) Make sure that MIG is enabled on the node level (see dynamic MIG quickstart for details) and set: kubectl label node <node-name> node-role.kubernetes.io/runai-mig-enabled=true (use kubectl to unset) Limitations Once a node has been marked as dynamic MIG enabled, it can only be used via the Run:ai scheduler. Run:ai currently supports H100 or A100 nodes with 40GB/80GB RAM. GPU utilization, shown on the Run:ai dashboards, may not be accurate while MIG jobs are running.","title":"Setting up Dynamic MIG"},{"location":"Researcher/scheduling/fractions/#mixing-fractions-and-dynamic-mig","text":"Given a specific node, the IT administrator can decide whether to use one technology or the other. When the Researcher asks for a specific amount of GPU memory, Run:ai will either provide it on an annotated node by dynamically allocating a MIG partition, or use a different node using the fractions technology.","title":"Mixing Fractions and Dynamic MIG"},{"location":"Researcher/scheduling/fractions/#see-also","text":"Fractions quickstart . Dynamic MIG quickstart","title":"See Also"},{"location":"Researcher/scheduling/hpo/","text":"Researcher Library: Hyperparameter Optimization Support \u00b6 The Run:ai Researcher Library is a python library you can add to your deep learning python code. The hyperparameter optimization(HPO) support module of the library is a helper library for hyperparameter optimization (HPO) experiments Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. Example hyperparameters: Learning rate, Batch size, Different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine the results to decide what works best. With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch, and more. In addition, you can externalize custom metrics of your choosing. Getting Started \u00b6 Prerequisites \u00b6 Run:ai HPO library is dependent on PyYAML . Install it using the command: pip install pyyaml Installing \u00b6 Install the runai Python library using pip using the following command: pip install runai Make sure to use the correct pip installer (you might need to use pip3 for Python3) Usage \u00b6 Import the runai.hpo package. import runai.hpo Initialize the Run:ai HPO library with a path to a directory shared between all cluster nodes (typically using an NFS server). We recommend specifying a unique name for the experiment, the name will be used to create a sub-directory on the shared folder. To do so, we recommend using the environment variables JOB_NAME and JOB_UUID which are injected to the container by Run:ai. hpo_root = '/path/to/nfs' hpo_experiment = ' %s _ %s ' % ( os . getenv ( 'JOB_NAME' ), os . getenv ( 'JOB_UUID' )) runai . hpo . init ( hpo_root , hpo_experiment ) Decide on an HPO strategy: Random search - randomly pick a set of hyperparameter values Grid search - pick the next set of hyperparameter values, iterating through all sets across multiple experiments strategy = runai . hpo . Strategy . GridSearch Call the Run:ai HPO library to specify a set of hyperparameters and pick a specific configuration for this experiment. config = runai . hpo . pick ( grid = dict ( batch_size = [ 32 , 64 , 128 ], lr = [ 1 , 0.1 , 0.01 , 0.001 ]), strategy = strategy ) Use the returned configuration in your code. For example: optimizer = keras.optimizers.SGD(lr=config['lr']) Metrics could be reported and saved in the experiment directory under the fule runai.yaml using runai.hpo.report . You should pass the epoch number and a dictionary with metrics to be reported. For example: runai . hpo . report ( epoch = 5 , metrics = { 'accuracy' : 0.87 }) See Also \u00b6 See hyperparameter Optimization Quickstart Sample code in Github","title":"Hyperparameter Optimization"},{"location":"Researcher/scheduling/hpo/#researcher-library-hyperparameter-optimization-support","text":"The Run:ai Researcher Library is a python library you can add to your deep learning python code. The hyperparameter optimization(HPO) support module of the library is a helper library for hyperparameter optimization (HPO) experiments Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. Example hyperparameters: Learning rate, Batch size, Different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine the results to decide what works best. With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch, and more. In addition, you can externalize custom metrics of your choosing.","title":"Researcher Library: Hyperparameter Optimization Support"},{"location":"Researcher/scheduling/hpo/#getting-started","text":"","title":"Getting Started"},{"location":"Researcher/scheduling/hpo/#prerequisites","text":"Run:ai HPO library is dependent on PyYAML . Install it using the command: pip install pyyaml","title":"Prerequisites"},{"location":"Researcher/scheduling/hpo/#installing","text":"Install the runai Python library using pip using the following command: pip install runai Make sure to use the correct pip installer (you might need to use pip3 for Python3)","title":"Installing"},{"location":"Researcher/scheduling/hpo/#usage","text":"Import the runai.hpo package. import runai.hpo Initialize the Run:ai HPO library with a path to a directory shared between all cluster nodes (typically using an NFS server). We recommend specifying a unique name for the experiment, the name will be used to create a sub-directory on the shared folder. To do so, we recommend using the environment variables JOB_NAME and JOB_UUID which are injected to the container by Run:ai. hpo_root = '/path/to/nfs' hpo_experiment = ' %s _ %s ' % ( os . getenv ( 'JOB_NAME' ), os . getenv ( 'JOB_UUID' )) runai . hpo . init ( hpo_root , hpo_experiment ) Decide on an HPO strategy: Random search - randomly pick a set of hyperparameter values Grid search - pick the next set of hyperparameter values, iterating through all sets across multiple experiments strategy = runai . hpo . Strategy . GridSearch Call the Run:ai HPO library to specify a set of hyperparameters and pick a specific configuration for this experiment. config = runai . hpo . pick ( grid = dict ( batch_size = [ 32 , 64 , 128 ], lr = [ 1 , 0.1 , 0.01 , 0.001 ]), strategy = strategy ) Use the returned configuration in your code. For example: optimizer = keras.optimizers.SGD(lr=config['lr']) Metrics could be reported and saved in the experiment directory under the fule runai.yaml using runai.hpo.report . You should pass the epoch number and a dictionary with metrics to be reported. For example: runai . hpo . report ( epoch = 5 , metrics = { 'accuracy' : 0.87 })","title":"Usage"},{"location":"Researcher/scheduling/hpo/#see-also","text":"See hyperparameter Optimization Quickstart Sample code in Github","title":"See Also"},{"location":"Researcher/scheduling/job-statuses/","text":"Introduction \u00b6 The runai submit function and its sibling the runai submit-mpi function submit Run:ai Jobs for execution. A Job has a status . Once a Job is submitted it goes through several statuses before ending in an End State . Most of these statuses originate in the underlying Kubernetes infrastructure, but some are Run:ai-specific. The purpose of this document is to explain these statuses as well as the lifecycle of a Job. Successful Flow \u00b6 A regular, training Job that has no errors and executes without preemption would go through the following statuses: Pending - the Job is waiting to be scheduled. ContainerCreating - the Job has been scheduled, the Job docker image is now downloading. Running - the Job is now executing. Succeeded - the Job has finished with exit code 0 (success). The Job can be preempted, in which case it can go through other statuses: Terminating - the Job is now being preempted. Pending - the Job is waiting in queue again to receive resources. An interactive Job, by definition, needs to be closed by the Researcher and will thus never reach the Succeeded status. Rather, it would be moved by the Researcher to status Deleted . For a further explanation of the additional statuses, see the table below. Error flow \u00b6 A regular, training Job may encounter an error inside the running process (exit code is non-zero). In which case the following will happen: The Job enters an Error status and then immediately tries to reschedule itself for another attempted run. The reschedule can happen on another node in the system. After a specified number of retries, the Job will enter a final status of Fail An interactive Job, enters an Error status and then moves immediately to CrashLoopBackOff trying to reschedule itself. The reschedule attempt has no 'back-off' limit and will continue to retry indefinitely Jobs may be submitted with an image that cannot be downloaded. There are special statuses for such Jobs. See table below Status Table \u00b6 Below is a list of statuses. For each status the list shows: Name End State - this status is the final status in the lifecycle of the Job Resource Allocation - when the Job is in this status, does the system allocate resources to it Description Color - Status color as can be seen in the Run:ai User Interface Job list p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000; min-height: 15.0px} span.s1 {font-kerning: none} table.t1 {border-collapse: collapse; table-layout: fixed} td.td1 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td2 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td3 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td4 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td5 {width: 93.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td6 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td7 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td8 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td9 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td10 {width: 93.0px; background-color: #599b3e; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td11 {width: 172.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td12 {width: 48.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td13 {width: 82.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td14 {width: 456.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td15 {width: 93.0px; background-color: #fd8608; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td16 {width: 93.0px; background-color: #0000ff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td17 {width: 93.0px; background-color: #afafaf; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td18 {width: 93.0px; background-color: #fb0007; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td19 {width: 172.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td20 {width: 48.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td21 {width: 82.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td22 {width: 456.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td23 {width: 93.0px; background-color: #d0d0d0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} Status End State Resource Allocation Description Color Running Yes Job is running successfully Terminating Yes Pod is being evicted at the moment (e.g. due to an over-quota allocation, the reason will be written once eviction finishes). A new pod will be created shortly ContainerCreating Yes Image is being pulled from registry. Pending - Job is pending. Possible reasons: - Not enough resources - Waiting in Queue (over quota etc). Succeeded Yes - An Unattended (training) Job has ran and finished successfully. Deleted Yes - Job has been deleted. TimedOut Yes - Interactive Job has reached the defined timeout of the project. Preempted Yes - Interactive preemptible Job has been evicted. ContainerCannotRun Yes - Container has failed to start running. This is typically a problem within the docker image itself. Error Yes for interactive only The Job has returned an exit code different than zero. It is now waiting for another run attempt (retry). Fail Yes - Job has failed after a number of retries (according to \"--backoffLimit\" field) and will not be trying again. CrashLoopBackOff Yes Interactive Only: During backoff after Error, before a retry attempt to run pod on the same node. ErrImagePull, ImagePullBackOff Yes Failing to retrieve docker image Unknown Yes - The Run:ai Scheduler wasn't running when the Job has finished. How to get more information \u00b6 The system stores various events during the Job's lifecycle. These events can be helpful in diagnosing issues around Job scheduling. To view these events run: runai describe job <workload-name> Sometimes, useful information can be found by looking at logs emitted from the process running inside the container. For example, Jobs that have exited with an exit code different than zero may write an exit reason in this log. To see Job logs run: runai logs <job-name> Distributed Training (mpi) Jobs \u00b6 A distributed (mpi) Job, which has no errors will be slightly more complicated and has additional statuses associated with it. Distributed Jobs start with an \"init container\" which sets the stage for a distributed run. When the init container finishes, the main \"launcher\" container is created. The launcher is responsible for coordinating between the different workers Workers run and do the actual work. A successful flow of distribute training would look as: Additional Statuses: Status End State Resource Allocation Description Color Init:<number A>/<number B> Yes The Pod has B Init Containers, and A have completed so far. PodInitializing Yes The pod has finished executing Init Containers. The system is creating the main 'launcher' container Init:Error An Init Container has failed to execute. Init:CrashLoopBackOff An Init Container has failed repeatedly to execute","title":"Job Statuses"},{"location":"Researcher/scheduling/job-statuses/#introduction","text":"The runai submit function and its sibling the runai submit-mpi function submit Run:ai Jobs for execution. A Job has a status . Once a Job is submitted it goes through several statuses before ending in an End State . Most of these statuses originate in the underlying Kubernetes infrastructure, but some are Run:ai-specific. The purpose of this document is to explain these statuses as well as the lifecycle of a Job.","title":"Introduction"},{"location":"Researcher/scheduling/job-statuses/#successful-flow","text":"A regular, training Job that has no errors and executes without preemption would go through the following statuses: Pending - the Job is waiting to be scheduled. ContainerCreating - the Job has been scheduled, the Job docker image is now downloading. Running - the Job is now executing. Succeeded - the Job has finished with exit code 0 (success). The Job can be preempted, in which case it can go through other statuses: Terminating - the Job is now being preempted. Pending - the Job is waiting in queue again to receive resources. An interactive Job, by definition, needs to be closed by the Researcher and will thus never reach the Succeeded status. Rather, it would be moved by the Researcher to status Deleted . For a further explanation of the additional statuses, see the table below.","title":"Successful Flow"},{"location":"Researcher/scheduling/job-statuses/#error-flow","text":"A regular, training Job may encounter an error inside the running process (exit code is non-zero). In which case the following will happen: The Job enters an Error status and then immediately tries to reschedule itself for another attempted run. The reschedule can happen on another node in the system. After a specified number of retries, the Job will enter a final status of Fail An interactive Job, enters an Error status and then moves immediately to CrashLoopBackOff trying to reschedule itself. The reschedule attempt has no 'back-off' limit and will continue to retry indefinitely Jobs may be submitted with an image that cannot be downloaded. There are special statuses for such Jobs. See table below","title":"Error flow"},{"location":"Researcher/scheduling/job-statuses/#status-table","text":"Below is a list of statuses. For each status the list shows: Name End State - this status is the final status in the lifecycle of the Job Resource Allocation - when the Job is in this status, does the system allocate resources to it Description Color - Status color as can be seen in the Run:ai User Interface Job list p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000; min-height: 15.0px} span.s1 {font-kerning: none} table.t1 {border-collapse: collapse; table-layout: fixed} td.td1 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td2 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td3 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td4 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td5 {width: 93.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td6 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td7 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td8 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td9 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td10 {width: 93.0px; background-color: #599b3e; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td11 {width: 172.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td12 {width: 48.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td13 {width: 82.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td14 {width: 456.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td15 {width: 93.0px; background-color: #fd8608; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td16 {width: 93.0px; background-color: #0000ff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td17 {width: 93.0px; background-color: #afafaf; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td18 {width: 93.0px; background-color: #fb0007; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td19 {width: 172.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td20 {width: 48.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td21 {width: 82.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td22 {width: 456.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td23 {width: 93.0px; background-color: #d0d0d0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} Status End State Resource Allocation Description Color Running Yes Job is running successfully Terminating Yes Pod is being evicted at the moment (e.g. due to an over-quota allocation, the reason will be written once eviction finishes). A new pod will be created shortly ContainerCreating Yes Image is being pulled from registry. Pending - Job is pending. Possible reasons: - Not enough resources - Waiting in Queue (over quota etc). Succeeded Yes - An Unattended (training) Job has ran and finished successfully. Deleted Yes - Job has been deleted. TimedOut Yes - Interactive Job has reached the defined timeout of the project. Preempted Yes - Interactive preemptible Job has been evicted. ContainerCannotRun Yes - Container has failed to start running. This is typically a problem within the docker image itself. Error Yes for interactive only The Job has returned an exit code different than zero. It is now waiting for another run attempt (retry). Fail Yes - Job has failed after a number of retries (according to \"--backoffLimit\" field) and will not be trying again. CrashLoopBackOff Yes Interactive Only: During backoff after Error, before a retry attempt to run pod on the same node. ErrImagePull, ImagePullBackOff Yes Failing to retrieve docker image Unknown Yes - The Run:ai Scheduler wasn't running when the Job has finished.","title":"Status Table"},{"location":"Researcher/scheduling/job-statuses/#how-to-get-more-information","text":"The system stores various events during the Job's lifecycle. These events can be helpful in diagnosing issues around Job scheduling. To view these events run: runai describe job <workload-name> Sometimes, useful information can be found by looking at logs emitted from the process running inside the container. For example, Jobs that have exited with an exit code different than zero may write an exit reason in this log. To see Job logs run: runai logs <job-name>","title":"How to get more information"},{"location":"Researcher/scheduling/job-statuses/#distributed-training-mpi-jobs","text":"A distributed (mpi) Job, which has no errors will be slightly more complicated and has additional statuses associated with it. Distributed Jobs start with an \"init container\" which sets the stage for a distributed run. When the init container finishes, the main \"launcher\" container is created. The launcher is responsible for coordinating between the different workers Workers run and do the actual work. A successful flow of distribute training would look as: Additional Statuses: Status End State Resource Allocation Description Color Init:<number A>/<number B> Yes The Pod has B Init Containers, and A have completed so far. PodInitializing Yes The pod has finished executing Init Containers. The system is creating the main 'launcher' container Init:Error An Init Container has failed to execute. Init:CrashLoopBackOff An Init Container has failed repeatedly to execute","title":"Distributed Training (mpi) Jobs"},{"location":"Researcher/scheduling/strategies/","text":"Introduction \u00b6 When the Run:ai scheduler schedules Jobs, it can use two alternate placement strategies : Strategy Description Bin Packing Fill up a GPU and/or a node before moving on to the next one Spreading Equally spread Jobs amongst GPUs and nodes Bin Packing \u00b6 Bin packing is the default strategy. With bin packing, the scheduler tries to: Fill up a node with Jobs before allocating Jobs to second and third nodes. In a multi GPU node, when using fractions , fill up a GPU before allocating Jobs to a second GPU. The advantage of this strategy is that the scheduler, over time, can package more Jobs into the cluster. As the strategy minimizes fragmentation. For example, if we have 2 GPUs in a single node on the cluster, and 2 tasks requiring 0.5 GPUs each, using bin-packing, we would place both Jobs on the same GPU and remain with a full GPU ready for the next Job. Spreading \u00b6 There are disadvantages to bin-packing: Within a single GPU, two fractional Jobs compete for the same onboard compute power. Within a single node, two Jobs (even on separate GPUs) compete for networking resources, compute power and memory. When there are more resources available than requested, it sometimes makes sense to spread Jobs amongst nodes and GPUs, to allow higher utilization of computing resources and network resources. Returning to the example above, if we have 2 GPUs in a single node on the cluster, and 2 Jobs requiring 0.5 GPUs each, using spread scheduling we would place each Job on a separate GPU, allowing both to benefit from the computing power of a full GPU. Changing Scheduler Strategy \u00b6 The strategy affects the entire cluster. To change the strategy run: kubectl edit runaiconfig -n runai Find `runai-scheduler' and add: runai-scheduler: placementStrategy: spread","title":"Scheduling Strategies"},{"location":"Researcher/scheduling/strategies/#introduction","text":"When the Run:ai scheduler schedules Jobs, it can use two alternate placement strategies : Strategy Description Bin Packing Fill up a GPU and/or a node before moving on to the next one Spreading Equally spread Jobs amongst GPUs and nodes","title":"Introduction"},{"location":"Researcher/scheduling/strategies/#bin-packing","text":"Bin packing is the default strategy. With bin packing, the scheduler tries to: Fill up a node with Jobs before allocating Jobs to second and third nodes. In a multi GPU node, when using fractions , fill up a GPU before allocating Jobs to a second GPU. The advantage of this strategy is that the scheduler, over time, can package more Jobs into the cluster. As the strategy minimizes fragmentation. For example, if we have 2 GPUs in a single node on the cluster, and 2 tasks requiring 0.5 GPUs each, using bin-packing, we would place both Jobs on the same GPU and remain with a full GPU ready for the next Job.","title":"Bin Packing"},{"location":"Researcher/scheduling/strategies/#spreading","text":"There are disadvantages to bin-packing: Within a single GPU, two fractional Jobs compete for the same onboard compute power. Within a single node, two Jobs (even on separate GPUs) compete for networking resources, compute power and memory. When there are more resources available than requested, it sometimes makes sense to spread Jobs amongst nodes and GPUs, to allow higher utilization of computing resources and network resources. Returning to the example above, if we have 2 GPUs in a single node on the cluster, and 2 Jobs requiring 0.5 GPUs each, using spread scheduling we would place each Job on a separate GPU, allowing both to benefit from the computing power of a full GPU.","title":"Spreading"},{"location":"Researcher/scheduling/strategies/#changing-scheduler-strategy","text":"The strategy affects the entire cluster. To change the strategy run: kubectl edit runaiconfig -n runai Find `runai-scheduler' and add: runai-scheduler: placementStrategy: spread","title":"Changing Scheduler Strategy"},{"location":"Researcher/scheduling/the-runai-scheduler/","text":"Introduction \u00b6 At the heart of the Run:ai solution is the Run:ai scheduler. The scheduler is the gatekeeper of your organization's hardware resources. It makes decisions on resource allocations according to pre-created rules. The purpose of this document is to describe the Run:ai scheduler and explain how resource management works. Terminology \u00b6 Workload Types \u00b6 Run:ai differentiates between three types of deep learning workloads: Interactive build workloads. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Build workloads typically do not tax the GPU for a long duration. There are also typically real users behind an interactive workload that need an immediate scheduling response. Unattended (or \"non-interactive\") training workloads. Training is characterized by a deep learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. Training workloads typically utilize large percentages of the GPU. During the execution, the Researcher can examine the results. A Training session can take anything from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. It follows that a good practice for the Researcher is to save checkpoints and allow the code to restore from the last checkpoint. Inference workloads. These are production workloads that serve requests. The Run:ai scheduler treats these workloads as Interactive workloads. Projects \u00b6 Projects are quota entities that associate a Project name with a deserved GPU quota as well as other preferences. A Researcher submitting a workload must associate a Project with any workload request. The Run:ai scheduler will then compare the request against the current allocations and the Project's deserved quota and determine whether the workload can be allocated with resources or whether it should remain in a pending state. For further information on Projects and how to configure them, see: Working with Projects Departments \u00b6 A Department is the second hierarchy of resource allocation above Project . A Department quota supersedes a Project quota in the sense that if the sum of Project quotas for Department A exceeds the Department quota -- the scheduler will use the Department quota rather than the Projects' quota. For further information on Departments and how to configure them, see: Working with Departments Pods \u00b6 Pods are units of work within a Job. Typically, each Job has a single Pod. However, in some scenarios (see Hyperparameter Optimization and Distribute Training below) there will be multiple Pods per Job. All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, memory. Basic Scheduling Concepts \u00b6 Interactive, Training and Inference \u00b6 The Researcher uses the --interactive flag to specify whether the workload is an unattended \"train\" workload or an interactive \"build\" workload. Interactive & Inference workloads will get precedence over training workloads. Training workloads can be preempted when the scheduler determines a more urgent need for resources. Interactive workloads are never preempted. Guaranteed Quota and Over-Quota \u00b6 There are two use cases for Quota and Over-Quota: Node pools are disabled Every new workload is associated with a Project. The Project contains a deserved GPU quota. During scheduling: If the newly required resources, together with currently used resources, end up within the Project's quota, then the workload is ready to be scheduled as part of the guaranteed quota. If the newly required resources together with currently used resources end up above the Project's quota, the workload will only be scheduled if there are 'spare' GPU resources. There are nuances in this flow that are meant to ensure that a Project does not end up with an over-quota made fully of interactive workloads. For additional details see below. Node pools are enabled Every new workload is associated with a Project. The Project contains a deserved GPU quota that is the sum off all node pools GPU quotas. During scheduling: If the newly required resources, together with currently used resources, end up within the overall Project's quota and within the requested node pool(s) quota, then the workload is ready to be scheduled as part of the guaranteed quota. If the newly required resources together with currently used resources end up above the Project's quota or above the requested node pool(s) quota, the workload will only be scheduled if there are 'spare' GPU resources within the same node pool but not part of this Project. There are nuances in this flow that are meant to ensure that a Project does not end up with an over-quota made fully of interactive workloads. For additional details see below. Quota with Multiple Resources \u00b6 A project may have a quota set for more than one resource (GPU, CPU or CPU Memory). For a project then to be \"Over Quota\" it will have to have at least one resource over its quota. For a project to be under quota it needs to have all of its resoruces under quota. Scheduler Details \u00b6 Allocation & Preemption \u00b6 The Run:ai scheduler wakes up periodically to perform allocation tasks on pending workloads: The scheduler looks at each Project separately and selects the most 'deprived' Project. For this deprived Project it chooses a single workload to work on: Interactive & Inference workloads are tried first, but only up to the Project's guaranteed quota. If such a workload exists, it is scheduled even if it means preempting a running unattended workload in this Project. Else, it looks for an unattended workload and schedules it on guaranteed quota or over-quota. The scheduler then recalculates the next 'deprived' Project and continues with the same flow until it finishes attempting to schedule all workloads Node Pools \u00b6 A Node Pool is a set of nodes grouped by an Administrator into a distinct group of resources from which resources can be allocated to Projects and Departments. By default, any node pool created in the system is automatically associated with all Projects and Departments using zero quota resource (GPUs, CPUs, Memory) allocation. This allows any Project and Department to use any node pool with Over-Quota (for Preemptible workloads), thus maximizing the system resource utilization. An Administrator can allocate resources from a specific node pool to chosen Projects and Departments. See Project Setup The Researcher can use node-pools in two ways. The first one is where a Project has guaranteed resources on node-pools - Researcher can then submit a workload and specify a single node-pool or a prioritized list of node-pools to use and recieve guaranteed resources. The second is by using node-pool(s) with no guaranteed resource for that Project (zero allocated resources), and in practice using Over-Quota resources of node-pools. This means a Workload must be Preemptible as it uses resources out of the Project's or node pool quota. The same scenario occurs if a Researcher uses more resources than allocated to a specific node-pool and goes Over-Quota. By default, if a Researcher doesn't specify a node-pool to use by a workload, the scheduler assigns the workload to run using the Project's 'Default node-pool list'. Node Affinity \u00b6 Both the Administrator and the Researcher can provide limitations as to which nodes can be selected for the Job. Limits are managed via Kubernetes labels : The Administrator can set limits at the Project level. Example: Project team-a can only run interactive Jobs on machines with a label of v-100 or a-100 . See Project Setup for more information. The Researcher can set a limit at the Job level, by using the command-line interface flag --node-type . The flag acts as a subset to the Project setting. Node affinity constraints are used during the Allocation phase to filter out candidate nodes for running the Job. For more information on how nodes are filtered see the Filtering section under Node selection in kube-scheduler . The Run:ai scheduler works similarly. Reclaim \u00b6 During the above process, there may be a pending workload whose Project is below the deserved capacity. Still, it cannot be allocated due to the lack of GPU resources. The scheduler will then look for alternative allocations at the expense of another Project which has gone over-quota while preserving fairness between Projects. Fairness \u00b6 The Run:ai scheduler determines fairness between multiple over-quota Projects according to their GPU quota. Consider for example two Projects, each spawning a significant amount of workloads (e.g. for Hyperparameter tuning) all of which wait in the queue to be executed. The Run:ai Scheduler allocates resources while preserving fairness between the different Projects regardless of the time they entered the system. The fairness works according to the relative portion of the GPU quota for each Project. To further illustrate that, suppose that: Project A has been allocated with a quota of 3 GPUs. Project B has been allocated with a quota of 1 GPU. Then, if both Projects go over quota, Project A will receive 75% (=3/(1+3)) of the idle GPUs and Project B will receive 25% (=1/(1+3)) of the idle GPUs. This ratio will be recalculated every time a new Job is submitted to the system or an existing Job ends. This fairness equivalence will also be maintained amongst running Jobs. The scheduler will preempt training sessions to maintain this equivalence Over-Quota Priority \u00b6 When Over-Quota Priority feature is enabled, Run:ai scheduler allocates GPUs within-quota and over-quota using different weights. Within quota, GPUs are allocated based on assigned GPUs. The remaining over-quota GPUs are allocated based on their relative portion of GPU Over Quota Priority for each Project. GPUs Over-Quota Priority values are translated into numeric values as follows: None-0, Low-1, Medium-2, High-3. Let's examine the previous example with Over-Quota Weights: Project A has been allocated with a quota of 3 GPUs and GPU over quota weight is set to Low. Project B has been allocated with a quota of 1 GPU and GPU over quota weight is set to High. Then, Project A is allocated with 3 GPUs and project B is allocated with 1 GPU. if both Projects go over quota, Project A will receive an additional 25% (=1/(1+3)) of the idle GPUs and Project B will receive an additional 75% (=3/(1+3)) of the idle GPUs. With the addition of node pools, the principles of Over-Quota and Over-Quota priority remain unchanged. However, the number of resources that are allocated with Over-Quota and Over-Quota Priority is calculated against node pool resources instead of the whole Project resources. Note: Over-Quota On/Off and Over-Quota Priority settings remain at the Project and Department level. Bin-packing & Consolidation \u00b6 Part of an efficient scheduler is the ability to eliminate fragmentation: The first step in avoiding fragmentation is bin packing: try and fill nodes (machines) up before allocating workloads to new machines. The next step is to consolidate Jobs on demand. If a workload cannot be allocated due to fragmentation, the scheduler will try and move unattended workloads from node to node in order to get the required amount of GPUs to schedule the pending workload. Advanced \u00b6 GPU Fractions \u00b6 Run:ai provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU. Run:ai\u2019s fractional GPU system effectively creates logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. One important thing to note is that fraction scheduling divides up GPU memory . As such the GPU memory is divided up between Jobs. If a Job asks for 0.5 GPU, and the GPU has 32GB of memory, then the Job will see only 16GB. An attempt to allocate more than 16GB will result in an out-of-memory exception. GPU Fractions are scheduled as regular GPUs in the sense that: Allocation is made using fractions such that the total of the GPU allocation for a single GPU is smaller or equal to 1. Preemption is available for non-interactive workloads. Bin-packing & Consolidation work the same for fractions. Support: Hyperparameter Optimization supports fractions. Distributed Training \u00b6 Distributed Training, is the ability to split the training of a model among multiple processors. It is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Each such split is a pod (see definition above). Run:ai spawns an additional launcher process that manages and coordinates the other worker pods. Distribute Training utilizes a practice sometimes known as Gang Scheduling : The scheduler must ensure that multiple pods are started on what are typically multiple Nodes before the Job can start. If one pod is preempted, the others are also immediately preempted. When node pools are enabled, all pods must be scheduled to the same node pool. Gang Scheduling essentially prevents scenarios where part of the pods are scheduled while other pods belonging to the same Job are pending for resources to become available; scenarios that can cause deadlock situations and major inefficiencies in cluster utilization. The Run:ai system provides: Inter-pod communication. Command-line interface to access logs and an interactive shell. For more information on Distributed Training in Run:ai see here Hyperparameter Optimization \u00b6 Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine the results to decide what works best. With HPO, the Researcher provides a single script that is used with multiple, varying, parameters. Each run is a pod (see definition above). Unlike Gang Scheduling, with HPO, pods are independent . They are scheduled independently, started, and end independently, and if preempted, the other pods are unaffected. The scheduling behavior for individual pods is exactly as described in the Scheduler Details section above for Jobs. In case node pools are enabled, if the HPO workload designated more than one node pool, the different pods might end up running on different node pools. For more information on Hyperparameter Optimization in Run:ai see here","title":"The Run:ai Scheduler"},{"location":"Researcher/scheduling/the-runai-scheduler/#introduction","text":"At the heart of the Run:ai solution is the Run:ai scheduler. The scheduler is the gatekeeper of your organization's hardware resources. It makes decisions on resource allocations according to pre-created rules. The purpose of this document is to describe the Run:ai scheduler and explain how resource management works.","title":"Introduction"},{"location":"Researcher/scheduling/the-runai-scheduler/#terminology","text":"","title":"Terminology"},{"location":"Researcher/scheduling/the-runai-scheduler/#workload-types","text":"Run:ai differentiates between three types of deep learning workloads: Interactive build workloads. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Build workloads typically do not tax the GPU for a long duration. There are also typically real users behind an interactive workload that need an immediate scheduling response. Unattended (or \"non-interactive\") training workloads. Training is characterized by a deep learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. Training workloads typically utilize large percentages of the GPU. During the execution, the Researcher can examine the results. A Training session can take anything from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. It follows that a good practice for the Researcher is to save checkpoints and allow the code to restore from the last checkpoint. Inference workloads. These are production workloads that serve requests. The Run:ai scheduler treats these workloads as Interactive workloads.","title":"Workload Types"},{"location":"Researcher/scheduling/the-runai-scheduler/#projects","text":"Projects are quota entities that associate a Project name with a deserved GPU quota as well as other preferences. A Researcher submitting a workload must associate a Project with any workload request. The Run:ai scheduler will then compare the request against the current allocations and the Project's deserved quota and determine whether the workload can be allocated with resources or whether it should remain in a pending state. For further information on Projects and how to configure them, see: Working with Projects","title":"Projects"},{"location":"Researcher/scheduling/the-runai-scheduler/#departments","text":"A Department is the second hierarchy of resource allocation above Project . A Department quota supersedes a Project quota in the sense that if the sum of Project quotas for Department A exceeds the Department quota -- the scheduler will use the Department quota rather than the Projects' quota. For further information on Departments and how to configure them, see: Working with Departments","title":"Departments"},{"location":"Researcher/scheduling/the-runai-scheduler/#pods","text":"Pods are units of work within a Job. Typically, each Job has a single Pod. However, in some scenarios (see Hyperparameter Optimization and Distribute Training below) there will be multiple Pods per Job. All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, memory.","title":"Pods"},{"location":"Researcher/scheduling/the-runai-scheduler/#basic-scheduling-concepts","text":"","title":"Basic Scheduling Concepts"},{"location":"Researcher/scheduling/the-runai-scheduler/#interactive-training-and-inference","text":"The Researcher uses the --interactive flag to specify whether the workload is an unattended \"train\" workload or an interactive \"build\" workload. Interactive & Inference workloads will get precedence over training workloads. Training workloads can be preempted when the scheduler determines a more urgent need for resources. Interactive workloads are never preempted.","title":"Interactive, Training and Inference"},{"location":"Researcher/scheduling/the-runai-scheduler/#guaranteed-quota-and-over-quota","text":"There are two use cases for Quota and Over-Quota: Node pools are disabled Every new workload is associated with a Project. The Project contains a deserved GPU quota. During scheduling: If the newly required resources, together with currently used resources, end up within the Project's quota, then the workload is ready to be scheduled as part of the guaranteed quota. If the newly required resources together with currently used resources end up above the Project's quota, the workload will only be scheduled if there are 'spare' GPU resources. There are nuances in this flow that are meant to ensure that a Project does not end up with an over-quota made fully of interactive workloads. For additional details see below. Node pools are enabled Every new workload is associated with a Project. The Project contains a deserved GPU quota that is the sum off all node pools GPU quotas. During scheduling: If the newly required resources, together with currently used resources, end up within the overall Project's quota and within the requested node pool(s) quota, then the workload is ready to be scheduled as part of the guaranteed quota. If the newly required resources together with currently used resources end up above the Project's quota or above the requested node pool(s) quota, the workload will only be scheduled if there are 'spare' GPU resources within the same node pool but not part of this Project. There are nuances in this flow that are meant to ensure that a Project does not end up with an over-quota made fully of interactive workloads. For additional details see below.","title":"Guaranteed Quota and Over-Quota"},{"location":"Researcher/scheduling/the-runai-scheduler/#quota-with-multiple-resources","text":"A project may have a quota set for more than one resource (GPU, CPU or CPU Memory). For a project then to be \"Over Quota\" it will have to have at least one resource over its quota. For a project to be under quota it needs to have all of its resoruces under quota.","title":"Quota with Multiple Resources"},{"location":"Researcher/scheduling/the-runai-scheduler/#scheduler-details","text":"","title":"Scheduler Details"},{"location":"Researcher/scheduling/the-runai-scheduler/#allocation-preemption","text":"The Run:ai scheduler wakes up periodically to perform allocation tasks on pending workloads: The scheduler looks at each Project separately and selects the most 'deprived' Project. For this deprived Project it chooses a single workload to work on: Interactive & Inference workloads are tried first, but only up to the Project's guaranteed quota. If such a workload exists, it is scheduled even if it means preempting a running unattended workload in this Project. Else, it looks for an unattended workload and schedules it on guaranteed quota or over-quota. The scheduler then recalculates the next 'deprived' Project and continues with the same flow until it finishes attempting to schedule all workloads","title":"Allocation &amp; Preemption"},{"location":"Researcher/scheduling/the-runai-scheduler/#node-pools","text":"A Node Pool is a set of nodes grouped by an Administrator into a distinct group of resources from which resources can be allocated to Projects and Departments. By default, any node pool created in the system is automatically associated with all Projects and Departments using zero quota resource (GPUs, CPUs, Memory) allocation. This allows any Project and Department to use any node pool with Over-Quota (for Preemptible workloads), thus maximizing the system resource utilization. An Administrator can allocate resources from a specific node pool to chosen Projects and Departments. See Project Setup The Researcher can use node-pools in two ways. The first one is where a Project has guaranteed resources on node-pools - Researcher can then submit a workload and specify a single node-pool or a prioritized list of node-pools to use and recieve guaranteed resources. The second is by using node-pool(s) with no guaranteed resource for that Project (zero allocated resources), and in practice using Over-Quota resources of node-pools. This means a Workload must be Preemptible as it uses resources out of the Project's or node pool quota. The same scenario occurs if a Researcher uses more resources than allocated to a specific node-pool and goes Over-Quota. By default, if a Researcher doesn't specify a node-pool to use by a workload, the scheduler assigns the workload to run using the Project's 'Default node-pool list'.","title":"Node Pools"},{"location":"Researcher/scheduling/the-runai-scheduler/#node-affinity","text":"Both the Administrator and the Researcher can provide limitations as to which nodes can be selected for the Job. Limits are managed via Kubernetes labels : The Administrator can set limits at the Project level. Example: Project team-a can only run interactive Jobs on machines with a label of v-100 or a-100 . See Project Setup for more information. The Researcher can set a limit at the Job level, by using the command-line interface flag --node-type . The flag acts as a subset to the Project setting. Node affinity constraints are used during the Allocation phase to filter out candidate nodes for running the Job. For more information on how nodes are filtered see the Filtering section under Node selection in kube-scheduler . The Run:ai scheduler works similarly.","title":"Node Affinity"},{"location":"Researcher/scheduling/the-runai-scheduler/#reclaim","text":"During the above process, there may be a pending workload whose Project is below the deserved capacity. Still, it cannot be allocated due to the lack of GPU resources. The scheduler will then look for alternative allocations at the expense of another Project which has gone over-quota while preserving fairness between Projects.","title":"Reclaim"},{"location":"Researcher/scheduling/the-runai-scheduler/#fairness","text":"The Run:ai scheduler determines fairness between multiple over-quota Projects according to their GPU quota. Consider for example two Projects, each spawning a significant amount of workloads (e.g. for Hyperparameter tuning) all of which wait in the queue to be executed. The Run:ai Scheduler allocates resources while preserving fairness between the different Projects regardless of the time they entered the system. The fairness works according to the relative portion of the GPU quota for each Project. To further illustrate that, suppose that: Project A has been allocated with a quota of 3 GPUs. Project B has been allocated with a quota of 1 GPU. Then, if both Projects go over quota, Project A will receive 75% (=3/(1+3)) of the idle GPUs and Project B will receive 25% (=1/(1+3)) of the idle GPUs. This ratio will be recalculated every time a new Job is submitted to the system or an existing Job ends. This fairness equivalence will also be maintained amongst running Jobs. The scheduler will preempt training sessions to maintain this equivalence","title":"Fairness"},{"location":"Researcher/scheduling/the-runai-scheduler/#over-quota-priority","text":"When Over-Quota Priority feature is enabled, Run:ai scheduler allocates GPUs within-quota and over-quota using different weights. Within quota, GPUs are allocated based on assigned GPUs. The remaining over-quota GPUs are allocated based on their relative portion of GPU Over Quota Priority for each Project. GPUs Over-Quota Priority values are translated into numeric values as follows: None-0, Low-1, Medium-2, High-3. Let's examine the previous example with Over-Quota Weights: Project A has been allocated with a quota of 3 GPUs and GPU over quota weight is set to Low. Project B has been allocated with a quota of 1 GPU and GPU over quota weight is set to High. Then, Project A is allocated with 3 GPUs and project B is allocated with 1 GPU. if both Projects go over quota, Project A will receive an additional 25% (=1/(1+3)) of the idle GPUs and Project B will receive an additional 75% (=3/(1+3)) of the idle GPUs. With the addition of node pools, the principles of Over-Quota and Over-Quota priority remain unchanged. However, the number of resources that are allocated with Over-Quota and Over-Quota Priority is calculated against node pool resources instead of the whole Project resources. Note: Over-Quota On/Off and Over-Quota Priority settings remain at the Project and Department level.","title":"Over-Quota Priority"},{"location":"Researcher/scheduling/the-runai-scheduler/#bin-packing-consolidation","text":"Part of an efficient scheduler is the ability to eliminate fragmentation: The first step in avoiding fragmentation is bin packing: try and fill nodes (machines) up before allocating workloads to new machines. The next step is to consolidate Jobs on demand. If a workload cannot be allocated due to fragmentation, the scheduler will try and move unattended workloads from node to node in order to get the required amount of GPUs to schedule the pending workload.","title":"Bin-packing &amp; Consolidation"},{"location":"Researcher/scheduling/the-runai-scheduler/#advanced","text":"","title":"Advanced"},{"location":"Researcher/scheduling/the-runai-scheduler/#gpu-fractions","text":"Run:ai provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU. Run:ai\u2019s fractional GPU system effectively creates logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. One important thing to note is that fraction scheduling divides up GPU memory . As such the GPU memory is divided up between Jobs. If a Job asks for 0.5 GPU, and the GPU has 32GB of memory, then the Job will see only 16GB. An attempt to allocate more than 16GB will result in an out-of-memory exception. GPU Fractions are scheduled as regular GPUs in the sense that: Allocation is made using fractions such that the total of the GPU allocation for a single GPU is smaller or equal to 1. Preemption is available for non-interactive workloads. Bin-packing & Consolidation work the same for fractions. Support: Hyperparameter Optimization supports fractions.","title":"GPU Fractions"},{"location":"Researcher/scheduling/the-runai-scheduler/#distributed-training","text":"Distributed Training, is the ability to split the training of a model among multiple processors. It is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Each such split is a pod (see definition above). Run:ai spawns an additional launcher process that manages and coordinates the other worker pods. Distribute Training utilizes a practice sometimes known as Gang Scheduling : The scheduler must ensure that multiple pods are started on what are typically multiple Nodes before the Job can start. If one pod is preempted, the others are also immediately preempted. When node pools are enabled, all pods must be scheduled to the same node pool. Gang Scheduling essentially prevents scenarios where part of the pods are scheduled while other pods belonging to the same Job are pending for resources to become available; scenarios that can cause deadlock situations and major inefficiencies in cluster utilization. The Run:ai system provides: Inter-pod communication. Command-line interface to access logs and an interactive shell. For more information on Distributed Training in Run:ai see here","title":"Distributed Training"},{"location":"Researcher/scheduling/the-runai-scheduler/#hyperparameter-optimization","text":"Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine the results to decide what works best. With HPO, the Researcher provides a single script that is used with multiple, varying, parameters. Each run is a pod (see definition above). Unlike Gang Scheduling, with HPO, pods are independent . They are scheduled independently, started, and end independently, and if preempted, the other pods are unaffected. The scheduling behavior for individual pods is exactly as described in the Scheduler Details section above for Jobs. In case node pools are enabled, if the HPO workload designated more than one node pool, the different pods might end up running on different node pools. For more information on Hyperparameter Optimization in Run:ai see here","title":"Hyperparameter Optimization"},{"location":"Researcher/scheduling/using-node-pools/","text":"Introduction \u00b6 Run:ai version 2.8 introduced a new building block to assist in managing resources effectively - node-pools. A node-pool is a set of nodes grouped into a bucket of resources using a predefined (e.g. GPU-Type) or administrator-defined label (key & value). Typically, those nodes share a common feature or property, such as GPU type or other HW capability (such as Infiniband connectivity) or represent a proximity group (i.e. nodes interconnected via a local ultra-fast switch). Those nodes would typically be used by researchers to run specific workloads on specific resource types, or by MLops engineers to run specific Inference workloads that require specific node types. Enabling Node-Pools \u00b6 After upgrading from version 2.7 or older to version 2.8 or newer, the \u2018Node Pools\u2019 feature is disabled by default. Once the feature is enabled by the administrator, all nodes in each of your upgraded clusters are associated with the \u2018Default\u2019 node pool. * To use node-pools - enable this feature under \u2018Settings->General\u2019, set \u2018Enable Node Pools\u2019 switch. * To manage CPU resources - enable this feature under \u2018Settings->General\u2019, set \u2018Enable CPU Resources Quota\u2019. Creating and using Node-Pools \u00b6 An administrator creates logical groups of nodes by specifying a unique label (key & value) and associating it with a node-pool. Run:ai allows an administrator to use any label key and value as the designated node-pool label (e.g. gpu-type = A100 or faculty = computer-science). Each node-pool has its unique name and label used identify and group nodes into a node-pool. Once a new node-pool is created, it is automatically assigned to all Projects and Departments with a quota of zero GPU resources and CPU resources. This allows any Project and Department to use any node pool when over-quota is enabled, even if the administrator did not assign a quota for a specific node-pool in a Project or Department. Using resources with over-quota means those resources might be reclaimed by other Projects or Departments that have assigned quota in place for those node-pools. On the other hand, this pattern allows to maximize the utilization of GPU and CPU resources by the system. An administrator should assign resources from a node-pool to a project for which the administrator wants to guarantee reserved resources on that node pool. The reservation should be done for GPU resources and CPU resources. Projects and Departments with no reserved resources for a specific node-pool can still use node-pools resources, but the resources are not reserved and can be reclaimed by the resources owner Project (or Department). Creating a new node-pool and assigning resources from a node-pool to Projects and Departments is an operation limited to Administrators only. Researchers can use node-pools when submitting a new workload. By specifying the node pool from which a workload allocates resources, the scheduler shell launch that workload on a node that is part of the specified node pool. If no node-pool is selected by a workload, the \u2018Default\u2019 node-pool is used. Multi Node Pools Selection \u00b6 Starting version 2.9, Run:ai system supports scheduling workloads to a node pool using a list of priortized node pools. The scheduler will try to schedule the workload to the most prioritzed node pool first, if it fails, it will try the second one and so forth. If the scheduler tried the all list and failed to schedule the workload, it will start from the most prioritzed node pool again. This patten allows to maximize the odds that a workload will be schdeuled. Defininig Project level 'default node pool priority list' \u00b6 If the Reseacher did not specify any node pool within the workload specification, the system will use the 'default node pool priority list' as defined by the administrator. If the administrator did not define the 'default node pool priority list', the system will use 'Default' node pool. Node-Pools Best Practices \u00b6 Node-pools give administrators the ability to manage quotas in a more granular manner than the Project level, allowing them to specify which Projects are assigned guaranteed resources on specific sets of nodes to be then used by Workloads that need specific node characteristics. Any Project can use any Node-Pool, even if a quota was not assigned to the Node-Pool, it can still be used in Over-Quota manner. As a rule of thumb, it is best for the administrator to split the organization's GPU deployment to the smallest number of node-pools that still serves its purpose, this would help in keeping each pool large enough and minimize the probability that the Run:ai scheduler would not be able to find available resources on a specific node-pool. It is a good practice for researchers to use multi node pools where applicable, to maximize their workload odds to get scheduled in a timely manner or in cases where resources are scarce in specific node pool. Administrators should set Projects' 'default node pool priority list' to make sure that incase a workload was scheduled with no node pool selection, it is scheduled to the preferences of the Administrator, and to increase the workload's odds to get scheduled and in a timely manner. Common use-cases \u00b6 Training workloads that require specific GPU-type nodes, either because of the scale of parameters (computation time) or for other specific GPU capabilities Inference workloads that require specific GPU-type nodes to comply with constraints such as execution time Workloads that require proximity of nodes for purposes of local ultra-fast networking Organizations where specific nodes belong to specific a department, and while assuring quota for that department and its subordinated projects, the administrator also wants to let other departments and projects use those nodes when not used by the resource owner Projects that need to use specific resources, but also ensure others will not occupy those resources While the upper use cases refer to a single node pool, they are applicable to multi node pools as well. In cases where a workload's spcification could be statisfied by more than one type of node, using multiple node pools selection potentially increases the probability of a workload to find resources to allocate and shorten the time it will take to get those resources.","title":"Using Node Pools"},{"location":"Researcher/scheduling/using-node-pools/#introduction","text":"Run:ai version 2.8 introduced a new building block to assist in managing resources effectively - node-pools. A node-pool is a set of nodes grouped into a bucket of resources using a predefined (e.g. GPU-Type) or administrator-defined label (key & value). Typically, those nodes share a common feature or property, such as GPU type or other HW capability (such as Infiniband connectivity) or represent a proximity group (i.e. nodes interconnected via a local ultra-fast switch). Those nodes would typically be used by researchers to run specific workloads on specific resource types, or by MLops engineers to run specific Inference workloads that require specific node types.","title":"Introduction"},{"location":"Researcher/scheduling/using-node-pools/#enabling-node-pools","text":"After upgrading from version 2.7 or older to version 2.8 or newer, the \u2018Node Pools\u2019 feature is disabled by default. Once the feature is enabled by the administrator, all nodes in each of your upgraded clusters are associated with the \u2018Default\u2019 node pool. * To use node-pools - enable this feature under \u2018Settings->General\u2019, set \u2018Enable Node Pools\u2019 switch. * To manage CPU resources - enable this feature under \u2018Settings->General\u2019, set \u2018Enable CPU Resources Quota\u2019.","title":"Enabling Node-Pools"},{"location":"Researcher/scheduling/using-node-pools/#creating-and-using-node-pools","text":"An administrator creates logical groups of nodes by specifying a unique label (key & value) and associating it with a node-pool. Run:ai allows an administrator to use any label key and value as the designated node-pool label (e.g. gpu-type = A100 or faculty = computer-science). Each node-pool has its unique name and label used identify and group nodes into a node-pool. Once a new node-pool is created, it is automatically assigned to all Projects and Departments with a quota of zero GPU resources and CPU resources. This allows any Project and Department to use any node pool when over-quota is enabled, even if the administrator did not assign a quota for a specific node-pool in a Project or Department. Using resources with over-quota means those resources might be reclaimed by other Projects or Departments that have assigned quota in place for those node-pools. On the other hand, this pattern allows to maximize the utilization of GPU and CPU resources by the system. An administrator should assign resources from a node-pool to a project for which the administrator wants to guarantee reserved resources on that node pool. The reservation should be done for GPU resources and CPU resources. Projects and Departments with no reserved resources for a specific node-pool can still use node-pools resources, but the resources are not reserved and can be reclaimed by the resources owner Project (or Department). Creating a new node-pool and assigning resources from a node-pool to Projects and Departments is an operation limited to Administrators only. Researchers can use node-pools when submitting a new workload. By specifying the node pool from which a workload allocates resources, the scheduler shell launch that workload on a node that is part of the specified node pool. If no node-pool is selected by a workload, the \u2018Default\u2019 node-pool is used.","title":"Creating and using Node-Pools"},{"location":"Researcher/scheduling/using-node-pools/#multi-node-pools-selection","text":"Starting version 2.9, Run:ai system supports scheduling workloads to a node pool using a list of priortized node pools. The scheduler will try to schedule the workload to the most prioritzed node pool first, if it fails, it will try the second one and so forth. If the scheduler tried the all list and failed to schedule the workload, it will start from the most prioritzed node pool again. This patten allows to maximize the odds that a workload will be schdeuled.","title":"Multi Node Pools Selection"},{"location":"Researcher/scheduling/using-node-pools/#defininig-project-level-default-node-pool-priority-list","text":"If the Reseacher did not specify any node pool within the workload specification, the system will use the 'default node pool priority list' as defined by the administrator. If the administrator did not define the 'default node pool priority list', the system will use 'Default' node pool.","title":"Defininig Project level 'default node pool priority list'"},{"location":"Researcher/scheduling/using-node-pools/#node-pools-best-practices","text":"Node-pools give administrators the ability to manage quotas in a more granular manner than the Project level, allowing them to specify which Projects are assigned guaranteed resources on specific sets of nodes to be then used by Workloads that need specific node characteristics. Any Project can use any Node-Pool, even if a quota was not assigned to the Node-Pool, it can still be used in Over-Quota manner. As a rule of thumb, it is best for the administrator to split the organization's GPU deployment to the smallest number of node-pools that still serves its purpose, this would help in keeping each pool large enough and minimize the probability that the Run:ai scheduler would not be able to find available resources on a specific node-pool. It is a good practice for researchers to use multi node pools where applicable, to maximize their workload odds to get scheduled in a timely manner or in cases where resources are scarce in specific node pool. Administrators should set Projects' 'default node pool priority list' to make sure that incase a workload was scheduled with no node pool selection, it is scheduled to the preferences of the Administrator, and to increase the workload's odds to get scheduled and in a timely manner.","title":"Node-Pools Best Practices"},{"location":"Researcher/scheduling/using-node-pools/#common-use-cases","text":"Training workloads that require specific GPU-type nodes, either because of the scale of parameters (computation time) or for other specific GPU capabilities Inference workloads that require specific GPU-type nodes to comply with constraints such as execution time Workloads that require proximity of nodes for purposes of local ultra-fast networking Organizations where specific nodes belong to specific a department, and while assuring quota for that department and its subordinated projects, the administrator also wants to let other departments and projects use those nodes when not used by the resource owner Projects that need to use specific resources, but also ensure others will not occupy those resources While the upper use cases refer to a single node pool, they are applicable to multi node pools as well. In cases where a workload's spcification could be statisfied by more than one type of node, using multiple node pools selection potentially increases the probability of a workload to find resources to allocate and shorten the time it will take to get those resources.","title":"Common use-cases"},{"location":"Researcher/tools/dev-jupyter/","text":"Use a Jupyter Notebook with a Run:ai Job \u00b6 A Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code. Uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Jupyter Notebooks are popular with Researchers as a way to code and run deep-learning code. A Jupyter Notebook runs inside the user container . This document is about accessing the remote container created by Run:ai via such a notebook. Alternatively, Run:ai provides integration with JupyterHub. JupyterHub is a separate service that makes it possible to serve pre-configured data science environments. For more information see Connecting JupyterHub with Run:ai . Submit a Jupyter Notebook Workload \u00b6 There are two ways to submit a Jupyter Notebook Job: via the Command-line interface or the user interface Submit via the User interface \u00b6 Within the user interface go to the Job list. Select New Job on the top right. Select Interactive at the top. Add an image that supports Jupyter Notebook. For example jupyter/scipy-notebook . Select the Jupyter Notebook button. Submit the Job. When running, select the job and press Connect on the top right. Submit a Workload \u00b6 Run the following command to connect to the Jupyter Notebook container as if it were running locally: runai submit build-jupyter --jupyter -g 1 The terminal will show the following: ~> runai submit build-jupyter --jupyter -g 1 --attach INFO [ 0001 ] Exposing default jupyter notebook port 8888 INFO [ 0001 ] Using default jupyter notebook image \"jupyter/scipy-notebook\" INFO [ 0001 ] Using default jupyter notebook service type portforward The job 'build-jupyter' has been submitted successfully You can run ` runai describe job build-jupyter -p team-a ` to check the job status INFO [ 0006 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0081 ] Job started Jupyter notebook token: 428dc561a5431bd383eff17714460de478d673deec57c045 Open access point ( s ) to service from localhost:8888 Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 The Job starts a Jupyter notebook container. The connection is redirected to the local machine (127.0.0.1) on port 8888 Browse to http://localhost:8888 . Use the token in the output to log into the notebook. Alternatives \u00b6 The above flag --jupyter is a shortcut with a predefined image. If you want to run your own notebook, use the quickstart on running a build workload with connected ports .","title":"Jupyter Notebook"},{"location":"Researcher/tools/dev-jupyter/#use-a-jupyter-notebook-with-a-runai-job","text":"A Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code. Uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Jupyter Notebooks are popular with Researchers as a way to code and run deep-learning code. A Jupyter Notebook runs inside the user container . This document is about accessing the remote container created by Run:ai via such a notebook. Alternatively, Run:ai provides integration with JupyterHub. JupyterHub is a separate service that makes it possible to serve pre-configured data science environments. For more information see Connecting JupyterHub with Run:ai .","title":"Use a Jupyter Notebook with a Run:ai Job"},{"location":"Researcher/tools/dev-jupyter/#submit-a-jupyter-notebook-workload","text":"There are two ways to submit a Jupyter Notebook Job: via the Command-line interface or the user interface","title":"Submit a Jupyter Notebook Workload"},{"location":"Researcher/tools/dev-jupyter/#submit-via-the-user-interface","text":"Within the user interface go to the Job list. Select New Job on the top right. Select Interactive at the top. Add an image that supports Jupyter Notebook. For example jupyter/scipy-notebook . Select the Jupyter Notebook button. Submit the Job. When running, select the job and press Connect on the top right.","title":"Submit via the User interface"},{"location":"Researcher/tools/dev-jupyter/#submit-a-workload","text":"Run the following command to connect to the Jupyter Notebook container as if it were running locally: runai submit build-jupyter --jupyter -g 1 The terminal will show the following: ~> runai submit build-jupyter --jupyter -g 1 --attach INFO [ 0001 ] Exposing default jupyter notebook port 8888 INFO [ 0001 ] Using default jupyter notebook image \"jupyter/scipy-notebook\" INFO [ 0001 ] Using default jupyter notebook service type portforward The job 'build-jupyter' has been submitted successfully You can run ` runai describe job build-jupyter -p team-a ` to check the job status INFO [ 0006 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0081 ] Job started Jupyter notebook token: 428dc561a5431bd383eff17714460de478d673deec57c045 Open access point ( s ) to service from localhost:8888 Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 The Job starts a Jupyter notebook container. The connection is redirected to the local machine (127.0.0.1) on port 8888 Browse to http://localhost:8888 . Use the token in the output to log into the notebook.","title":"Submit a Workload"},{"location":"Researcher/tools/dev-jupyter/#alternatives","text":"The above flag --jupyter is a shortcut with a predefined image. If you want to run your own notebook, use the quickstart on running a build workload with connected ports .","title":"Alternatives"},{"location":"Researcher/tools/dev-pycharm/","text":"Use PyCharm with a Run:ai Job \u00b6 Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:ai, from JetBrain's PyCharm . Submit a Workload \u00b6 You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/pycharm-demo . The image runs both python and ssh. Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'build-remote' has been submitted successfully You can run ` runai describe job build-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222 Note It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22 The Job starts an sshd server on port 22. The Job redirects the external port 30022 to port 22 and uses a Node Port service type. Run: runai list worklaods Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222 PyCharm \u00b6 Under PyCharm | Preferences go to: Project | Python Interpreter Add a new SSH Interpreter. As Host, use the IP address above. Change the port to the above and use the Username root You will be prompted for a password. Enter root Apply settings and run the code via this interpreter. You will see your project uploaded to the container and running remotely.","title":"PyCharm"},{"location":"Researcher/tools/dev-pycharm/#use-pycharm-with-a-runai-job","text":"Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:ai, from JetBrain's PyCharm .","title":"Use PyCharm with a Run:ai Job"},{"location":"Researcher/tools/dev-pycharm/#submit-a-workload","text":"You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/pycharm-demo . The image runs both python and ssh. Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'build-remote' has been submitted successfully You can run ` runai describe job build-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222 Note It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22 The Job starts an sshd server on port 22. The Job redirects the external port 30022 to port 22 and uses a Node Port service type. Run: runai list worklaods Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222","title":"Submit a Workload"},{"location":"Researcher/tools/dev-pycharm/#pycharm","text":"Under PyCharm | Preferences go to: Project | Python Interpreter Add a new SSH Interpreter. As Host, use the IP address above. Change the port to the above and use the Username root You will be prompted for a password. Enter root Apply settings and run the code via this interpreter. You will see your project uploaded to the container and running remotely.","title":"PyCharm"},{"location":"Researcher/tools/dev-tensorboard/","text":"Connecting to TensorBoard \u00b6 Once you launch a Deep Learning workload using Run:ai, you may want to view its progress. A popular tool for viewing progress is TensorBoard . The document below explains how to use TensorBoard to view the progress or a Run:ai Job. Submit a Workload \u00b6 When you submit a workload, your workload must save TensorBoard logs which can later be viewed. Follow this document on how to do this. You can also view the Run:ai sample code here . The code shows: A reference to a log directory: log_dir = \"logs/fit/\" + datetime . datetime . now () . strftime ( \"%Y%m %d -%H%M%S\" ) A registered Keras callback for TensorBoard: tensorboard_callback = TensorBoard ( log_dir = log_dir , histogram_freq = 1 ) model . fit ( x_train , y_train , .... callbacks = [ ... , tensorboard_callback ]) The logs directory must be saved on a Network File Server such that it can be accessed by the TensorBoard Job. For example, by running the Job as follows: runai submit train-with-logs -i tensorflow/tensorflow:1.14.0-gpu-py3 \\ -v /mnt/nfs_share/john:/mydir -g 1 --working-dir /mydir --command -- ./startup.sh Note the volume flag ( -v ) and working directory flag ( --working-dir ). The logs directory will be created on /mnt/nfs_share/john/logs/fit . Submit a TensorBoard Workload \u00b6 There are two ways to submit a TensorBoard Workload: via the Command-line interface or the user interface Submit via the User interface \u00b6 Within the user interface go to the Job list. Select New Job on the top right. Select Interactive at the top. Add an image that supports TensorBoard. For example: tensorflow/tensorflow:latest . Select the TensorBoard button. Add a mounted volume on which TensorBoard logs exist. The example above uses /mnt/nfs_share/john . Map to /mydir Add /mydir to the TensorBoard Logs Directory . Submit the Job. When running, select the job and press Connect on the top right. Submit via the Command-line interface \u00b6 Run the following: runai submit tb -i tensorflow/tensorflow:latest --interactive --service-type=portforward --port 8888:8888 --working-dir /mydir -v /mnt/nfs_share/john:/mydir -- tensorboard --logdir logs/fit --port 8888 --host 0.0.0.0 The terminal will show the following: The job 'tb' has been submitted successfully You can run ` runai describe job tb -p team-a ` to check the job status INFO [ 0006 ] Waiting for job to start Waiting for job to start INFO [ 0014 ] Job started Open access point ( s ) to service from localhost:8888 Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 Browse to http://localhost:8888/ to view TensorBoard. Note A single TensorBoard Job can be used to view multiple deep learning Jobs, provided it has access to the logs directory for these Jobs. You can also submit a TensorBoard Job via the user interface. In which case, instead of portforward you will need to select a different service type. If the URL to the TensorBoard job includes a path, you may need to use the TensorBoard flag --path_prefix . For example, if your access point is acme.com/tensorboard1 add --path_prefix /tensorboard1 .","title":"TensorBoard"},{"location":"Researcher/tools/dev-tensorboard/#connecting-to-tensorboard","text":"Once you launch a Deep Learning workload using Run:ai, you may want to view its progress. A popular tool for viewing progress is TensorBoard . The document below explains how to use TensorBoard to view the progress or a Run:ai Job.","title":"Connecting to TensorBoard"},{"location":"Researcher/tools/dev-tensorboard/#submit-a-workload","text":"When you submit a workload, your workload must save TensorBoard logs which can later be viewed. Follow this document on how to do this. You can also view the Run:ai sample code here . The code shows: A reference to a log directory: log_dir = \"logs/fit/\" + datetime . datetime . now () . strftime ( \"%Y%m %d -%H%M%S\" ) A registered Keras callback for TensorBoard: tensorboard_callback = TensorBoard ( log_dir = log_dir , histogram_freq = 1 ) model . fit ( x_train , y_train , .... callbacks = [ ... , tensorboard_callback ]) The logs directory must be saved on a Network File Server such that it can be accessed by the TensorBoard Job. For example, by running the Job as follows: runai submit train-with-logs -i tensorflow/tensorflow:1.14.0-gpu-py3 \\ -v /mnt/nfs_share/john:/mydir -g 1 --working-dir /mydir --command -- ./startup.sh Note the volume flag ( -v ) and working directory flag ( --working-dir ). The logs directory will be created on /mnt/nfs_share/john/logs/fit .","title":"Submit a Workload"},{"location":"Researcher/tools/dev-tensorboard/#submit-a-tensorboard-workload","text":"There are two ways to submit a TensorBoard Workload: via the Command-line interface or the user interface","title":"Submit a TensorBoard Workload"},{"location":"Researcher/tools/dev-tensorboard/#submit-via-the-user-interface","text":"Within the user interface go to the Job list. Select New Job on the top right. Select Interactive at the top. Add an image that supports TensorBoard. For example: tensorflow/tensorflow:latest . Select the TensorBoard button. Add a mounted volume on which TensorBoard logs exist. The example above uses /mnt/nfs_share/john . Map to /mydir Add /mydir to the TensorBoard Logs Directory . Submit the Job. When running, select the job and press Connect on the top right.","title":"Submit via the User interface"},{"location":"Researcher/tools/dev-tensorboard/#submit-via-the-command-line-interface","text":"Run the following: runai submit tb -i tensorflow/tensorflow:latest --interactive --service-type=portforward --port 8888:8888 --working-dir /mydir -v /mnt/nfs_share/john:/mydir -- tensorboard --logdir logs/fit --port 8888 --host 0.0.0.0 The terminal will show the following: The job 'tb' has been submitted successfully You can run ` runai describe job tb -p team-a ` to check the job status INFO [ 0006 ] Waiting for job to start Waiting for job to start INFO [ 0014 ] Job started Open access point ( s ) to service from localhost:8888 Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 Browse to http://localhost:8888/ to view TensorBoard. Note A single TensorBoard Job can be used to view multiple deep learning Jobs, provided it has access to the logs directory for these Jobs. You can also submit a TensorBoard Job via the user interface. In which case, instead of portforward you will need to select a different service type. If the URL to the TensorBoard job includes a path, you may need to use the TensorBoard flag --path_prefix . For example, if your access point is acme.com/tensorboard1 add --path_prefix /tensorboard1 .","title":"Submit via the Command-line interface"},{"location":"Researcher/tools/dev-vscode/","text":"Use Visual Studio Code with a Run:ai Job \u00b6 Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:ai, from Visual Studio Code . Submit a Workload \u00b6 You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/pycharm-demo . The image runs both python and ssh. Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'build-remote' has been submitted successfully You can run ` runai describe job build-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222 Note It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22 The Job starts an sshd server on port 22. The Job redirects the external port 30022 to port 22 and uses a Node Port service type. Run: runai list jobs Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222 Visual Studio Code \u00b6 Under Visual Studio code install the Remote SSH extension. Create an ssh entry to the service by editing .ssh/config file or use the command Remote-SSH: Connect to Host... from the Command Palette. Enter the IP address and port from above (e.g. ssh root@35.34.212.12 -p 30022 or ssh root@127.0.0.1 -p 2222). User and password are root Using VS Code, install the Python extension on the remote machine Write your first python code and run it remotely.","title":"Visual Studio Code"},{"location":"Researcher/tools/dev-vscode/#use-visual-studio-code-with-a-runai-job","text":"Once you launch a workload using Run:ai, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:ai, from Visual Studio Code .","title":"Use Visual Studio Code with a Run:ai Job"},{"location":"Researcher/tools/dev-vscode/#submit-a-workload","text":"You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/pycharm-demo . The image runs both python and ssh. Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'build-remote' has been submitted successfully You can run ` runai describe job build-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222 Note It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22 The Job starts an sshd server on port 22. The Job redirects the external port 30022 to port 22 and uses a Node Port service type. Run: runai list jobs Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222","title":"Submit a Workload"},{"location":"Researcher/tools/dev-vscode/#visual-studio-code","text":"Under Visual Studio code install the Remote SSH extension. Create an ssh entry to the service by editing .ssh/config file or use the command Remote-SSH: Connect to Host... from the Command Palette. Enter the IP address and port from above (e.g. ssh root@35.34.212.12 -p 30022 or ssh root@127.0.0.1 -p 2222). User and password are root Using VS Code, install the Python extension on the remote machine Write your first python code and run it remotely.","title":"Visual Studio Code"},{"location":"Researcher/tools/dev-x11forward-pycharm/","text":"Use PyCharm with X11 Forwarding and Run:ai \u00b6 X11 is a window system for the Unix operating systems. X11 forwarding allows executing a program remotely through an SSH connection. Meaning, the executable file itself is hosted on a different machine than where the graphical interface is being displayed. The graphical windows are forwarded to your local machine through the SSH connection. This section is about setting up X11 forwarding from a Run:ai-based container to a PyCharm IDE on a remote machine. Submit a Workload \u00b6 You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/quickstart-x-forwarding . The image runs: Python SSH Daemon configured for X11Forwarding OpenCV python library for image handling Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit xforward-remote -i gcr.io/run-ai-demo/quickstart-x-forwarding --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'xforward-remote' has been submitted successfully You can run ` runai describe job xforward-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222 Setup the X11 Forwarding Tunnel \u00b6 Connect to the new Job by running: ssh -X root@127.0.0.1 -p 2222 Note the -X flag. Run: echo $DISPLAY Copy the value. It will be used as a PyCharm environment variable. Important The ssh terminal should remain active throughout the session. PyCharm \u00b6 Under PyCharm | Preferences go to: Project | Python Interpreter Add a new SSH Interpreter. As Host, use localhost . Change the port to the above ( 2222 ) and use the Username root . You will be prompted for a password. Enter root . Make sure to set the correct path of the Python binary. In our case it's /usr/local/bin/python . Apply your settings. Under PyCharm configuration set the following environment variables: DISPLAY - set environment variable you copied before HOME - In our case it's /root . This is required for the X11 authentication to work. Run your code. You can use our sample code here .","title":"X11 & PyCharm"},{"location":"Researcher/tools/dev-x11forward-pycharm/#use-pycharm-with-x11-forwarding-and-runai","text":"X11 is a window system for the Unix operating systems. X11 forwarding allows executing a program remotely through an SSH connection. Meaning, the executable file itself is hosted on a different machine than where the graphical interface is being displayed. The graphical windows are forwarded to your local machine through the SSH connection. This section is about setting up X11 forwarding from a Run:ai-based container to a PyCharm IDE on a remote machine.","title":"Use PyCharm with X11 Forwarding and Run:ai"},{"location":"Researcher/tools/dev-x11forward-pycharm/#submit-a-workload","text":"You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/quickstart-x-forwarding . The image runs: Python SSH Daemon configured for X11Forwarding OpenCV python library for image handling Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit xforward-remote -i gcr.io/run-ai-demo/quickstart-x-forwarding --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'xforward-remote' has been submitted successfully You can run ` runai describe job xforward-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222","title":"Submit a Workload"},{"location":"Researcher/tools/dev-x11forward-pycharm/#setup-the-x11-forwarding-tunnel","text":"Connect to the new Job by running: ssh -X root@127.0.0.1 -p 2222 Note the -X flag. Run: echo $DISPLAY Copy the value. It will be used as a PyCharm environment variable. Important The ssh terminal should remain active throughout the session.","title":"Setup the X11 Forwarding Tunnel"},{"location":"Researcher/tools/dev-x11forward-pycharm/#pycharm","text":"Under PyCharm | Preferences go to: Project | Python Interpreter Add a new SSH Interpreter. As Host, use localhost . Change the port to the above ( 2222 ) and use the Username root . You will be prompted for a password. Enter root . Make sure to set the correct path of the Python binary. In our case it's /usr/local/bin/python . Apply your settings. Under PyCharm configuration set the following environment variables: DISPLAY - set environment variable you copied before HOME - In our case it's /root . This is required for the X11 authentication to work. Run your code. You can use our sample code here .","title":"PyCharm"},{"location":"Researcher/user-interface/workspaces/overview/","text":"Getting familiar with workspaces \u00b6 Workspace is a simplified tool for researchers to conduct experiments, build AI models, access standard MLOps tools, and collaborate with their peers. Run:ai workspaces abstract complex concepts related to running containerized workloads in a Kubernetes environment. Aspects such as networking, storage, and secrets, are built from predefined abstracted setups, that ease and streamline the researcher's AI model development. A workspace consists of all the setup and configuration needed for the research, including container images, data sets, resource requests, as well as all required tools for the research, in a single place. This setup is set to facilitate the research needs and yet to ensure infrastructure owners keep control and efficiency when supporting the various needs. A workspace is associated with a specific Run:ai project (internally: a Kubernetes namespace). A researcher can create multiple workspaces under a specific project. Researchers can only view and use workspaces that are created under projects they are assigned to. Workspaces can be created with just a few clicks of a button. See Workspace creation . Workspaces can be stopped and started to save expensive resources without losing complex environment configurations. Only when a workspace is in status active (see also Workspace Statuses ) does it consume resources. When the workspace is active it exposes the connections to the tools (for example, a Jupyter notebook) within the workspace. An active workspace is a Run:ai interactive workload . The interactive workload starts when the workspace is started and stopped when the workspace is stopped. Workspaces can be used via the user interface or programmatically via the Run:ai Admin API . Workspaces are not supported via the command line interface. You can still run an interactive workload via the command line. Next Steps \u00b6 Workspaces are made from building blocks . Read about the various building block See how to create a Workspace .","title":"Introduction"},{"location":"Researcher/user-interface/workspaces/overview/#getting-familiar-with-workspaces","text":"Workspace is a simplified tool for researchers to conduct experiments, build AI models, access standard MLOps tools, and collaborate with their peers. Run:ai workspaces abstract complex concepts related to running containerized workloads in a Kubernetes environment. Aspects such as networking, storage, and secrets, are built from predefined abstracted setups, that ease and streamline the researcher's AI model development. A workspace consists of all the setup and configuration needed for the research, including container images, data sets, resource requests, as well as all required tools for the research, in a single place. This setup is set to facilitate the research needs and yet to ensure infrastructure owners keep control and efficiency when supporting the various needs. A workspace is associated with a specific Run:ai project (internally: a Kubernetes namespace). A researcher can create multiple workspaces under a specific project. Researchers can only view and use workspaces that are created under projects they are assigned to. Workspaces can be created with just a few clicks of a button. See Workspace creation . Workspaces can be stopped and started to save expensive resources without losing complex environment configurations. Only when a workspace is in status active (see also Workspace Statuses ) does it consume resources. When the workspace is active it exposes the connections to the tools (for example, a Jupyter notebook) within the workspace. An active workspace is a Run:ai interactive workload . The interactive workload starts when the workspace is started and stopped when the workspace is stopped. Workspaces can be used via the user interface or programmatically via the Run:ai Admin API . Workspaces are not supported via the command line interface. You can still run an interactive workload via the command line.","title":"Getting familiar with workspaces"},{"location":"Researcher/user-interface/workspaces/overview/#next-steps","text":"Workspaces are made from building blocks . Read about the various building block See how to create a Workspace .","title":"Next Steps"},{"location":"Researcher/user-interface/workspaces/statuses/","text":"Workspace Statuses \u00b6 The Workspace\u2019s status mechanism displays the state of the workspace by aggregating various Kubernetes statuses into the following list: Status Description Pending The workspace is waiting in queue and does not consume any resources. Initializing The workspace has been scheduled and it is consuming resources. Active The workspace is ready to be used and allows the researcher to connect. Stopped The workspace is currently unused and does not consume any resources Failed Something went wrong and the workspace is not usable. This allows the researcher to quickly understand whether the workspace is ready to use and if resources are allocated to it. You can hover over the status column to see additional details about the workspace status. Pending workspace \u00b6 The Pending status indicates that the workspace is waiting in queue and does not consume any resources. The workspace will always end up in this state if the workspace was successfully activated but the relevant resources are unavailable. Initializing workspace \u00b6 The Initializing status indicates that the workspace has been scheduled and is consuming resources. However, it is not active yet as its container is still initializing (so it is not possible to connect to the container tools). This step can take anything from a few seconds to a couple of minutes depending on several factors such as the image size to be pulled. The workspace always goes through this state before the workspace turns active. Active workspace \u00b6 The Active status indicates that the workspace is ready to be used and allows the researcher to connect to its tools. At this status, the workspace is consuming resources and affecting the project\u2019s quota. The workspace will turn to active status once the Active button is pressed, the activation process ends up successfully and relevant resources are available and vacant. Stopped workspace \u00b6 The Stopped status indicates that the workspace is currently unused and does not consume any resources. A workspace can be stopped either manually, or automatically if triggered by idleness criteria set by the admin (see Limit duration of interactive Jobs ). Failed workspace \u00b6 The Failed status indicates that something went wrong and the workspace is not usable. You must recreate the workspace and try again. Transitioning states \u00b6 When the user attempts to delete, stop, or activate a workspace, the status column indicates a transition state which will either be successful or will fail. If the action fails, the workspace will stay in its original status. For example, if the user tries to delete an active workspace and fails, the workspace is left in active status. Transitioning states are only visible in the browser of the user.","title":"Statuses"},{"location":"Researcher/user-interface/workspaces/statuses/#workspace-statuses","text":"The Workspace\u2019s status mechanism displays the state of the workspace by aggregating various Kubernetes statuses into the following list: Status Description Pending The workspace is waiting in queue and does not consume any resources. Initializing The workspace has been scheduled and it is consuming resources. Active The workspace is ready to be used and allows the researcher to connect. Stopped The workspace is currently unused and does not consume any resources Failed Something went wrong and the workspace is not usable. This allows the researcher to quickly understand whether the workspace is ready to use and if resources are allocated to it. You can hover over the status column to see additional details about the workspace status.","title":"Workspace Statuses"},{"location":"Researcher/user-interface/workspaces/statuses/#pending-workspace","text":"The Pending status indicates that the workspace is waiting in queue and does not consume any resources. The workspace will always end up in this state if the workspace was successfully activated but the relevant resources are unavailable.","title":"Pending workspace"},{"location":"Researcher/user-interface/workspaces/statuses/#initializing-workspace","text":"The Initializing status indicates that the workspace has been scheduled and is consuming resources. However, it is not active yet as its container is still initializing (so it is not possible to connect to the container tools). This step can take anything from a few seconds to a couple of minutes depending on several factors such as the image size to be pulled. The workspace always goes through this state before the workspace turns active.","title":"Initializing workspace"},{"location":"Researcher/user-interface/workspaces/statuses/#active-workspace","text":"The Active status indicates that the workspace is ready to be used and allows the researcher to connect to its tools. At this status, the workspace is consuming resources and affecting the project\u2019s quota. The workspace will turn to active status once the Active button is pressed, the activation process ends up successfully and relevant resources are available and vacant.","title":"Active workspace"},{"location":"Researcher/user-interface/workspaces/statuses/#stopped-workspace","text":"The Stopped status indicates that the workspace is currently unused and does not consume any resources. A workspace can be stopped either manually, or automatically if triggered by idleness criteria set by the admin (see Limit duration of interactive Jobs ).","title":"Stopped workspace"},{"location":"Researcher/user-interface/workspaces/statuses/#failed-workspace","text":"The Failed status indicates that something went wrong and the workspace is not usable. You must recreate the workspace and try again.","title":"Failed workspace"},{"location":"Researcher/user-interface/workspaces/statuses/#transitioning-states","text":"When the user attempts to delete, stop, or activate a workspace, the status column indicates a transition state which will either be successful or will fail. If the action fails, the workspace will stay in its original status. For example, if the user tries to delete an active workspace and fails, the workspace is left in active status. Transitioning states are only visible in the browser of the user.","title":"Transitioning states"},{"location":"Researcher/user-interface/workspaces/blocks/building-blocks/","text":"Workspace Building Blocks \u00b6 Workspace building blocks are a layer that abstracts complex containers and Kubernetes concepts and provides simple and reusable tools to quickly allocate resources to the workspace. This way researchers need to interact only with the building blocks, and do not need to be aware of technical setups and configurations. Workspaces are built from the following building blocks: Environment Data source Compute resource When a workspace is created, the researcher chooses from preconfigured building blocks or can create a new one on the fly. For example, a workspace can be composed of the following blocks: Environment: Jupyter, Tensor Board and Cude 11.2 Compute resource: 0.5 GPU, 8 cores and 200 Megabytes of CPU memory Data source: A Git branch with the relevant dataset needed A building block has a scope . The scope links a building block to a specific Run:ai project or to all projects: When a building block scope is a specific project. It can be viewed and used only within the project. A building block scope can also be set to all projects (current projects and also any future ones). Typically, building blocks are created by the administrator and then assigned to a project. You can grant permission to the researchers to create their own building blocks. These building blocks will only be available to the projects that are assigned to the researcher that created them. Next Steps \u00b6 Read about the various building blocks Environments , Compute Resources and Data Sources .","title":"Overview"},{"location":"Researcher/user-interface/workspaces/blocks/building-blocks/#workspace-building-blocks","text":"Workspace building blocks are a layer that abstracts complex containers and Kubernetes concepts and provides simple and reusable tools to quickly allocate resources to the workspace. This way researchers need to interact only with the building blocks, and do not need to be aware of technical setups and configurations. Workspaces are built from the following building blocks: Environment Data source Compute resource When a workspace is created, the researcher chooses from preconfigured building blocks or can create a new one on the fly. For example, a workspace can be composed of the following blocks: Environment: Jupyter, Tensor Board and Cude 11.2 Compute resource: 0.5 GPU, 8 cores and 200 Megabytes of CPU memory Data source: A Git branch with the relevant dataset needed A building block has a scope . The scope links a building block to a specific Run:ai project or to all projects: When a building block scope is a specific project. It can be viewed and used only within the project. A building block scope can also be set to all projects (current projects and also any future ones). Typically, building blocks are created by the administrator and then assigned to a project. You can grant permission to the researchers to create their own building blocks. These building blocks will only be available to the projects that are assigned to the researcher that created them.","title":"Workspace Building Blocks"},{"location":"Researcher/user-interface/workspaces/blocks/building-blocks/#next-steps","text":"Read about the various building blocks Environments , Compute Resources and Data Sources .","title":"Next Steps"},{"location":"Researcher/user-interface/workspaces/blocks/compute/","text":"Compute resource introduction \u00b6 A compute resource building block represents a resource request to be used by the workspace (for example 0.5 GPU, 8 cores and 200 Megabytes of CPU memory). When a workspace is activated, the scheduler looks for a node that can fullfil the request. The compute resource is a mandatory building block for Workspace. A request is composed of the following resources: GPU resources CPU memory resources CPU cores resources Note GPU resources can be requested as either a memory request, a full GPU request or a fraction of a GPU . A fraction of a GPU also supports the selection of a dynamic MIG profile if configured See Also \u00b6 Create a Compute resource .","title":"Compute Resources"},{"location":"Researcher/user-interface/workspaces/blocks/compute/#compute-resource-introduction","text":"A compute resource building block represents a resource request to be used by the workspace (for example 0.5 GPU, 8 cores and 200 Megabytes of CPU memory). When a workspace is activated, the scheduler looks for a node that can fullfil the request. The compute resource is a mandatory building block for Workspace. A request is composed of the following resources: GPU resources CPU memory resources CPU cores resources Note GPU resources can be requested as either a memory request, a full GPU request or a fraction of a GPU . A fraction of a GPU also supports the selection of a dynamic MIG profile if configured","title":"Compute resource introduction"},{"location":"Researcher/user-interface/workspaces/blocks/compute/#see-also","text":"Create a Compute resource .","title":"See Also"},{"location":"Researcher/user-interface/workspaces/blocks/datasources/","text":"Data source introduction \u00b6 A data source is a location where data sets relevant to the research are stored. Workspaces can be attached to several data sources for reading and writing. The data can be located locally or in the cloud. Run:ai data sources can use a variety of storage technologies such as Git, S3, NFS, PVC, and more. The data source is an optional building block for the creation of a workspace. See Also \u00b6 Create a Data source .","title":"Data Sources"},{"location":"Researcher/user-interface/workspaces/blocks/datasources/#data-source-introduction","text":"A data source is a location where data sets relevant to the research are stored. Workspaces can be attached to several data sources for reading and writing. The data can be located locally or in the cloud. Run:ai data sources can use a variety of storage technologies such as Git, S3, NFS, PVC, and more. The data source is an optional building block for the creation of a workspace.","title":"Data source introduction"},{"location":"Researcher/user-interface/workspaces/blocks/datasources/#see-also","text":"Create a Data source .","title":"See Also"},{"location":"Researcher/user-interface/workspaces/blocks/environments/","text":"Environment introduction \u00b6 The environment block consists of the URL path for the container image and the image pull policy. It exposes all the necessary tools (open source, 3rd party, or custom tools) along with their connection interfaces (See also external node port and the container ports. An environment is a mandatory building block for the creation of a workspace. You can also include commands, arguments, and environment variables, as well as the user identity with permission to run the commands in the container. Note Additional arguments and environment variables can be added to workspaces even if they were not defined in the environment building block used by the workspace. This ensures that the same environment can still serve many workspaces, even if they differ in their arguments and environment variables. See Also \u00b6 Create an Environment .","title":"Environments"},{"location":"Researcher/user-interface/workspaces/blocks/environments/#environment-introduction","text":"The environment block consists of the URL path for the container image and the image pull policy. It exposes all the necessary tools (open source, 3rd party, or custom tools) along with their connection interfaces (See also external node port and the container ports. An environment is a mandatory building block for the creation of a workspace. You can also include commands, arguments, and environment variables, as well as the user identity with permission to run the commands in the container. Note Additional arguments and environment variables can be added to workspaces even if they were not defined in the environment building block used by the workspace. This ensures that the same environment can still serve many workspaces, even if they differ in their arguments and environment variables.","title":"Environment introduction"},{"location":"Researcher/user-interface/workspaces/blocks/environments/#see-also","text":"Create an Environment .","title":"See Also"},{"location":"Researcher/user-interface/workspaces/create/create-compute/","text":"Create a new Compute Resource \u00b6 To create a compute resource: Select the New Compute Resource button Select the project the resource will reside in Give the resource a meaningful name. A compute resource, is assigned to a single project or all projects (current and future ones). The latter option can only be created by a Run:ai administrator. A compute resource, by design, is shared with all project members. Set the resources request \u00b6 A resources request is composed of 3 types of resources: GPU CPU Memory CPU Compute The user can select one or more resources. For example, one compute resource may consist of a CPU resource request only, whereas a different request can consist of a CPU memory request and a GPU request. Note Selecting resources more than the cluster can supply will result in a permanently failed workspace. Set GPU resources \u00b6 GPU resources can be expressed in various ways: Request GPU devices: this option supports whole GPUs (e.g. 1 GPU, 2 GPUs, 3 GPUs) or a fraction of GPU (e.g. 0.1 GPU, 0.5 GPU, 0.93 GPU, etc.) Request partial memory of a single GPU device: this option allows to explicitly state the amount of memory needed (e.g. 5GB GPU RAM). Request a MIG profile: this option will dynamically provision the requested MIG profile (if the relevant hardware exists). Note Selecting a GPU fraction (e.g. 0.5 GPU) in a heterogeneous cluster may result in inconsistent results: For example, half of a V100 16GB GPU memory is different than A100 with 40GB). In such scenarios. Requesting specific GPU memory is a better strategy. When selecting partial memory of a single GPU device, if NVIDIA MIG is enabled on a node, then the memory can be provided as a MIG profile. For more information see Dynamic MIG . If GPUs are not requested, they will not be allocated even if resources are available. In that case, the project's GPU quota will not be affected. Set CPU resources \u00b6 A CPU resource consists of cores and memory. When GPU resources are requested the user interface will automatically present a proportional amount of CPU cores and memory (as set on the cluster side). Note If no GPU, CPU and memory resources are defined, the request will not be allocated any GPUs. The scheduler will create a container with no minimal CPU and memory. Such a job will run but is likely to be preempted at any time by other jobs. The scheme is relevant for testing and debugging purposes.","title":"Compute Resources"},{"location":"Researcher/user-interface/workspaces/create/create-compute/#create-a-new-compute-resource","text":"To create a compute resource: Select the New Compute Resource button Select the project the resource will reside in Give the resource a meaningful name. A compute resource, is assigned to a single project or all projects (current and future ones). The latter option can only be created by a Run:ai administrator. A compute resource, by design, is shared with all project members.","title":"Create a new Compute Resource"},{"location":"Researcher/user-interface/workspaces/create/create-compute/#set-the-resources-request","text":"A resources request is composed of 3 types of resources: GPU CPU Memory CPU Compute The user can select one or more resources. For example, one compute resource may consist of a CPU resource request only, whereas a different request can consist of a CPU memory request and a GPU request. Note Selecting resources more than the cluster can supply will result in a permanently failed workspace.","title":"Set the resources request"},{"location":"Researcher/user-interface/workspaces/create/create-compute/#set-gpu-resources","text":"GPU resources can be expressed in various ways: Request GPU devices: this option supports whole GPUs (e.g. 1 GPU, 2 GPUs, 3 GPUs) or a fraction of GPU (e.g. 0.1 GPU, 0.5 GPU, 0.93 GPU, etc.) Request partial memory of a single GPU device: this option allows to explicitly state the amount of memory needed (e.g. 5GB GPU RAM). Request a MIG profile: this option will dynamically provision the requested MIG profile (if the relevant hardware exists). Note Selecting a GPU fraction (e.g. 0.5 GPU) in a heterogeneous cluster may result in inconsistent results: For example, half of a V100 16GB GPU memory is different than A100 with 40GB). In such scenarios. Requesting specific GPU memory is a better strategy. When selecting partial memory of a single GPU device, if NVIDIA MIG is enabled on a node, then the memory can be provided as a MIG profile. For more information see Dynamic MIG . If GPUs are not requested, they will not be allocated even if resources are available. In that case, the project's GPU quota will not be affected.","title":"Set GPU resources"},{"location":"Researcher/user-interface/workspaces/create/create-compute/#set-cpu-resources","text":"A CPU resource consists of cores and memory. When GPU resources are requested the user interface will automatically present a proportional amount of CPU cores and memory (as set on the cluster side). Note If no GPU, CPU and memory resources are defined, the request will not be allocated any GPUs. The scheduler will create a container with no minimal CPU and memory. Such a job will run but is likely to be preempted at any time by other jobs. The scheme is relevant for testing and debugging purposes.","title":"Set CPU resources"},{"location":"Researcher/user-interface/workspaces/create/create-ds/","text":"Create a new data source \u00b6 When you select New Compute Resource you will be presented with various data source options described below. Create an NFS data source \u00b6 To create an NFS data source, provide: A data source name A Run:ai project scope An NFS server The path to the data within the server. The path within the container where the data will be mounted. The data can be set as read-write or limited to read-only permission regardless of any other user privileges. Create a PVC data source \u00b6 To create an PVC data source, provide: A data source name A Run:ai project scope Select an existing PVC or create a new one by providing a claim name, a storage class, access mode, and a required storage size. The path within the container where the data will be mounted. Create an S3 data source \u00b6 S3 storage saves data in buckets . S3 is typically attributed to AWS cloud service but can also be used as a separate service unrelated to Amazon. To create an S3 data source, provide A data source name A Run:ai project scope The relevant S3 service URL server The bucket name of the data. The path within the container where the data will be mounted. Note that an S3 data source can be public or private. For the latter option, please select the relevant credentials associated with the project to allow access to the data. Create a Git data source \u00b6 To create a Git data source, provide: A data source name A Run:ai project scope The relevant repository URL. The path within the container where the data will be mounted. The Git data source can be public or private. To allow access to a private Git data source, you must select the relevant credentials associated with the project. Create a host path data source \u00b6 To create a host path data source, provide: A data source name A Run:ai project scope The relevant path on the host. The path within the container where the data will be mounted. Note that the data can be limited to read-only permission regardless of any other user privileges.","title":"Data Sources"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-a-new-data-source","text":"When you select New Compute Resource you will be presented with various data source options described below.","title":"Create a new data source"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-an-nfs-data-source","text":"To create an NFS data source, provide: A data source name A Run:ai project scope An NFS server The path to the data within the server. The path within the container where the data will be mounted. The data can be set as read-write or limited to read-only permission regardless of any other user privileges.","title":"Create an NFS data source"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-a-pvc-data-source","text":"To create an PVC data source, provide: A data source name A Run:ai project scope Select an existing PVC or create a new one by providing a claim name, a storage class, access mode, and a required storage size. The path within the container where the data will be mounted.","title":"Create a PVC data source"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-an-s3-data-source","text":"S3 storage saves data in buckets . S3 is typically attributed to AWS cloud service but can also be used as a separate service unrelated to Amazon. To create an S3 data source, provide A data source name A Run:ai project scope The relevant S3 service URL server The bucket name of the data. The path within the container where the data will be mounted. Note that an S3 data source can be public or private. For the latter option, please select the relevant credentials associated with the project to allow access to the data.","title":"Create an S3 data source"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-a-git-data-source","text":"To create a Git data source, provide: A data source name A Run:ai project scope The relevant repository URL. The path within the container where the data will be mounted. The Git data source can be public or private. To allow access to a private Git data source, you must select the relevant credentials associated with the project.","title":"Create a Git data source"},{"location":"Researcher/user-interface/workspaces/create/create-ds/#create-a-host-path-data-source","text":"To create a host path data source, provide: A data source name A Run:ai project scope The relevant path on the host. The path within the container where the data will be mounted. Note that the data can be limited to read-only permission regardless of any other user privileges.","title":"Create a host path data source"},{"location":"Researcher/user-interface/workspaces/create/create-env/","text":"Creating a new environment \u00b6 To create an environment: Select the New Environment button. Give the environment a meaningful name. Select the project the environment will reside in. An Environment is assigned to a single project or all projects (current and future ones). The latter option can only be created by a Run:ai administrator. A compute resource, by design, is shared with all project members. Set the container image \u00b6 Enter the image URL path as well as a policy for pulling the image from the image repository. Select the relevant tools \u00b6 In a single environment, it is possible to add as many tools as needed (or none at all). Tools can be: Different applications such as Code editor IDEs (e.g VS Code), Experiment tracking (e.g. Weight and Biases), visualization tools (e.g. Tensor Board), and more. Open source tools (e.g Jupyter notebook) or commercial 3rd party tools (e.g. MATLAB) It is also possible to set up a custom tool used by the organization. For each tool, you must set the type of connection interface and port. If not set, default values are provided. The supported connection types are: External URL: This connection type allows you to connect to your tool either by inserting a custom URL or having one generated for you. Either way, the URL should be unique per workspace as many workspaces may use the same environment. If the URL type was set to custom, the URL will be requested from the Researcher upon creating the workspace. External node port: A NodePort exposes your application externally on every host of the cluster, access the tool using http://<HOST_IP>:<NODEPORT> (e.g http://203.0.113.20:30556). Note Selecting a tool is not sufficient to have it up and running. To actually configure a tool you need additional steps: The container image needs to support the tool. The administrator must configure a DNS record and certificate as described here . Configure runtime settings \u00b6 Per environment, the creating user (either Researcher or administrator) is allowed to set the command running in the container. This command will be visible in the workspace creation form, although it will not be editable (e.g. a python command). In addition, the researcher can add arguments that can be edited upon creating a workspace using this environment. Similarly, environment variables can be added to the environments, but these can be edited in the workspace creation form. Note The value of an environment variable can be left empty for the researcher to fill in upon workplace creation. Examples: WANDB UID and GID of the user when launching a Jupyter notebook. In addition, in the environment, it is possible to set the path to the working directory that will be used as the current directory when the container running the created workload starts. It is possible to either use the exact UID and GID defined in the image. However, in many cases, it can be with root privileges so it is possible to override it. If single sign on is configured, the UID and GID to be used will be the ones of the logged researcher that creates the workspace, otherwise, the researcher creating the workspace will be guided to provide it upon workspace creation form.","title":"Environments"},{"location":"Researcher/user-interface/workspaces/create/create-env/#creating-a-new-environment","text":"To create an environment: Select the New Environment button. Give the environment a meaningful name. Select the project the environment will reside in. An Environment is assigned to a single project or all projects (current and future ones). The latter option can only be created by a Run:ai administrator. A compute resource, by design, is shared with all project members.","title":"Creating a new environment"},{"location":"Researcher/user-interface/workspaces/create/create-env/#set-the-container-image","text":"Enter the image URL path as well as a policy for pulling the image from the image repository.","title":"Set the container image"},{"location":"Researcher/user-interface/workspaces/create/create-env/#select-the-relevant-tools","text":"In a single environment, it is possible to add as many tools as needed (or none at all). Tools can be: Different applications such as Code editor IDEs (e.g VS Code), Experiment tracking (e.g. Weight and Biases), visualization tools (e.g. Tensor Board), and more. Open source tools (e.g Jupyter notebook) or commercial 3rd party tools (e.g. MATLAB) It is also possible to set up a custom tool used by the organization. For each tool, you must set the type of connection interface and port. If not set, default values are provided. The supported connection types are: External URL: This connection type allows you to connect to your tool either by inserting a custom URL or having one generated for you. Either way, the URL should be unique per workspace as many workspaces may use the same environment. If the URL type was set to custom, the URL will be requested from the Researcher upon creating the workspace. External node port: A NodePort exposes your application externally on every host of the cluster, access the tool using http://<HOST_IP>:<NODEPORT> (e.g http://203.0.113.20:30556). Note Selecting a tool is not sufficient to have it up and running. To actually configure a tool you need additional steps: The container image needs to support the tool. The administrator must configure a DNS record and certificate as described here .","title":"Select the relevant tools"},{"location":"Researcher/user-interface/workspaces/create/create-env/#configure-runtime-settings","text":"Per environment, the creating user (either Researcher or administrator) is allowed to set the command running in the container. This command will be visible in the workspace creation form, although it will not be editable (e.g. a python command). In addition, the researcher can add arguments that can be edited upon creating a workspace using this environment. Similarly, environment variables can be added to the environments, but these can be edited in the workspace creation form. Note The value of an environment variable can be left empty for the researcher to fill in upon workplace creation. Examples: WANDB UID and GID of the user when launching a Jupyter notebook. In addition, in the environment, it is possible to set the path to the working directory that will be used as the current directory when the container running the created workload starts. It is possible to either use the exact UID and GID defined in the image. However, in many cases, it can be with root privileges so it is possible to override it. If single sign on is configured, the UID and GID to be used will be the ones of the logged researcher that creates the workspace, otherwise, the researcher creating the workspace will be guided to provide it upon workspace creation form.","title":"Configure runtime settings"},{"location":"Researcher/user-interface/workspaces/create/workspace/","text":"Workspaces actions and use cases \u00b6 Create a new workspace \u00b6 A Workspace is assigned to a project and is affected by the project\u2019s quota just like any other workload. A workspace is shared with all project members for collaboration. To create a workspace, you must provide: At least one project A researcher assigned to at least one project To create a workspace, the researcher must select building blocks in one of two ways: Create a workspace from scratch : this allows you to either select an existing building block or create them on the fly (pending the right permissions). Create a workspace from a template : a template contains a set of predefined building blocks as well as additional configurations which allow the user to immediately create a templated-based workspace. To create a workspace: Press New Workspace Select a project for the new workspace. The project visualization contains information about the project such as how much of the quota is being allocated and indicates the likelihood of the workspace being scheduled or left in the queue Create a new workspace from scratch \u00b6 See picture: Note The building block can also be created (and then selected) directly from within the workspace creation form. Select an Environment for a new workspace \u00b6 An environment is a mandatory element of a workspace. All environments created for the project will be shown to researchers in the form of a gallery view (see also Creating a new environment ). Each tile shows the tools as well as the image. When selecting an environment, the command, arguments and environment variables defined in the environment are visible for review. The researcher can edit arguments and environment variables that are specific to the current workspace and that are not part of the common shared environment. In some cases, it would even be expected that the researcher will provide additional information (for example, values for environment variables) to successfully create the workspace (see also Create new environment ). You can also decide whether the workspace is preemptable or not (see also create a preemptable worksapce ). By default, interactive sessions are limited to the project\u2019s GPU, meaning that they can only be scheduled (and activated) when there is an available and sufficient GPU quota. With the following parameter, the researcher can determine whether the workspace is allowed to go over quota with the understanding that it can be preempted if other projects would demand back their quota. Select a compute resource for a new workspace \u00b6 Selecting compute resources for the workspace is a mandatory step. If compute resources are created for the project (see also creating a new compute resource ), those will be offered to researchers in the form of a gallery view. Each tile shows the amount of GPU, CPU and Memory in the request. Select a data source for a new workspace \u00b6 Selecting a data source for the workspace is a non-mandatory step. If data sources are created for the project (see also creating a new compute resource ), those will be offered to researchers in the form of a gallery view. Each tile shows the unique name of the building block and the type of data source. Create a new workspace from a template \u00b6 Templates ease the way of creating a new workspace in a few clicks. In contrast to creating a workspace from scratch (selecting manually which building blocks to use in your workspace), a template aggregates all building blocks under a single entity for researchers to use for the creation of workspaces. A Template consists of the building blocks and other parameters that are exposed in a workspace creation form. Templates can be fully defined to a point researcher can select and create the workspace without providing any additional information or partially defined, hence, leaving some degree of freedom in the creation of the workspace via the template. This can help in cases where only part of the configuration is selected in the template and the rest is expected to be provided by the user creating a workspace from the template. Few examples: A template can have the value of an environment variable empty for the researcher to edit later during the workspace creation. A template can consist of an environment with a tool that requests a custom URL. This URL field stays empty until the researcher fills it upon creating the workspace For collaboration purposes, templates are assigned to a specific project and are shared with all project members by design. Create a preemptible workspace \u00b6 For a better experience, workspaces, as they are built for interactive research, are designed to not be preempted (because the researchers actively interact with GPU resources). Thus, non-preemptable workspaces can be only scheduled if the project has a sufficient vacant quota. However, if that\u2019s not the case (the project does not have a sufficient vacant quota) and the researcher still needs to create and activate a workspace (if cluster resources are available) he/she can allow the workspace to go over quota, thus be scheduled, but with the cost of preemption without prior notice.","title":"Workspaces"},{"location":"Researcher/user-interface/workspaces/create/workspace/#workspaces-actions-and-use-cases","text":"","title":"Workspaces actions and use cases"},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-new-workspace","text":"A Workspace is assigned to a project and is affected by the project\u2019s quota just like any other workload. A workspace is shared with all project members for collaboration. To create a workspace, you must provide: At least one project A researcher assigned to at least one project To create a workspace, the researcher must select building blocks in one of two ways: Create a workspace from scratch : this allows you to either select an existing building block or create them on the fly (pending the right permissions). Create a workspace from a template : a template contains a set of predefined building blocks as well as additional configurations which allow the user to immediately create a templated-based workspace. To create a workspace: Press New Workspace Select a project for the new workspace. The project visualization contains information about the project such as how much of the quota is being allocated and indicates the likelihood of the workspace being scheduled or left in the queue","title":"Create a new workspace"},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-new-workspace-from-scratch","text":"See picture: Note The building block can also be created (and then selected) directly from within the workspace creation form.","title":"Create a new workspace from scratch"},{"location":"Researcher/user-interface/workspaces/create/workspace/#select-an-environment-for-a-new-workspace","text":"An environment is a mandatory element of a workspace. All environments created for the project will be shown to researchers in the form of a gallery view (see also Creating a new environment ). Each tile shows the tools as well as the image. When selecting an environment, the command, arguments and environment variables defined in the environment are visible for review. The researcher can edit arguments and environment variables that are specific to the current workspace and that are not part of the common shared environment. In some cases, it would even be expected that the researcher will provide additional information (for example, values for environment variables) to successfully create the workspace (see also Create new environment ). You can also decide whether the workspace is preemptable or not (see also create a preemptable worksapce ). By default, interactive sessions are limited to the project\u2019s GPU, meaning that they can only be scheduled (and activated) when there is an available and sufficient GPU quota. With the following parameter, the researcher can determine whether the workspace is allowed to go over quota with the understanding that it can be preempted if other projects would demand back their quota.","title":"Select an Environment for a new workspace"},{"location":"Researcher/user-interface/workspaces/create/workspace/#select-a-compute-resource-for-a-new-workspace","text":"Selecting compute resources for the workspace is a mandatory step. If compute resources are created for the project (see also creating a new compute resource ), those will be offered to researchers in the form of a gallery view. Each tile shows the amount of GPU, CPU and Memory in the request.","title":"Select a compute resource for a new workspace"},{"location":"Researcher/user-interface/workspaces/create/workspace/#select-a-data-source-for-a-new-workspace","text":"Selecting a data source for the workspace is a non-mandatory step. If data sources are created for the project (see also creating a new compute resource ), those will be offered to researchers in the form of a gallery view. Each tile shows the unique name of the building block and the type of data source.","title":"Select a data source for a new workspace"},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-new-workspace-from-a-template","text":"Templates ease the way of creating a new workspace in a few clicks. In contrast to creating a workspace from scratch (selecting manually which building blocks to use in your workspace), a template aggregates all building blocks under a single entity for researchers to use for the creation of workspaces. A Template consists of the building blocks and other parameters that are exposed in a workspace creation form. Templates can be fully defined to a point researcher can select and create the workspace without providing any additional information or partially defined, hence, leaving some degree of freedom in the creation of the workspace via the template. This can help in cases where only part of the configuration is selected in the template and the rest is expected to be provided by the user creating a workspace from the template. Few examples: A template can have the value of an environment variable empty for the researcher to edit later during the workspace creation. A template can consist of an environment with a tool that requests a custom URL. This URL field stays empty until the researcher fills it upon creating the workspace For collaboration purposes, templates are assigned to a specific project and are shared with all project members by design.","title":"Create a new workspace from a template"},{"location":"Researcher/user-interface/workspaces/create/workspace/#create-a-preemptible-workspace","text":"For a better experience, workspaces, as they are built for interactive research, are designed to not be preempted (because the researchers actively interact with GPU resources). Thus, non-preemptable workspaces can be only scheduled if the project has a sufficient vacant quota. However, if that\u2019s not the case (the project does not have a sufficient vacant quota) and the researcher still needs to create and activate a workspace (if cluster resources are available) he/she can allow the workspace to go over quota, thus be scheduled, but with the cost of preemption without prior notice.","title":"Create a preemptible workspace"},{"location":"admin/overview-administrator/","text":"Overview: Administrator Documentation \u00b6 The role of Administrators is to set up Run:ai and perform day-to-day monitoring and maintenance. As part of the Administrator documentation you will find: Run:ai Setup How to set up and modify a GPU cluster with Run:ai. Researcher Setup How to set up Researchers to work with Run:ai. Setting and maintaining the cluster via the Run:ai User Interface . Integrations of Run:ai with a variety of other systems.","title":"Overview"},{"location":"admin/overview-administrator/#overview-administrator-documentation","text":"The role of Administrators is to set up Run:ai and perform day-to-day monitoring and maintenance. As part of the Administrator documentation you will find: Run:ai Setup How to set up and modify a GPU cluster with Run:ai. Researcher Setup How to set up Researchers to work with Run:ai. Setting and maintaining the cluster via the Run:ai User Interface . Integrations of Run:ai with a variety of other systems.","title":"Overview: Administrator Documentation"},{"location":"admin/admin-ui-setup/admin-ui-users/","text":"Adding, Updating and Deleting Users \u00b6 Introduction \u00b6 The Run:ai User Interface allows the creation of Run:ai Users. Run:ai Users can receive varying levels of access to the Administration UI and submit Jobs on the Cluster. Tip It is possible to connect the Run:ai user interface to the organization's directory and use single sign-on. This allows you to set Run:ai roles for users and groups from the organizational directory. For further information see single sign-on configuration . Working with Users \u00b6 You can create users, as well as update and delete users. Create a User \u00b6 Note To be able to review, add, update and delete users, you must have an Administrator access. If you do not have such access, please contact an Administrator. Login to the Users area of the Run:ai User interface at company-name.run.ai . On the top right, select \"Add New Users\". Choose a User name and email. Select Roles. More than one role can be selected. The available roles are: Administrator : Can manage Users and install Clusters. Editor : Can manage Projects and Departments. Viewer : View-only access to the Run:ai User Interface. Researcher : Can submit ML workloads. Setting a user as a Researcher also requires assigning the user to projects . Research Manager : Can act as Researcher in all projects, including new ones to be created in the future. ML Engineer : Can view and manage deployments and cluster resources. Available only when Inference module is installed . (Optional) Select Cluster(s). This determines what Clusters are accessible to this User. Press \"Save\". You will get the new user credentials and have the option to send the credentials by email.","title":"Users"},{"location":"admin/admin-ui-setup/admin-ui-users/#adding-updating-and-deleting-users","text":"","title":"Adding, Updating and Deleting Users"},{"location":"admin/admin-ui-setup/admin-ui-users/#introduction","text":"The Run:ai User Interface allows the creation of Run:ai Users. Run:ai Users can receive varying levels of access to the Administration UI and submit Jobs on the Cluster. Tip It is possible to connect the Run:ai user interface to the organization's directory and use single sign-on. This allows you to set Run:ai roles for users and groups from the organizational directory. For further information see single sign-on configuration .","title":"Introduction"},{"location":"admin/admin-ui-setup/admin-ui-users/#working-with-users","text":"You can create users, as well as update and delete users.","title":"Working with Users"},{"location":"admin/admin-ui-setup/admin-ui-users/#create-a-user","text":"Note To be able to review, add, update and delete users, you must have an Administrator access. If you do not have such access, please contact an Administrator. Login to the Users area of the Run:ai User interface at company-name.run.ai . On the top right, select \"Add New Users\". Choose a User name and email. Select Roles. More than one role can be selected. The available roles are: Administrator : Can manage Users and install Clusters. Editor : Can manage Projects and Departments. Viewer : View-only access to the Run:ai User Interface. Researcher : Can submit ML workloads. Setting a user as a Researcher also requires assigning the user to projects . Research Manager : Can act as Researcher in all projects, including new ones to be created in the future. ML Engineer : Can view and manage deployments and cluster resources. Available only when Inference module is installed . (Optional) Select Cluster(s). This determines what Clusters are accessible to this User. Press \"Save\". You will get the new user credentials and have the option to send the credentials by email.","title":"Create a User"},{"location":"admin/admin-ui-setup/dashboard-analysis/","text":"The Run:ai Administration User Interface provides a set of dashboards that help you monitor Clusters, Cluster Nodes, Projects, and Jobs. This document provides the key metrics to monitor, how to assess them as well as suggested actions. There are 3 dashboards: Overview dashboard - Provides information about what is happening right now in the cluster Analytics dashboard - Provides long term analysis of cluster behavior Multi-Cluster Overview dashboard - Provides a more holistic, multi-cluster view of what is happening right now. The dashboard is intended for organizations that have more than one connected cluster. Overview Dashboard \u00b6 The Overview dashboard provides information about what is happening right now in the cluster. Administrators can view high-level information on the state of the cluster, including: The number of available and allocated resources and their cluster-wide utilization The number of running and pending Jobs , their utilization, information on Jobs with errors or Jobs with idle GPUs Active Projects , their assigned and allocated GPUs and number of running and pending Jobs Cluster administrators can use the Overview dashboard to find issues and fix them. Below are a few examples: Jobs with idle GPUs \u00b6 Locate Jobs with idle GPUs, defined as GPUs with 0% GPU utilization for more than 5 minutes. How to : view the following panel: Analysis and Suggested actions : Review Analysis & Actions Interactive Jobs are too frequently idle * Consider setting time limits for interactive Jobs through the Projects tab. * Consider also reducing GPU quotas for specific Projects to encourage users to run more training Jobs as opposed to interactive Jobs (note that interactive Jobs can not use more than the GPU quota assigned to their Project). Training Jobs are too frequently idle Identify and notify the right users and work with them to improve the utilization of their training scripts Jobs with an Error \u00b6 Search for Jobs with an error status. These Jobs may be holding GPUs without actually using them. How to : view the following panel: Analysis and Suggested actions : Search for Jobs with an Error status on the Jobs view and discuss with the Job owner. Consider deleting these Jobs to free up the resources for other users. Jobs with a Long Duration \u00b6 View list of 5 longest Jobs. How to : view the following panel: Analysis and Suggested actions : Review Analysis & Actions Training Jobs run for too long Ask users to view their Jobs and analyze whether useful work is being done. If needed, stop their Jobs. Interactive Jobs run for too long Consider setting time limits for interactive Jobs via the Project editor. Job Queue \u00b6 Identify queueing bottlenecks. How to : view the following panel: Analysis and Suggested actions : Review Analysis & Actions Cluster is fully loaded Go over the table of active Projects and check that fairness between Projects was enforced, by reviewing the number of allocated GPUs for each Project, ensuring each Project was allocated with its fair-share portion of the cluster. Cluster is not fully loaded Go to the Jobs view to review the resources requested for that Job (CPU, CPU memory, GPU, GPU memory). Go to the Nodes view to verify that there is no Node with enough free resources that can host that Job. Also, check the command that the user used to submit the job. The Researcher may have requested a specific Node for that Job. Analytics Dashboard \u00b6 The Analytics dashboard provides means for viewing historical data on cluster information such as: Utilization across the cluster GPU usage by different Projects , including allocation and utilization, broken down into interactive and training Jobs Breakdown of running Jobs into interactive, training, and GPU versus CPU-only Jobs, including information on queueing (number of pending Jobs and requested GPUs), Status of Nodes in terms of availability and allocated and utilized resources. The information presented in Analytics can be used in different ways for identifying problems and fixing them. Below are a few examples. Node Downtime \u00b6 View the overall available resources per Node and identify cases where a Node is down and there was a reduction in the number of available resources. How to : view the following panel. Analysis and Suggested actions : Filter according to time range to understand for how long the Node is down. GPU Allocation \u00b6 Track GPU allocation across time. How to : view the following panels. The panel on the right-hand side shows the cluster-wide GPU allocation and utilization versus time, whereas the panels on the left-hand side show the cluster-wide GPU allocation and utilization averaged across the filtered time range. Analysis and Suggested actions : If the allocation is too low for a long period, work with users to run more workloads and to better utilize the Cluster. Track GPU utilization \u00b6 Track whether Researchers efficiently use the GPU resources they have allocated for themselves. How to : view the following panel: Analysis and Suggested actions : If utilization is too low for a long period, you will want to identify the source of the problem: Go to \u201cAverage GPU Allocation & Utilization\u201d Look for Projects with large GPU allocations for interactive Jobs or Projects that poorly utilize their training Jobs. Users tend to poorly utilize their GPUs in interactive sessions because of the dev & debug nature of their work which typically is an iterative process with long idle GPU time. On many occasions users also don\u2019t shut down their interactive Jobs, holding their GPUs idle and preventing others from using them. Review Analysis & Actions Low GPU utilization is due to interactive Jobs being used too frequently Consider setting time limits for interactive Jobs through the Projects tab or reducing GPU quotas to encourage users to run more training Jobs as opposed to interactive Jobs (note that interactive Jobs can not use more than the GPU quota assigned to their Project). Low GPU utilization is due to users poorly utilizing their GPUs in training sessions Identify Projects with bad GPU utilization in training Jobs, notify the users and work with them to improve their code and the way they utilize their GPUs. Training vs. Interactive -- Researcher maturity \u00b6 Track the number of running Jobs and the breakdown into interactive, training, and CPU-only Jobs. How to : view the following panel: Analysis and Suggested actions : We would want to encourage users to run more training Jobs than interactive Jobs, as it is the key to achieving high GPU utilization across the Cluster: Training Jobs run to completion and free up their resources automatically when training ends Training Jobs can be preempted, queued, and resumed automatically by the Run:ai system according to predefined policies which increases fairness and Cluster utilization. Pending Queue Size \u00b6 Track how long is the queue for pending Jobs How to : view the following panels: Analysis and Suggested actions : Consider buying more GPUs if, Too many Jobs are waiting in queue for too long With a large number of requested GPUs While the Cluster is fully loaded and well utilized. CPU & Memory Utilization \u00b6 Track CPU and memory Node utilization and identify times where the load on specific Nodes is high. How to : view the following panel: Analysis and Suggested actions : If the load on specific Nodes is too high, it may cause problems with the proper operation of the Cluster and the way jobs are running. Consider adding more CPUs, or adding additional CPU-only nodes for Jobs that do only CPU processing. Multi-Cluster Overview Dashboard \u00b6 Provides a holistic, aggregated view across Clusters, including information about Cluster and Node utilization, available resources, and allocated resources. With this dashboard, you can identify Clusters that are down or underutilized and go to the Overview of that Cluster to explore further.","title":"Dashboard Analysis"},{"location":"admin/admin-ui-setup/dashboard-analysis/#overview-dashboard","text":"The Overview dashboard provides information about what is happening right now in the cluster. Administrators can view high-level information on the state of the cluster, including: The number of available and allocated resources and their cluster-wide utilization The number of running and pending Jobs , their utilization, information on Jobs with errors or Jobs with idle GPUs Active Projects , their assigned and allocated GPUs and number of running and pending Jobs Cluster administrators can use the Overview dashboard to find issues and fix them. Below are a few examples:","title":"Overview Dashboard"},{"location":"admin/admin-ui-setup/dashboard-analysis/#jobs-with-idle-gpus","text":"Locate Jobs with idle GPUs, defined as GPUs with 0% GPU utilization for more than 5 minutes. How to : view the following panel: Analysis and Suggested actions : Review Analysis & Actions Interactive Jobs are too frequently idle * Consider setting time limits for interactive Jobs through the Projects tab. * Consider also reducing GPU quotas for specific Projects to encourage users to run more training Jobs as opposed to interactive Jobs (note that interactive Jobs can not use more than the GPU quota assigned to their Project). Training Jobs are too frequently idle Identify and notify the right users and work with them to improve the utilization of their training scripts","title":"Jobs with idle GPUs"},{"location":"admin/admin-ui-setup/dashboard-analysis/#jobs-with-an-error","text":"Search for Jobs with an error status. These Jobs may be holding GPUs without actually using them. How to : view the following panel: Analysis and Suggested actions : Search for Jobs with an Error status on the Jobs view and discuss with the Job owner. Consider deleting these Jobs to free up the resources for other users.","title":"Jobs with an Error"},{"location":"admin/admin-ui-setup/dashboard-analysis/#jobs-with-a-long-duration","text":"View list of 5 longest Jobs. How to : view the following panel: Analysis and Suggested actions : Review Analysis & Actions Training Jobs run for too long Ask users to view their Jobs and analyze whether useful work is being done. If needed, stop their Jobs. Interactive Jobs run for too long Consider setting time limits for interactive Jobs via the Project editor.","title":"Jobs with a Long Duration"},{"location":"admin/admin-ui-setup/dashboard-analysis/#job-queue","text":"Identify queueing bottlenecks. How to : view the following panel: Analysis and Suggested actions : Review Analysis & Actions Cluster is fully loaded Go over the table of active Projects and check that fairness between Projects was enforced, by reviewing the number of allocated GPUs for each Project, ensuring each Project was allocated with its fair-share portion of the cluster. Cluster is not fully loaded Go to the Jobs view to review the resources requested for that Job (CPU, CPU memory, GPU, GPU memory). Go to the Nodes view to verify that there is no Node with enough free resources that can host that Job. Also, check the command that the user used to submit the job. The Researcher may have requested a specific Node for that Job.","title":"Job Queue"},{"location":"admin/admin-ui-setup/dashboard-analysis/#analytics-dashboard","text":"The Analytics dashboard provides means for viewing historical data on cluster information such as: Utilization across the cluster GPU usage by different Projects , including allocation and utilization, broken down into interactive and training Jobs Breakdown of running Jobs into interactive, training, and GPU versus CPU-only Jobs, including information on queueing (number of pending Jobs and requested GPUs), Status of Nodes in terms of availability and allocated and utilized resources. The information presented in Analytics can be used in different ways for identifying problems and fixing them. Below are a few examples.","title":"Analytics Dashboard"},{"location":"admin/admin-ui-setup/dashboard-analysis/#node-downtime","text":"View the overall available resources per Node and identify cases where a Node is down and there was a reduction in the number of available resources. How to : view the following panel. Analysis and Suggested actions : Filter according to time range to understand for how long the Node is down.","title":"Node Downtime"},{"location":"admin/admin-ui-setup/dashboard-analysis/#gpu-allocation","text":"Track GPU allocation across time. How to : view the following panels. The panel on the right-hand side shows the cluster-wide GPU allocation and utilization versus time, whereas the panels on the left-hand side show the cluster-wide GPU allocation and utilization averaged across the filtered time range. Analysis and Suggested actions : If the allocation is too low for a long period, work with users to run more workloads and to better utilize the Cluster.","title":"GPU Allocation"},{"location":"admin/admin-ui-setup/dashboard-analysis/#track-gpu-utilization","text":"Track whether Researchers efficiently use the GPU resources they have allocated for themselves. How to : view the following panel: Analysis and Suggested actions : If utilization is too low for a long period, you will want to identify the source of the problem: Go to \u201cAverage GPU Allocation & Utilization\u201d Look for Projects with large GPU allocations for interactive Jobs or Projects that poorly utilize their training Jobs. Users tend to poorly utilize their GPUs in interactive sessions because of the dev & debug nature of their work which typically is an iterative process with long idle GPU time. On many occasions users also don\u2019t shut down their interactive Jobs, holding their GPUs idle and preventing others from using them. Review Analysis & Actions Low GPU utilization is due to interactive Jobs being used too frequently Consider setting time limits for interactive Jobs through the Projects tab or reducing GPU quotas to encourage users to run more training Jobs as opposed to interactive Jobs (note that interactive Jobs can not use more than the GPU quota assigned to their Project). Low GPU utilization is due to users poorly utilizing their GPUs in training sessions Identify Projects with bad GPU utilization in training Jobs, notify the users and work with them to improve their code and the way they utilize their GPUs.","title":"Track GPU utilization"},{"location":"admin/admin-ui-setup/dashboard-analysis/#training-vs-interactive-researcher-maturity","text":"Track the number of running Jobs and the breakdown into interactive, training, and CPU-only Jobs. How to : view the following panel: Analysis and Suggested actions : We would want to encourage users to run more training Jobs than interactive Jobs, as it is the key to achieving high GPU utilization across the Cluster: Training Jobs run to completion and free up their resources automatically when training ends Training Jobs can be preempted, queued, and resumed automatically by the Run:ai system according to predefined policies which increases fairness and Cluster utilization.","title":"Training vs. Interactive -- Researcher maturity"},{"location":"admin/admin-ui-setup/dashboard-analysis/#pending-queue-size","text":"Track how long is the queue for pending Jobs How to : view the following panels: Analysis and Suggested actions : Consider buying more GPUs if, Too many Jobs are waiting in queue for too long With a large number of requested GPUs While the Cluster is fully loaded and well utilized.","title":"Pending Queue Size"},{"location":"admin/admin-ui-setup/dashboard-analysis/#cpu-memory-utilization","text":"Track CPU and memory Node utilization and identify times where the load on specific Nodes is high. How to : view the following panel: Analysis and Suggested actions : If the load on specific Nodes is too high, it may cause problems with the proper operation of the Cluster and the way jobs are running. Consider adding more CPUs, or adding additional CPU-only nodes for Jobs that do only CPU processing.","title":"CPU &amp; Memory Utilization"},{"location":"admin/admin-ui-setup/dashboard-analysis/#multi-cluster-overview-dashboard","text":"Provides a holistic, aggregated view across Clusters, including information about Cluster and Node utilization, available resources, and allocated resources. With this dashboard, you can identify Clusters that are down or underutilized and go to the Overview of that Cluster to explore further.","title":"Multi-Cluster Overview Dashboard"},{"location":"admin/admin-ui-setup/department-setup/","text":"Introduction \u00b6 Researchers submit Jobs. To streamline resource allocation and prioritize work, Run:ai introduces the concept of Projects . Projects are the tool to implement resource allocation policies as well as create segregation between different initiatives. A project in most cases represents a team, an individual, or an initiative that shares resources or has a specific resources budget (quota). A Researcher submitting a Job needs to associate a Project name with the request. The Run:ai scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in the queue for future allocation. In some organizations, Projects may not be enough, this is because: There are simply too many individual entities that are attached to a quota. There are organizational quotas at a higher level. Departments \u00b6 Departments create a secondary hierarchy of resource allocation: A Project is associated with a single Department. Multiple Projects can be associated with the same Department. A Department, like a Project is associated with a Quota. It is recommended that a Department's quota supersedes the sum of all its associated Projects' quota. Node Pools and Quota settings \u00b6 The Run:ai system associates each node with a single node pool. By default, the Run:ai system associates all nodes to 'Default' node-pool. If 'Enable Node Pools' flag is disabled, all GPU and CPU resources are directly associated to the Department's Quotas. Once an Administrator enables 'Enable Node Pools' flag, all GPU and CPU resources will be included in 'Default' node-pool and summed up to the Department's overall Quotas. An administrator can create new node-pool and associate nodes into this pool. Any new pool is automatically associated with all Departments and Projects within a cluster, with a GPU and CPU resource Quota of zero. The Administrator can then change the Quota of any node-pool resource per Department and Project. The Quota of node-pool X within Department Y should be at least the sum of the same node-pool X Quota across all associated Projects. This means an administrator should carefully plan the resource Quota allocation from the Department to its descendent Projects. The overall Quota of the Department is the sum of all its associated node-pools. Over-quota behavior \u00b6 Consider an example from an academic use case: the Computer Science Department and the GeoPhysics Department have each purchased 10 nodes with 8 GPUs for each node, totaling a cluster of 160 GPUs for both departments. The two Departments do not mind sharing GPUs as long as they always get their 80 GPUs when they truly need them. As such, there could be many Projects in the GeoPhysics Department, totaling an allocation of 100 GPUs, but anything above 80 GPUs will be considered by the Run:ai scheduler as over-quota. For more details on over-quota scheduling see the Run:ai Scheduler . In case node pools are enabled, the same rule applies per node pool, i.e. if a job tries to use resources that supersede a node pool Department's quota - it will be considered as Over-Quota. Important best practice: As a rule, the sum of the Departments' Qouta allocations should be equal to the number of GPUs in the cluster. Creating and Managing Departments \u00b6 Enable Departments \u00b6 Departments are disabled by default. To start working with Departments: Go to Settings | General Enable Departments Once Departments are enabled, the left-side menu will have a new item named \"Departments\". Under Departments there will be a single Department named default . All Projects created before the Department feature was enabled will belong to the default Department. Adding Departments \u00b6 You can add new Departments by pressing the Add New Department at the top right of the Department view. Add Department name and quota allocation. Assigning Projects to Departments \u00b6 Under Projects edit an existing Project. You will see a new Department drop-down with which you can associate a Project with a Department.","title":"Departments"},{"location":"admin/admin-ui-setup/department-setup/#introduction","text":"Researchers submit Jobs. To streamline resource allocation and prioritize work, Run:ai introduces the concept of Projects . Projects are the tool to implement resource allocation policies as well as create segregation between different initiatives. A project in most cases represents a team, an individual, or an initiative that shares resources or has a specific resources budget (quota). A Researcher submitting a Job needs to associate a Project name with the request. The Run:ai scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in the queue for future allocation. In some organizations, Projects may not be enough, this is because: There are simply too many individual entities that are attached to a quota. There are organizational quotas at a higher level.","title":"Introduction"},{"location":"admin/admin-ui-setup/department-setup/#departments","text":"Departments create a secondary hierarchy of resource allocation: A Project is associated with a single Department. Multiple Projects can be associated with the same Department. A Department, like a Project is associated with a Quota. It is recommended that a Department's quota supersedes the sum of all its associated Projects' quota.","title":"Departments"},{"location":"admin/admin-ui-setup/department-setup/#node-pools-and-quota-settings","text":"The Run:ai system associates each node with a single node pool. By default, the Run:ai system associates all nodes to 'Default' node-pool. If 'Enable Node Pools' flag is disabled, all GPU and CPU resources are directly associated to the Department's Quotas. Once an Administrator enables 'Enable Node Pools' flag, all GPU and CPU resources will be included in 'Default' node-pool and summed up to the Department's overall Quotas. An administrator can create new node-pool and associate nodes into this pool. Any new pool is automatically associated with all Departments and Projects within a cluster, with a GPU and CPU resource Quota of zero. The Administrator can then change the Quota of any node-pool resource per Department and Project. The Quota of node-pool X within Department Y should be at least the sum of the same node-pool X Quota across all associated Projects. This means an administrator should carefully plan the resource Quota allocation from the Department to its descendent Projects. The overall Quota of the Department is the sum of all its associated node-pools.","title":"Node Pools and Quota settings"},{"location":"admin/admin-ui-setup/department-setup/#over-quota-behavior","text":"Consider an example from an academic use case: the Computer Science Department and the GeoPhysics Department have each purchased 10 nodes with 8 GPUs for each node, totaling a cluster of 160 GPUs for both departments. The two Departments do not mind sharing GPUs as long as they always get their 80 GPUs when they truly need them. As such, there could be many Projects in the GeoPhysics Department, totaling an allocation of 100 GPUs, but anything above 80 GPUs will be considered by the Run:ai scheduler as over-quota. For more details on over-quota scheduling see the Run:ai Scheduler . In case node pools are enabled, the same rule applies per node pool, i.e. if a job tries to use resources that supersede a node pool Department's quota - it will be considered as Over-Quota. Important best practice: As a rule, the sum of the Departments' Qouta allocations should be equal to the number of GPUs in the cluster.","title":"Over-quota behavior"},{"location":"admin/admin-ui-setup/department-setup/#creating-and-managing-departments","text":"","title":"Creating and Managing Departments"},{"location":"admin/admin-ui-setup/department-setup/#enable-departments","text":"Departments are disabled by default. To start working with Departments: Go to Settings | General Enable Departments Once Departments are enabled, the left-side menu will have a new item named \"Departments\". Under Departments there will be a single Department named default . All Projects created before the Department feature was enabled will belong to the default Department.","title":"Enable Departments"},{"location":"admin/admin-ui-setup/department-setup/#adding-departments","text":"You can add new Departments by pressing the Add New Department at the top right of the Department view. Add Department name and quota allocation.","title":"Adding Departments"},{"location":"admin/admin-ui-setup/department-setup/#assigning-projects-to-departments","text":"Under Projects edit an existing Project. You will see a new Department drop-down with which you can associate a Project with a Department.","title":"Assigning Projects to Departments"},{"location":"admin/admin-ui-setup/deployments/","text":"Viewing and Submitting Deployments \u00b6 The Run:ai User interface Deployment area allows the viewing and submitting of Deployments for serving inference workloads. Submitting inference workloads can only be done if your user has ML Engineer access. Deployment list \u00b6 The main view shows a list of Deployments: Submit a Deployment \u00b6 On the top right, you can choose to Submit a new Deployment. Note If knative is not installed in your cluster the button will be grayed out. A Deployment form will open: Note If the Deploy button is disabled or does not exist, then your cluster is not installed or configured to connect to the cluster see here for more information. Deployment Properties \u00b6 When selecting a single Deployment, a right-pane appears: This multi-tab view provides information about Deployment details, related Pods, Deployment status history, and various utilization graphs.","title":"Deployments"},{"location":"admin/admin-ui-setup/deployments/#viewing-and-submitting-deployments","text":"The Run:ai User interface Deployment area allows the viewing and submitting of Deployments for serving inference workloads. Submitting inference workloads can only be done if your user has ML Engineer access.","title":"Viewing and Submitting Deployments"},{"location":"admin/admin-ui-setup/deployments/#deployment-list","text":"The main view shows a list of Deployments:","title":"Deployment list"},{"location":"admin/admin-ui-setup/deployments/#submit-a-deployment","text":"On the top right, you can choose to Submit a new Deployment. Note If knative is not installed in your cluster the button will be grayed out. A Deployment form will open: Note If the Deploy button is disabled or does not exist, then your cluster is not installed or configured to connect to the cluster see here for more information.","title":"Submit a Deployment"},{"location":"admin/admin-ui-setup/deployments/#deployment-properties","text":"When selecting a single Deployment, a right-pane appears: This multi-tab view provides information about Deployment details, related Pods, Deployment status history, and various utilization graphs.","title":"Deployment Properties"},{"location":"admin/admin-ui-setup/jobs/","text":"Viewing and Submitting Jobs \u00b6 The Run:ai User interface Job area allows the viewing of Jobs and Job details. It also allows the Researcher to submit Jobs, suspend and resume Jobs and delete Jobs. Job list \u00b6 The main view shows a list of Jobs. The list can be filtered and sorted: Submit Job \u00b6 On the top right, you can choose to Submit a new Job. A Job form will open: Note If the Submit Job button is disabled or does not exist, then your cluster is not installed or configured to connect to the cluster see here for more information. Job Properties \u00b6 When selecting a single Job, a right-pane appears: This multi-tab view provides information about Job details, related Pods, Job status history, and various utilization graphs. You can also view internal Job logs as shown here: Other operations \u00b6 You can also delete a selected Job or suspend/resume a selected Job.","title":"Jobs"},{"location":"admin/admin-ui-setup/jobs/#viewing-and-submitting-jobs","text":"The Run:ai User interface Job area allows the viewing of Jobs and Job details. It also allows the Researcher to submit Jobs, suspend and resume Jobs and delete Jobs.","title":"Viewing and Submitting Jobs"},{"location":"admin/admin-ui-setup/jobs/#job-list","text":"The main view shows a list of Jobs. The list can be filtered and sorted:","title":"Job list"},{"location":"admin/admin-ui-setup/jobs/#submit-job","text":"On the top right, you can choose to Submit a new Job. A Job form will open: Note If the Submit Job button is disabled or does not exist, then your cluster is not installed or configured to connect to the cluster see here for more information.","title":"Submit Job"},{"location":"admin/admin-ui-setup/jobs/#job-properties","text":"When selecting a single Job, a right-pane appears: This multi-tab view provides information about Job details, related Pods, Job status history, and various utilization graphs. You can also view internal Job logs as shown here:","title":"Job Properties"},{"location":"admin/admin-ui-setup/jobs/#other-operations","text":"You can also delete a selected Job or suspend/resume a selected Job.","title":"Other operations"},{"location":"admin/admin-ui-setup/overview/","text":"User Interface Overview \u00b6 Run:ai provides a single user interface that, depending on your role, serves both as a control-plane management tool and a researcher workbench. The control-plane part of the tool allows the administrator to: Analyze cluster status using dashboards . Manage Run:ai metadata such as users , departments , and projects . View Job details to be able to help researchers solve Job-related issues. The researcher workbench part of the tool allows Researchers to submit, delete and pause Jobs , view Job logs etc. Setup \u00b6 The cluster installation process requires configuring a new cluster and downloading a YAML file. On SaaS-based installations, the cluster creation wizard requires a URL to the cluster as explained here . Architecture \u00b6 Run:ai saves metadata such as users, projects, departments, clusters, and tenant settings, in the control plane residing on the Run:ai cloud. Workload information resides on (sometimes multiple) GPU clusters. The Run:ai user interface needs to work with both sources of information. As such, the chosen architecture of the user interface is: The user interface is served from the management backend. The user interface connects directly to multiple GPU clusters using cross-origin access . This works using CORS : Cross-origin resource sharing. This allows submitting workloads and getting extended logging information directly from the GPU clusters. Meta-data, such as Projects, Settings, and Job information is synced into the management backend via a cluster-sync service. Cluster-sync creates an outbound-only channel with no incoming HTTPS connections. Important One corollary of this architecture is that for SaaS-based tenants, the user interface will only be able to access the cluster when the browser is inside the corporate firewall. When working outside the firewall. Workload-related functionality such as Submitting a Job, viewing Job lots etc, is disabled.","title":"Overview"},{"location":"admin/admin-ui-setup/overview/#user-interface-overview","text":"Run:ai provides a single user interface that, depending on your role, serves both as a control-plane management tool and a researcher workbench. The control-plane part of the tool allows the administrator to: Analyze cluster status using dashboards . Manage Run:ai metadata such as users , departments , and projects . View Job details to be able to help researchers solve Job-related issues. The researcher workbench part of the tool allows Researchers to submit, delete and pause Jobs , view Job logs etc.","title":"User Interface Overview"},{"location":"admin/admin-ui-setup/overview/#setup","text":"The cluster installation process requires configuring a new cluster and downloading a YAML file. On SaaS-based installations, the cluster creation wizard requires a URL to the cluster as explained here .","title":"Setup"},{"location":"admin/admin-ui-setup/overview/#architecture","text":"Run:ai saves metadata such as users, projects, departments, clusters, and tenant settings, in the control plane residing on the Run:ai cloud. Workload information resides on (sometimes multiple) GPU clusters. The Run:ai user interface needs to work with both sources of information. As such, the chosen architecture of the user interface is: The user interface is served from the management backend. The user interface connects directly to multiple GPU clusters using cross-origin access . This works using CORS : Cross-origin resource sharing. This allows submitting workloads and getting extended logging information directly from the GPU clusters. Meta-data, such as Projects, Settings, and Job information is synced into the management backend via a cluster-sync service. Cluster-sync creates an outbound-only channel with no incoming HTTPS connections. Important One corollary of this architecture is that for SaaS-based tenants, the user interface will only be able to access the cluster when the browser is inside the corporate firewall. When working outside the firewall. Workload-related functionality such as Submitting a Job, viewing Job lots etc, is disabled.","title":"Architecture"},{"location":"admin/admin-ui-setup/project-setup/","text":"Introduction \u00b6 Researchers submit Jobs. To streamline resource allocation and prioritize work, Run:ai introduces the concept of Projects . Projects are the tool to implement resource allocation policies as well as create segregation between different initiatives. A project in most cases represents a team, an individual, or an initiative that shares resources or has a specific resources budget (quota). A Researcher submitting a Job needs to associate a Project name with the request. The Run:ai scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in the queue for future allocation. Modeling Projects \u00b6 As an Admin, you need to determine how to model Projects. You can: Set a Project per user. Set a Project per team of users. Set a Project per a real organizational Project. Node Pools \u00b6 By default all nodes on a cluster are part of 'Default' node pool. Administrator can choose to create new node pools and include a set of nodes to a node pool by associating a label that is shared by all those nodes. Each node pool is automatically associated with all Projects and Departments with zero resources allocation (Quotas). When submitting a Job (or Deployment), the Researcher (or ML Engineer) can chose a single node pool to use, or multiple node pools to use. When chosing more than one node pool, the researcher sets the order of priority between the chosen node pools, the scheduler will try to schedule the Job to the first node pool, then if not successful, to the second node pool in the list, and so on, until it finds a node pool that can provide the Job's specification. Administrator can set a 'Project's default priority list' of node pools, so incase the Researcher did not specify any node pool (or node pool list), the scheduler will use the Project's default node pool priority list to determine the order that the scheduler will use when scheduling the Job. Project Quotas \u00b6 Each Project is associated with a total quota of GPU and CPU resources (CPU Compute & CPU Memory) that can be allocated for the Project at the same time. This total is the sum of all node pools' quotas associated with this Project. This is guaranteed quota in the sense that Researchers using this Project are guaranteed to get this amount of GPU and CPU resources, no matter what the status in the cluster is. Beyond that, a user of this Project can receive an over-quota (Administrator needs to enable over-quota per project). As long as GPUs are unused, a Researcher using this Project can get more GPUs. However, these GPUs can be taken away at a moment's notice . When node-pools flag is enabled, over-quota is effective and calculated per node-pool, this means that a workload requesting resources from a certain node pool, can get its resources from a quota that belongs to another Project for the same node pool, if the resources are exhaused for this Project and avaialble on another Project. For more details on over-quota scheduling see: The Run AI Scheduler . Important best practice: As a rule, the sum of the Projects' allocations should be equal to the number of GPUs in the cluster. Controlling Over-Quota Behavior \u00b6 By default, the amount of over-quota available for Project members is proportional to the original quota provided above. The Run:ai scheduler document provides further examples which show how over-quota is distributed amongst competing Projects. As an administrator, you may want to disconnect the two parameters. So that, for example, a Project with a high quota will receive little or no over -quota. To perform this: Under General | Settings turn on the Enable Over-quota Priority feature When creating a new Project, you can now see a slider for over-quota priority ranging from None to High Create a Project \u00b6 Note To be able to create or edit Projects, you must have Editor access. See the Users documentation. Login to the Projects area of the Run:ai user interface at <company-name>.run.ai . On the top right, select \"Add New Project\" Choose a Project name and a Project quota Press \"Save\" Assign Users to Project \u00b6 When Researcher Authentication is enabled, the Project form will contain an additional Access Control tab. The tab will allow you to assign Researchers to their Projects. If you are using Single-sign-on, you can also assign Groups Other Project Properties \u00b6 Limit Jobs to run on Specific Node Groups \u00b6 You can assign a Project to run on specific nodes (machines).This is achieved by two different mechnisms: Node Pools: All node pools in the system are associated with each Project. Each node pool can allocate GPU and CPU resources (CPU Compute & CPU Memory) to a Project. By associating a quota on specific node pools for a Project, you can control which nodes a Project can utilize and which default priority order the scheduler will use (in case the workload did chose so by itself). Each workload should choose the node pool(s) to use, if no choice is made, it will use the Project's default 'node pool priority list'. Note that node pools with zero resources associated to a Project or node pools with exhausted resources, can still be used by a Project when Over-Quota flag is enabled. Node Affinities (aka Node Type) Administrator can assosciate specific node sets characterized by a shared run-ai/node-type label value to a Project. This means descendant workloads can only use nodes from one of those node affinity groups. A workload can specify which node affinity to use, out of the list bounded to its parent Project. There are many use cases and reasons to use specific nodes for a Project and its descendant workloads, here are some examples: The project team needs specialized hardware (e.g. with enough memory). The project team is the owner of specific hardware which was acquired with a specialized budget. We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes. The difference between node pools and affinities \u00b6 Node pools represent an independent schduling domain per Project, therefore are completly segragated from each other. To use a specific node pool (or node pools), any workload must specify the node pool(s) it would like to use. While for affinites, workloads that ask for a specific affinity will only be scheduled to nodes marked with that affinity, while workloads that did not specify any affinity might be schduled as well to those nodes with an affinity. Therefore the schduler cannot guarantee quota for node affinities, only to node pools. Note that using node pools and affinities narrows down the scope of nodes a specific project is eligible to use. It, therefore, reduces the odds of a specific workload under that Project getting scheduled. In some cases, this may reduce the overall system utilization. Grouping Nodes using Node Pools \u00b6 To create a node pool you must first annotate nodes with a label or use an existing node label, as the key for grouping nodes into pools. You can use any unique label (in the format key:value ) to form a node pool. a node pool is characterized by a label but also has its own unique node pool name. To get the list of nodes and their current labels, run: kubectl get nodes --show-labels To annotate a specific node with the label dgx-2 , run: kubectl label node <node-name> node-model=dgx-2 You can annotate multiple nodes with the same label. To create a node pool with the chosen common label use the create node pool Run:ai API. Setting Node Pools for a Specific Project \u00b6 By default all node-pools are associated with every Project and Department using zero resource allocation. This means that by default any Project can use any node-pool if Over-Quota is set for that Project, but only for preemptible workloads (i.e. Training workloads or Interactive using Preemptible flag). To gaurantee resources for all workloads including non-preemptible workloads, administrator should allocate resources in node-pools. Go to the Node Pools tab under Project and set a quota to any of the node pools (GPU resources, CPU resources) you want to use. To set the Project's default node pools order of priority, you should set the precednece of each node pool, this is done in the Project's node pool tab. The node pool default priority order is used if the workload did not specify its own preffered node pool(s) list of priority. To mandate a Workload to run on a specific node pools, Researcher should specify the node-pool to use for a workload. If no node-pool is specified - the Project's 'Default' node-pool priority list is used. Press 'Save' to save your changes. Grouping Nodes using Node Affinities \u00b6 To set node affinities, you must first annotate nodes with labels. These labels will later be associated with Projects. To get the list of nodes, run: kubectl get nodes To annotate a specific node with the label \"dgx-2\", run: kubectl label node <node-name> run.ai/type=dgx-2 Each node can only be annotated with a single label. You can annotate multiple nodes with the same label. Setting Affinity for a Specific Project \u00b6 To mandate training Jobs to run on specific node groups: Create a Project or edit an existing Project. Go to the Node Affinity tab and set a limit to specific node groups. If the label does not yet exist, press the + sign and add the label. Press Enter to save the label. Select the label. To mandate interactive Jobs to run on specific node groups, perform the same steps under the \"interactive\" section in the Project dialog. Further Affinity Refinement by the Researcher \u00b6 The Researcher can limit the selection of node groups by using the CLI flag --node-type with a specific label. When setting specific Project affinity, the CLI flag can only be used with a node group out of the previously chosen list. See CLI reference for further information runai submit Limit Duration of Interactive and Training Jobs \u00b6 As interactive sessions involve human interaction, Run:ai provides an additional tool to enforce a policy that sets the time limit for such sessions. This policy is often used to handle situations like researchers leaving sessions open even when they do not need to access the resources. Warning This feature will cause containers to automatically stop. Any work not saved to a shared volume will be lost To set a duration limit for interactive Jobs: Create a Project or edit an existing Project. Go to the Time Limit tab You can limit interactive Jobs using two criteria: Set a hard time limit (day, hour, minute) to an Interactive Job, regardless of the activity of this Job, e.g. stop the Job after 1 day of work. Set a time limit for Idle Interactive Jobs, i.e. an Interactive Job idle for X time is stopped. Idle means no GPU activity. You can set if this idle time limit is effective for Interactive Jobs that are Preemptible, non-Preemptible, or both. The setting only takes effect for Jobs that have started after the duration has been changed. On some use cases you would like to stop Training Jobs if X time elapsed since they have started to run. This can be to clean up stale Training Jobs or Jobs that are running for too long probbaly because of wrong parameters set or other errors of the model. To set a duration limit for Training Jobs: Create a Project or edit an existing Project. Go to the Time Limit tab: Set a time limit for Idle Training Jobs, i.e. a Training Job idle for X time is stopped. Idle means no GPU activity. The setting only takes effect for Jobs that have started after the duration has been changed. See Also \u00b6 Run:ai supports an additional (optional) level of resource allocation called Departments .","title":"Projects"},{"location":"admin/admin-ui-setup/project-setup/#introduction","text":"Researchers submit Jobs. To streamline resource allocation and prioritize work, Run:ai introduces the concept of Projects . Projects are the tool to implement resource allocation policies as well as create segregation between different initiatives. A project in most cases represents a team, an individual, or an initiative that shares resources or has a specific resources budget (quota). A Researcher submitting a Job needs to associate a Project name with the request. The Run:ai scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in the queue for future allocation.","title":"Introduction"},{"location":"admin/admin-ui-setup/project-setup/#modeling-projects","text":"As an Admin, you need to determine how to model Projects. You can: Set a Project per user. Set a Project per team of users. Set a Project per a real organizational Project.","title":"Modeling Projects"},{"location":"admin/admin-ui-setup/project-setup/#node-pools","text":"By default all nodes on a cluster are part of 'Default' node pool. Administrator can choose to create new node pools and include a set of nodes to a node pool by associating a label that is shared by all those nodes. Each node pool is automatically associated with all Projects and Departments with zero resources allocation (Quotas). When submitting a Job (or Deployment), the Researcher (or ML Engineer) can chose a single node pool to use, or multiple node pools to use. When chosing more than one node pool, the researcher sets the order of priority between the chosen node pools, the scheduler will try to schedule the Job to the first node pool, then if not successful, to the second node pool in the list, and so on, until it finds a node pool that can provide the Job's specification. Administrator can set a 'Project's default priority list' of node pools, so incase the Researcher did not specify any node pool (or node pool list), the scheduler will use the Project's default node pool priority list to determine the order that the scheduler will use when scheduling the Job.","title":"Node Pools"},{"location":"admin/admin-ui-setup/project-setup/#project-quotas","text":"Each Project is associated with a total quota of GPU and CPU resources (CPU Compute & CPU Memory) that can be allocated for the Project at the same time. This total is the sum of all node pools' quotas associated with this Project. This is guaranteed quota in the sense that Researchers using this Project are guaranteed to get this amount of GPU and CPU resources, no matter what the status in the cluster is. Beyond that, a user of this Project can receive an over-quota (Administrator needs to enable over-quota per project). As long as GPUs are unused, a Researcher using this Project can get more GPUs. However, these GPUs can be taken away at a moment's notice . When node-pools flag is enabled, over-quota is effective and calculated per node-pool, this means that a workload requesting resources from a certain node pool, can get its resources from a quota that belongs to another Project for the same node pool, if the resources are exhaused for this Project and avaialble on another Project. For more details on over-quota scheduling see: The Run AI Scheduler . Important best practice: As a rule, the sum of the Projects' allocations should be equal to the number of GPUs in the cluster.","title":"Project Quotas"},{"location":"admin/admin-ui-setup/project-setup/#controlling-over-quota-behavior","text":"By default, the amount of over-quota available for Project members is proportional to the original quota provided above. The Run:ai scheduler document provides further examples which show how over-quota is distributed amongst competing Projects. As an administrator, you may want to disconnect the two parameters. So that, for example, a Project with a high quota will receive little or no over -quota. To perform this: Under General | Settings turn on the Enable Over-quota Priority feature When creating a new Project, you can now see a slider for over-quota priority ranging from None to High","title":"Controlling Over-Quota Behavior"},{"location":"admin/admin-ui-setup/project-setup/#create-a-project","text":"Note To be able to create or edit Projects, you must have Editor access. See the Users documentation. Login to the Projects area of the Run:ai user interface at <company-name>.run.ai . On the top right, select \"Add New Project\" Choose a Project name and a Project quota Press \"Save\"","title":"Create a Project"},{"location":"admin/admin-ui-setup/project-setup/#assign-users-to-project","text":"When Researcher Authentication is enabled, the Project form will contain an additional Access Control tab. The tab will allow you to assign Researchers to their Projects. If you are using Single-sign-on, you can also assign Groups","title":"Assign Users to Project"},{"location":"admin/admin-ui-setup/project-setup/#other-project-properties","text":"","title":"Other Project Properties"},{"location":"admin/admin-ui-setup/project-setup/#limit-jobs-to-run-on-specific-node-groups","text":"You can assign a Project to run on specific nodes (machines).This is achieved by two different mechnisms: Node Pools: All node pools in the system are associated with each Project. Each node pool can allocate GPU and CPU resources (CPU Compute & CPU Memory) to a Project. By associating a quota on specific node pools for a Project, you can control which nodes a Project can utilize and which default priority order the scheduler will use (in case the workload did chose so by itself). Each workload should choose the node pool(s) to use, if no choice is made, it will use the Project's default 'node pool priority list'. Note that node pools with zero resources associated to a Project or node pools with exhausted resources, can still be used by a Project when Over-Quota flag is enabled. Node Affinities (aka Node Type) Administrator can assosciate specific node sets characterized by a shared run-ai/node-type label value to a Project. This means descendant workloads can only use nodes from one of those node affinity groups. A workload can specify which node affinity to use, out of the list bounded to its parent Project. There are many use cases and reasons to use specific nodes for a Project and its descendant workloads, here are some examples: The project team needs specialized hardware (e.g. with enough memory). The project team is the owner of specific hardware which was acquired with a specialized budget. We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes.","title":"Limit Jobs to run on Specific Node Groups"},{"location":"admin/admin-ui-setup/project-setup/#the-difference-between-node-pools-and-affinities","text":"Node pools represent an independent schduling domain per Project, therefore are completly segragated from each other. To use a specific node pool (or node pools), any workload must specify the node pool(s) it would like to use. While for affinites, workloads that ask for a specific affinity will only be scheduled to nodes marked with that affinity, while workloads that did not specify any affinity might be schduled as well to those nodes with an affinity. Therefore the schduler cannot guarantee quota for node affinities, only to node pools. Note that using node pools and affinities narrows down the scope of nodes a specific project is eligible to use. It, therefore, reduces the odds of a specific workload under that Project getting scheduled. In some cases, this may reduce the overall system utilization.","title":"The difference between node pools and affinities"},{"location":"admin/admin-ui-setup/project-setup/#grouping-nodes-using-node-pools","text":"To create a node pool you must first annotate nodes with a label or use an existing node label, as the key for grouping nodes into pools. You can use any unique label (in the format key:value ) to form a node pool. a node pool is characterized by a label but also has its own unique node pool name. To get the list of nodes and their current labels, run: kubectl get nodes --show-labels To annotate a specific node with the label dgx-2 , run: kubectl label node <node-name> node-model=dgx-2 You can annotate multiple nodes with the same label. To create a node pool with the chosen common label use the create node pool Run:ai API.","title":"Grouping Nodes using Node Pools"},{"location":"admin/admin-ui-setup/project-setup/#setting-node-pools-for-a-specific-project","text":"By default all node-pools are associated with every Project and Department using zero resource allocation. This means that by default any Project can use any node-pool if Over-Quota is set for that Project, but only for preemptible workloads (i.e. Training workloads or Interactive using Preemptible flag). To gaurantee resources for all workloads including non-preemptible workloads, administrator should allocate resources in node-pools. Go to the Node Pools tab under Project and set a quota to any of the node pools (GPU resources, CPU resources) you want to use. To set the Project's default node pools order of priority, you should set the precednece of each node pool, this is done in the Project's node pool tab. The node pool default priority order is used if the workload did not specify its own preffered node pool(s) list of priority. To mandate a Workload to run on a specific node pools, Researcher should specify the node-pool to use for a workload. If no node-pool is specified - the Project's 'Default' node-pool priority list is used. Press 'Save' to save your changes.","title":"Setting Node Pools for a Specific Project"},{"location":"admin/admin-ui-setup/project-setup/#grouping-nodes-using-node-affinities","text":"To set node affinities, you must first annotate nodes with labels. These labels will later be associated with Projects. To get the list of nodes, run: kubectl get nodes To annotate a specific node with the label \"dgx-2\", run: kubectl label node <node-name> run.ai/type=dgx-2 Each node can only be annotated with a single label. You can annotate multiple nodes with the same label.","title":"Grouping Nodes using Node Affinities"},{"location":"admin/admin-ui-setup/project-setup/#setting-affinity-for-a-specific-project","text":"To mandate training Jobs to run on specific node groups: Create a Project or edit an existing Project. Go to the Node Affinity tab and set a limit to specific node groups. If the label does not yet exist, press the + sign and add the label. Press Enter to save the label. Select the label. To mandate interactive Jobs to run on specific node groups, perform the same steps under the \"interactive\" section in the Project dialog.","title":"Setting Affinity for a Specific Project"},{"location":"admin/admin-ui-setup/project-setup/#further-affinity-refinement-by-the-researcher","text":"The Researcher can limit the selection of node groups by using the CLI flag --node-type with a specific label. When setting specific Project affinity, the CLI flag can only be used with a node group out of the previously chosen list. See CLI reference for further information runai submit","title":"Further Affinity Refinement by the Researcher"},{"location":"admin/admin-ui-setup/project-setup/#limit-duration-of-interactive-and-training-jobs","text":"As interactive sessions involve human interaction, Run:ai provides an additional tool to enforce a policy that sets the time limit for such sessions. This policy is often used to handle situations like researchers leaving sessions open even when they do not need to access the resources. Warning This feature will cause containers to automatically stop. Any work not saved to a shared volume will be lost To set a duration limit for interactive Jobs: Create a Project or edit an existing Project. Go to the Time Limit tab You can limit interactive Jobs using two criteria: Set a hard time limit (day, hour, minute) to an Interactive Job, regardless of the activity of this Job, e.g. stop the Job after 1 day of work. Set a time limit for Idle Interactive Jobs, i.e. an Interactive Job idle for X time is stopped. Idle means no GPU activity. You can set if this idle time limit is effective for Interactive Jobs that are Preemptible, non-Preemptible, or both. The setting only takes effect for Jobs that have started after the duration has been changed. On some use cases you would like to stop Training Jobs if X time elapsed since they have started to run. This can be to clean up stale Training Jobs or Jobs that are running for too long probbaly because of wrong parameters set or other errors of the model. To set a duration limit for Training Jobs: Create a Project or edit an existing Project. Go to the Time Limit tab: Set a time limit for Idle Training Jobs, i.e. a Training Job idle for X time is stopped. Idle means no GPU activity. The setting only takes effect for Jobs that have started after the duration has been changed.","title":"Limit Duration of Interactive and Training Jobs"},{"location":"admin/admin-ui-setup/project-setup/#see-also","text":"Run:ai supports an additional (optional) level of resource allocation called Departments .","title":"See Also"},{"location":"admin/integration/airflow/","text":"Integrate Run:ai with Apache Airflow \u00b6 Airflow is a platform to programmatically author, schedule, and monitor workflows. Specifically, it is used in Machine Learning to create pipelines. Airflow DAG \u00b6 In Airflow, a DAG \u2013 or a Directed Acyclic Graph \u2013 is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies. A DAG is defined in a Python script, which represents the DAGs structure (tasks and their dependencies) as code. For example, a simple DAG could consist of three tasks: A, B, and C. It could say that A has to run successfully before B can run, but C can run anytime. It could say that task A times out after 5 minutes, and B can be restarted up to 5 times in case it fails. It might also say that the workflow will run every night at 10 pm, but shouldn\u2019t start until a certain date. Airflow tasks are sent for execution. Specifically, the Airflow - Kubernetes integration allows Airflow tasks to be scheduled on a Kubernetes cluster. Run:ai - Airflow Integration \u00b6 DAGs are defined in Python. Airflow tasks based on Kubernetes are defined via the KubernetesPodOperator class. To run an Airflow task with Run:ai you must provide additional, Run:ai-related, properties to dag = DAG ( ... ) resources = { \"limit_gpu\" : < number - of - GPUs > } job = KubernetesPodOperator ( namespace = 'runai-<project-name>' , image = '<image-name>' , labels = { \"project\" : '<project-name>' }, name = '<task-name>' , task_id = '<task-name>' , get_logs = True , schedulername = 'runai-scheduler' , resources = resources , dag = dag ) The code: Specifies the runai-scheduler which directs the task to be scheduled with the Run:ai scheduler Specifies a Run:ai Project . A Project in Run:ai specifies guaranteed GPU & CPU quota. Once you run the DAG, you can see Airflow tasks shown in the Run:ai UI.","title":"Airflow"},{"location":"admin/integration/airflow/#integrate-runai-with-apache-airflow","text":"Airflow is a platform to programmatically author, schedule, and monitor workflows. Specifically, it is used in Machine Learning to create pipelines.","title":"Integrate Run:ai with Apache Airflow"},{"location":"admin/integration/airflow/#airflow-dag","text":"In Airflow, a DAG \u2013 or a Directed Acyclic Graph \u2013 is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies. A DAG is defined in a Python script, which represents the DAGs structure (tasks and their dependencies) as code. For example, a simple DAG could consist of three tasks: A, B, and C. It could say that A has to run successfully before B can run, but C can run anytime. It could say that task A times out after 5 minutes, and B can be restarted up to 5 times in case it fails. It might also say that the workflow will run every night at 10 pm, but shouldn\u2019t start until a certain date. Airflow tasks are sent for execution. Specifically, the Airflow - Kubernetes integration allows Airflow tasks to be scheduled on a Kubernetes cluster.","title":"Airflow DAG"},{"location":"admin/integration/airflow/#runai-airflow-integration","text":"DAGs are defined in Python. Airflow tasks based on Kubernetes are defined via the KubernetesPodOperator class. To run an Airflow task with Run:ai you must provide additional, Run:ai-related, properties to dag = DAG ( ... ) resources = { \"limit_gpu\" : < number - of - GPUs > } job = KubernetesPodOperator ( namespace = 'runai-<project-name>' , image = '<image-name>' , labels = { \"project\" : '<project-name>' }, name = '<task-name>' , task_id = '<task-name>' , get_logs = True , schedulername = 'runai-scheduler' , resources = resources , dag = dag ) The code: Specifies the runai-scheduler which directs the task to be scheduled with the Run:ai scheduler Specifies a Run:ai Project . A Project in Run:ai specifies guaranteed GPU & CPU quota. Once you run the DAG, you can see Airflow tasks shown in the Run:ai UI.","title":"Run:ai - Airflow Integration"},{"location":"admin/integration/argo-workflows/","text":"Integrate Run:ai with Argo Workflows \u00b6 Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. This document describes the process of using Argo Workflows in conjunction with Run:ai. Argo Workflows submits jobs that are scheduled via Run:ai. Install Argo Workflows \u00b6 Use the default installation to install Argo Workflows. As described in the documentation, open the Argo Workflows UI by running: kubectl -n argo port-forward deployment/argo-workflows-server 2746:2746 Then browse to localhost:2746 Create a Run:ai Project \u00b6 Using the Run:ai user interface, create a Run:ai Project. A Project named team-a will create a Kubernetes namespace named runai-team-a . Run an Argo Workflow with Run:ai \u00b6 Create an Argo Workflows Template \u00b6 Within the Argo Workflows user interface, go to Templates and create a new Template. Add the following metadata: spec : templates : - name : <WORKFLOW-NAME> metadata : labels : project : team-a # (1) Name of Project. Create and Run the Workflow \u00b6 Create an Argo Workflow from the template and run it. Open the Run:ai user interface, go to Jobs , and verify that you can see the new Job. Using GPU Fractions with Argo Workflows \u00b6 To run an Argo Workflow using GPU Fractions , you will need to add an annotation : spec : templates : - name : <WORKFLOW-NAME> metadata : annotations : gpu-fraction : '0.5' # (1) labels : project : team-a # (2) Size of required GPU Fraction. Name of Project.","title":"Argo Workflows"},{"location":"admin/integration/argo-workflows/#integrate-runai-with-argo-workflows","text":"Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. This document describes the process of using Argo Workflows in conjunction with Run:ai. Argo Workflows submits jobs that are scheduled via Run:ai.","title":"Integrate Run:ai with Argo Workflows"},{"location":"admin/integration/argo-workflows/#install-argo-workflows","text":"Use the default installation to install Argo Workflows. As described in the documentation, open the Argo Workflows UI by running: kubectl -n argo port-forward deployment/argo-workflows-server 2746:2746 Then browse to localhost:2746","title":"Install Argo Workflows"},{"location":"admin/integration/argo-workflows/#create-a-runai-project","text":"Using the Run:ai user interface, create a Run:ai Project. A Project named team-a will create a Kubernetes namespace named runai-team-a .","title":"Create a Run:ai Project"},{"location":"admin/integration/argo-workflows/#run-an-argo-workflow-with-runai","text":"","title":"Run an Argo Workflow with Run:ai"},{"location":"admin/integration/argo-workflows/#create-an-argo-workflows-template","text":"Within the Argo Workflows user interface, go to Templates and create a new Template. Add the following metadata: spec : templates : - name : <WORKFLOW-NAME> metadata : labels : project : team-a # (1) Name of Project.","title":"Create an Argo Workflows Template"},{"location":"admin/integration/argo-workflows/#create-and-run-the-workflow","text":"Create an Argo Workflow from the template and run it. Open the Run:ai user interface, go to Jobs , and verify that you can see the new Job.","title":"Create and Run the Workflow"},{"location":"admin/integration/argo-workflows/#using-gpu-fractions-with-argo-workflows","text":"To run an Argo Workflow using GPU Fractions , you will need to add an annotation : spec : templates : - name : <WORKFLOW-NAME> metadata : annotations : gpu-fraction : '0.5' # (1) labels : project : team-a # (2) Size of required GPU Fraction. Name of Project.","title":"Using GPU Fractions with Argo Workflows"},{"location":"admin/integration/clearml/","text":"Integrate Run:ai with ClearML \u00b6 ClearML is an open-source and commercial platform to manage the ML lifecycle. The purpose of this document is to explain how to run Jobs with MLflow using the Run:ai scheduler. Overview \u00b6 ClearML concepts are discussed here . Specifically see ClearML Kubernetes architecture. Terminology \u00b6 Run:ai uses Projects . A Project is assigned to users and contains information such as quota, affinity, and more. A Run:ai Project is implemented as a Kubernetes namespace. ClearML allows the Reesearcher to run Experiments . Experiment is equivalent to a Run:ai Job. A ClearML Experiment is sent to a ClearML Queue for execution. ClearML execute Agents . An agent runs on a Kubernetes namespace. An Agent is configured to watch a Queue. The Agent fetches an experiment from the queue for execution within the Kubernetes namespace. Step by Step Instructions \u00b6 Prerequisites \u00b6 A working Run:ai cluster. Install ClearML via ClearML helm charts . Once ClearML is installed, verify that the installation is working by running: kubectl get pod -n clearml See that all pods are up. Preparations \u00b6 To prepare a Run:ai Project and a ClearML Queue do the following: In ClearML, create a queue named runai-clearml . In Run:ai, create a project named clearml . This will create a namespace called runai-clearml Associate the queue and the project by running: kubectl get role -n clearml k8sagent-pods-access -ojson | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | kubectl create -f - kubectl get rolebinding -n clearml k8sagent-pods-access -ojson | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | kubectl create -f - kubectl get secret -n clearml clearml-conf -ojson | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | kubectl create -f - kubectl get configmap -n clearml k8sagent-pod-template -ojson | sed 's@tolerations:\\\\n {}@tolerations:\\\\n []@g' | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | jq '.data[\"template.yaml\"]=(.data[\"template.yaml\"] + \" schedulerName: runai-scheduler\")' | kubectl create -f - kubectl get deployment -n clearml clearml-k8sagent -ojson | sed 's/clearml-apiserver/clearml-apiserver.clearml.svc.cluster.local/; s/clearml-webserver/clearml-webserver.clearml.svc.cluster.local/; s/clearml-fileserver/clearml-fileserver.clearml.svc.cluster.local/; s@--template-yaml /root/template/template.yaml@--template-yaml /root/template/template.yaml --namespace runai-clearml@; s/k8s-agent/runai-k8s-agent/; s/aws-instances/runai-clearml/' | jq 'del(.status)' | jq 'del(.metadata.creationTimestamp)' | jq 'del(.metadata.generation)' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq '.metadata.namespace=\"runai-clearml\"' | kubectl create -f - Note The script is hardcoded for the above queue name and Run:ai Project name. You can change the script accordingly. Validate that the Queue and the Project are connected by running: kubectl get pod -n runai-clearml You should see a ClearML agent running inside the Run:ai namespace. Running an Experiment \u00b6 Using the ClearML interface create an experiment and enqueue it to the runai-clearml queue. Go to the Run:ai user interface. Under Jobs see that the job was created.","title":"ClearML"},{"location":"admin/integration/clearml/#integrate-runai-with-clearml","text":"ClearML is an open-source and commercial platform to manage the ML lifecycle. The purpose of this document is to explain how to run Jobs with MLflow using the Run:ai scheduler.","title":"Integrate Run:ai with ClearML"},{"location":"admin/integration/clearml/#overview","text":"ClearML concepts are discussed here . Specifically see ClearML Kubernetes architecture.","title":"Overview"},{"location":"admin/integration/clearml/#terminology","text":"Run:ai uses Projects . A Project is assigned to users and contains information such as quota, affinity, and more. A Run:ai Project is implemented as a Kubernetes namespace. ClearML allows the Reesearcher to run Experiments . Experiment is equivalent to a Run:ai Job. A ClearML Experiment is sent to a ClearML Queue for execution. ClearML execute Agents . An agent runs on a Kubernetes namespace. An Agent is configured to watch a Queue. The Agent fetches an experiment from the queue for execution within the Kubernetes namespace.","title":"Terminology"},{"location":"admin/integration/clearml/#step-by-step-instructions","text":"","title":"Step by Step Instructions"},{"location":"admin/integration/clearml/#prerequisites","text":"A working Run:ai cluster. Install ClearML via ClearML helm charts . Once ClearML is installed, verify that the installation is working by running: kubectl get pod -n clearml See that all pods are up.","title":"Prerequisites"},{"location":"admin/integration/clearml/#preparations","text":"To prepare a Run:ai Project and a ClearML Queue do the following: In ClearML, create a queue named runai-clearml . In Run:ai, create a project named clearml . This will create a namespace called runai-clearml Associate the queue and the project by running: kubectl get role -n clearml k8sagent-pods-access -ojson | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | kubectl create -f - kubectl get rolebinding -n clearml k8sagent-pods-access -ojson | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | kubectl create -f - kubectl get secret -n clearml clearml-conf -ojson | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | kubectl create -f - kubectl get configmap -n clearml k8sagent-pod-template -ojson | sed 's@tolerations:\\\\n {}@tolerations:\\\\n []@g' | jq '.metadata.namespace=\"runai-clearml\"' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq 'del(.metadata.creationTimestamp)' | jq '.data[\"template.yaml\"]=(.data[\"template.yaml\"] + \" schedulerName: runai-scheduler\")' | kubectl create -f - kubectl get deployment -n clearml clearml-k8sagent -ojson | sed 's/clearml-apiserver/clearml-apiserver.clearml.svc.cluster.local/; s/clearml-webserver/clearml-webserver.clearml.svc.cluster.local/; s/clearml-fileserver/clearml-fileserver.clearml.svc.cluster.local/; s@--template-yaml /root/template/template.yaml@--template-yaml /root/template/template.yaml --namespace runai-clearml@; s/k8s-agent/runai-k8s-agent/; s/aws-instances/runai-clearml/' | jq 'del(.status)' | jq 'del(.metadata.creationTimestamp)' | jq 'del(.metadata.generation)' | jq 'del(.metadata.uid)' | jq 'del(.metadata.resourceVersion)' | jq '.metadata.namespace=\"runai-clearml\"' | kubectl create -f - Note The script is hardcoded for the above queue name and Run:ai Project name. You can change the script accordingly. Validate that the Queue and the Project are connected by running: kubectl get pod -n runai-clearml You should see a ClearML agent running inside the Run:ai namespace.","title":"Preparations"},{"location":"admin/integration/clearml/#running-an-experiment","text":"Using the ClearML interface create an experiment and enqueue it to the runai-clearml queue. Go to the Run:ai user interface. Under Jobs see that the job was created.","title":"Running an Experiment"},{"location":"admin/integration/jupyterhub/","text":"Connect JupyterHub with Run:ai \u00b6 Overview \u00b6 A Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code. Uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Jupyter Notebooks are popular with Researchers as a way to code and run deep-learning code. A Jupyter Notebook runs inside the user container . For more information, see Using a Jupyter Notebook within a Run:ai Job . JupyterHub is a separate service that makes it possible to serve pre-configured data science environments. This document explains how to set up JupyterHub to integrate with Run:ai such that Notebooks spawned via JuptyerHub will use resources scheduled by Run:ai. Installing JupyterHub \u00b6 This document follows the JupyterHub installation documentation Create a namespace \u00b6 Run: kubectl create namespace jhub Provide access roles \u00b6 kubectl apply -f https://raw.githubusercontent.com/run-ai/docs/master/install/jupyterhub/jhubroles.yaml Create storage \u00b6 JupyterHub requires storage in the form of a PersistentVolume (PV). For an example of a local PV: Download https://raw.githubusercontent.com/run-ai/docs/master/install/jupyterhub/pv-example.yaml Replace <NODE-NAME> with one of your worker nodes. The example PV refers to /srv/jupyterhub . Log on to <NODE-NAME> and create the folder and run sudo chmod 777 -R /srv/jupyterhub Then run: kubectl apply -f pv-example.yaml Note The JupyterHub installation will create a PersistentVolumeClaim named hub-db-dir that should be referred to by any PV you create. Create a configuration file \u00b6 Create a configuration file for JupyterHub. An example configuration file for Run:ai can be found in https://raw.githubusercontent.com/run-ai/docs/master/install/jupyterhub/config.yaml . It contains 3 sample Run:ai configurations. Download the file Replace <SECRET-TOKEN> with a random number generated, by running openssl rand -hex 32 Install \u00b6 Run: helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/ helm repo update helm install jhub jupyterhub/jupyterhub -n jhub --version = 0 .11.1 --values config.yaml Verify Installation \u00b6 Run: kubectl get pods -n jhub Verify that all pods are running Access JupyterHub \u00b6 Run: kubectl get service -n jhub proxy-public Use the External IP of the service to access the service. Login with Run:ai Project name as user name. Troubleshooting the JupyterHub Installation \u00b6 If the External IP of the proxy-public service remains in the Pending status, it might mean that this service is not configured with an External IP by default. To fix, find out which pod is the proxy pod running on. Run: kubectl get pods -n jhub -l component = proxy -o = jsonpath = '{.items[0].spec.nodeName}{\"\\n\"}' This will print the node that the proxy pod is running on. You will need to get both the internal and external IPs of this node for the next step. Now, let's check the proxy-public service definition. Run: kubectl edit svc proxy-public -n jhub Under spec You should see a section externalIPs . If it does not exist, you must add it there. The section must contain both the external and the internal IPs of the proxy pod, for example: spec : externalIPs : - 35.224.44.230 - 10.8.0.9 Save the file and then try to access JupyterHub by using the external IP from the previous step in your browser. Caution Jupyter hub integration does not currently work properly when the Run:ai Project name includes a hyphen ('-'). We are working to fix that.","title":"JupyterHub"},{"location":"admin/integration/jupyterhub/#connect-jupyterhub-with-runai","text":"","title":"Connect JupyterHub with Run:ai"},{"location":"admin/integration/jupyterhub/#overview","text":"A Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code. Uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Jupyter Notebooks are popular with Researchers as a way to code and run deep-learning code. A Jupyter Notebook runs inside the user container . For more information, see Using a Jupyter Notebook within a Run:ai Job . JupyterHub is a separate service that makes it possible to serve pre-configured data science environments. This document explains how to set up JupyterHub to integrate with Run:ai such that Notebooks spawned via JuptyerHub will use resources scheduled by Run:ai.","title":"Overview"},{"location":"admin/integration/jupyterhub/#installing-jupyterhub","text":"This document follows the JupyterHub installation documentation","title":"Installing JupyterHub"},{"location":"admin/integration/jupyterhub/#create-a-namespace","text":"Run: kubectl create namespace jhub","title":"Create a namespace"},{"location":"admin/integration/jupyterhub/#provide-access-roles","text":"kubectl apply -f https://raw.githubusercontent.com/run-ai/docs/master/install/jupyterhub/jhubroles.yaml","title":"Provide access roles"},{"location":"admin/integration/jupyterhub/#create-storage","text":"JupyterHub requires storage in the form of a PersistentVolume (PV). For an example of a local PV: Download https://raw.githubusercontent.com/run-ai/docs/master/install/jupyterhub/pv-example.yaml Replace <NODE-NAME> with one of your worker nodes. The example PV refers to /srv/jupyterhub . Log on to <NODE-NAME> and create the folder and run sudo chmod 777 -R /srv/jupyterhub Then run: kubectl apply -f pv-example.yaml Note The JupyterHub installation will create a PersistentVolumeClaim named hub-db-dir that should be referred to by any PV you create.","title":"Create storage"},{"location":"admin/integration/jupyterhub/#create-a-configuration-file","text":"Create a configuration file for JupyterHub. An example configuration file for Run:ai can be found in https://raw.githubusercontent.com/run-ai/docs/master/install/jupyterhub/config.yaml . It contains 3 sample Run:ai configurations. Download the file Replace <SECRET-TOKEN> with a random number generated, by running openssl rand -hex 32","title":"Create a configuration file"},{"location":"admin/integration/jupyterhub/#install","text":"Run: helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/ helm repo update helm install jhub jupyterhub/jupyterhub -n jhub --version = 0 .11.1 --values config.yaml","title":"Install"},{"location":"admin/integration/jupyterhub/#verify-installation","text":"Run: kubectl get pods -n jhub Verify that all pods are running","title":"Verify Installation"},{"location":"admin/integration/jupyterhub/#access-jupyterhub","text":"Run: kubectl get service -n jhub proxy-public Use the External IP of the service to access the service. Login with Run:ai Project name as user name.","title":"Access JupyterHub"},{"location":"admin/integration/jupyterhub/#troubleshooting-the-jupyterhub-installation","text":"If the External IP of the proxy-public service remains in the Pending status, it might mean that this service is not configured with an External IP by default. To fix, find out which pod is the proxy pod running on. Run: kubectl get pods -n jhub -l component = proxy -o = jsonpath = '{.items[0].spec.nodeName}{\"\\n\"}' This will print the node that the proxy pod is running on. You will need to get both the internal and external IPs of this node for the next step. Now, let's check the proxy-public service definition. Run: kubectl edit svc proxy-public -n jhub Under spec You should see a section externalIPs . If it does not exist, you must add it there. The section must contain both the external and the internal IPs of the proxy pod, for example: spec : externalIPs : - 35.224.44.230 - 10.8.0.9 Save the file and then try to access JupyterHub by using the external IP from the previous step in your browser. Caution Jupyter hub integration does not currently work properly when the Run:ai Project name includes a hyphen ('-'). We are working to fix that.","title":"Troubleshooting the JupyterHub Installation"},{"location":"admin/integration/kubeflow/","text":"Integrate Run:ai with Kubeflow \u00b6 Kubeflow is a platform for data scientists who want to build and experiment with ML pipelines. Kubeflow is also for ML engineers and operational teams who want to deploy ML systems to various environments for development, testing, and production-level serving. This document describes the process of using Kubeflow in conjunction with Run:ai. Kubeflow submits jobs that are scheduled via Run:ai. Kubeflow is a set of technologies. This document discusses Kubeflow Notebooks and Kubeflow Pipelines . Install Kubeflow \u00b6 Use the default installation to install Kubeflow. Install Run:ai Cluster \u00b6 When installing Run:ai, customize the cluster installation as follows: Set createNamespaces to false , as Kubeflow uses its own namespace convention. Create Run:ai Projects \u00b6 Kubeflow uses the namespace convention kubeflow-<username> . Use the 4 steps here to set up Run:ai projects and link them with Kubeflow namespaces. Verify that the association has worked by running: kubectl get rolebindings -n <KUBEFLOW-USER-NAMESPACE> See that role bindings starting with runai- were created. Kubeflow, Users and Kubernetes Namespaces \u00b6 Kubeflow has a multi-user architecture. A user has a Kubeflow profile which maps to a Kubernetes Namespace. This is similar to the Run:ai concept where a Run:ai Project is mapped to a Kubernetes namespace. Kubeflow Notebooks \u00b6 When starting a Kubeflow Notebook , you select a Kubeflow configuration . A Kubeflow configuration allows you to inject additional settings into the notebook, such as environment variables. To use Kubeflow with Run:ai you will use configurations to inject: The name of the Run:ai project Allocation of a fraction of a GPU, if required Whole GPUs \u00b6 To use Run:ai with whole GPUs (no fractions), apply the following configuration: apiVersion : kubeflow.org/v1alpha1 kind : PodDefault metadata : name : runai-non-fractional namespace : <KUBEFLOW-USER-NAMESPACE> spec : desc : \"Use Run:ai scheduler (whole GPUs)\" env : - name : RUNAI_PROJECT value : \"<PROJECT>\" selector : matchLabels : runai-non-fractional : \"true\" # key must be identical to metadata.name Where <KUBEFLOW-USER-NAMESPACE> is the name of the namespace associated with the Kubeflow user and <PROJECT> is the name of the Run:ai project. Important Jobs should not be submitted within the same namespace where Kubeflow Operator is installed. Within the Kubeflow Notebook creation form, select the new configuration as well as the number of GPUs required. Fractions \u00b6 The Kubeflow Notebook creation form only allows the selection of 1, 2, 4, or 8 GPUs. It is not possible to select a portion of a GPU (e.g. 0.5). As such, within the form, select None in the GPU box together with the following configuration: apiVersion : kubeflow.org/v1alpha1 kind : PodDefault metadata : name : runai-half-gpu namespace : <KUBEFLOW-USER-NAMESPACE> spec : desc : \"Allocate 0.5 GPUs via Run:ai scheduler\" env : - name : RUNAI_PROJECT value : \"<PROJECT>\" - name : RUNAI_GPU_FRACTION value : \"0.5\" selector : matchLabels : runai-half-gpu : \"true\" # key must be identical to metadata.name Similar configurations can be created for fractional configurations, other than 0.5. Kubeflow Pipelines \u00b6 Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers. As with Kubeflow Notebooks, the goal of this section is to run pipelines jobs within the context of Run:ai. To create a Kubeflow pipeline, you: Write code using the Kubeflow Pipeline SDK . Package it into a single compressed file. Upload the file into Kubeflow and set it up. The example code provided here shows how to augment pipeline code to use Run:ai Whole GPUs \u00b6 To the pipeline code add: _training = training_op () ... _training . add_pod_label ( 'runai' , 'true' ) _training . add_pod_label ( 'project' , '<PROJECT>' ) Where <Project> is the Run:ai project name. See example code here Compile the code by running: dsl-compile --py kubeflow-runai-one-gpu.py --output kubeflow-runai-one-gpu.tar.gz (dsl-compile is part of the Kubeflow Pipeline Python SDK). Fractions \u00b6 To allocate half a GPU, add the following to the pipeline code: _training = training_op () ... _training . add_pod_label ( 'runai' , 'true' ) _training . add_pod_label ( 'project' , '<PROJECT>' ) _training . add_pod_annotation ( 'gpu-fraction' , '0.5' ) Where <Project> is the Run:ai project name. See example code here . Compile the code as described above.","title":"Kubeflow"},{"location":"admin/integration/kubeflow/#integrate-runai-with-kubeflow","text":"Kubeflow is a platform for data scientists who want to build and experiment with ML pipelines. Kubeflow is also for ML engineers and operational teams who want to deploy ML systems to various environments for development, testing, and production-level serving. This document describes the process of using Kubeflow in conjunction with Run:ai. Kubeflow submits jobs that are scheduled via Run:ai. Kubeflow is a set of technologies. This document discusses Kubeflow Notebooks and Kubeflow Pipelines .","title":"Integrate Run:ai with Kubeflow"},{"location":"admin/integration/kubeflow/#install-kubeflow","text":"Use the default installation to install Kubeflow.","title":"Install Kubeflow"},{"location":"admin/integration/kubeflow/#install-runai-cluster","text":"When installing Run:ai, customize the cluster installation as follows: Set createNamespaces to false , as Kubeflow uses its own namespace convention.","title":"Install Run:ai Cluster"},{"location":"admin/integration/kubeflow/#create-runai-projects","text":"Kubeflow uses the namespace convention kubeflow-<username> . Use the 4 steps here to set up Run:ai projects and link them with Kubeflow namespaces. Verify that the association has worked by running: kubectl get rolebindings -n <KUBEFLOW-USER-NAMESPACE> See that role bindings starting with runai- were created.","title":"Create Run:ai Projects"},{"location":"admin/integration/kubeflow/#kubeflow-users-and-kubernetes-namespaces","text":"Kubeflow has a multi-user architecture. A user has a Kubeflow profile which maps to a Kubernetes Namespace. This is similar to the Run:ai concept where a Run:ai Project is mapped to a Kubernetes namespace.","title":"Kubeflow, Users and Kubernetes Namespaces"},{"location":"admin/integration/kubeflow/#kubeflow-notebooks","text":"When starting a Kubeflow Notebook , you select a Kubeflow configuration . A Kubeflow configuration allows you to inject additional settings into the notebook, such as environment variables. To use Kubeflow with Run:ai you will use configurations to inject: The name of the Run:ai project Allocation of a fraction of a GPU, if required","title":"Kubeflow Notebooks"},{"location":"admin/integration/kubeflow/#whole-gpus","text":"To use Run:ai with whole GPUs (no fractions), apply the following configuration: apiVersion : kubeflow.org/v1alpha1 kind : PodDefault metadata : name : runai-non-fractional namespace : <KUBEFLOW-USER-NAMESPACE> spec : desc : \"Use Run:ai scheduler (whole GPUs)\" env : - name : RUNAI_PROJECT value : \"<PROJECT>\" selector : matchLabels : runai-non-fractional : \"true\" # key must be identical to metadata.name Where <KUBEFLOW-USER-NAMESPACE> is the name of the namespace associated with the Kubeflow user and <PROJECT> is the name of the Run:ai project. Important Jobs should not be submitted within the same namespace where Kubeflow Operator is installed. Within the Kubeflow Notebook creation form, select the new configuration as well as the number of GPUs required.","title":"Whole GPUs"},{"location":"admin/integration/kubeflow/#fractions","text":"The Kubeflow Notebook creation form only allows the selection of 1, 2, 4, or 8 GPUs. It is not possible to select a portion of a GPU (e.g. 0.5). As such, within the form, select None in the GPU box together with the following configuration: apiVersion : kubeflow.org/v1alpha1 kind : PodDefault metadata : name : runai-half-gpu namespace : <KUBEFLOW-USER-NAMESPACE> spec : desc : \"Allocate 0.5 GPUs via Run:ai scheduler\" env : - name : RUNAI_PROJECT value : \"<PROJECT>\" - name : RUNAI_GPU_FRACTION value : \"0.5\" selector : matchLabels : runai-half-gpu : \"true\" # key must be identical to metadata.name Similar configurations can be created for fractional configurations, other than 0.5.","title":"Fractions"},{"location":"admin/integration/kubeflow/#kubeflow-pipelines","text":"Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers. As with Kubeflow Notebooks, the goal of this section is to run pipelines jobs within the context of Run:ai. To create a Kubeflow pipeline, you: Write code using the Kubeflow Pipeline SDK . Package it into a single compressed file. Upload the file into Kubeflow and set it up. The example code provided here shows how to augment pipeline code to use Run:ai","title":"Kubeflow Pipelines"},{"location":"admin/integration/kubeflow/#whole-gpus_1","text":"To the pipeline code add: _training = training_op () ... _training . add_pod_label ( 'runai' , 'true' ) _training . add_pod_label ( 'project' , '<PROJECT>' ) Where <Project> is the Run:ai project name. See example code here Compile the code by running: dsl-compile --py kubeflow-runai-one-gpu.py --output kubeflow-runai-one-gpu.tar.gz (dsl-compile is part of the Kubeflow Pipeline Python SDK).","title":"Whole GPUs"},{"location":"admin/integration/kubeflow/#fractions_1","text":"To allocate half a GPU, add the following to the pipeline code: _training = training_op () ... _training . add_pod_label ( 'runai' , 'true' ) _training . add_pod_label ( 'project' , '<PROJECT>' ) _training . add_pod_annotation ( 'gpu-fraction' , '0.5' ) Where <Project> is the Run:ai project name. See example code here . Compile the code as described above.","title":"Fractions"},{"location":"admin/integration/kubevirt/","text":"Scheduling Virtual Machines using Run:ai \u00b6 Many organizations use virtual machines (VMs) to provide operating system abstraction to users. Containers are different than VMs but serve a similar purpose. Containers at a large scale are best managed by Kubernetes and Run:ai is based on Kubernetes. It is possible to mix and match containers and VMs to some extent using a technology called KubeVirt . KubeVirt allows running VMs inside containers on top of Kubernetes. This article describes how to use Kubevirt to schedule VMs with GPUs. Limitations \u00b6 Each node in the cluster will be able to support either VMs or containers - not combined. GPU fractions are not supported. Preparations \u00b6 Making GPUs visible to VMs is not trivial. It requires either a license for NVIDIA software called NVIDIA vGPU or creating a GPU passthrough by the explicit mapping of GPU devices to virtual machines. This guide relates to the latter option. Install KubeVirt \u00b6 Install KubeVirt using the following guide . Dedicate specific nodes for VMs \u00b6 Dedicate specific nodes within the cluster to be used for VMs and not containers - following the guide . Specifically, restrict virt-controller , virt-api and virt-handler pods to only run on the nodes you want to be used for VMs. Assign host devices to virtual machines \u00b6 For each node in the cluster that we want to use with VMs we must: Identify all GPU cards we want to dedicate to be used by VMs. Map GPU cards for KubeVirt to pick up (called assigning host devices to a virtual machine ). Instructions for identifying GPU cards are operating-system-specific. For Ubuntu 20.04 run: lspci -nnk -d 10de: Search for GPU cards that are marked with the text Kernel driver in use . Save the PCI Address, for example: 10de:1e04 Important Once exposed, these GPUs cannot be used by regular pods. Only VMs. To expose the GPUs and map them to KubeVirt follow the instructions here . Specifically, run: kubectl edit kubevirt -n kubevirt -o yaml And add all of the PCI Addresses of all GPUs of all Nodes concatenated by commas, with the resource name kubevirt/vmgpu : spec : certificateRotateStrategy : {} configuration : developerConfiguration : featureGates : - GPU - HostDevices permittedHostDevices : pciHostDevices : - pciVendorSelector : <PCI-ADDRESS>,<PCI-ADDRESS>, resourceName : kubevirt/vmgpu Assign GPUs to VMs \u00b6 You must create a CRD called vm for each virtual machine. vm is a reference to a virtual machine and its capabilities. The Run:ai project is matched to a Kubernetes namespace. Unless manually configured, the namespace is runai-<PROJECT-NAME> . Per Run:ai Project , create a vm object. See KubeVirt documentation example. Specifically, the created YAML should look like this: spec : running : false template : metadata : creationTimestamp : null labels : .... priorityClassName : <WORKLOAD-TYPE> project : <PROJECT-NAME> spec : schedulerName : runai-scheduler domain : devices : gpus : - deviceName : kubevirt/vmgpu # identical name to resourceName above name : gpu1 # name here is arbitrary and is not used. Where <WORKLOAD-TYPE> is train or build Turn on Kubevirt feature in Runai \u00b6 If you want to upgrade the runai cluster, use the instructions . During the upgrade, customize the cluster installation by adding the following to the values.yaml file: global : kubevirtCluster : enabled : true If you don't want to upgrade the whole cluster, you can add those values to your existing values.yaml file. Then, run the command: helm upgrade runai-cluster runai/runai-cluster -n runai -f values.yaml Make sure the kubevirtCluster: enabled flag is still turned on in runaiconfig : kubectl edit runaiconfig runai -n runai Start a VM \u00b6 Run: virtctl start testvm -n runai-test You can now see the VMs pod in Run:ai. runai list -A NAME STATUS AGE NODE IMAGE TYPE PROJECT USER GPUs Allocated (Requested) PODs Running (Pending) SERVICE URL(S) testvm Running 0s master-node quay.io/kubevirt/virt-launcher:v0.47.1 test 1 (1) 1 (0)","title":"KubeVirt (VM)"},{"location":"admin/integration/kubevirt/#scheduling-virtual-machines-using-runai","text":"Many organizations use virtual machines (VMs) to provide operating system abstraction to users. Containers are different than VMs but serve a similar purpose. Containers at a large scale are best managed by Kubernetes and Run:ai is based on Kubernetes. It is possible to mix and match containers and VMs to some extent using a technology called KubeVirt . KubeVirt allows running VMs inside containers on top of Kubernetes. This article describes how to use Kubevirt to schedule VMs with GPUs.","title":"Scheduling Virtual Machines using Run:ai"},{"location":"admin/integration/kubevirt/#limitations","text":"Each node in the cluster will be able to support either VMs or containers - not combined. GPU fractions are not supported.","title":"Limitations"},{"location":"admin/integration/kubevirt/#preparations","text":"Making GPUs visible to VMs is not trivial. It requires either a license for NVIDIA software called NVIDIA vGPU or creating a GPU passthrough by the explicit mapping of GPU devices to virtual machines. This guide relates to the latter option.","title":"Preparations"},{"location":"admin/integration/kubevirt/#install-kubevirt","text":"Install KubeVirt using the following guide .","title":"Install KubeVirt"},{"location":"admin/integration/kubevirt/#dedicate-specific-nodes-for-vms","text":"Dedicate specific nodes within the cluster to be used for VMs and not containers - following the guide . Specifically, restrict virt-controller , virt-api and virt-handler pods to only run on the nodes you want to be used for VMs.","title":"Dedicate specific nodes for VMs"},{"location":"admin/integration/kubevirt/#assign-host-devices-to-virtual-machines","text":"For each node in the cluster that we want to use with VMs we must: Identify all GPU cards we want to dedicate to be used by VMs. Map GPU cards for KubeVirt to pick up (called assigning host devices to a virtual machine ). Instructions for identifying GPU cards are operating-system-specific. For Ubuntu 20.04 run: lspci -nnk -d 10de: Search for GPU cards that are marked with the text Kernel driver in use . Save the PCI Address, for example: 10de:1e04 Important Once exposed, these GPUs cannot be used by regular pods. Only VMs. To expose the GPUs and map them to KubeVirt follow the instructions here . Specifically, run: kubectl edit kubevirt -n kubevirt -o yaml And add all of the PCI Addresses of all GPUs of all Nodes concatenated by commas, with the resource name kubevirt/vmgpu : spec : certificateRotateStrategy : {} configuration : developerConfiguration : featureGates : - GPU - HostDevices permittedHostDevices : pciHostDevices : - pciVendorSelector : <PCI-ADDRESS>,<PCI-ADDRESS>, resourceName : kubevirt/vmgpu","title":"Assign host devices to virtual machines"},{"location":"admin/integration/kubevirt/#assign-gpus-to-vms","text":"You must create a CRD called vm for each virtual machine. vm is a reference to a virtual machine and its capabilities. The Run:ai project is matched to a Kubernetes namespace. Unless manually configured, the namespace is runai-<PROJECT-NAME> . Per Run:ai Project , create a vm object. See KubeVirt documentation example. Specifically, the created YAML should look like this: spec : running : false template : metadata : creationTimestamp : null labels : .... priorityClassName : <WORKLOAD-TYPE> project : <PROJECT-NAME> spec : schedulerName : runai-scheduler domain : devices : gpus : - deviceName : kubevirt/vmgpu # identical name to resourceName above name : gpu1 # name here is arbitrary and is not used. Where <WORKLOAD-TYPE> is train or build","title":"Assign GPUs to VMs"},{"location":"admin/integration/kubevirt/#turn-on-kubevirt-feature-in-runai","text":"If you want to upgrade the runai cluster, use the instructions . During the upgrade, customize the cluster installation by adding the following to the values.yaml file: global : kubevirtCluster : enabled : true If you don't want to upgrade the whole cluster, you can add those values to your existing values.yaml file. Then, run the command: helm upgrade runai-cluster runai/runai-cluster -n runai -f values.yaml Make sure the kubevirtCluster: enabled flag is still turned on in runaiconfig : kubectl edit runaiconfig runai -n runai","title":"Turn on Kubevirt feature in Runai"},{"location":"admin/integration/kubevirt/#start-a-vm","text":"Run: virtctl start testvm -n runai-test You can now see the VMs pod in Run:ai. runai list -A NAME STATUS AGE NODE IMAGE TYPE PROJECT USER GPUs Allocated (Requested) PODs Running (Pending) SERVICE URL(S) testvm Running 0s master-node quay.io/kubevirt/virt-launcher:v0.47.1 test 1 (1) 1 (0)","title":"Start a VM"},{"location":"admin/integration/mlflow/","text":"Integrate Run:ai with MLflow \u00b6 MLflow is an open-source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. The purpose of this document is to explain how to run Jobs with MLflow using the Run:ai scheduler. Overview \u00b6 MLflow concepts and alternative architectures are discussed here . MLflow can run on various platforms. To work with Run:ai we would use the MLflow Kubernetes integration . The MLflow documentation describes the Kubernetes integration as such: Quote When you run an MLflow Project on Kubernetes, MLflow constructs a new Docker image containing the Project\u2019s contents; this image inherits from the Project\u2019s Docker environment. MLflow then pushes the new Project image to your specified Docker registry and starts a Kubernetes Job on your specified Kubernetes cluster. This Kubernetes Job downloads the Project image and starts a corresponding Docker container. Finally, the container invokes your Project\u2019s entry point, logging parameters, tags, metrics, and artifacts to your MLflow tracking server. To run an MLflow job via Kubernetes, you specify an MLflow Kubernetes configuration file that contains a template. Here is an example from the MLflow documentation: { \"kube-context\" : ... , \"repository-uri\" : ... , \"kube-job-template-path\" : \"/username/path/to/kubernetes_job_template.yaml\" } The essence of the Run:ai integration is the modification of the kubernetes_job_template.yaml file. Specifically adding the Run:ai scheduler name and the Run:ai Project (Kubernetes namespace). Step by Step Instructions \u00b6 Prerequisites \u00b6 Install MLflow . Make sure you have push access to a Docker repository from your local machine. Make sure you are connected to Run:ai via the Run:ai Command-line interface. The sample MLflow Project \u00b6 The relevant sample files are here . These contain: A Dockerfile . This file builds a base docker image containing python3 and the required MLflow dependencies. The Docker file is already compiled and available at gcr.io/run-ai-demo/mlflow-demo . An MLflow project file MLproject . The project file contains the base image above as well as the python command-line to run. The training python code train.py MLflow Kubernetes configuration files as in the MLflow documentation . Kubernetes configuration file kubernetes_config.json An MLflow Kubernetes Job template kubernetes_job_template.yaml Preparations \u00b6 Edit kubernetes_config.json . Set kube-context to the name of the Kubernetes context. You can find the context name by running runai list clusters or kubectl config get-contexts . Set repository-uri to a repository and name of a docker image that will be used by MLflow (this is a different image than the base docker image described above). Your local machine needs permissions to be able to push this image to the Docker registry. Edit kubernetes_job_template.yaml . Set the value of namespace to runai-<name of Run:ai project> . Note the last line which adds the Run:ai scheduler to the configuration. Do not change the lines marked by {replaced with... . Set the requested resources including GPUs. You can use the --dry-run flag of the runai submit command to gain insight on additional configurations Running \u00b6 Perform docker login if required. Run: mlflow run mlproject -P alpha=5.0 -P l1-ratio=0.1 \\ --backend kubernetes --backend-config kubernetes_config.json MLflow Tracking \u00b6 The sample training code above does not contain references to an MLflow tracking server. This has been done to simplify the required setup. With MLflow-Kubernetes you will need a remote server architecture . Once you have such an architecture set up, you can use MLflow Tracking in your code. Using Interactive Workloads \u00b6 With Run:ai you can also run interactive workloads. To run the Job as interactive, add the following to kubernetes_job_template.yaml : metadata : labels : priorityClassName : \"build\" See Also \u00b6 You can use MLflow together with Fractional GPUs. For more information see Launch Job via YAML . To map additional Run:ai options to the YAML, see Launch Job via YAML .","title":"MLflow"},{"location":"admin/integration/mlflow/#integrate-runai-with-mlflow","text":"MLflow is an open-source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. The purpose of this document is to explain how to run Jobs with MLflow using the Run:ai scheduler.","title":"Integrate Run:ai with MLflow"},{"location":"admin/integration/mlflow/#overview","text":"MLflow concepts and alternative architectures are discussed here . MLflow can run on various platforms. To work with Run:ai we would use the MLflow Kubernetes integration . The MLflow documentation describes the Kubernetes integration as such: Quote When you run an MLflow Project on Kubernetes, MLflow constructs a new Docker image containing the Project\u2019s contents; this image inherits from the Project\u2019s Docker environment. MLflow then pushes the new Project image to your specified Docker registry and starts a Kubernetes Job on your specified Kubernetes cluster. This Kubernetes Job downloads the Project image and starts a corresponding Docker container. Finally, the container invokes your Project\u2019s entry point, logging parameters, tags, metrics, and artifacts to your MLflow tracking server. To run an MLflow job via Kubernetes, you specify an MLflow Kubernetes configuration file that contains a template. Here is an example from the MLflow documentation: { \"kube-context\" : ... , \"repository-uri\" : ... , \"kube-job-template-path\" : \"/username/path/to/kubernetes_job_template.yaml\" } The essence of the Run:ai integration is the modification of the kubernetes_job_template.yaml file. Specifically adding the Run:ai scheduler name and the Run:ai Project (Kubernetes namespace).","title":"Overview"},{"location":"admin/integration/mlflow/#step-by-step-instructions","text":"","title":"Step by Step Instructions"},{"location":"admin/integration/mlflow/#prerequisites","text":"Install MLflow . Make sure you have push access to a Docker repository from your local machine. Make sure you are connected to Run:ai via the Run:ai Command-line interface.","title":"Prerequisites"},{"location":"admin/integration/mlflow/#the-sample-mlflow-project","text":"The relevant sample files are here . These contain: A Dockerfile . This file builds a base docker image containing python3 and the required MLflow dependencies. The Docker file is already compiled and available at gcr.io/run-ai-demo/mlflow-demo . An MLflow project file MLproject . The project file contains the base image above as well as the python command-line to run. The training python code train.py MLflow Kubernetes configuration files as in the MLflow documentation . Kubernetes configuration file kubernetes_config.json An MLflow Kubernetes Job template kubernetes_job_template.yaml","title":"The sample MLflow Project"},{"location":"admin/integration/mlflow/#preparations","text":"Edit kubernetes_config.json . Set kube-context to the name of the Kubernetes context. You can find the context name by running runai list clusters or kubectl config get-contexts . Set repository-uri to a repository and name of a docker image that will be used by MLflow (this is a different image than the base docker image described above). Your local machine needs permissions to be able to push this image to the Docker registry. Edit kubernetes_job_template.yaml . Set the value of namespace to runai-<name of Run:ai project> . Note the last line which adds the Run:ai scheduler to the configuration. Do not change the lines marked by {replaced with... . Set the requested resources including GPUs. You can use the --dry-run flag of the runai submit command to gain insight on additional configurations","title":"Preparations"},{"location":"admin/integration/mlflow/#running","text":"Perform docker login if required. Run: mlflow run mlproject -P alpha=5.0 -P l1-ratio=0.1 \\ --backend kubernetes --backend-config kubernetes_config.json","title":"Running"},{"location":"admin/integration/mlflow/#mlflow-tracking","text":"The sample training code above does not contain references to an MLflow tracking server. This has been done to simplify the required setup. With MLflow-Kubernetes you will need a remote server architecture . Once you have such an architecture set up, you can use MLflow Tracking in your code.","title":"MLflow Tracking"},{"location":"admin/integration/mlflow/#using-interactive-workloads","text":"With Run:ai you can also run interactive workloads. To run the Job as interactive, add the following to kubernetes_job_template.yaml : metadata : labels : priorityClassName : \"build\"","title":"Using Interactive Workloads"},{"location":"admin/integration/mlflow/#see-also","text":"You can use MLflow together with Fractional GPUs. For more information see Launch Job via YAML . To map additional Run:ai options to the YAML, see Launch Job via YAML .","title":"See Also"},{"location":"admin/integration/seldon/","text":"Integrate Run:ai with Seldon Core \u00b6 Seldon Core is software that deploys machine learning models to production over Kubernetes. The purpose of this document is to explain how to use Seldon Core together with Run:ai. Of special importance, is the usage of Seldon together with the Run:ai fractions technology: Machine learning production tends to take less GPU Memory. As such, allocating a fraction of the GPU per job allows for better GPU Utilization. Prerequisites \u00b6 Install Seldon Core as described here . We recommend using the helm-based installation of both Seldon Core and Istio. Create a Seldon deployment \u00b6 The instructions below follow a sample machine learning model that tests the Run:ai - Seldon Core integration. Save the following in a file named <FILE-NAME>.yaml apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : seldon-model namespace : runai-<PROJECT-NAME> spec : name : test-deployment predictors : - componentSpecs : - spec : containers : - name : classifier image : seldonio/mock_classifier:1.5.0-dev resources : limits : nvidia.com/gpu : <GPUs> schedulerName : runai-scheduler graph : children : [] endpoint : type : REST name : classifier type : MODEL name : example replicas : 1 apiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: seldon-model namespace: runai- spec: name: test-deployment predictors: - componentSpecs: - spec: containers: - name: classifier image: seldonio/mock_classifier:1.0 resources: limits: nvidia.com/gpu: schedulerName: runai-scheduler graph: children: [] endpoint: type: REST name: classifier type: MODEL name: example replicas: 1 Replace <PROJECT-NAME> with the Run:ai projects and <GPUs> with the amount of GPUs you want to allocate (e.g. 0.5 GPUs). kubectl apply -f <FILE-NAME>.yaml Verification \u00b6 Run: runai list jobs and verify that the job is running Delete a deployment \u00b6 Run: kubectl delete -f <FILE-NAME>.yaml","title":"Seldon Core"},{"location":"admin/integration/seldon/#integrate-runai-with-seldon-core","text":"Seldon Core is software that deploys machine learning models to production over Kubernetes. The purpose of this document is to explain how to use Seldon Core together with Run:ai. Of special importance, is the usage of Seldon together with the Run:ai fractions technology: Machine learning production tends to take less GPU Memory. As such, allocating a fraction of the GPU per job allows for better GPU Utilization.","title":"Integrate Run:ai with Seldon Core"},{"location":"admin/integration/seldon/#prerequisites","text":"Install Seldon Core as described here . We recommend using the helm-based installation of both Seldon Core and Istio.","title":"Prerequisites"},{"location":"admin/integration/seldon/#create-a-seldon-deployment","text":"The instructions below follow a sample machine learning model that tests the Run:ai - Seldon Core integration. Save the following in a file named <FILE-NAME>.yaml apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : seldon-model namespace : runai-<PROJECT-NAME> spec : name : test-deployment predictors : - componentSpecs : - spec : containers : - name : classifier image : seldonio/mock_classifier:1.5.0-dev resources : limits : nvidia.com/gpu : <GPUs> schedulerName : runai-scheduler graph : children : [] endpoint : type : REST name : classifier type : MODEL name : example replicas : 1 apiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: seldon-model namespace: runai- spec: name: test-deployment predictors: - componentSpecs: - spec: containers: - name: classifier image: seldonio/mock_classifier:1.0 resources: limits: nvidia.com/gpu: schedulerName: runai-scheduler graph: children: [] endpoint: type: REST name: classifier type: MODEL name: example replicas: 1 Replace <PROJECT-NAME> with the Run:ai projects and <GPUs> with the amount of GPUs you want to allocate (e.g. 0.5 GPUs). kubectl apply -f <FILE-NAME>.yaml","title":"Create a Seldon deployment"},{"location":"admin/integration/seldon/#verification","text":"Run: runai list jobs and verify that the job is running","title":"Verification"},{"location":"admin/integration/seldon/#delete-a-deployment","text":"Run: kubectl delete -f <FILE-NAME>.yaml","title":"Delete a deployment"},{"location":"admin/integration/weights-and-biases/","text":"Workspace Weights and Biases \u00b6 Weights and Biases is a commercial tool that provides experiment tracking, model visualization, and collaboration for machine learning projects. It helps researchers and developers keep track of their experiments, visualize their results, and compare different models to make informed decisions. When Wights and Biases are integrated into Run:ai Workspaces, researchers can easily create their custom work environments and have access to a toolbox of many researcher-relevant tools in a single place. Researchers can now create useful connectivity between the running Workspace and the relevant project using Weights Biases for experiment tracking. To configure Weights and Biases: Login to your account in Weights and Biases . If you do not have a valid account, you will need to create one. Setup your Weights and Biases account here In your Run:ai account, create an environment and set Weights and Biases as a tool then: Link it to https://wandb-c.ACME.com/${WANDB_PROJECT} Add an environment variable: ```Key = WANDB_PROJECT``` ```Value =``` leave empty for researcher to fill it in when creating a Workspace in the next step. Create a Workspace using the Environment you just created. In the Workspace, add the URL for your project in your Weights and Biases account to the value environment variable. This will create a link, that will automatically open a new tab directly from your Workspace to your exact Weights and Biases project.","title":"Weights & Biases"},{"location":"admin/integration/weights-and-biases/#workspace-weights-and-biases","text":"Weights and Biases is a commercial tool that provides experiment tracking, model visualization, and collaboration for machine learning projects. It helps researchers and developers keep track of their experiments, visualize their results, and compare different models to make informed decisions. When Wights and Biases are integrated into Run:ai Workspaces, researchers can easily create their custom work environments and have access to a toolbox of many researcher-relevant tools in a single place. Researchers can now create useful connectivity between the running Workspace and the relevant project using Weights Biases for experiment tracking. To configure Weights and Biases: Login to your account in Weights and Biases . If you do not have a valid account, you will need to create one. Setup your Weights and Biases account here In your Run:ai account, create an environment and set Weights and Biases as a tool then: Link it to https://wandb-c.ACME.com/${WANDB_PROJECT} Add an environment variable: ```Key = WANDB_PROJECT``` ```Value =``` leave empty for researcher to fill it in when creating a Workspace in the next step. Create a Workspace using the Environment you just created. In the Workspace, add the URL for your project in your Weights and Biases account to the value environment variable. This will create a link, that will automatically open a new tab directly from your Workspace to your exact Weights and Biases project.","title":"Workspace Weights and Biases"},{"location":"admin/researcher-setup/cli-install/","text":"Install the Run:ai Command-line Interface \u00b6 The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc. The instructions below will guide you through the process of installing the CLI. The Run:ai CLI runs on Mac and Linux. You can run the CLI on Windows by using Docker for Windows. See the end of this document. Researcher Authentication \u00b6 When enabled, Researcher authentication requires additional setup when installing the CLI. To configure authentication see Setup Project-based Researcher Access Control . Use the modified Kubernetes configuration file described in the article. Prerequisites \u00b6 When installing the command-line interface, it is worth considering future upgrades: Install the CLI on a dedicated Jumpbox machine. Researchers will connect to the Jumpbox from which they can submit Run:ai commands Install the CLI on a shared directory that is mounted on Researchers' machines. A Kubernetes configuration file . Setup \u00b6 Kubernetes Configuration \u00b6 On the Researcher's root folder, create a directory .kube . Copy the Kubernetes configuration file into the directory. Each Researcher should have a separate copy of the configuration file. The Researcher should have write access to the configuration file as it stores user defaults. If you choose to locate the file at a different location than ~/.kube/config , you must create a shell variable to point to the configuration file as follows: export KUBECONFIG=<Kubernetes-config-file> Test the connection by running: kubectl get nodes Install Run:ai CLI \u00b6 Mac or Linux Windows (Run:ai Version 2.9) Windows (Run:ai Version 2.8 or lower) Go to the Run:ai user interface. On the top right select Researcher Command Line Interface . Select Mac or Linux . Download directly using the button or copy the command and run it on a remote machine Run: chmod +x runai sudo mv runai /usr/local/bin/runai Go to the Run:ai user interface. On the top right select Researcher Command Line Interface . Select Windows Download directly using the button or copy the command and run it on a remote machine Rename the downloaded file to have a .exe extension and move the file to a folder that is a part of the PATH . Install Docker for Windows . Get the following folder from GitHub: https://github.com/run-ai/docs/tree/master/cli/windows . Replace config with your Kubernetes Configuration file. Replace <CLUSTER-URL> in the Dockerfile with the URL of the cluster. The URL can be found in the Clusters view of the Run:ai user interface. Run: build.sh to create a docker image named runai-cli . Test the image by running: docker run -it runai-cli bash Try and connect to your cluster from inside the docker by running a Run:ai CLI command. E.g. runai list projects . Distribute the image to Windows users. In case you want to use port-forward feature please use the following command docker run -it -p <PORT>:<PORT> runai-cli bash And when using runai submit command add the following flag: --address 0.0.0.0 Note An alternative way of downloading the CLI is provided under the CLI Troubleshooting section. To verify the installation run: runai list jobs Install Command Auto-Completion \u00b6 It is possible to configure your Linux/Mac shell to complete Run:ai CLI commands. This feature works on bash and zsh shells only. Zsh \u00b6 Edit the file ~/.zshrc . Add the lines: autoload -U compinit; compinit -i source <(runai completion zsh) Bash \u00b6 Install the bash-completion package: Mac: brew install bash-completion Ubuntu/Debian: sudo apt-get install bash-completion Fedora/Centos: sudo yum install bash-completion Edit the file ~/.bashrc . Add the lines: [[ -r \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d ]] && . \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d source < ( runai completion bash ) Troubleshoot the CLI Installation \u00b6 See Troubleshooting a CLI installation Update the Run:ai CLI \u00b6 To update the CLI to the latest version perform the same install process again. Delete the Run:ai CLI \u00b6 If you have installed using the default path, run: sudo rm /usr/local/bin/runai","title":"Install the CLI"},{"location":"admin/researcher-setup/cli-install/#install-the-runai-command-line-interface","text":"The Run:ai Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc. The instructions below will guide you through the process of installing the CLI. The Run:ai CLI runs on Mac and Linux. You can run the CLI on Windows by using Docker for Windows. See the end of this document.","title":"Install the Run:ai Command-line Interface"},{"location":"admin/researcher-setup/cli-install/#researcher-authentication","text":"When enabled, Researcher authentication requires additional setup when installing the CLI. To configure authentication see Setup Project-based Researcher Access Control . Use the modified Kubernetes configuration file described in the article.","title":"Researcher Authentication"},{"location":"admin/researcher-setup/cli-install/#prerequisites","text":"When installing the command-line interface, it is worth considering future upgrades: Install the CLI on a dedicated Jumpbox machine. Researchers will connect to the Jumpbox from which they can submit Run:ai commands Install the CLI on a shared directory that is mounted on Researchers' machines. A Kubernetes configuration file .","title":"Prerequisites"},{"location":"admin/researcher-setup/cli-install/#setup","text":"","title":"Setup"},{"location":"admin/researcher-setup/cli-install/#kubernetes-configuration","text":"On the Researcher's root folder, create a directory .kube . Copy the Kubernetes configuration file into the directory. Each Researcher should have a separate copy of the configuration file. The Researcher should have write access to the configuration file as it stores user defaults. If you choose to locate the file at a different location than ~/.kube/config , you must create a shell variable to point to the configuration file as follows: export KUBECONFIG=<Kubernetes-config-file> Test the connection by running: kubectl get nodes","title":"Kubernetes Configuration"},{"location":"admin/researcher-setup/cli-install/#install-runai-cli","text":"Mac or Linux Windows (Run:ai Version 2.9) Windows (Run:ai Version 2.8 or lower) Go to the Run:ai user interface. On the top right select Researcher Command Line Interface . Select Mac or Linux . Download directly using the button or copy the command and run it on a remote machine Run: chmod +x runai sudo mv runai /usr/local/bin/runai Go to the Run:ai user interface. On the top right select Researcher Command Line Interface . Select Windows Download directly using the button or copy the command and run it on a remote machine Rename the downloaded file to have a .exe extension and move the file to a folder that is a part of the PATH . Install Docker for Windows . Get the following folder from GitHub: https://github.com/run-ai/docs/tree/master/cli/windows . Replace config with your Kubernetes Configuration file. Replace <CLUSTER-URL> in the Dockerfile with the URL of the cluster. The URL can be found in the Clusters view of the Run:ai user interface. Run: build.sh to create a docker image named runai-cli . Test the image by running: docker run -it runai-cli bash Try and connect to your cluster from inside the docker by running a Run:ai CLI command. E.g. runai list projects . Distribute the image to Windows users. In case you want to use port-forward feature please use the following command docker run -it -p <PORT>:<PORT> runai-cli bash And when using runai submit command add the following flag: --address 0.0.0.0 Note An alternative way of downloading the CLI is provided under the CLI Troubleshooting section. To verify the installation run: runai list jobs","title":"Install Run:ai CLI"},{"location":"admin/researcher-setup/cli-install/#install-command-auto-completion","text":"It is possible to configure your Linux/Mac shell to complete Run:ai CLI commands. This feature works on bash and zsh shells only.","title":"Install Command Auto-Completion"},{"location":"admin/researcher-setup/cli-install/#zsh","text":"Edit the file ~/.zshrc . Add the lines: autoload -U compinit; compinit -i source <(runai completion zsh)","title":"Zsh"},{"location":"admin/researcher-setup/cli-install/#bash","text":"Install the bash-completion package: Mac: brew install bash-completion Ubuntu/Debian: sudo apt-get install bash-completion Fedora/Centos: sudo yum install bash-completion Edit the file ~/.bashrc . Add the lines: [[ -r \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d ]] && . \u201c/usr/local/etc/profile.d/bash_completion.sh\u201d source < ( runai completion bash )","title":"Bash"},{"location":"admin/researcher-setup/cli-install/#troubleshoot-the-cli-installation","text":"See Troubleshooting a CLI installation","title":"Troubleshoot the CLI Installation"},{"location":"admin/researcher-setup/cli-install/#update-the-runai-cli","text":"To update the CLI to the latest version perform the same install process again.","title":"Update the Run:ai CLI"},{"location":"admin/researcher-setup/cli-install/#delete-the-runai-cli","text":"If you have installed using the default path, run: sudo rm /usr/local/bin/runai","title":"Delete the Run:ai CLI"},{"location":"admin/researcher-setup/docker-registry-config/","text":"Using a Docker Registry with Credentials \u00b6 Why? \u00b6 Some Docker images are stored in private docker registries. For the Researcher to access the images, we will need to provide credentials for the registry. How? \u00b6 There could be two business scenarios: All researchers use single credentials for the registry. There exist separate registry credentials per Run:ai Project. Single Credentials \u00b6 For each private registry you must perform the following (The example below uses Docker Hub): kubectl create secret docker-registry <secret_name> -n runai \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=<user_name> --docker-password=<password> Then: kubectl label secret <secret_name> runai/cluster-wide=\"true\" -n runai <secret_name> may be any arbitrary string <user_name> and <password> are the repository user and password Notes The secret may take up to a minute to update in the system. The above scheme relies on the cluster setting clusterWideSecret to be set to true Credentials per Project \u00b6 For each Run:ai Project create a secret: kubectl create secret docker-registry <secret_name> -n <NAMESPACE> \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=<user_name> --docker-password=<password> Where <NAMESPACE> is the namespace associated with the Project (typically its runai-<PROJECT-NAME> ). Then apply the secret to Run:ai by running: kubectl patch serviceaccount default -n <NAMESPACE> -p '{\"imagePullSecrets\": [{\"name\": \"<secret_name>\"}]}' Google Cloud Registry \u00b6 Follow the steps below to access private images in the Google Container Registry (GCR): Create a service-account in GCP. Provide it Viewer permissions and download a JSON key. Under GCR, go to image and locate the domain name. Example GCR domains can be gcr.io , eu.gcr.io etc. On your local machine, log in to docker with the new credentials: docker login -u _json_key -p \"$(cat <config.json>)\" <gcr-domain> Where <gcr-domain> is the GCR domain we have located, <config.json> is the GCP configuration file. This will generate an entry for the GCR domain in your ~/.docker/config.json file . Open the ~/.docker/config.json file. Copy the JSON structure under the GCR domain into a new file called ~/docker-config.json . When doing so, take care to remove all newlines . For example: {\"https://eu.gcr.io\": { \"auth\": \"<key>\"}} Convert the file into base64: cat ~/docker-config.json | base64 Create a new file called secret.yaml : apiVersion : v1 kind : Secret metadata : name : gcr-secret namespace : runai labels : runai/cluster-wide : \"true\" data : .dockerconfigjson : << PASTE_HERE_THE_LONG_BASE64_ENCODED_STRING >> type : kubernetes.io/dockerconfigjson Apply to Kubernetes by running the command: kubectl create -f ~/secret.yaml Test your settings by submitting a which references an image from the GCR repository","title":"Use a Docker Registry with Credentials"},{"location":"admin/researcher-setup/docker-registry-config/#using-a-docker-registry-with-credentials","text":"","title":"Using a Docker Registry with Credentials"},{"location":"admin/researcher-setup/docker-registry-config/#why","text":"Some Docker images are stored in private docker registries. For the Researcher to access the images, we will need to provide credentials for the registry.","title":"Why?"},{"location":"admin/researcher-setup/docker-registry-config/#how","text":"There could be two business scenarios: All researchers use single credentials for the registry. There exist separate registry credentials per Run:ai Project.","title":"How?"},{"location":"admin/researcher-setup/docker-registry-config/#single-credentials","text":"For each private registry you must perform the following (The example below uses Docker Hub): kubectl create secret docker-registry <secret_name> -n runai \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=<user_name> --docker-password=<password> Then: kubectl label secret <secret_name> runai/cluster-wide=\"true\" -n runai <secret_name> may be any arbitrary string <user_name> and <password> are the repository user and password Notes The secret may take up to a minute to update in the system. The above scheme relies on the cluster setting clusterWideSecret to be set to true","title":"Single Credentials"},{"location":"admin/researcher-setup/docker-registry-config/#credentials-per-project","text":"For each Run:ai Project create a secret: kubectl create secret docker-registry <secret_name> -n <NAMESPACE> \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=<user_name> --docker-password=<password> Where <NAMESPACE> is the namespace associated with the Project (typically its runai-<PROJECT-NAME> ). Then apply the secret to Run:ai by running: kubectl patch serviceaccount default -n <NAMESPACE> -p '{\"imagePullSecrets\": [{\"name\": \"<secret_name>\"}]}'","title":"Credentials per Project"},{"location":"admin/researcher-setup/docker-registry-config/#google-cloud-registry","text":"Follow the steps below to access private images in the Google Container Registry (GCR): Create a service-account in GCP. Provide it Viewer permissions and download a JSON key. Under GCR, go to image and locate the domain name. Example GCR domains can be gcr.io , eu.gcr.io etc. On your local machine, log in to docker with the new credentials: docker login -u _json_key -p \"$(cat <config.json>)\" <gcr-domain> Where <gcr-domain> is the GCR domain we have located, <config.json> is the GCP configuration file. This will generate an entry for the GCR domain in your ~/.docker/config.json file . Open the ~/.docker/config.json file. Copy the JSON structure under the GCR domain into a new file called ~/docker-config.json . When doing so, take care to remove all newlines . For example: {\"https://eu.gcr.io\": { \"auth\": \"<key>\"}} Convert the file into base64: cat ~/docker-config.json | base64 Create a new file called secret.yaml : apiVersion : v1 kind : Secret metadata : name : gcr-secret namespace : runai labels : runai/cluster-wide : \"true\" data : .dockerconfigjson : << PASTE_HERE_THE_LONG_BASE64_ENCODED_STRING >> type : kubernetes.io/dockerconfigjson Apply to Kubernetes by running the command: kubectl create -f ~/secret.yaml Test your settings by submitting a which references an image from the GCR repository","title":"Google Cloud Registry"},{"location":"admin/researcher-setup/docker-to-runai/","text":"Dockers, Images, and Kubernetes \u00b6 Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image . You create a container by starting a docker image on a machine. Run:ai is based on Kubernetes . At its core, Kubernetes is an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the Researcher's workflow as follows. Image Repository \u00b6 If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when runai submit workloads, the Researcher must use the flag --local-image ). If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself. It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub . Alternatively, the organization can install a private repository on-prem. Day-to-day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, nvcr.io/nvidia/pytorch:19.12-py_3 is a PyTorch image that is located in nvcr.io . This is the Nvidia image repository as found on the web. Data \u00b6 Deep learning is about data. It can be your code, the training data, saved checkpoints, etc. If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself. If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command). Working with Containers \u00b6 Starting a container using docker usually involves a single command-line with multiple flags. A typical example: docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME='hostname' \\ -v /raid/public/my_datasets:/root/dataset:ro -i nvcr.io/nvidia/pytorch:19.12-py3 The docker command docker run should be replaced with a Run:ai command runai submit . The flags are usually the same but some adaptation is required. A complete list of flags can be found here: runai submit . There are similar commands to get a shell into the container ( runai bash ), get the container logs ( runai logs ), and more. For a complete list see the Run:ai CLI reference . Schedule an Onboarding Session \u00b6 It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline Researchers' work as well as save money for the organization.","title":"From Docker to Run:ai "},{"location":"admin/researcher-setup/docker-to-runai/#dockers-images-and-kubernetes","text":"Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image . You create a container by starting a docker image on a machine. Run:ai is based on Kubernetes . At its core, Kubernetes is an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the Researcher's workflow as follows.","title":"Dockers, Images, and Kubernetes"},{"location":"admin/researcher-setup/docker-to-runai/#image-repository","text":"If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when runai submit workloads, the Researcher must use the flag --local-image ). If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself. It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub . Alternatively, the organization can install a private repository on-prem. Day-to-day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, nvcr.io/nvidia/pytorch:19.12-py_3 is a PyTorch image that is located in nvcr.io . This is the Nvidia image repository as found on the web.","title":"Image Repository"},{"location":"admin/researcher-setup/docker-to-runai/#data","text":"Deep learning is about data. It can be your code, the training data, saved checkpoints, etc. If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself. If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command).","title":"Data"},{"location":"admin/researcher-setup/docker-to-runai/#working-with-containers","text":"Starting a container using docker usually involves a single command-line with multiple flags. A typical example: docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME='hostname' \\ -v /raid/public/my_datasets:/root/dataset:ro -i nvcr.io/nvidia/pytorch:19.12-py3 The docker command docker run should be replaced with a Run:ai command runai submit . The flags are usually the same but some adaptation is required. A complete list of flags can be found here: runai submit . There are similar commands to get a shell into the container ( runai bash ), get the container logs ( runai logs ), and more. For a complete list see the Run:ai CLI reference .","title":"Working with Containers"},{"location":"admin/researcher-setup/docker-to-runai/#schedule-an-onboarding-session","text":"It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline Researchers' work as well as save money for the organization.","title":"Schedule an Onboarding Session"},{"location":"admin/researcher-setup/limit-to-node-group/","text":"Why? \u00b6 In some business scenarios, you may want to direct the Run:ai scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions, Hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Another example is an inference workload that is optimized to a specific GPU type and must have dedicated resources reserved to ensure enough capacity. Run:ai provides two methods to designate, and group, specific resources: Node Pools: Run:ai allows administrators to group specific nodes into a node pool. A node pool is a group of nodes identified by a given name (node pool name) and grouped by any label (key and value combination). The label can be chosen by the administrator or can be an existing, pre-set, label (such as an NVIDIA GPU type label). Node Affinity: Run:ai allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag --node-type <label> to force this allocation. Important One can set and use both node pool and node affinity combined as a prerequisite to the scheduler, for example, if a researcher wants to use a T4 node with an Infiniband card - he or she can use a node pool of T4 and from that group, choose only the nodes with Infiniband card (node-type = infiniband). There is a tradeoff in place when allowing Researchers to designate specific nodes. Overuse of this feature limits the scheduler in finding an optimal resource and thus reduces overall cluster utilization. Configuring Node Groups \u00b6 To configure a node pool: Find the label key & value you want to use for Run:ai to create the node pool. Check that the nodes you want to group as a pool have a unique label to use, otherwise you should mark those nodes with your own uniquely identifiable label. Get the names of the nodes you want Run:ai to group. To get a list of nodes, run: kubectl get nodes Kubectl get nodes --show-labels If you chose to set your own label, run the following: kubectl label node <node-name> <label-key>=<label-value> The same value can be set to a single node, or multiple nodes. Node Pool can only use one label (key & value) at a time. * To create a node pool use the create node pool Run:ai API. To configure a node affinity: Get the names of the nodes where you want to limit Run:ai. To get a list of nodes, run: kubectl get nodes For each node run the following: kubectl label node <node-name> run.ai/type=<label> The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value. Using Node Groups via the CLI \u00b6 To use Run:ai node pool with a workload, use Run:ai CLI command \u2018node-pool\u2019: runai submit job1 ... --node-pools \"my-pool\" ... To use Run:ai multi node pools with a workload, use Run:ai CLI command : runai submit job1 ... --node-pools \"my-pool my-pool2 my-pool3\" ... With Multi node pools the researcher creates a list of prioritized node-pools and let the scheduler to try and chose from any of the node pools in the list, according to the given priority. To use node affinity, use the node type label with the --node-type flag: runai submit job1 ... --node-type \"my-nodes\" A researcher may combine the two flags to select both a node pool and a specific set of nodes out of that node pool (e.g. gpu-type=t4 and node-type=infiniband): runai submit job1 ... --node-pool-name \u201cmy pool\u201d --node-type \"my-nodes\" Note When submitting a workload, if you chose a node pool label and a node affinity (node type) label which do not intersect, the Run:ai scheduler will not be able to schedule that workload as it represents an empty nodes group. See the runai submit documentation for further information. Assigning Node Groups to a Project \u00b6 Node Pools are automatically assigned to all Projects and Departments with zero resource allocation as default. Allocating resources to a node pool can be done for each Project and Department. Submitting a workload to a node pool that has zero allocation for a specific project (or department) results in that workload running as an over-quota workload. To assign and configure specific node affinity groups or node pools to a Project see working with Projects . When the command-line interface flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the Project.","title":"Group Nodes"},{"location":"admin/researcher-setup/limit-to-node-group/#why","text":"In some business scenarios, you may want to direct the Run:ai scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions, Hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Another example is an inference workload that is optimized to a specific GPU type and must have dedicated resources reserved to ensure enough capacity. Run:ai provides two methods to designate, and group, specific resources: Node Pools: Run:ai allows administrators to group specific nodes into a node pool. A node pool is a group of nodes identified by a given name (node pool name) and grouped by any label (key and value combination). The label can be chosen by the administrator or can be an existing, pre-set, label (such as an NVIDIA GPU type label). Node Affinity: Run:ai allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag --node-type <label> to force this allocation. Important One can set and use both node pool and node affinity combined as a prerequisite to the scheduler, for example, if a researcher wants to use a T4 node with an Infiniband card - he or she can use a node pool of T4 and from that group, choose only the nodes with Infiniband card (node-type = infiniband). There is a tradeoff in place when allowing Researchers to designate specific nodes. Overuse of this feature limits the scheduler in finding an optimal resource and thus reduces overall cluster utilization.","title":"Why?"},{"location":"admin/researcher-setup/limit-to-node-group/#configuring-node-groups","text":"To configure a node pool: Find the label key & value you want to use for Run:ai to create the node pool. Check that the nodes you want to group as a pool have a unique label to use, otherwise you should mark those nodes with your own uniquely identifiable label. Get the names of the nodes you want Run:ai to group. To get a list of nodes, run: kubectl get nodes Kubectl get nodes --show-labels If you chose to set your own label, run the following: kubectl label node <node-name> <label-key>=<label-value> The same value can be set to a single node, or multiple nodes. Node Pool can only use one label (key & value) at a time. * To create a node pool use the create node pool Run:ai API. To configure a node affinity: Get the names of the nodes where you want to limit Run:ai. To get a list of nodes, run: kubectl get nodes For each node run the following: kubectl label node <node-name> run.ai/type=<label> The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value.","title":"Configuring Node Groups"},{"location":"admin/researcher-setup/limit-to-node-group/#using-node-groups-via-the-cli","text":"To use Run:ai node pool with a workload, use Run:ai CLI command \u2018node-pool\u2019: runai submit job1 ... --node-pools \"my-pool\" ... To use Run:ai multi node pools with a workload, use Run:ai CLI command : runai submit job1 ... --node-pools \"my-pool my-pool2 my-pool3\" ... With Multi node pools the researcher creates a list of prioritized node-pools and let the scheduler to try and chose from any of the node pools in the list, according to the given priority. To use node affinity, use the node type label with the --node-type flag: runai submit job1 ... --node-type \"my-nodes\" A researcher may combine the two flags to select both a node pool and a specific set of nodes out of that node pool (e.g. gpu-type=t4 and node-type=infiniband): runai submit job1 ... --node-pool-name \u201cmy pool\u201d --node-type \"my-nodes\" Note When submitting a workload, if you chose a node pool label and a node affinity (node type) label which do not intersect, the Run:ai scheduler will not be able to schedule that workload as it represents an empty nodes group. See the runai submit documentation for further information.","title":"Using Node Groups via the CLI"},{"location":"admin/researcher-setup/limit-to-node-group/#assigning-node-groups-to-a-project","text":"Node Pools are automatically assigned to all Projects and Departments with zero resource allocation as default. Allocating resources to a node pool can be done for each Project and Department. Submitting a workload to a node pool that has zero allocation for a specific project (or department) results in that workload running as an over-quota workload. To assign and configure specific node affinity groups or node pools to a Project see working with Projects . When the command-line interface flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the Project.","title":"Assigning Node Groups to a Project"},{"location":"admin/researcher-setup/researcher-setup-intro/","text":"Following is a step-by-step guide for getting a new Researcher up to speed with Run:ai and Kubernetes. Change of Paradigms: from Docker to Kubernetes \u00b6 As part of Run:ai, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the Researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:ai CLI. Setup the Run:ai Command-Line Interface \u00b6 Run:ai CLI needs to be installed on the Researcher's machine. This document provides step by step instructions. Provide the Researcher with a GPU Quota \u00b6 To submit workloads with Run:ai, the Researcher must be provided with a Project that contains a GPU quota. Please see Working with Projects document on how to create Projects and set a quota. Provide access to the Run:ai User Interface \u00b6 See Setting up users for further information on how to provide access to users. Schedule an Onboarding Session \u00b6 It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline the Researchers' work as well as save money for the organization.","title":"Introduction"},{"location":"admin/researcher-setup/researcher-setup-intro/#change-of-paradigms-from-docker-to-kubernetes","text":"As part of Run:ai, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the Researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:ai CLI.","title":"Change of Paradigms: from Docker to Kubernetes"},{"location":"admin/researcher-setup/researcher-setup-intro/#setup-the-runai-command-line-interface","text":"Run:ai CLI needs to be installed on the Researcher's machine. This document provides step by step instructions.","title":"Setup the Run:ai Command-Line Interface"},{"location":"admin/researcher-setup/researcher-setup-intro/#provide-the-researcher-with-a-gpu-quota","text":"To submit workloads with Run:ai, the Researcher must be provided with a Project that contains a GPU quota. Please see Working with Projects document on how to create Projects and set a quota.","title":"Provide the Researcher with a GPU Quota"},{"location":"admin/researcher-setup/researcher-setup-intro/#provide-access-to-the-runai-user-interface","text":"See Setting up users for further information on how to provide access to users.","title":"Provide access to the Run:ai User Interface"},{"location":"admin/researcher-setup/researcher-setup-intro/#schedule-an-onboarding-session","text":"It is highly recommended to schedule an onboarding session for Researchers with a Run:ai customer success professional. Run:ai can help with the above transition, but adding to that, we at Run:ai have also acquired a large body of knowledge on data science best practices which can help streamline the Researchers' work as well as save money for the organization.","title":"Schedule an Onboarding Session"},{"location":"admin/runai-setup/installation-types/","text":"Installation Types \u00b6 Run:ai consists of two components: The Run:ai Cluster . One or more data-science GPU clusters hosted by the customer (on-prem or cloud). The Run:ai Control plane . A single entity that monitors clusters, sets priorities, and business policies. There are two main installation options: Installation Type Description Classic (SaaS) Run:ai is installed on the customer's data science GPU clusters. The cluster connects to the Run:ai control plane on the cloud (https:// .run.ai). With this installation, the cluster requires an outbound connection to the Run:ai cloud. Self-hosted The Run:ai control plane is also installed in the customer's data center The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns. The self-hosted installation is priced differently. For further information please talk to Run:ai sales. Self-hosted Installation \u00b6 Run:ai self-hosting comes with two variants: Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet Self-hosting with Kubernetes vs OpenShift \u00b6 Kubernetes has many Certified Kubernetes Providers . Run:ai has been certified with several of them (see the Kubernetes prerequisites section). The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections: OpenShift-based installation. See Run:ai OpenShift installation . Kubernetes-based installation. See Run:ai Kubernetes installation . Secure Installation \u00b6 In many organizations, Kubernetes is governed by IT compliance rules. In this scenario, there are strict access control rules during the installation and running of workloads: OpenShift is secured using Security Context Constraints (SCC). The Run:ai installation supports SCC. Kubernetes Pod Security Policy (PSP) has been deprecated by Kubernetes. Support for PSP will be removed on Run:ai versions higher than 2.8.","title":"Installation Types"},{"location":"admin/runai-setup/installation-types/#installation-types","text":"Run:ai consists of two components: The Run:ai Cluster . One or more data-science GPU clusters hosted by the customer (on-prem or cloud). The Run:ai Control plane . A single entity that monitors clusters, sets priorities, and business policies. There are two main installation options: Installation Type Description Classic (SaaS) Run:ai is installed on the customer's data science GPU clusters. The cluster connects to the Run:ai control plane on the cloud (https:// .run.ai). With this installation, the cluster requires an outbound connection to the Run:ai cloud. Self-hosted The Run:ai control plane is also installed in the customer's data center The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns. The self-hosted installation is priced differently. For further information please talk to Run:ai sales.","title":"Installation Types"},{"location":"admin/runai-setup/installation-types/#self-hosted-installation","text":"Run:ai self-hosting comes with two variants: Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet","title":"Self-hosted Installation"},{"location":"admin/runai-setup/installation-types/#self-hosting-with-kubernetes-vs-openshift","text":"Kubernetes has many Certified Kubernetes Providers . Run:ai has been certified with several of them (see the Kubernetes prerequisites section). The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections: OpenShift-based installation. See Run:ai OpenShift installation . Kubernetes-based installation. See Run:ai Kubernetes installation .","title":"Self-hosting with Kubernetes vs OpenShift"},{"location":"admin/runai-setup/installation-types/#secure-installation","text":"In many organizations, Kubernetes is governed by IT compliance rules. In this scenario, there are strict access control rules during the installation and running of workloads: OpenShift is secured using Security Context Constraints (SCC). The Run:ai installation supports SCC. Kubernetes Pod Security Policy (PSP) has been deprecated by Kubernetes. Support for PSP will be removed on Run:ai versions higher than 2.8.","title":"Secure Installation"},{"location":"admin/runai-setup/try-azure/","text":"Try Run:ai on Azure Cloud \u00b6 You can try Run:ai by starting a virtual machine on Azure. This option is currently limited to a single GPU node. To install a cluster with multiple nodes or for running a formal pilot with Run:ai, use Cluster Installation . Prerequisites \u00b6 You will need: An account in Azure with a quota for GPUs. Run:ai will work with any modern GPU. Tenant credentials and data, provided by Run:ai customer support. Create an instance in Azure \u00b6 Go to Run:ai Quickstart in the Azure marketplace. Press the \"Create\" button. Select a name, subscription, and machine size with GPUs. The machine should have at least 8 CPUs. Under the Advanced tab select Enable user data . Paste the user data provided by Run:ai customer support. It should be in the format: export RUNAI_TENANT = <tenant-name> export RUNAI_CLIENTID = <client-id> export RUNAI_SECRET = <secret> Create the machine. Use Run:ai \u00b6 Go to https://<tenant-name>.run.ai . Use credentials provided by Run:ai support. After ~30 minutes you should have a working Run:ai cluster. You can submit Jobs via the user interface. Command-line is not provided. Limitations \u00b6 This setup does not support single-sign-on .","title":"Try on the Cloud"},{"location":"admin/runai-setup/try-azure/#try-runai-on-azure-cloud","text":"You can try Run:ai by starting a virtual machine on Azure. This option is currently limited to a single GPU node. To install a cluster with multiple nodes or for running a formal pilot with Run:ai, use Cluster Installation .","title":"Try Run:ai on Azure Cloud"},{"location":"admin/runai-setup/try-azure/#prerequisites","text":"You will need: An account in Azure with a quota for GPUs. Run:ai will work with any modern GPU. Tenant credentials and data, provided by Run:ai customer support.","title":"Prerequisites"},{"location":"admin/runai-setup/try-azure/#create-an-instance-in-azure","text":"Go to Run:ai Quickstart in the Azure marketplace. Press the \"Create\" button. Select a name, subscription, and machine size with GPUs. The machine should have at least 8 CPUs. Under the Advanced tab select Enable user data . Paste the user data provided by Run:ai customer support. It should be in the format: export RUNAI_TENANT = <tenant-name> export RUNAI_CLIENTID = <client-id> export RUNAI_SECRET = <secret> Create the machine.","title":"Create an instance in Azure"},{"location":"admin/runai-setup/try-azure/#use-runai","text":"Go to https://<tenant-name>.run.ai . Use credentials provided by Run:ai support. After ~30 minutes you should have a working Run:ai cluster. You can submit Jobs via the user interface. Command-line is not provided.","title":"Use Run:ai"},{"location":"admin/runai-setup/try-azure/#limitations","text":"This setup does not support single-sign-on .","title":"Limitations"},{"location":"admin/runai-setup/authentication/authentication-overview/","text":"Authentication Overview \u00b6 To access Run:ai resources, you have to authenticate. The purpose of this document is to explain how authentication works at Run:ai. Authentication Endpoints \u00b6 Generally speaking, there are two authentication endpoints: The Run:ai control plane. Run:ai GPU clusters. Both endpoints are accessible via APIs as well as a user interface. Identity Service \u00b6 Run:ai includes an internal identity service. The identity service ensures users are who they claim to be and gives them the right kind of access to Run:ai. Users \u00b6 Out of the box, The Run:ai identity service provides a way to create users and associate them with access roles. It is also possible to configure the Run:ai identity service to connect to a company directory using the SAML protocol. For more information see single sign-on . Authentication Method \u00b6 Both endpoints described above are protected via time-limited oauth2-like JWT authentication tokens. There are two ways of getting a token: Using a user/password combination. Using client applications for API access. Authentication Flows \u00b6 Run:ai control plane \u00b6 You can use the Run:ai user interface to provide user/password. These are validated against the identity service. Run:ai will return a token with the right access rights for continued operation. You can also use a client application to get a token and then connect directly to the administration API endpoint . Run:ai GPU Clusters \u00b6 The Run:ai GPU cluster is a Kubernetes cluster. All communication into Kubernetes flows through the Kubernetes API server . To facilitate authentication via Run:ai the Kubernetes API server must be configured to use the Run:ai identity service to validate authentication tokens. For more information on how to configure the Kubernetes API server see Kubernetes configuration under researcher authentication . See also \u00b6 To configure authentication for researchers researcher authentication . To configure single sign-on, see single sign-on .","title":"Overview"},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-overview","text":"To access Run:ai resources, you have to authenticate. The purpose of this document is to explain how authentication works at Run:ai.","title":"Authentication Overview"},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-endpoints","text":"Generally speaking, there are two authentication endpoints: The Run:ai control plane. Run:ai GPU clusters. Both endpoints are accessible via APIs as well as a user interface.","title":"Authentication Endpoints"},{"location":"admin/runai-setup/authentication/authentication-overview/#identity-service","text":"Run:ai includes an internal identity service. The identity service ensures users are who they claim to be and gives them the right kind of access to Run:ai.","title":"Identity Service"},{"location":"admin/runai-setup/authentication/authentication-overview/#users","text":"Out of the box, The Run:ai identity service provides a way to create users and associate them with access roles. It is also possible to configure the Run:ai identity service to connect to a company directory using the SAML protocol. For more information see single sign-on .","title":"Users"},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-method","text":"Both endpoints described above are protected via time-limited oauth2-like JWT authentication tokens. There are two ways of getting a token: Using a user/password combination. Using client applications for API access.","title":"Authentication Method"},{"location":"admin/runai-setup/authentication/authentication-overview/#authentication-flows","text":"","title":"Authentication Flows"},{"location":"admin/runai-setup/authentication/authentication-overview/#runai-control-plane","text":"You can use the Run:ai user interface to provide user/password. These are validated against the identity service. Run:ai will return a token with the right access rights for continued operation. You can also use a client application to get a token and then connect directly to the administration API endpoint .","title":"Run:ai control plane"},{"location":"admin/runai-setup/authentication/authentication-overview/#runai-gpu-clusters","text":"The Run:ai GPU cluster is a Kubernetes cluster. All communication into Kubernetes flows through the Kubernetes API server . To facilitate authentication via Run:ai the Kubernetes API server must be configured to use the Run:ai identity service to validate authentication tokens. For more information on how to configure the Kubernetes API server see Kubernetes configuration under researcher authentication .","title":"Run:ai GPU Clusters"},{"location":"admin/runai-setup/authentication/authentication-overview/#see-also","text":"To configure authentication for researchers researcher authentication . To configure single sign-on, see single sign-on .","title":"See also"},{"location":"admin/runai-setup/authentication/researcher-authentication/","text":"Setup Researcher Access Control \u00b6 Introduction \u00b6 The following instructions explain how to complete the configuration of access control for Researchers. Run:ai access control is at the Project level. When you assign Users to Projects - only these users are allowed to submit Jobs and access Jobs details. This requires several steps: Assign users to their Projects. (Mandatory) Modify the Kubernetes entry point (called the Kubernetes API server ) to validate credentials of incoming requests against the Run:ai Authentication authority. (Command-line Interface usage only) Modify the Kubernetes profile to prompt the Researcher for credentials when running runai login (or oc login for OpenShift). Administration User Interface Setup \u00b6 Enable Researcher Authentication \u00b6 Open the Run:ai user interface and navigate to General | Settings . Enable the flag Researcher Authentication (should be enabled by default for new tenants). There are values for Realm , client configuration , and server configuration which appear on the screen. Use them as below. Assign Users to Projects \u00b6 Assign Researchers to Projects: Open the Run:ai user interface and navigate to Users . Add a Researcher and assign it a Researcher role. Navigate to Projects . Edit or create a Project. Use the Access Control tab to assign the Researcher to the Project. If you are using Single Sign-On, you can also assign Groups . For more information see the Single Sign-On documentation. (Mandatory) Kubernetes Configuration \u00b6 As described in authentication overview , you must direct the Kubernetes API server to authenticate via Run:ai. This requires adding flags to the Kubernetes API Server. Modifying the API Server configuration differs between Kubernetes distributions: Native Kubernetes OpenShift RKE RKE2 GKE EKS Bright Other Locate the Kubernetes API Server configuration file. The file's location may defer between different Kubernetes distributions. The location for vanilla Kubernetes is /etc/kubernetes/manifests/kube-apiserver.yaml Edit the document, under the command tag, add the server configuration text from General | Settings | Researcher Authentication described above. Verify that the kube-apiserver-<master-node-name> pod in the kube-system namespace has been restarted and that changes have been incorporated. Run the below and verify that the oidc flags you have added: kubectl get pods -n kube-system kube-apiserver-<master-node-name> -o yaml No configuration is needed. Instead, Run:ai assumes that an Identity Provider has been defined at the OpenShift level and that the Run:ai Cluster installation has set the OpenshiftIdp flag to true. For more information see the Run:ai OpenShift control-plane setup. Edit Rancher cluster.yml (with Rancher UI, follow this ). Add the following: cluster.yml kube-api : always_pull_images : false extra_args : oidc-client-id : runai # (1) oidc-issuer-url : https://example.com/auth oidc-username-prefix : \"-\" These are example parameters. Copy the actual parameters from from General | Settings | Researcher Authentication as described above. You can verify that the flags have been incorporated into the RKE cluster by following the instructions here and running docker inspect <kube-api-server-container-id> , where <kube-api-server-container-id> is the container ID of api-server via obtained in the Rancher document. If working via the RKE2 Quickstart , edit /etc/rancher/rke2/config.yaml . Add the parameters provided in the server configuration section as described above in the following fashion: /etc/rancher/rke2/config.yaml kube-apiserver-arg : - \"oidc-client-id=<CLIENT-ID>\" - \"oidc-issuer-url=<URL>\" - \"oidc-username-prefix=-\" If working via Rancher UI, need to add the flag as part of the cluster provisioning. At the time of writing, the flags cannot be changed after the cluster has been provisioned due to a Rancher bug. Under Cluster Management | Create , turn on RKE2 and select a platform. Under Cluster Configuration | Advanced | Additional API Server Args . Add the Run:ai flags as <key>=<value> (e.g. oidc-username-prefix=- ). Install Anthos identity service by running: gcloud container clusters update <gke-cluster-name> \\ --enable-identity-service --project=<gcp-project-name> --zone=<gcp-zone-name> Install the yq utility and run: kubectl get clientconfig default -n kube-public -o yaml > login-config.yaml yq -i e \".spec +={\\\"authentication\\\":[{\\\"name\\\":\\\"oidc\\\",\\\"oidc\\\":{\\\"clientID\\\":\\\"$OIDC_CLIENT_ID\\\",\\\"issuerURI\\\":\\\"$OIDC_ISSUER_URL\\\",\\\"kubectlRedirectURI\\\":\\\"http://localhost:8000/callback\\\",\\\"userClaim\\\":\\\"sub\\\",\\\"userPrefix\\\":\\\"$OIDC_USERNAME_PREFIX\\\"}}]}\" login-config.yaml kubectl apply -f login-config.yaml Where the OIDC flags are provided in the Run:ai server configuration section as described above. To create a kubeconfig profile for Researchers run: kubectl oidc login --cluster=CLUSTER_NAME --login-config=login-config.yaml \\ --kubeconfig=developer-kubeconfig Then modify the developer-kubeconfig file as described in the Command-line Inteface Access section below. In the AWS Console, under EKS, find your cluster. Go to Configuration and then to Authentication . Associate a new identity provider . Use the parameters provided in the server configuration section as described above. The process can take up to 30 minutes. Run the following. Replace <TENANT-NAME> and <REALM-NAME> with the appropriate values: # start cmsh [ root@headnode ~ ] # cmsh # go to the configurationoverlay submode [ headnode ] % configurationoverlay [ headnode->configurationoverlay ] % list # use list here to list overlays ... # go to the overlay for kube master nodes [ headnode->configurationoverlay ] % use kube-default-master [ headnode->configurationoverlay [ kube-default-master ]] % show # use show here to show the selected overlay ... # go to the kube apiserver role [ headnode->configurationoverlay [ kube-default-master ]] % roles [ headnode->configurationoverlay [ kube-default-master ] ->roles ] % list # ... [ headnode->configurationoverlay [ kube-default-master ] ->roles ] % use kubernetes::apiserver # we can check the current value of \"options\" [ headnode->configurationoverlay [ kube-default-master ] ->roles [ Kubernetes::ApiServer ]] % show # ... [ headnode->configurationoverlay [ kube-default-master ] ->roles [ Kubernetes::ApiServer ]] % get options --anonymous-auth = false --service-account-issuer = https://kubernetes.default.svc.cluster.local --service-account-signing-key-file = /cm/local/apps/kubernetes/var/etc/sa-default.key --feature-gates = LegacyServiceAccountTokenNoAutoGeneration = false # we can append our flags like this [ headnode->configurationoverlay [ kube-default-master ] ->roles [ Kubernetes::ApiServer ]] % append options \"--oidc-client-id=runai\" [ headnode->configurationoverlay* [ kube-default-master* ] ->roles* [ Kubernetes::ApiServer* ]] % append options \"--oidc-issuer-url=https://app.run.ai/auth/realms/<REALM-NAME>\" [ headnode->configurationoverlay* [ kube-default-master* ] ->roles* [ Kubernetes::ApiServer* ]] % append options \"--oidc-username-prefix=-\" # commit the changes [ headnode->configurationoverlay [ kube-default-master ] ->roles [ Kubernetes::ApiServer ]] % ]] % commit # view updated list of options [ headnode->configurationoverlay [ kube-default-master ] ->roles [ Kubernetes::ApiServer ]] % get options --anonymous-auth = false --service-account-issuer = https://kubernetes.default.svc.cluster.local --service-account-signing-key-file = /cm/local/apps/kubernetes/var/etc/sa-default.key --feature-gates = LegacyServiceAccountTokenNoAutoGeneration = false --cors-allowed-origins =[ \\\" https://<TENANT-NAME>.run.ai \\\" ] --oidc-client-id = runai --oidc-issuer-url = https://app.run.ai/auth/realms/<REALM-NAME> --oidc-username-prefix = - All nodes with the kube api server role will automatically restart with the new flag. See specific instructions in the documentation of the Kubernetes distribution. Command-line Interface Access \u00b6 To control access to Run:ai (and Kubernetes) resources, you must modify the Kubernetes configuration file. The file is distributed to users as part of the Command-line interface installation . When making changes to the file, keep a copy of the original file to be used for cluster administration. After making the modifications, distribute the modified file to Researchers. Under the ~/.kube directory edit the config file, remove the administrative user, and replace it with the client configuration text from General | Settings | Researcher Authentication described above. Under contexts | context | user change the user to runai-authenticated-user . Test via Command-line interface \u00b6 Run: runai login (in OpenShift environments use oc login rather than runai login ). You will be prompted for a username and password. In a single sign-on flow, you will be asked to copy a link to a browser, log in and return a code. Once login is successful, submit a Job. If the Job was submitted with a Project to which you have no access, your access will be denied. If the Job was submitted with a Project to which you have access, your access will be granted. You can also submit a Job from the Run:ai User interface and verify that the new job shows on the job list with your user name. Test via User Interface \u00b6 Open the Run:ai user interface, go to Jobs . On the top-right, select Submit Job . Tip If you do not see the button or it is disabled, then you either do not have Researcher access or the cluster has not been set up correctly. For more information, refer to user interface overview .","title":"Researcher Authentication"},{"location":"admin/runai-setup/authentication/researcher-authentication/#setup-researcher-access-control","text":"","title":"Setup Researcher Access Control"},{"location":"admin/runai-setup/authentication/researcher-authentication/#introduction","text":"The following instructions explain how to complete the configuration of access control for Researchers. Run:ai access control is at the Project level. When you assign Users to Projects - only these users are allowed to submit Jobs and access Jobs details. This requires several steps: Assign users to their Projects. (Mandatory) Modify the Kubernetes entry point (called the Kubernetes API server ) to validate credentials of incoming requests against the Run:ai Authentication authority. (Command-line Interface usage only) Modify the Kubernetes profile to prompt the Researcher for credentials when running runai login (or oc login for OpenShift).","title":"Introduction"},{"location":"admin/runai-setup/authentication/researcher-authentication/#administration-user-interface-setup","text":"","title":"Administration User Interface Setup"},{"location":"admin/runai-setup/authentication/researcher-authentication/#enable-researcher-authentication","text":"Open the Run:ai user interface and navigate to General | Settings . Enable the flag Researcher Authentication (should be enabled by default for new tenants). There are values for Realm , client configuration , and server configuration which appear on the screen. Use them as below.","title":"Enable Researcher Authentication"},{"location":"admin/runai-setup/authentication/researcher-authentication/#assign-users-to-projects","text":"Assign Researchers to Projects: Open the Run:ai user interface and navigate to Users . Add a Researcher and assign it a Researcher role. Navigate to Projects . Edit or create a Project. Use the Access Control tab to assign the Researcher to the Project. If you are using Single Sign-On, you can also assign Groups . For more information see the Single Sign-On documentation.","title":"Assign Users to Projects"},{"location":"admin/runai-setup/authentication/researcher-authentication/#mandatory-kubernetes-configuration","text":"As described in authentication overview , you must direct the Kubernetes API server to authenticate via Run:ai. This requires adding flags to the Kubernetes API Server. Modifying the API Server configuration differs between Kubernetes distributions: Native Kubernetes OpenShift RKE RKE2 GKE EKS Bright Other Locate the Kubernetes API Server configuration file. The file's location may defer between different Kubernetes distributions. The location for vanilla Kubernetes is /etc/kubernetes/manifests/kube-apiserver.yaml Edit the document, under the command tag, add the server configuration text from General | Settings | Researcher Authentication described above. Verify that the kube-apiserver-<master-node-name> pod in the kube-system namespace has been restarted and that changes have been incorporated. Run the below and verify that the oidc flags you have added: kubectl get pods -n kube-system kube-apiserver-<master-node-name> -o yaml No configuration is needed. Instead, Run:ai assumes that an Identity Provider has been defined at the OpenShift level and that the Run:ai Cluster installation has set the OpenshiftIdp flag to true. For more information see the Run:ai OpenShift control-plane setup. Edit Rancher cluster.yml (with Rancher UI, follow this ). Add the following: cluster.yml kube-api : always_pull_images : false extra_args : oidc-client-id : runai # (1) oidc-issuer-url : https://example.com/auth oidc-username-prefix : \"-\" These are example parameters. Copy the actual parameters from from General | Settings | Researcher Authentication as described above. You can verify that the flags have been incorporated into the RKE cluster by following the instructions here and running docker inspect <kube-api-server-container-id> , where <kube-api-server-container-id> is the container ID of api-server via obtained in the Rancher document. If working via the RKE2 Quickstart , edit /etc/rancher/rke2/config.yaml . Add the parameters provided in the server configuration section as described above in the following fashion: /etc/rancher/rke2/config.yaml kube-apiserver-arg : - \"oidc-client-id=<CLIENT-ID>\" - \"oidc-issuer-url=<URL>\" - \"oidc-username-prefix=-\" If working via Rancher UI, need to add the flag as part of the cluster provisioning. At the time of writing, the flags cannot be changed after the cluster has been provisioned due to a Rancher bug. Under Cluster Management | Create , turn on RKE2 and select a platform. Under Cluster Configuration | Advanced | Additional API Server Args . Add the Run:ai flags as <key>=<value> (e.g. oidc-username-prefix=- ). Install Anthos identity service by running: gcloud container clusters update <gke-cluster-name> \\ --enable-identity-service --project=<gcp-project-name> --zone=<gcp-zone-name> Install the yq utility and run: kubectl get clientconfig default -n kube-public -o yaml > login-config.yaml yq -i e \".spec +={\\\"authentication\\\":[{\\\"name\\\":\\\"oidc\\\",\\\"oidc\\\":{\\\"clientID\\\":\\\"$OIDC_CLIENT_ID\\\",\\\"issuerURI\\\":\\\"$OIDC_ISSUER_URL\\\",\\\"kubectlRedirectURI\\\":\\\"http://localhost:8000/callback\\\",\\\"userClaim\\\":\\\"sub\\\",\\\"userPrefix\\\":\\\"$OIDC_USERNAME_PREFIX\\\"}}]}\" login-config.yaml kubectl apply -f login-config.yaml Where the OIDC flags are provided in the Run:ai server configuration section as described above. To create a kubeconfig profile for Researchers run: kubectl oidc login --cluster=CLUSTER_NAME --login-config=login-config.yaml \\ --kubeconfig=developer-kubeconfig Then modify the developer-kubeconfig file as described in the Command-line Inteface Access section below. In the AWS Console, under EKS, find your cluster. Go to Configuration and then to Authentication . Associate a new identity provider . Use the parameters provided in the server configuration section as described above. The process can take up to 30 minutes. Run the following. Replace <TENANT-NAME> and <REALM-NAME> with the appropriate values: # start cmsh [ root@headnode ~ ] # cmsh # go to the configurationoverlay submode [ headnode ] % configurationoverlay [ headnode->configurationoverlay ] % list # use list here to list overlays ... # go to the overlay for kube master nodes [ headnode->configurationoverlay ] % use kube-default-master [ headnode->configurationoverlay [ kube-default-master ]] % show # use show here to show the selected overlay ... # go to the kube apiserver role [ headnode->configurationoverlay [ kube-default-master ]] % roles [ headnode->configurationoverlay [ kube-default-master ] ->roles ] % list # ... [ headnode->configurationoverlay [ kube-default-master ] ->roles ] % use kubernetes::apiserver # we can check the current value of \"options\" [ headnode->configurationoverlay [ kube-default-master ] ->roles [ Kubernetes::ApiServer ]] % show # ... [ headnode->configurationoverlay [ kube-default-master ] ->roles [ Kubernetes::ApiServer ]] % get options --anonymous-auth = false --service-account-issuer = https://kubernetes.default.svc.cluster.local --service-account-signing-key-file = /cm/local/apps/kubernetes/var/etc/sa-default.key --feature-gates = LegacyServiceAccountTokenNoAutoGeneration = false # we can append our flags like this [ headnode->configurationoverlay [ kube-default-master ] ->roles [ Kubernetes::ApiServer ]] % append options \"--oidc-client-id=runai\" [ headnode->configurationoverlay* [ kube-default-master* ] ->roles* [ Kubernetes::ApiServer* ]] % append options \"--oidc-issuer-url=https://app.run.ai/auth/realms/<REALM-NAME>\" [ headnode->configurationoverlay* [ kube-default-master* ] ->roles* [ Kubernetes::ApiServer* ]] % append options \"--oidc-username-prefix=-\" # commit the changes [ headnode->configurationoverlay [ kube-default-master ] ->roles [ Kubernetes::ApiServer ]] % ]] % commit # view updated list of options [ headnode->configurationoverlay [ kube-default-master ] ->roles [ Kubernetes::ApiServer ]] % get options --anonymous-auth = false --service-account-issuer = https://kubernetes.default.svc.cluster.local --service-account-signing-key-file = /cm/local/apps/kubernetes/var/etc/sa-default.key --feature-gates = LegacyServiceAccountTokenNoAutoGeneration = false --cors-allowed-origins =[ \\\" https://<TENANT-NAME>.run.ai \\\" ] --oidc-client-id = runai --oidc-issuer-url = https://app.run.ai/auth/realms/<REALM-NAME> --oidc-username-prefix = - All nodes with the kube api server role will automatically restart with the new flag. See specific instructions in the documentation of the Kubernetes distribution.","title":"(Mandatory) Kubernetes Configuration"},{"location":"admin/runai-setup/authentication/researcher-authentication/#command-line-interface-access","text":"To control access to Run:ai (and Kubernetes) resources, you must modify the Kubernetes configuration file. The file is distributed to users as part of the Command-line interface installation . When making changes to the file, keep a copy of the original file to be used for cluster administration. After making the modifications, distribute the modified file to Researchers. Under the ~/.kube directory edit the config file, remove the administrative user, and replace it with the client configuration text from General | Settings | Researcher Authentication described above. Under contexts | context | user change the user to runai-authenticated-user .","title":"Command-line Interface Access"},{"location":"admin/runai-setup/authentication/researcher-authentication/#test-via-command-line-interface","text":"Run: runai login (in OpenShift environments use oc login rather than runai login ). You will be prompted for a username and password. In a single sign-on flow, you will be asked to copy a link to a browser, log in and return a code. Once login is successful, submit a Job. If the Job was submitted with a Project to which you have no access, your access will be denied. If the Job was submitted with a Project to which you have access, your access will be granted. You can also submit a Job from the Run:ai User interface and verify that the new job shows on the job list with your user name.","title":"Test via Command-line interface"},{"location":"admin/runai-setup/authentication/researcher-authentication/#test-via-user-interface","text":"Open the Run:ai user interface, go to Jobs . On the top-right, select Submit Job . Tip If you do not see the button or it is disabled, then you either do not have Researcher access or the cluster has not been set up correctly. For more information, refer to user interface overview .","title":"Test via User Interface"},{"location":"admin/runai-setup/authentication/sso/","text":"Single Sign-On \u00b6 Single Sign-On (SSO) is an authentication scheme that allows a user to log in with a single ID to other, independent, software systems. SSO solves security issues involving multiple user/password data entries, multiple compliance schemes, etc. Run:ai supports SSO using the SAML 2.0 protocol. When SSO is configured, the system is accessible via single-sign-on only . Caution Single sign-on is only available with SaaS installations where the tenant has been created post-January 2022 or any Self-hosted installation of release 2.0.58 or upwards. If you are using single sign-on with older versions of Run:ai, please contact Run:ai customer support Terminology \u00b6 The term Identity Provider (or IdP) below relates to the system which creates, maintains, and manages identity information. Example IdPs: Google, Keycloak, Salesforce, Auth0. Prerequisites \u00b6 XML Metadata : You must have an XML Metadata file retrieved from your IdP. Upload the file to a web server such that you will have a URL to the file. The URL must have the XML file extension. For example, to connect using Google, you must create a custom SAML App here , download the Metadata file, and upload it to a web server. Organization Name : You must have a Run:ai Organization Name . This is the name that appears on the top right of the Run:ai user interface. Additional attribute mapping : Configure your IdP to map several IdP attributes: IdP attribute Run:ai required name Description User email email (Mandatory) e-mail is the user identifier with Run:ai. User role groups GROUPS (Optional) If exists, allows assigning Run:ai role groups via the IdP. The IdP attribute must be of a type of list of strings. See more below Linux User ID UID (Optional) If exists in IdP, allows Researcher containers to start with the Linux User UID . Used to map access to network resources such as file systems to users. The IdP attribute must be of integer type. Linux Group ID GID (Optional) If exists in IdP, allows Researcher containers to start with the Linux Group GID . The IdP attribute must be of integer type. Linux Supplementary Groups SUPPLEMENTARYGROUPS (Optional) If exists in IdP, allows Researcher containers to start with the relevant Linux supplementary groups. The IdP attribute must be of a type of list of integers. User first name firstName (Optional) Used as the first name showing in the Run:ai user interface. User last name lastName (Optional) Used as the last name showing in the Run:ai user interface Example attribute mapping for Google Suite \u00b6 If you are using Google Suite as your Identity provider, to map custom attributes follow the Google support article . Use the Whole Number attribute type. For Supplementary Groups use the Multi-value designation. Step 1: UI Configuration \u00b6 Open the Administration User interface. Go to Settings | General . Turn on Login with SSO . Under Metadata XML Url enter the URL to the XML Metadata file obtained above. Under Administrator email, enter the first administrator user. Press Save . Once you press Save you will receive a Redirect URI and an Entity ID . Both values must be set on the IdP side. Important Upon pressing Save , all existing users will be rendered non-functional, and the only valid user will be the Administrator email entered above. You can always revert by disabling Login via SSO . Test \u00b6 Test Connectivity to Administration User Interface: Using an incognito browser tab and open the Run:ai user interface. Select the Login with SSO button. Provide the Organization name obtained above. You will be redirected to the IdP login page. Use the previously entered Administrator email to log in. Troubleshooting \u00b6 Single sign-on log in can be separated into two parts: Run:ai redirects to the IdP (e.g. Google) for login using a SAML Request . Upon successful login, IdP redirects back to Run:ai with a SAML Response . You can follow that by following the URL changes from app.run.ai to the IdP provider (e.g. accounts.google.com ) and back to app.run.ai : If there is an issue on the IdP site (e.g. app_is_not_configred error in Google), the problem is likely to be in the SAML Request. If the user is redirected back to Run:ai and something goes wrong, the problem is most likely in the SAML Response. Troubleshooting SAML Request \u00b6 When logging in, have the Chrome network inspector open (Open by Right-Click | Inspect on the page, then open the network tab). After the IdP login screen shows, search in the network tab for an HTTP request showing the SAML Request. Depending on the IdP this would be a request to the IdP domain name. E.g. accounts.google.com/idp?1234. When found, go to the \"Payload\" tab and copy the value of the SAML Request. Paste the value into a SAML decoder . A typical response should look like this: <?xml version=\"1.0\"?> <samlp:AuthnRequest xmlns:samlp= \"urn:oasis:names:tc:SAML:2.0:protocol\" xmlns= \"urn:oasis:names:tc:SAML:2.0:assertion\" xmlns:saml= \"urn:oasis:names:tc:SAML:2.0:assertion\" AssertionConsumerServiceURL= \"https://.../auth/realms/runai/broker/saml/endpoint\" Destination= \"https://accounts.google.com/o/saml2/idp?idpid=....\" ForceAuthn= \"false\" ID= \"ID_66da617d-b862-4cca-9ei5-b727a920f3cb\" IssueInstant= \"2022-01-12T12:54:22.907Z\" ProtocolBinding= \"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\" Version= \"2.0\" > <saml:Issuer> runai-jtqee5v8ob </saml:Issuer> <samlp:NameIDPolicy AllowCreate= \"true\" Format= \"urn:oasis:names:tc:SAML:2.0:nameid-format:persistent\" /> </samlp:AuthnRequest> Check in the above that: The content of the <saml:Issuer> tag is the same as Entity ID defined above. AssertionConsumerServiceURL is the same as the Redirect URI . Troubleshooting SAML Response \u00b6 When logging in, have the Chrome network inspector open (Open by Right-Click | Inspect on the page, then open the network tab). Search for \"endpoint\". When found, go to the \"Payload\" tab and copy the value of the SAML Response. Paste the value into a SAML decoder . A typical response should look like this: <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <saml2p:Response xmlns:saml2p= \"urn:oasis:names:tc:SAML:2.0:protocol\" Destination= \"https://.../auth/realms/runai/broker/saml/endpoint\" ID= \"_2d085ed4f45a7ab221a49e6c02e30cac\" InResponseTo= \"ID_295f2723-79f5-4410-99b2-5f4acb2d4f8e\" IssueInstant= \"2022-01-12T12:06:31.175Z\" Version= \"2.0\" > <saml2:Issuer xmlns:saml2= \"urn:oasis:names:tc:SAML:2.0:assertion\" > https://accounts.google.com/o/saml2?idpid=.... </saml2:Issuer> <saml2p:Status> <saml2p:StatusCode Value= \"urn:oasis:names:tc:SAML:2.0:status:Success\" /> </saml2p:Status> <saml2:Assertion xmlns:saml2= \"urn:oasis:names:tc:SAML:2.0:assertion\" ID= \"_befe8441fa06594b365c516558dc5636\" IssueInstant= \"2022-01-12T12:06:31.175Z\" Version= \"2.0\" > <saml2:Issuer> https://accounts.google.com/o/saml2?idpid=... </saml2:Issuer> <ds:Signature xmlns:ds= \"http://www.w3.org/2000/09/xmldsig#\" > <ds:SignedInfo> <ds:CanonicalizationMethod Algorithm= \"http://www.w3.org/2001/10/xml-exc-c14n#\" /> <ds:SignatureMethod Algorithm= \"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\" /> <ds:Reference URI= \"#_befe8441fa06594b365c516558dc5636\" > <ds:Transforms> <ds:Transform Algorithm= \"http://www.w3.org/2000/09/xmldsig#enveloped-signature\" /> <ds:Transform Algorithm= \"http://www.w3.org/2001/10/xml-exc-c14n#\" /> </ds:Transforms> <ds:DigestMethod Algorithm= \"http://www.w3.org/2001/04/xmlenc#sha256\" /> <ds:DigestValue> QxNCjtz9Gomv2qaz8Rb4X8cQJOSGkK+87CrHDkBPidM= </ds:DigestValue> </ds:Reference> </ds:SignedInfo> <ds:SignatureValue> ... </ds:SignatureValue> <ds:KeyInfo> <ds:X509Data> <ds:X509SubjectName> ST=California,C=US,OU=Google For Work,CN=Google,L=Mountain View,O=Google Inc. </ds:X509SubjectName> <ds:X509Certificate> ... </ds:X509Certificate> </ds:X509Data> </ds:KeyInfo> </ds:Signature> <saml2:Subject> <saml2:NameID Format= \"urn:oasis:names:tc:SAML:2.0:nameid-format:persistent\" > john@example.com </saml2:NameID> <saml2:SubjectConfirmation Method= \"urn:oasis:names:tc:SAML:2.0:cm:bearer\" > <saml2:SubjectConfirmationData InResponseTo= \"ID_295f2723-79f5-4410-99b2-5f4acb2d4f8e\" NotOnOrAfter= \"2022-01-12T12:11:31.175Z\" Recipient= \"https://.../auth/realms/runai/broker/saml/endpoint\" /> </saml2:SubjectConfirmation> </saml2:Subject> <saml2:Conditions NotBefore= \"2022-01-12T12:01:31.175Z\" NotOnOrAfter= \"2022-01-12T12:11:31.175Z\" > <saml2:AudienceRestriction> <saml2:Audience> runai-jtqee5v8ob </saml2:Audience> </saml2:AudienceRestriction> </saml2:Conditions> <saml2:AttributeStatement> <saml2:Attribute Name= \"email\" > <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > john@example.com </saml2:AttributeValue> </saml2:Attribute> <saml2:Attribute Name= \"GID\" > <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 8765 </saml2:AttributeValue> </saml2:Attribute> <saml2:Attribute Name= \"SUPPLEMENTARYGROUPS\" > <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 200 </saml2:AttributeValue> <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 300 </saml2:AttributeValue> <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 400 </saml2:AttributeValue> <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 100 </saml2:AttributeValue> </saml2:Attribute> <saml2:Attribute Name= \"UID\" > <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 4321 </saml2:AttributeValue> </saml2:Attribute> </saml2:AttributeStatement> <saml2:AuthnStatement AuthnInstant= \"2022-01-12T12:06:30.000Z\" SessionIndex= \"_befe8441fa06594b365c516558dc5636\" > <saml2:AuthnContext> <saml2:AuthnContextClassRef> urn:oasis:names:tc:SAML:2.0:ac:classes:unspecified </saml2:AuthnContextClassRef> </saml2:AuthnContext> </saml2:AuthnStatement> </saml2:Assertion> </saml2p:Response> Check in the above that: The content of the <saml2:Audience> tag is the same as Entity ID defined above. The Destination at the top is the same as the Redirect URI . The user email under the <saml2:Subject> tag is the same as the logged-in user. Make sure that under the <saml2:AttributeStatement> tag, there is an Attribute named email (lowercase). This attribute is mandatory. If other, optional attributes (such as UID, GID) are mapped, make sure they exist under <saml2:AttributeStatement> along with their respective values. Step 2: Cluster Authentication \u00b6 Researchers should be authenticated when accessing the Run:ai GPU Cluster. To perform that, the Kubernetes cluster and the user's Kubernetes profile must be aware of the IdP. Follow the instructions here . If you have followed these instructions in the past, you must do so again and replace the client-side and server-side configuration values with the new values as provided by on General | Settings | Researcher Authentication . Test \u00b6 Test connectivity to Run:ai command-line interface: In the command-line, run runai login . You will receive a link that you must copy and open in your browser. Post login you will receive a verification code which you must paste into the shell window. Verify successful login. Step 3: UID/GID Mapping \u00b6 Configure the IdP to add UID, GID, and Supplementary groups in the IdP. Test \u00b6 Test the mapping of UID/GID to within the container: Submit a job with the flag --run-as-user , for example: runai submit -i ubuntu --interactive --run-as-user --attach -- bash When a shell opens inside the container, run id and verify that UID, GID, and the supplementary groups are the same as in the user's profile in the organization's directory. Step 4: Adding Users \u00b6 You can add additional users, by either: Manually adding roles for each user. Mapping roles to IdP groups. The latter option is easier to maintain. Adding Roles for a User \u00b6 Go to Settings | Users . Select the Users button at the top. Map users as explained here . Mapping Role Groups \u00b6 Go to Settings | Users . Select the Groups button. Assuming you have mapped the IdP Groups attribute as described in the prerequisites section above, add a name of a group that has been created in the directory and create an equivalent Run:ai Group. If the role group contains the Researcher role, you can assign this group to a Run:ai Project. All members of the group will have access to the cluster. Note This feature also works in OpenShift. If you create a group in Run:ai with the same name as an OpenShift Group, the associated permissions will be applied to all users in the group. Logout URL \u00b6 It is possible to configure the redirect URL when the session ends. To perform this configuration please contact Run:ai customer support. Implementation Notes \u00b6 Run:ai SSO does not support single logout. As such, logging out from Run:ai will not log you out from other systems.","title":"Single Sign-On"},{"location":"admin/runai-setup/authentication/sso/#single-sign-on","text":"Single Sign-On (SSO) is an authentication scheme that allows a user to log in with a single ID to other, independent, software systems. SSO solves security issues involving multiple user/password data entries, multiple compliance schemes, etc. Run:ai supports SSO using the SAML 2.0 protocol. When SSO is configured, the system is accessible via single-sign-on only . Caution Single sign-on is only available with SaaS installations where the tenant has been created post-January 2022 or any Self-hosted installation of release 2.0.58 or upwards. If you are using single sign-on with older versions of Run:ai, please contact Run:ai customer support","title":"Single Sign-On"},{"location":"admin/runai-setup/authentication/sso/#terminology","text":"The term Identity Provider (or IdP) below relates to the system which creates, maintains, and manages identity information. Example IdPs: Google, Keycloak, Salesforce, Auth0.","title":"Terminology"},{"location":"admin/runai-setup/authentication/sso/#prerequisites","text":"XML Metadata : You must have an XML Metadata file retrieved from your IdP. Upload the file to a web server such that you will have a URL to the file. The URL must have the XML file extension. For example, to connect using Google, you must create a custom SAML App here , download the Metadata file, and upload it to a web server. Organization Name : You must have a Run:ai Organization Name . This is the name that appears on the top right of the Run:ai user interface. Additional attribute mapping : Configure your IdP to map several IdP attributes: IdP attribute Run:ai required name Description User email email (Mandatory) e-mail is the user identifier with Run:ai. User role groups GROUPS (Optional) If exists, allows assigning Run:ai role groups via the IdP. The IdP attribute must be of a type of list of strings. See more below Linux User ID UID (Optional) If exists in IdP, allows Researcher containers to start with the Linux User UID . Used to map access to network resources such as file systems to users. The IdP attribute must be of integer type. Linux Group ID GID (Optional) If exists in IdP, allows Researcher containers to start with the Linux Group GID . The IdP attribute must be of integer type. Linux Supplementary Groups SUPPLEMENTARYGROUPS (Optional) If exists in IdP, allows Researcher containers to start with the relevant Linux supplementary groups. The IdP attribute must be of a type of list of integers. User first name firstName (Optional) Used as the first name showing in the Run:ai user interface. User last name lastName (Optional) Used as the last name showing in the Run:ai user interface","title":"Prerequisites"},{"location":"admin/runai-setup/authentication/sso/#example-attribute-mapping-for-google-suite","text":"If you are using Google Suite as your Identity provider, to map custom attributes follow the Google support article . Use the Whole Number attribute type. For Supplementary Groups use the Multi-value designation.","title":"Example attribute mapping for Google Suite"},{"location":"admin/runai-setup/authentication/sso/#step-1-ui-configuration","text":"Open the Administration User interface. Go to Settings | General . Turn on Login with SSO . Under Metadata XML Url enter the URL to the XML Metadata file obtained above. Under Administrator email, enter the first administrator user. Press Save . Once you press Save you will receive a Redirect URI and an Entity ID . Both values must be set on the IdP side. Important Upon pressing Save , all existing users will be rendered non-functional, and the only valid user will be the Administrator email entered above. You can always revert by disabling Login via SSO .","title":"Step 1: UI Configuration"},{"location":"admin/runai-setup/authentication/sso/#test","text":"Test Connectivity to Administration User Interface: Using an incognito browser tab and open the Run:ai user interface. Select the Login with SSO button. Provide the Organization name obtained above. You will be redirected to the IdP login page. Use the previously entered Administrator email to log in.","title":"Test"},{"location":"admin/runai-setup/authentication/sso/#troubleshooting","text":"Single sign-on log in can be separated into two parts: Run:ai redirects to the IdP (e.g. Google) for login using a SAML Request . Upon successful login, IdP redirects back to Run:ai with a SAML Response . You can follow that by following the URL changes from app.run.ai to the IdP provider (e.g. accounts.google.com ) and back to app.run.ai : If there is an issue on the IdP site (e.g. app_is_not_configred error in Google), the problem is likely to be in the SAML Request. If the user is redirected back to Run:ai and something goes wrong, the problem is most likely in the SAML Response.","title":"Troubleshooting"},{"location":"admin/runai-setup/authentication/sso/#troubleshooting-saml-request","text":"When logging in, have the Chrome network inspector open (Open by Right-Click | Inspect on the page, then open the network tab). After the IdP login screen shows, search in the network tab for an HTTP request showing the SAML Request. Depending on the IdP this would be a request to the IdP domain name. E.g. accounts.google.com/idp?1234. When found, go to the \"Payload\" tab and copy the value of the SAML Request. Paste the value into a SAML decoder . A typical response should look like this: <?xml version=\"1.0\"?> <samlp:AuthnRequest xmlns:samlp= \"urn:oasis:names:tc:SAML:2.0:protocol\" xmlns= \"urn:oasis:names:tc:SAML:2.0:assertion\" xmlns:saml= \"urn:oasis:names:tc:SAML:2.0:assertion\" AssertionConsumerServiceURL= \"https://.../auth/realms/runai/broker/saml/endpoint\" Destination= \"https://accounts.google.com/o/saml2/idp?idpid=....\" ForceAuthn= \"false\" ID= \"ID_66da617d-b862-4cca-9ei5-b727a920f3cb\" IssueInstant= \"2022-01-12T12:54:22.907Z\" ProtocolBinding= \"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\" Version= \"2.0\" > <saml:Issuer> runai-jtqee5v8ob </saml:Issuer> <samlp:NameIDPolicy AllowCreate= \"true\" Format= \"urn:oasis:names:tc:SAML:2.0:nameid-format:persistent\" /> </samlp:AuthnRequest> Check in the above that: The content of the <saml:Issuer> tag is the same as Entity ID defined above. AssertionConsumerServiceURL is the same as the Redirect URI .","title":"Troubleshooting SAML Request"},{"location":"admin/runai-setup/authentication/sso/#troubleshooting-saml-response","text":"When logging in, have the Chrome network inspector open (Open by Right-Click | Inspect on the page, then open the network tab). Search for \"endpoint\". When found, go to the \"Payload\" tab and copy the value of the SAML Response. Paste the value into a SAML decoder . A typical response should look like this: <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <saml2p:Response xmlns:saml2p= \"urn:oasis:names:tc:SAML:2.0:protocol\" Destination= \"https://.../auth/realms/runai/broker/saml/endpoint\" ID= \"_2d085ed4f45a7ab221a49e6c02e30cac\" InResponseTo= \"ID_295f2723-79f5-4410-99b2-5f4acb2d4f8e\" IssueInstant= \"2022-01-12T12:06:31.175Z\" Version= \"2.0\" > <saml2:Issuer xmlns:saml2= \"urn:oasis:names:tc:SAML:2.0:assertion\" > https://accounts.google.com/o/saml2?idpid=.... </saml2:Issuer> <saml2p:Status> <saml2p:StatusCode Value= \"urn:oasis:names:tc:SAML:2.0:status:Success\" /> </saml2p:Status> <saml2:Assertion xmlns:saml2= \"urn:oasis:names:tc:SAML:2.0:assertion\" ID= \"_befe8441fa06594b365c516558dc5636\" IssueInstant= \"2022-01-12T12:06:31.175Z\" Version= \"2.0\" > <saml2:Issuer> https://accounts.google.com/o/saml2?idpid=... </saml2:Issuer> <ds:Signature xmlns:ds= \"http://www.w3.org/2000/09/xmldsig#\" > <ds:SignedInfo> <ds:CanonicalizationMethod Algorithm= \"http://www.w3.org/2001/10/xml-exc-c14n#\" /> <ds:SignatureMethod Algorithm= \"http://www.w3.org/2001/04/xmldsig-more#rsa-sha256\" /> <ds:Reference URI= \"#_befe8441fa06594b365c516558dc5636\" > <ds:Transforms> <ds:Transform Algorithm= \"http://www.w3.org/2000/09/xmldsig#enveloped-signature\" /> <ds:Transform Algorithm= \"http://www.w3.org/2001/10/xml-exc-c14n#\" /> </ds:Transforms> <ds:DigestMethod Algorithm= \"http://www.w3.org/2001/04/xmlenc#sha256\" /> <ds:DigestValue> QxNCjtz9Gomv2qaz8Rb4X8cQJOSGkK+87CrHDkBPidM= </ds:DigestValue> </ds:Reference> </ds:SignedInfo> <ds:SignatureValue> ... </ds:SignatureValue> <ds:KeyInfo> <ds:X509Data> <ds:X509SubjectName> ST=California,C=US,OU=Google For Work,CN=Google,L=Mountain View,O=Google Inc. </ds:X509SubjectName> <ds:X509Certificate> ... </ds:X509Certificate> </ds:X509Data> </ds:KeyInfo> </ds:Signature> <saml2:Subject> <saml2:NameID Format= \"urn:oasis:names:tc:SAML:2.0:nameid-format:persistent\" > john@example.com </saml2:NameID> <saml2:SubjectConfirmation Method= \"urn:oasis:names:tc:SAML:2.0:cm:bearer\" > <saml2:SubjectConfirmationData InResponseTo= \"ID_295f2723-79f5-4410-99b2-5f4acb2d4f8e\" NotOnOrAfter= \"2022-01-12T12:11:31.175Z\" Recipient= \"https://.../auth/realms/runai/broker/saml/endpoint\" /> </saml2:SubjectConfirmation> </saml2:Subject> <saml2:Conditions NotBefore= \"2022-01-12T12:01:31.175Z\" NotOnOrAfter= \"2022-01-12T12:11:31.175Z\" > <saml2:AudienceRestriction> <saml2:Audience> runai-jtqee5v8ob </saml2:Audience> </saml2:AudienceRestriction> </saml2:Conditions> <saml2:AttributeStatement> <saml2:Attribute Name= \"email\" > <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > john@example.com </saml2:AttributeValue> </saml2:Attribute> <saml2:Attribute Name= \"GID\" > <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 8765 </saml2:AttributeValue> </saml2:Attribute> <saml2:Attribute Name= \"SUPPLEMENTARYGROUPS\" > <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 200 </saml2:AttributeValue> <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 300 </saml2:AttributeValue> <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 400 </saml2:AttributeValue> <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 100 </saml2:AttributeValue> </saml2:Attribute> <saml2:Attribute Name= \"UID\" > <saml2:AttributeValue xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:type= \"xs:anyType\" > 4321 </saml2:AttributeValue> </saml2:Attribute> </saml2:AttributeStatement> <saml2:AuthnStatement AuthnInstant= \"2022-01-12T12:06:30.000Z\" SessionIndex= \"_befe8441fa06594b365c516558dc5636\" > <saml2:AuthnContext> <saml2:AuthnContextClassRef> urn:oasis:names:tc:SAML:2.0:ac:classes:unspecified </saml2:AuthnContextClassRef> </saml2:AuthnContext> </saml2:AuthnStatement> </saml2:Assertion> </saml2p:Response> Check in the above that: The content of the <saml2:Audience> tag is the same as Entity ID defined above. The Destination at the top is the same as the Redirect URI . The user email under the <saml2:Subject> tag is the same as the logged-in user. Make sure that under the <saml2:AttributeStatement> tag, there is an Attribute named email (lowercase). This attribute is mandatory. If other, optional attributes (such as UID, GID) are mapped, make sure they exist under <saml2:AttributeStatement> along with their respective values.","title":"Troubleshooting SAML Response"},{"location":"admin/runai-setup/authentication/sso/#step-2-cluster-authentication","text":"Researchers should be authenticated when accessing the Run:ai GPU Cluster. To perform that, the Kubernetes cluster and the user's Kubernetes profile must be aware of the IdP. Follow the instructions here . If you have followed these instructions in the past, you must do so again and replace the client-side and server-side configuration values with the new values as provided by on General | Settings | Researcher Authentication .","title":"Step 2: Cluster Authentication"},{"location":"admin/runai-setup/authentication/sso/#test_1","text":"Test connectivity to Run:ai command-line interface: In the command-line, run runai login . You will receive a link that you must copy and open in your browser. Post login you will receive a verification code which you must paste into the shell window. Verify successful login.","title":"Test"},{"location":"admin/runai-setup/authentication/sso/#step-3-uidgid-mapping","text":"Configure the IdP to add UID, GID, and Supplementary groups in the IdP.","title":"Step 3: UID/GID Mapping"},{"location":"admin/runai-setup/authentication/sso/#test_2","text":"Test the mapping of UID/GID to within the container: Submit a job with the flag --run-as-user , for example: runai submit -i ubuntu --interactive --run-as-user --attach -- bash When a shell opens inside the container, run id and verify that UID, GID, and the supplementary groups are the same as in the user's profile in the organization's directory.","title":"Test"},{"location":"admin/runai-setup/authentication/sso/#step-4-adding-users","text":"You can add additional users, by either: Manually adding roles for each user. Mapping roles to IdP groups. The latter option is easier to maintain.","title":"Step 4: Adding Users"},{"location":"admin/runai-setup/authentication/sso/#adding-roles-for-a-user","text":"Go to Settings | Users . Select the Users button at the top. Map users as explained here .","title":"Adding Roles for a User"},{"location":"admin/runai-setup/authentication/sso/#mapping-role-groups","text":"Go to Settings | Users . Select the Groups button. Assuming you have mapped the IdP Groups attribute as described in the prerequisites section above, add a name of a group that has been created in the directory and create an equivalent Run:ai Group. If the role group contains the Researcher role, you can assign this group to a Run:ai Project. All members of the group will have access to the cluster. Note This feature also works in OpenShift. If you create a group in Run:ai with the same name as an OpenShift Group, the associated permissions will be applied to all users in the group.","title":"Mapping Role Groups"},{"location":"admin/runai-setup/authentication/sso/#logout-url","text":"It is possible to configure the redirect URL when the session ends. To perform this configuration please contact Run:ai customer support.","title":"Logout URL"},{"location":"admin/runai-setup/authentication/sso/#implementation-notes","text":"Run:ai SSO does not support single logout. As such, logging out from Run:ai will not log you out from other systems.","title":"Implementation Notes"},{"location":"admin/runai-setup/cluster-setup/cluster-delete/","text":"Deleting a Cluster Installation \u00b6 To delete a Run:ai Cluster installation while retaining existing running jobs, run the following commands: Version 2.9 or later Version 2.8 Version 2.7 or earlier helm delete runai-cluster -n runai kubectl delete RunaiConfig runai -n runai helm delete runai-cluster -n runai kubectl patch RunaiConfig runai -n runai -p '{\"metadata\":{\"finalizers\":[]}}' --type=\"merge\" kubectl delete RunaiConfig runai -n runai helm delete runai-cluster runai -n runai The commands will not delete existing Jobs submitted by users.","title":"Cluster Delete"},{"location":"admin/runai-setup/cluster-setup/cluster-delete/#deleting-a-cluster-installation","text":"To delete a Run:ai Cluster installation while retaining existing running jobs, run the following commands: Version 2.9 or later Version 2.8 Version 2.7 or earlier helm delete runai-cluster -n runai kubectl delete RunaiConfig runai -n runai helm delete runai-cluster -n runai kubectl patch RunaiConfig runai -n runai -p '{\"metadata\":{\"finalizers\":[]}}' --type=\"merge\" kubectl delete RunaiConfig runai -n runai helm delete runai-cluster runai -n runai The commands will not delete existing Jobs submitted by users.","title":"Deleting a Cluster Installation"},{"location":"admin/runai-setup/cluster-setup/cluster-install/","text":"Below are instructions on how to install a Run:ai cluster. Before installing, please review the installation prerequisites here: Run:ai GPU Cluster Prerequisites . Important We strongly recommend running the Run:ai pre-install script to verify that all prerequisites are met. Starting version 2.9 you must pre-install NGINX ingress controller Starting version 2.9 you must pre-install the Prometheus stack . Install Run:ai \u00b6 Log in to Run:ai user interface at <company-name>.run.ai . Use credentials provided by Run:ai Customer Support: If no clusters are currently configured, you will see a Cluster installation wizard If a cluster has already been configured, use the menu on the top left and select \"Clusters\". On the top right, click \"Add New Cluster\". Using the Wizard: Choose a target Kubernetes platform (see table above) (SaaS only) Provide a domain name for your cluster as described here . (SaaS only) Install a trusted certificate to the domain within Kubernetes. Download a Helm values YAML file runai-<cluster-name>.yaml (Optional) customize the values file. See Customize Cluster Installation Install Helm For RKE and EKS, perform the steps here Run the helm commands as provided in the wizard. Info To install a specific version, add --version <version> to the install command. You can find available versions by running helm search repo -l runai-cluster . Verify your Installation \u00b6 Go to <company-name>.run.ai/dashboards/now . Verify that the number of GPUs on the top right reflects your GPU resources on your cluster and the list of machines with GPU resources appears on the bottom line. Version 2.9 and up Run: kubectl get cm runai-public -n runai -o jsonpath='{.data}' | yq -P (assumes the yq is instaled) Example output: cluster-version : 2.9.0 runai-public : version : 2.9.0 runaiConfigStatus : # (1) conditions : - type : DependenciesFulfilled status : \"True\" reason : dependencies_fulfilled message : Dependencies are fulfilled - type : Deployed status : \"True\" reason : deployed message : Resources Deployed - type : Available status : \"True\" reason : available message : System Available - type : Reconciled status : \"True\" reason : reconciled message : Reconciliation completed successfully optional : # (2) knative : # (3) components : hpa : available : true knative : available : true kourier : available : true mpi : # (4) available : true Verifies that all mandatory dependencies are met: NVIDIA GPU Operator, Prometheus and NGINX controller. Checks whether optional product dependencies have been met. See Inference prerequisites . See distributed training prerequisites . For a more extensive verification of cluster health, see Determining the health of a cluster . Researcher Authentication \u00b6 You must now set up Researcher Access Control . (Optional) Set Node Roles \u00b6 When installing a production cluster you may want to: Set one or more Run:ai system nodes. These are nodes dedicated to Run:ai software. Machine learning frequently requires jobs that require CPU but not GPU . You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. Limit Run:ai to specific nodes in the cluster. To perform these tasks. See Set Node Roles . Next Steps \u00b6 Set up Run:ai Users Working with Users . Set up Projects for Researchers Working with Projects . Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users. Review advanced setup and maintenance scenarios.","title":"Cluster Install"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#install-runai","text":"Log in to Run:ai user interface at <company-name>.run.ai . Use credentials provided by Run:ai Customer Support: If no clusters are currently configured, you will see a Cluster installation wizard If a cluster has already been configured, use the menu on the top left and select \"Clusters\". On the top right, click \"Add New Cluster\". Using the Wizard: Choose a target Kubernetes platform (see table above) (SaaS only) Provide a domain name for your cluster as described here . (SaaS only) Install a trusted certificate to the domain within Kubernetes. Download a Helm values YAML file runai-<cluster-name>.yaml (Optional) customize the values file. See Customize Cluster Installation Install Helm For RKE and EKS, perform the steps here Run the helm commands as provided in the wizard. Info To install a specific version, add --version <version> to the install command. You can find available versions by running helm search repo -l runai-cluster .","title":"Install Run:ai"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#verify-your-installation","text":"Go to <company-name>.run.ai/dashboards/now . Verify that the number of GPUs on the top right reflects your GPU resources on your cluster and the list of machines with GPU resources appears on the bottom line. Version 2.9 and up Run: kubectl get cm runai-public -n runai -o jsonpath='{.data}' | yq -P (assumes the yq is instaled) Example output: cluster-version : 2.9.0 runai-public : version : 2.9.0 runaiConfigStatus : # (1) conditions : - type : DependenciesFulfilled status : \"True\" reason : dependencies_fulfilled message : Dependencies are fulfilled - type : Deployed status : \"True\" reason : deployed message : Resources Deployed - type : Available status : \"True\" reason : available message : System Available - type : Reconciled status : \"True\" reason : reconciled message : Reconciliation completed successfully optional : # (2) knative : # (3) components : hpa : available : true knative : available : true kourier : available : true mpi : # (4) available : true Verifies that all mandatory dependencies are met: NVIDIA GPU Operator, Prometheus and NGINX controller. Checks whether optional product dependencies have been met. See Inference prerequisites . See distributed training prerequisites . For a more extensive verification of cluster health, see Determining the health of a cluster .","title":"Verify your Installation"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#researcher-authentication","text":"You must now set up Researcher Access Control .","title":"Researcher Authentication"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#optional-set-node-roles","text":"When installing a production cluster you may want to: Set one or more Run:ai system nodes. These are nodes dedicated to Run:ai software. Machine learning frequently requires jobs that require CPU but not GPU . You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. Limit Run:ai to specific nodes in the cluster. To perform these tasks. See Set Node Roles .","title":"(Optional) Set Node Roles"},{"location":"admin/runai-setup/cluster-setup/cluster-install/#next-steps","text":"Set up Run:ai Users Working with Users . Set up Projects for Researchers Working with Projects . Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users. Review advanced setup and maintenance scenarios.","title":"Next Steps"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/","text":"Below are the prerequisites of a cluster installed with Run:ai. Prerequisites in a Nutshell \u00b6 The following is a checklist of the Run:ai prerequisites: Prerequisite Details Kubernetes Verify certified vendor and correct version. NVIDIA GPU Operator Different Kubernetes flavors have slightly different setup instructions. Verify correct version. Ingress Controller Install and configure NGINX (some Kubernetes flavors have NGINX pre-installed). Version 2.7 or earlier of Run:ai already installs NGINX as part of the Run:ai cluster installation. Prometheus Install Prometheus. Version 2.8 or earlier of Run:ai already installs NGINX as part of the Run:ai cluster installation. Trusted domain name You must provide a trusted domain name (Version 2.7: a cluster IP). Accessible only inside the organization Cert manager For RKE and EKS, you must install cert management if not already installed and configure Run:ai to use it (Optional) Distributed Training Install Kubeflow MPI if required. (Optional) Inference Some third party software needs to be installed to use the Inference module. There are also specific hardware , operating system and network access requirements. A pre-install script is available to test if the prerequisites are met before installation. Software Requirements \u00b6 Operating System \u00b6 Run:ai will work on any Linux operating system that is supported by both Kubernetes and NVIDIA . Having said that, Run:ai performs its internal tests on Ubuntu 20.04 (and CoreOS for OpenShift). The exception is GKE (Google Kubernetes Engine) where Run:ai has only been certified on an Ubuntu image. Kubernetes \u00b6 Run:ai requires Kubernetes. The latest Run:ai version supports Kubernetes versions 1.21 through 1.26 and OpenShift 4.8 to 4.11. Run:ai does not support Pod Security Admission . Run:ai has been tested with the following Kubernetes distributions: Target Platform Description Installation Notes Vanilla Kubernetes Using no specific distribution but rather k8s native installation OCP OpenShift Container Platform The Run:ai operator is certified for OpenShift by Red Hat. EKS Amazon Elastic Kubernetes Service AKS Azure Kubernetes Services GKE Google Kubernetes Engine RKE Rancher Kubernetes Engine When installing Run:ai, select On Premise . RKE2 has a defect which requires a specific installation flow. Please contact Run:ai customer support for additional details. Bright NVIDIA Bright Cluster Manager Ezmeral HPE Ezmeral Container Platform See Run:ai at Ezmeral marketplace Tanzu VMWare Kubernetes Tanzu supports containerd rather than docker . See the NVIDIA prerequisites below as well as cluster customization for changes required for containerd Run:ai provides instructions for a simple (non-production-ready) Kubernetes Installation . Notes Kubernetes recommends the usage of the systemd as the container runtime cgroup driver . Kubernetes 1.22 and above defaults to systemd . Run:ai Supports Kubernetes Pos Security Policy if used. Pod Security Policy is deprecated and will be removed from Kubernetes 1.25. As such, Run:ai has removed support for PSP in Run:ai 2.9 NVIDIA \u00b6 Run:ai requires NVIDIA GPU Operator version 1.9 and 22.9.0. The interim versions (1.10 and 1.11) have a documented issue as per the note below. Important NVIDIA GPU Operator has a bug that affects metrics and scheduling. The bug affects NVIDIA GPU Operator versions 1.10 and 1.11 but does not exist in 1.9 and is resolved in 22.9.0. For more details see NVIDIA bug report . On Prem EKS GKE RKE Follow the Getting Started guide to install the NVIDIA GPU Operator . Do not install the NVIDIA device plug-in (as we want the NVIDIA GPU Operator to install it instead). When using the eksctl tool to create an AWS EKS cluster, use the flag --install-nvidia-plugin=false to disable this install. Follow the Getting Started guide to install the NVIDIA GPU Operator . For GPU nodes, EKS uses an AMI which already contains the NVIDIA drivers. As such, you must use the GPU Operator flags: --set driver.enabled=false . Create the gpu-operator namespace by running kubectl create ns gpu-operator Before installing the GPU Operator you must create the following file: resourcequota.yaml apiVersion : v1 kind : ResourceQuota metadata : name : gcp-critical-pods namespace : gpu-operator spec : scopeSelector : matchExpressions : - operator : In scopeName : PriorityClass values : - system-node-critical - system-cluster-critical Then run: kubectl apply -f resourcequota.yaml Important Run:ai on GKE has only been tested with GPU Operator version 1.11.1 and up. The above only works for Run:ai 2.7.16 and above. Install the NVIDIA GPU Operator as discussed here . Notes Use the default namespace gpu-operator . Otherwise, you must specify the target namespace using the flag runai-operator.config.nvidiaDcgmExporter.namespace as described in customized cluster installation . NVIDIA drivers may already be installed on the nodes. In such cases, use the NVIDIA GPU Operator flags --set driver.enabled=false . DGX OS is one such example as it comes bundled with NVIDIA Drivers. To work with containerd (e.g. for Tanzu), use the defaultRuntime flag accordingly. To use Dynamic MIG , the GPU Operator must be installed with the flag mig.strategy=mixed . If the GPU Operator is already installed, edit the clusterPolicy by running kubectl patch clusterPolicy cluster-policy -n gpu-operator --type=merge -p '{\"spec\":{\"mig\":{\"strategy\": \"mixed\"}}} Ingress Controller \u00b6 Version 2.8 and up. Run:ai requires an ingress controller as a prerequisite. The Run:ai cluster installation configures one or more ingress objects on top of the controller. There are many ways to install and configure an ingress controller and configuration is environment-dependent. A simple solution is to install & configure NGINX : On Prem Managed Kubernetes helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm upgrade -i nginx-ingress ingress-nginx/ingress-nginx \\ --namespace nginx-ingress --create-namespace \\ --set controller.kind = DaemonSet \\ --set controller.daemonset.useHostPort = true For managed Kubernetes such as EKS: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install nginx-ingress ingress-nginx/ingress-nginx \\ --namespace nginx-ingress --create-namespace For support of ingress controllers different than NGINX please contact Run:ai customer support. Note In a self-hosted installation, the typical scenario is to install the first Run:ai cluster on the same Kubernetes cluster as the control plane. In this case, there is no need to install an ingress controller as it is pre-installed by the control plane. Cluster URL \u00b6 The Run:ai user interface requires a URL to the Kubernetes cluster. You can use a domain name (FQDN) or an IP Address (deprecated). Note In a self-hosted installation, the typical scenario is to install the first Run:ai cluster on the same Kubernetes cluster as the control plane. In this case, the cluster URL need not be provided as it will be the same as the control-plane URL. Domain Name \u00b6 Version 2.8 and up. You must supply a domain name as well as a trusted certificate for that domain. Use an HTTPS-based domain (e.g. https://my-cluster.com ) as the cluster URL. Make sure that the DNS is configured with the cluster IP. In addition, to configure HTTPS for your URL, you must create a TLS secret named runai-cluster-domain-tls-secret in the runai namespace. The secret should contain a trusted certificate for the domain: kubectl create ns runai kubectl create secret tls runai-cluster-domain-tls-secret -n runai \\ --cert /path/to/fullchain.pem \\ # (1) --key /path/to/private.pem # (2) The domain's cert (public key). The domain's private key. For more information on how to create a TLS secret see: https://kubernetes.io/docs/concepts/configuration/secret/#tls-secrets . Cluster IP \u00b6 (Deprecated in version 2.8, no longer available in version 2.9) Following are instructions on how to get the IP and set firewall settings. Unmanaged Kubernetes Unmanaged Kubernetes on the cloud EKS GKE Use the node IP of any of the Kubernetes nodes. Set up the firewall such that the IP is available to Researchers running within the organization (but not outside the organization). Use the node IPs of any of the Kubernetes nodes. Both internal and external IP in the format external-IP,internal-IP . Set up the firewall such that the external IP is available to Researchers running within the organization (but not outside the organization). You will need to externalize an IP address via a load balancer. If you do not have an existing load balancer already, install NGINX as follows: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install nginx-ingress ingress-nginx/ingress-nginx Find the Cluster IP by running: echo $( kubectl get svc nginx-ingress-ingress-nginx-controller -o = jsonpath = '{.status.loadBalancer.ingress[0].hostname}' ) You will need to externalize an IP address via a load balancer. If you do not have an existing load balancer already, install NGINX as follows: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install nginx-ingress ingress-nginx/ingress-nginx Find the Cluster IP by running: echo $( kubectl get svc nginx-ingress-ingress-nginx-controller -o = jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) Prometheus \u00b6 Version 2.9 or later Version 2.8 or below If not already installed on your cluster, install the full kube-prometheus-stack through the Prometheus community Operator Due to a Prometheus bug describe here you may need to apply the Prometheus CRDs as follows: git clone https://github.com/prometheus-community/helm-charts.git kubectl apply --server-side -f helm-charts/charts/kube-prometheus-stack/crds/ --force-conflicts helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring --create-namespace The Run:ai Cluster installation will, by default, install Prometheus , but it can also connect to an existing Prometheus instance installed by the organization. Such a configuration can only be performed together with Run:ai support. In the latter case, it's important to: Verify that both Prometheus Node Exporter and kube-state-metrics are installed. Both are part of the default Prometheus installation Understand how Prometheus has been installed. Whether directly or with the Prometheus Operator . The distinction is important during the Run:ai Cluster installation. Cert Manager \u00b6 Rancher Kubernetes Engine (RKE) and Amazon Elastic Kubernetes Engine (EKS) require a certificate manager as described here . You must then configure Run:ai to use the cert-manager. When creating a cluster on the Run:ai user interface: Download the \"On Premise\" Kubernetes type. Edit the cluster values file and change useCertManager to true init-ca : enabled : true useCertManager : true Distributed Training \u00b6 Distributed training is the ability to run workloads on multiple nodes (not just multiple GPUs on the same node). Run:ai provides this capability via Kubeflow MPI. If you need this functionality, you will need to install the Kubeflow MPI Operator . Run:ai supports MPI version 0.3.0 (the latest version at the time of writing). Use the following installation guide . As per instruction, run: Verify that the mpijob custom resource does not currently exist in the cluster by running kubectl get crds | grep mpijobs . If it does, delete it by running kubectl delete crd mpijobs.kubeflow.org run kubectl apply -f https://raw.githubusercontent.com/kubeflow/mpi-operator/master/deploy/v2beta1/mpi-operator.yaml Inference \u00b6 To use the Run:ai inference module you must pre-install Knative Serving . Follow the instructions here to install. Run:ai is certified on Knative 1.4 to 1.8 with Kubernetes 1.22 or later. Post-install, you must configure Knative to use the Run:ai scheduler and allow pod affinity, by running: kubectl patch configmap/config-features \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"kubernetes.podspec-schedulername\":\"enabled\",\"kubernetes.podspec-affinity\":\"enabled\"}}' Inference Autoscaling \u00b6 Run:ai allows to autoscale a deployment according to various metrics: GPU Utilization (%) CPU Utilization (%) Latency (milliseconds) Throughput (requests/second) Concurrency Any custom metric Additional installation may be needed for some of the metrics as follows: Using Throughput or Concurrency does not require any additional installation. Any other metric will require installing the HPA Autoscaler . Using GPU Utilization , Latency or Custom metric will also require the Prometheus adapter. The Prometheus adapter is part of the Run:ai installer and can be added by setting the prometheus-adapter.enabled flag to true . See Customizing the Run:ai installation for further information. If you wish to use an existing Prometheus adapter installation, you will need to configure it manually with the Run:ai Prometheus rules, specified in the Run:ai chart values under prometheus-adapter.rules field. For further information please contact Run:ai customer support. Accessing Inference from outside the Cluster \u00b6 Inference workloads will typically be accessed by consumers residing outside the cluster. You will hence want to provide consumers with a URL to access the workload. The URL can be found in the Run:ai user interface under the deployment screen (alternatively, run kubectl get ksvc -n <project-namespace> ). However, for the URL to be accessible outside the cluster you must configure your DNS as described here . Altenative Configuration When the above DNS configuration is not possible, you can manually add the Host header to the REST request as follows: Get an <external-ip> by running kubectl get service -n kourier-system kourier . If you have been using istio during Run:ai installation, run: kubectl -n istio-system get service istio-ingressgateway instead. Send a request to your workload by using the external ip, and place the workload url as a Host header. For example curl http://<external-ip>/<container-specific-path> -H 'Host: <host-name>' Hardware Requirements \u00b6 (see picture below) (Production only) Run:ai System Nodes : To reduce downtime and save CPU cycles on expensive GPU Machines, we recommend that production deployments will contain two or more worker machines, designated for Run:ai Software. The nodes do not have to be dedicated to Run:ai, but for Run:ai purposes we would need: 4 CPUs 8GB of RAM 50GB of Disk space Shared data volume: Run:ai uses Kubernetes to abstract away the machine on which a container is running: Researcher containers: The Researcher's containers need to be able to access data from any machine in a uniform way, to access training data and code as well as save checkpoints, weights, and other machine-learning-related artifacts. The Run:ai system needs to save data on a storage device that is not dependent on a specific node. Typically, this is achieved via Network File Storage (NFS) or Network-attached storage (NAS). Docker Registry: With Run:ai, Workloads are based on Docker images. For container images to run on any machine, these images must be downloaded from a docker registry rather than reside on the local machine (though this also is possible ). You can use a public registry such as docker hub or set up a local registry on-prem (preferably on a dedicated machine). Run:ai can assist with setting up the repository. Kubernetes: Production Kubernetes installation requires separate nodes for the Kubernetes master. For more details see your specific Kubernetes distribution documentation. User requirements \u00b6 Usage of containers and images: The individual Researcher's work should be based on container images. Network Access Requirements \u00b6 Internal networking: Kubernetes networking is an add-on rather than a core part of Kubernetes. Different add-ons have different network requirements. You should consult the documentation of the specific add-on on which ports to open. It is however important to note that unless special provisions are made, Kubernetes assumes all cluster nodes can interconnect using all ports. Outbound network: Run:ai user interface runs from the cloud. All container nodes must be able to connect to the Run:ai cloud. Inbound connectivity (connecting from the cloud into nodes) is not required. If outbound connectivity is proxied/limited, the following exceptions should be applied: During Installation \u00b6 Run:ai requires an installation over the Kubernetes cluster. The installation access the web to download various images and registries. Some organizations place limitations on what you can pull from the internet. The following list shows the various solution components and their origin: Name Description URLs Ports Run:ai Repository Run:ai Helm Package Repository runai-charts.storage.googleapis.com 443 Docker Images Repository Run:ai images gcr.io/run-ai-prod 443 Docker Images Repository Third party Images hub.docker.com quay.io 443 Cert Manager (Run:ai version 2.7 or lower only) Creates a letsencrypt-based certificate for the cluster 8.8.8.8, 1.1.1.1, dynu.com 53 Post Installation \u00b6 In addition, once running, Run:ai requires an outbound network connection to the following targets: Name Description URLs Ports Grafana Grafana Metrics Server prometheus-us-central1.grafana.net 443 Run:ai Run:ai Cloud instance app.run.ai 443 Cert Manager (Run:ai version 2.7 or lower only) Creates a letsencrypt-based certificate for the cluster 8.8.8.8, 1.1.1.1, dynu.com 53 Pre-install Script \u00b6 Once you believe that the Run:ai prerequisites are met, we highly recommend installing and running the Run:ai pre-install diagnostics script . The tool: Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking. Looks at additional components installed and analyze their relevance to a successful Run:ai installation. To use the script download the latest version of the script and run: chmod +x preinstall-diagnostics-<platform> ./preinstall-diagnostics-<platform> If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file runai-preinstall-diagnostics.txt in the current directory and send it to Run:ai technical support. For more information on the script including additional command-line flags, see here .","title":"Prerequisites"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#prerequisites-in-a-nutshell","text":"The following is a checklist of the Run:ai prerequisites: Prerequisite Details Kubernetes Verify certified vendor and correct version. NVIDIA GPU Operator Different Kubernetes flavors have slightly different setup instructions. Verify correct version. Ingress Controller Install and configure NGINX (some Kubernetes flavors have NGINX pre-installed). Version 2.7 or earlier of Run:ai already installs NGINX as part of the Run:ai cluster installation. Prometheus Install Prometheus. Version 2.8 or earlier of Run:ai already installs NGINX as part of the Run:ai cluster installation. Trusted domain name You must provide a trusted domain name (Version 2.7: a cluster IP). Accessible only inside the organization Cert manager For RKE and EKS, you must install cert management if not already installed and configure Run:ai to use it (Optional) Distributed Training Install Kubeflow MPI if required. (Optional) Inference Some third party software needs to be installed to use the Inference module. There are also specific hardware , operating system and network access requirements. A pre-install script is available to test if the prerequisites are met before installation.","title":"Prerequisites in a Nutshell"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#software-requirements","text":"","title":"Software Requirements"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#operating-system","text":"Run:ai will work on any Linux operating system that is supported by both Kubernetes and NVIDIA . Having said that, Run:ai performs its internal tests on Ubuntu 20.04 (and CoreOS for OpenShift). The exception is GKE (Google Kubernetes Engine) where Run:ai has only been certified on an Ubuntu image.","title":"Operating System"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#kubernetes","text":"Run:ai requires Kubernetes. The latest Run:ai version supports Kubernetes versions 1.21 through 1.26 and OpenShift 4.8 to 4.11. Run:ai does not support Pod Security Admission . Run:ai has been tested with the following Kubernetes distributions: Target Platform Description Installation Notes Vanilla Kubernetes Using no specific distribution but rather k8s native installation OCP OpenShift Container Platform The Run:ai operator is certified for OpenShift by Red Hat. EKS Amazon Elastic Kubernetes Service AKS Azure Kubernetes Services GKE Google Kubernetes Engine RKE Rancher Kubernetes Engine When installing Run:ai, select On Premise . RKE2 has a defect which requires a specific installation flow. Please contact Run:ai customer support for additional details. Bright NVIDIA Bright Cluster Manager Ezmeral HPE Ezmeral Container Platform See Run:ai at Ezmeral marketplace Tanzu VMWare Kubernetes Tanzu supports containerd rather than docker . See the NVIDIA prerequisites below as well as cluster customization for changes required for containerd Run:ai provides instructions for a simple (non-production-ready) Kubernetes Installation . Notes Kubernetes recommends the usage of the systemd as the container runtime cgroup driver . Kubernetes 1.22 and above defaults to systemd . Run:ai Supports Kubernetes Pos Security Policy if used. Pod Security Policy is deprecated and will be removed from Kubernetes 1.25. As such, Run:ai has removed support for PSP in Run:ai 2.9","title":"Kubernetes"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#nvidia","text":"Run:ai requires NVIDIA GPU Operator version 1.9 and 22.9.0. The interim versions (1.10 and 1.11) have a documented issue as per the note below. Important NVIDIA GPU Operator has a bug that affects metrics and scheduling. The bug affects NVIDIA GPU Operator versions 1.10 and 1.11 but does not exist in 1.9 and is resolved in 22.9.0. For more details see NVIDIA bug report . On Prem EKS GKE RKE Follow the Getting Started guide to install the NVIDIA GPU Operator . Do not install the NVIDIA device plug-in (as we want the NVIDIA GPU Operator to install it instead). When using the eksctl tool to create an AWS EKS cluster, use the flag --install-nvidia-plugin=false to disable this install. Follow the Getting Started guide to install the NVIDIA GPU Operator . For GPU nodes, EKS uses an AMI which already contains the NVIDIA drivers. As such, you must use the GPU Operator flags: --set driver.enabled=false . Create the gpu-operator namespace by running kubectl create ns gpu-operator Before installing the GPU Operator you must create the following file: resourcequota.yaml apiVersion : v1 kind : ResourceQuota metadata : name : gcp-critical-pods namespace : gpu-operator spec : scopeSelector : matchExpressions : - operator : In scopeName : PriorityClass values : - system-node-critical - system-cluster-critical Then run: kubectl apply -f resourcequota.yaml Important Run:ai on GKE has only been tested with GPU Operator version 1.11.1 and up. The above only works for Run:ai 2.7.16 and above. Install the NVIDIA GPU Operator as discussed here . Notes Use the default namespace gpu-operator . Otherwise, you must specify the target namespace using the flag runai-operator.config.nvidiaDcgmExporter.namespace as described in customized cluster installation . NVIDIA drivers may already be installed on the nodes. In such cases, use the NVIDIA GPU Operator flags --set driver.enabled=false . DGX OS is one such example as it comes bundled with NVIDIA Drivers. To work with containerd (e.g. for Tanzu), use the defaultRuntime flag accordingly. To use Dynamic MIG , the GPU Operator must be installed with the flag mig.strategy=mixed . If the GPU Operator is already installed, edit the clusterPolicy by running kubectl patch clusterPolicy cluster-policy -n gpu-operator --type=merge -p '{\"spec\":{\"mig\":{\"strategy\": \"mixed\"}}}","title":"NVIDIA"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#ingress-controller","text":"Version 2.8 and up. Run:ai requires an ingress controller as a prerequisite. The Run:ai cluster installation configures one or more ingress objects on top of the controller. There are many ways to install and configure an ingress controller and configuration is environment-dependent. A simple solution is to install & configure NGINX : On Prem Managed Kubernetes helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm upgrade -i nginx-ingress ingress-nginx/ingress-nginx \\ --namespace nginx-ingress --create-namespace \\ --set controller.kind = DaemonSet \\ --set controller.daemonset.useHostPort = true For managed Kubernetes such as EKS: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install nginx-ingress ingress-nginx/ingress-nginx \\ --namespace nginx-ingress --create-namespace For support of ingress controllers different than NGINX please contact Run:ai customer support. Note In a self-hosted installation, the typical scenario is to install the first Run:ai cluster on the same Kubernetes cluster as the control plane. In this case, there is no need to install an ingress controller as it is pre-installed by the control plane.","title":"Ingress Controller"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#cluster-url","text":"The Run:ai user interface requires a URL to the Kubernetes cluster. You can use a domain name (FQDN) or an IP Address (deprecated). Note In a self-hosted installation, the typical scenario is to install the first Run:ai cluster on the same Kubernetes cluster as the control plane. In this case, the cluster URL need not be provided as it will be the same as the control-plane URL.","title":"Cluster URL"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#domain-name","text":"Version 2.8 and up. You must supply a domain name as well as a trusted certificate for that domain. Use an HTTPS-based domain (e.g. https://my-cluster.com ) as the cluster URL. Make sure that the DNS is configured with the cluster IP. In addition, to configure HTTPS for your URL, you must create a TLS secret named runai-cluster-domain-tls-secret in the runai namespace. The secret should contain a trusted certificate for the domain: kubectl create ns runai kubectl create secret tls runai-cluster-domain-tls-secret -n runai \\ --cert /path/to/fullchain.pem \\ # (1) --key /path/to/private.pem # (2) The domain's cert (public key). The domain's private key. For more information on how to create a TLS secret see: https://kubernetes.io/docs/concepts/configuration/secret/#tls-secrets .","title":"Domain Name"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#cluster-ip","text":"(Deprecated in version 2.8, no longer available in version 2.9) Following are instructions on how to get the IP and set firewall settings. Unmanaged Kubernetes Unmanaged Kubernetes on the cloud EKS GKE Use the node IP of any of the Kubernetes nodes. Set up the firewall such that the IP is available to Researchers running within the organization (but not outside the organization). Use the node IPs of any of the Kubernetes nodes. Both internal and external IP in the format external-IP,internal-IP . Set up the firewall such that the external IP is available to Researchers running within the organization (but not outside the organization). You will need to externalize an IP address via a load balancer. If you do not have an existing load balancer already, install NGINX as follows: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install nginx-ingress ingress-nginx/ingress-nginx Find the Cluster IP by running: echo $( kubectl get svc nginx-ingress-ingress-nginx-controller -o = jsonpath = '{.status.loadBalancer.ingress[0].hostname}' ) You will need to externalize an IP address via a load balancer. If you do not have an existing load balancer already, install NGINX as follows: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install nginx-ingress ingress-nginx/ingress-nginx Find the Cluster IP by running: echo $( kubectl get svc nginx-ingress-ingress-nginx-controller -o = jsonpath = '{.status.loadBalancer.ingress[0].ip}' )","title":"Cluster IP"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#prometheus","text":"Version 2.9 or later Version 2.8 or below If not already installed on your cluster, install the full kube-prometheus-stack through the Prometheus community Operator Due to a Prometheus bug describe here you may need to apply the Prometheus CRDs as follows: git clone https://github.com/prometheus-community/helm-charts.git kubectl apply --server-side -f helm-charts/charts/kube-prometheus-stack/crds/ --force-conflicts helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring --create-namespace The Run:ai Cluster installation will, by default, install Prometheus , but it can also connect to an existing Prometheus instance installed by the organization. Such a configuration can only be performed together with Run:ai support. In the latter case, it's important to: Verify that both Prometheus Node Exporter and kube-state-metrics are installed. Both are part of the default Prometheus installation Understand how Prometheus has been installed. Whether directly or with the Prometheus Operator . The distinction is important during the Run:ai Cluster installation.","title":"Prometheus"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#cert-manager","text":"Rancher Kubernetes Engine (RKE) and Amazon Elastic Kubernetes Engine (EKS) require a certificate manager as described here . You must then configure Run:ai to use the cert-manager. When creating a cluster on the Run:ai user interface: Download the \"On Premise\" Kubernetes type. Edit the cluster values file and change useCertManager to true init-ca : enabled : true useCertManager : true","title":"Cert Manager"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#distributed-training","text":"Distributed training is the ability to run workloads on multiple nodes (not just multiple GPUs on the same node). Run:ai provides this capability via Kubeflow MPI. If you need this functionality, you will need to install the Kubeflow MPI Operator . Run:ai supports MPI version 0.3.0 (the latest version at the time of writing). Use the following installation guide . As per instruction, run: Verify that the mpijob custom resource does not currently exist in the cluster by running kubectl get crds | grep mpijobs . If it does, delete it by running kubectl delete crd mpijobs.kubeflow.org run kubectl apply -f https://raw.githubusercontent.com/kubeflow/mpi-operator/master/deploy/v2beta1/mpi-operator.yaml","title":"Distributed Training"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#inference","text":"To use the Run:ai inference module you must pre-install Knative Serving . Follow the instructions here to install. Run:ai is certified on Knative 1.4 to 1.8 with Kubernetes 1.22 or later. Post-install, you must configure Knative to use the Run:ai scheduler and allow pod affinity, by running: kubectl patch configmap/config-features \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"kubernetes.podspec-schedulername\":\"enabled\",\"kubernetes.podspec-affinity\":\"enabled\"}}'","title":"Inference"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#inference-autoscaling","text":"Run:ai allows to autoscale a deployment according to various metrics: GPU Utilization (%) CPU Utilization (%) Latency (milliseconds) Throughput (requests/second) Concurrency Any custom metric Additional installation may be needed for some of the metrics as follows: Using Throughput or Concurrency does not require any additional installation. Any other metric will require installing the HPA Autoscaler . Using GPU Utilization , Latency or Custom metric will also require the Prometheus adapter. The Prometheus adapter is part of the Run:ai installer and can be added by setting the prometheus-adapter.enabled flag to true . See Customizing the Run:ai installation for further information. If you wish to use an existing Prometheus adapter installation, you will need to configure it manually with the Run:ai Prometheus rules, specified in the Run:ai chart values under prometheus-adapter.rules field. For further information please contact Run:ai customer support.","title":"Inference Autoscaling"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#accessing-inference-from-outside-the-cluster","text":"Inference workloads will typically be accessed by consumers residing outside the cluster. You will hence want to provide consumers with a URL to access the workload. The URL can be found in the Run:ai user interface under the deployment screen (alternatively, run kubectl get ksvc -n <project-namespace> ). However, for the URL to be accessible outside the cluster you must configure your DNS as described here . Altenative Configuration When the above DNS configuration is not possible, you can manually add the Host header to the REST request as follows: Get an <external-ip> by running kubectl get service -n kourier-system kourier . If you have been using istio during Run:ai installation, run: kubectl -n istio-system get service istio-ingressgateway instead. Send a request to your workload by using the external ip, and place the workload url as a Host header. For example curl http://<external-ip>/<container-specific-path> -H 'Host: <host-name>'","title":"Accessing Inference from outside the Cluster"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#hardware-requirements","text":"(see picture below) (Production only) Run:ai System Nodes : To reduce downtime and save CPU cycles on expensive GPU Machines, we recommend that production deployments will contain two or more worker machines, designated for Run:ai Software. The nodes do not have to be dedicated to Run:ai, but for Run:ai purposes we would need: 4 CPUs 8GB of RAM 50GB of Disk space Shared data volume: Run:ai uses Kubernetes to abstract away the machine on which a container is running: Researcher containers: The Researcher's containers need to be able to access data from any machine in a uniform way, to access training data and code as well as save checkpoints, weights, and other machine-learning-related artifacts. The Run:ai system needs to save data on a storage device that is not dependent on a specific node. Typically, this is achieved via Network File Storage (NFS) or Network-attached storage (NAS). Docker Registry: With Run:ai, Workloads are based on Docker images. For container images to run on any machine, these images must be downloaded from a docker registry rather than reside on the local machine (though this also is possible ). You can use a public registry such as docker hub or set up a local registry on-prem (preferably on a dedicated machine). Run:ai can assist with setting up the repository. Kubernetes: Production Kubernetes installation requires separate nodes for the Kubernetes master. For more details see your specific Kubernetes distribution documentation.","title":"Hardware Requirements"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#user-requirements","text":"Usage of containers and images: The individual Researcher's work should be based on container images.","title":"User requirements"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#network-access-requirements","text":"Internal networking: Kubernetes networking is an add-on rather than a core part of Kubernetes. Different add-ons have different network requirements. You should consult the documentation of the specific add-on on which ports to open. It is however important to note that unless special provisions are made, Kubernetes assumes all cluster nodes can interconnect using all ports. Outbound network: Run:ai user interface runs from the cloud. All container nodes must be able to connect to the Run:ai cloud. Inbound connectivity (connecting from the cloud into nodes) is not required. If outbound connectivity is proxied/limited, the following exceptions should be applied:","title":"Network Access Requirements"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#during-installation","text":"Run:ai requires an installation over the Kubernetes cluster. The installation access the web to download various images and registries. Some organizations place limitations on what you can pull from the internet. The following list shows the various solution components and their origin: Name Description URLs Ports Run:ai Repository Run:ai Helm Package Repository runai-charts.storage.googleapis.com 443 Docker Images Repository Run:ai images gcr.io/run-ai-prod 443 Docker Images Repository Third party Images hub.docker.com quay.io 443 Cert Manager (Run:ai version 2.7 or lower only) Creates a letsencrypt-based certificate for the cluster 8.8.8.8, 1.1.1.1, dynu.com 53","title":"During Installation"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#post-installation","text":"In addition, once running, Run:ai requires an outbound network connection to the following targets: Name Description URLs Ports Grafana Grafana Metrics Server prometheus-us-central1.grafana.net 443 Run:ai Run:ai Cloud instance app.run.ai 443 Cert Manager (Run:ai version 2.7 or lower only) Creates a letsencrypt-based certificate for the cluster 8.8.8.8, 1.1.1.1, dynu.com 53","title":"Post Installation"},{"location":"admin/runai-setup/cluster-setup/cluster-prerequisites/#pre-install-script","text":"Once you believe that the Run:ai prerequisites are met, we highly recommend installing and running the Run:ai pre-install diagnostics script . The tool: Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking. Looks at additional components installed and analyze their relevance to a successful Run:ai installation. To use the script download the latest version of the script and run: chmod +x preinstall-diagnostics-<platform> ./preinstall-diagnostics-<platform> If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file runai-preinstall-diagnostics.txt in the current directory and send it to Run:ai technical support. For more information on the script including additional command-line flags, see here .","title":"Pre-install Script"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/","text":"This section is a step-by-step guide for setting up a Run:ai cluster. A Run:ai cluster is installed on top of a Kubernetes cluster. A Run:ai cluster connects to the Run:ai control plane on the cloud. The control plane provides a control point as well as a monitoring and control user interface for Administrators. A customer may have multiple Run:ai Clusters, all connecting to a single control plane. For additional details see the Run:ai system components Documents \u00b6 Review Run:ai cluster prerequisites . Step-by-step installation instructions . Look for troubleshooting tips if required. Upgrade cluster and delete cluster instructions. Customization \u00b6 For a list of optional customizations see Customize Installation Additional Configuration \u00b6 For a list of advanced configuration scenarios such as configuring researcher authentication, Single sign-on limiting the installation to specific nodes, and more, see the Configuration Articles section. Next Steps \u00b6 After setting up the cluster, you may want to start setting up Researchers. See: Researcher Setup .","title":"Introduction"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#documents","text":"Review Run:ai cluster prerequisites . Step-by-step installation instructions . Look for troubleshooting tips if required. Upgrade cluster and delete cluster instructions.","title":"Documents"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#customization","text":"For a list of optional customizations see Customize Installation","title":"Customization"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#additional-configuration","text":"For a list of advanced configuration scenarios such as configuring researcher authentication, Single sign-on limiting the installation to specific nodes, and more, see the Configuration Articles section.","title":"Additional Configuration"},{"location":"admin/runai-setup/cluster-setup/cluster-setup-intro/#next-steps","text":"After setting up the cluster, you may want to start setting up Researchers. See: Researcher Setup .","title":"Next Steps"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/","text":"Upgrading a Cluster Installation \u00b6 Find out Run:ai Cluster version \u00b6 To find the Run:ai cluster version before and after upgrade run: helm list -n runai -f runai-cluster and record the chart version in the form of runai-cluster-<version-number> Upgrade Run:ai cluster \u00b6 Upgrade to version 2.9 \u00b6 The process of upgrading to 2.9 requires uninstalling and then installing again. No data is lost during the process. Note The reason for this process is that Run:ai 2.9 cluster installation no longer installs pre-requisites. As such ownership of dependencies such as Prometheus will be undefined if a helm upgrade is ran. After deleting the cluster, install the prerequisites. Specifically Ingress Controller and Prometheus . Upgrade \u00b6 Replace <version> with the new version number in the command below. Then run: kubectl apply -f https://raw.githubusercontent.com/run-ai/public/main/<version>/runai-crds.yaml The number should have 3 digits (for example 1.2.34 ). You can find Run:ai version numbers by running helm search repo -l runai-cluster . Then run: helm repo update helm get values runai-cluster -n runai > values.yaml helm upgrade runai-cluster runai/runai-cluster -n runai -f values.yaml Upgrade to a Specific Verison \u00b6 To upgrade to a specific version, add --version <version-number> to the helm upgrade command. Verify Successful Installation \u00b6 To verify that the upgrade has succeeded run: kubectl get pods -n runai Verify that all pods are running or completed.","title":"Cluster Upgrade"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrading-a-cluster-installation","text":"","title":"Upgrading a Cluster Installation"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#find-out-runai-cluster-version","text":"To find the Run:ai cluster version before and after upgrade run: helm list -n runai -f runai-cluster and record the chart version in the form of runai-cluster-<version-number>","title":"Find out Run:ai Cluster version"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade-runai-cluster","text":"","title":"Upgrade Run:ai cluster"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade-to-version-29","text":"The process of upgrading to 2.9 requires uninstalling and then installing again. No data is lost during the process. Note The reason for this process is that Run:ai 2.9 cluster installation no longer installs pre-requisites. As such ownership of dependencies such as Prometheus will be undefined if a helm upgrade is ran. After deleting the cluster, install the prerequisites. Specifically Ingress Controller and Prometheus .","title":"Upgrade to version 2.9"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade","text":"Replace <version> with the new version number in the command below. Then run: kubectl apply -f https://raw.githubusercontent.com/run-ai/public/main/<version>/runai-crds.yaml The number should have 3 digits (for example 1.2.34 ). You can find Run:ai version numbers by running helm search repo -l runai-cluster . Then run: helm repo update helm get values runai-cluster -n runai > values.yaml helm upgrade runai-cluster runai/runai-cluster -n runai -f values.yaml","title":"Upgrade"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#upgrade-to-a-specific-verison","text":"To upgrade to a specific version, add --version <version-number> to the helm upgrade command.","title":"Upgrade to a Specific Verison"},{"location":"admin/runai-setup/cluster-setup/cluster-upgrade/#verify-successful-installation","text":"To verify that the upgrade has succeeded run: kubectl get pods -n runai Verify that all pods are running or completed.","title":"Verify Successful Installation"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/","text":"(Optional) Customize Cluster Installation \u00b6 The Run:ai cluster creation wizard requires the download of a Helm values file runai-<cluster-name>.yaml . The file may be edited to customize the cluster installation. Configuration Flags \u00b6 Key Default Description runai-operator.config.project-controller.createNamespaces true Set to false if unwilling to provide Run:ai the ability to create namespaces. When set to false, will requires an additional manual step when creating new Run:ai Projects runai-operator.config.project-controller.clusterWideSecret true Set to false when using PodSecurityPolicy or OpenShift runai-operator.config.mps-server.enabled false Set to true to allow the use of NVIDIA MPS . MPS is useful with Inference workloads runai-operator.config.global.runtime docker Defines the container runtime of the cluster (supports docker and containerd ). Set to containerd when using Tanzu runai-operator.config.global.nvidiaDcgmExporter.namespace gpu-operator The namespace where dcgm-exporter (or gpu-operator) was installed runai-operator.config.global.nvidiaDcgmExporter.installedFromGpuOperator true Indicated whether the dcgm-exporter was installed via gpu-operator or not kube-prometheus-stack.enabled true (Version 2.8 or lower) Set to false when the cluster has an existing Prometheus installation that is not based on the Prometheus operator . This setting requires Run:ai customer support kube-prometheus-stack.prometheusOperator.enabled true (Version 2.8 or lower) Set to false when the cluster has an existing Prometheus installation based on the Prometheus operator and Run:ai should use the existing one rather than install a new one prometheus-adapter.enabled false (Version 2.8 or lower) Install Prometheus Adapter. Used for Inference workloads using a custom metric for autoscaling. Set to true if Prometheus Adapter is not already installed in the cluster prometheus-adapter.prometheus The address of the default Prometheus Service (Version 2.8 or lower) If you installed your own custom Prometheus Service, set this field accordingly with url and port Prometheus \u00b6 Version 2.9 or higher Version 2.8 or lower Not relevant The Run:ai Cluster installation uses Prometheus . There are 3 alternative configurations: Run:ai installs Prometheus (default). Run:ai uses an existing Prometheus installation based on the Prometheus operator. Run:ai uses an existing Prometheus installation based on a regular Prometheus installation. For option 2, disable the flag kube-prometheus-stack.prometheusOperator.enabled . For option 3, please contact Run:ai Customer support. For options 2 and 3, if you enabled prometheus-adapter , please configure it as described in the Prometheus Adapter documentation Understanding Custom Access Roles \u00b6 To review the access roles created by the Run:ai Cluster installation, see Understanding Access Roles . Manual Creation of Namespaces \u00b6 Run:ai Projects are implemented as Kubernetes namespaces. By default, the administrator creates a new Project via the Administration user interface which then triggers the creation of a Kubernetes namespace named runai-<PROJECT-NAME> . There are a couple of use cases that customers will want to disable this feature: Some organizations prefer to use their internal naming convention for Kubernetes namespaces, rather than Run:ai's default runai-<PROJECT-NAME> convention. Some organizations will not allow Run:ai to automatically create Kubernetes namespaces. Follow these steps to achieve this: Disable the namespace creation functionality. See the runai-operator.config.project-controller.createNamespaces flag above. Create a Project using the Run:ai User Interface. Create the namespace if needed by running: kubectl create ns <NAMESPACE> . The suggested Run:ai default is runai-<PROJECT-NAME> . Label the namespace to connect it to the Run:ai Project by running kubectl label ns <NAMESPACE> runai/queue=<PROJECT_NAME> , where <PROJECT_NAME> is the name of the project you have created in the Run:ai user interface above and <NAMESPACE> is the name you chose for your namespace. RKE-Specific Setup \u00b6 Rancher Kubernetes Engine (RKE) requires additional steps: Certificate Signing Request (RKE1 only) \u00b6 When creating a cluster on the Run:ai user interface: Download the \"On Premise\" Kubernetes type. Edit the cluster values file and change useCertManager to true init-ca : enabled : true useCertManager : true On the cluster, install Cert manager as follows: helm repo add jetstack https://charts.jetstack.io helm repo update helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true NGINX (both RKE1 and RKE2) \u00b6 RKE comes pre-installed with NGINX. Thus, the Run:ai prerequisite for ingress controller is not needed. Researcher Authentication \u00b6 See the RKE and RKE2 tabs in the Researcher Authentication document, on how to set up researcher authentication.","title":"Customize Installation"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#optional-customize-cluster-installation","text":"The Run:ai cluster creation wizard requires the download of a Helm values file runai-<cluster-name>.yaml . The file may be edited to customize the cluster installation.","title":"(Optional) Customize Cluster Installation"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#configuration-flags","text":"Key Default Description runai-operator.config.project-controller.createNamespaces true Set to false if unwilling to provide Run:ai the ability to create namespaces. When set to false, will requires an additional manual step when creating new Run:ai Projects runai-operator.config.project-controller.clusterWideSecret true Set to false when using PodSecurityPolicy or OpenShift runai-operator.config.mps-server.enabled false Set to true to allow the use of NVIDIA MPS . MPS is useful with Inference workloads runai-operator.config.global.runtime docker Defines the container runtime of the cluster (supports docker and containerd ). Set to containerd when using Tanzu runai-operator.config.global.nvidiaDcgmExporter.namespace gpu-operator The namespace where dcgm-exporter (or gpu-operator) was installed runai-operator.config.global.nvidiaDcgmExporter.installedFromGpuOperator true Indicated whether the dcgm-exporter was installed via gpu-operator or not kube-prometheus-stack.enabled true (Version 2.8 or lower) Set to false when the cluster has an existing Prometheus installation that is not based on the Prometheus operator . This setting requires Run:ai customer support kube-prometheus-stack.prometheusOperator.enabled true (Version 2.8 or lower) Set to false when the cluster has an existing Prometheus installation based on the Prometheus operator and Run:ai should use the existing one rather than install a new one prometheus-adapter.enabled false (Version 2.8 or lower) Install Prometheus Adapter. Used for Inference workloads using a custom metric for autoscaling. Set to true if Prometheus Adapter is not already installed in the cluster prometheus-adapter.prometheus The address of the default Prometheus Service (Version 2.8 or lower) If you installed your own custom Prometheus Service, set this field accordingly with url and port","title":"Configuration Flags"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#prometheus","text":"Version 2.9 or higher Version 2.8 or lower Not relevant The Run:ai Cluster installation uses Prometheus . There are 3 alternative configurations: Run:ai installs Prometheus (default). Run:ai uses an existing Prometheus installation based on the Prometheus operator. Run:ai uses an existing Prometheus installation based on a regular Prometheus installation. For option 2, disable the flag kube-prometheus-stack.prometheusOperator.enabled . For option 3, please contact Run:ai Customer support. For options 2 and 3, if you enabled prometheus-adapter , please configure it as described in the Prometheus Adapter documentation","title":"Prometheus"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#understanding-custom-access-roles","text":"To review the access roles created by the Run:ai Cluster installation, see Understanding Access Roles .","title":"Understanding Custom Access Roles"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#manual-creation-of-namespaces","text":"Run:ai Projects are implemented as Kubernetes namespaces. By default, the administrator creates a new Project via the Administration user interface which then triggers the creation of a Kubernetes namespace named runai-<PROJECT-NAME> . There are a couple of use cases that customers will want to disable this feature: Some organizations prefer to use their internal naming convention for Kubernetes namespaces, rather than Run:ai's default runai-<PROJECT-NAME> convention. Some organizations will not allow Run:ai to automatically create Kubernetes namespaces. Follow these steps to achieve this: Disable the namespace creation functionality. See the runai-operator.config.project-controller.createNamespaces flag above. Create a Project using the Run:ai User Interface. Create the namespace if needed by running: kubectl create ns <NAMESPACE> . The suggested Run:ai default is runai-<PROJECT-NAME> . Label the namespace to connect it to the Run:ai Project by running kubectl label ns <NAMESPACE> runai/queue=<PROJECT_NAME> , where <PROJECT_NAME> is the name of the project you have created in the Run:ai user interface above and <NAMESPACE> is the name you chose for your namespace.","title":"Manual Creation of Namespaces"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#rke-specific-setup","text":"Rancher Kubernetes Engine (RKE) requires additional steps:","title":"RKE-Specific Setup"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#certificate-signing-request-rke1-only","text":"When creating a cluster on the Run:ai user interface: Download the \"On Premise\" Kubernetes type. Edit the cluster values file and change useCertManager to true init-ca : enabled : true useCertManager : true On the cluster, install Cert manager as follows: helm repo add jetstack https://charts.jetstack.io helm repo update helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true","title":"Certificate Signing Request (RKE1 only)"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#nginx-both-rke1-and-rke2","text":"RKE comes pre-installed with NGINX. Thus, the Run:ai prerequisite for ingress controller is not needed.","title":"NGINX (both RKE1 and RKE2)"},{"location":"admin/runai-setup/cluster-setup/customize-cluster-install/#researcher-authentication","text":"See the RKE and RKE2 tabs in the Researcher Authentication document, on how to set up researcher authentication.","title":"Researcher Authentication"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/","text":"Run:ai & NVIDIA DGX Bundle \u00b6 NVIDIA DGX is a line of NVIDIA-produced servers and workstations which specialize in using GPUs to accelerate deep learning applications. NVIDIA DGX comes bundled out of the box with Run:ai. The purpose of this document is to guide you through the process of installing and configuring Run:ai in this scenario NVIDIA Bright Cluster Manager \u00b6 NVIDIA Bright Cluster Manager allows the deployment of software on NVIDIA DGX servers. During the installation of the DGX you will select Run:ai as well as Run:ai prerequisites from the Bright installer. Prerequisites \u00b6 Software Prerequisites \u00b6 Run:ai assumes the following components to be pre-installed: NVIDIA GPU Operator - available for installation via the bright installer Prometheus - available for installation via the bright installer Ingress controller - NGINX is available for installation via the bright installer. Run:ai prerequisites \u00b6 The Run:ai cluster installer will require the following: Run:ai tenant name - provided by Run:ai customer support. Run:ai install secret - provided by Run:ai customer support. Cluster URL - your organization should provide you with a domain name. Private and public keys -your organization should provide a trusted certificate for the above domain name. The Run:ai installer will require both private key and full-chain in PEM format. Post-installation - credentials for the Run:ai user interface. Provided by Run:ai customer support. Installing Run:ai installer \u00b6 Select Run:ai via the bright installer. Remember to select all of the above software prerequisites as well. Using the Run:ai installer \u00b6 Find out the cluster's IP address. Then browse to http://<CLUSTER-IP>:30080/runai-installer . Alternatively use the Bright landing page at http://<CLUSTER-IP>/#runai . Note Use http rather than https . Use the IP and not a domain name. A wizard would open up containing 3 pages: Prerequisites, setup, and installation. Prerequisites Page \u00b6 The first, verification page, verifies that all of the above software prerequisites are met. Press the \"Verify\" button. You will not be able to continue unless all prerequisites are met. When all are met, press the Continue button. Setup Page \u00b6 The setup page asks to provide all of the Run:ai prerequisites described above. The page will verify the Run:ai input (tenant name and install secret) but will not verify the validity of the cluster URL and certificate. If those are incorrect, the Run:ai installation will show as successful but certain aspects of Run:ai will not work. After filling up the form, press Continue . Installation page \u00b6 The Run:ai installation will start. Depending on your download network speed the installation can take from 2 to 10 minutes. When the installation is successful you will see a START USING RUN:AI button. Press the button and enter your credentials to enter the Run:ai user interface. Save the URL for future use. Post-installation. \u00b6 Post installation, you will want to: (Mandatory) Set up Researcher Access Control . Without this, the Job Submit form will not work. Note the Bright section in that document. Set up Run:ai Users Working with Users . Set up Projects for Researchers Working with Projects . Troubleshooting \u00b6 The cluster installer is a pod in Kubernetes. The pod is responsible for the installation preparation and prerequisite gathering phase. In case of an error during this pre-installation, you need to gather the pod's log. Once the Run:ai cluster installation has started, the behavior is identical to any Run:ai cluster installation flavor. See the troubleshooting page .","title":"NVIDIA DGX Bundle"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#runai-nvidia-dgx-bundle","text":"NVIDIA DGX is a line of NVIDIA-produced servers and workstations which specialize in using GPUs to accelerate deep learning applications. NVIDIA DGX comes bundled out of the box with Run:ai. The purpose of this document is to guide you through the process of installing and configuring Run:ai in this scenario","title":"Run:ai &amp; NVIDIA DGX Bundle"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#nvidia-bright-cluster-manager","text":"NVIDIA Bright Cluster Manager allows the deployment of software on NVIDIA DGX servers. During the installation of the DGX you will select Run:ai as well as Run:ai prerequisites from the Bright installer.","title":"NVIDIA Bright Cluster Manager"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#prerequisites","text":"","title":"Prerequisites"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#software-prerequisites","text":"Run:ai assumes the following components to be pre-installed: NVIDIA GPU Operator - available for installation via the bright installer Prometheus - available for installation via the bright installer Ingress controller - NGINX is available for installation via the bright installer.","title":"Software Prerequisites"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#runai-prerequisites","text":"The Run:ai cluster installer will require the following: Run:ai tenant name - provided by Run:ai customer support. Run:ai install secret - provided by Run:ai customer support. Cluster URL - your organization should provide you with a domain name. Private and public keys -your organization should provide a trusted certificate for the above domain name. The Run:ai installer will require both private key and full-chain in PEM format. Post-installation - credentials for the Run:ai user interface. Provided by Run:ai customer support.","title":"Run:ai prerequisites"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#installing-runai-installer","text":"Select Run:ai via the bright installer. Remember to select all of the above software prerequisites as well.","title":"Installing Run:ai installer"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#using-the-runai-installer","text":"Find out the cluster's IP address. Then browse to http://<CLUSTER-IP>:30080/runai-installer . Alternatively use the Bright landing page at http://<CLUSTER-IP>/#runai . Note Use http rather than https . Use the IP and not a domain name. A wizard would open up containing 3 pages: Prerequisites, setup, and installation.","title":"Using the Run:ai installer"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#prerequisites-page","text":"The first, verification page, verifies that all of the above software prerequisites are met. Press the \"Verify\" button. You will not be able to continue unless all prerequisites are met. When all are met, press the Continue button.","title":"Prerequisites Page"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#setup-page","text":"The setup page asks to provide all of the Run:ai prerequisites described above. The page will verify the Run:ai input (tenant name and install secret) but will not verify the validity of the cluster URL and certificate. If those are incorrect, the Run:ai installation will show as successful but certain aspects of Run:ai will not work. After filling up the form, press Continue .","title":"Setup Page"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#installation-page","text":"The Run:ai installation will start. Depending on your download network speed the installation can take from 2 to 10 minutes. When the installation is successful you will see a START USING RUN:AI button. Press the button and enter your credentials to enter the Run:ai user interface. Save the URL for future use.","title":"Installation page"},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#post-installation","text":"Post installation, you will want to: (Mandatory) Set up Researcher Access Control . Without this, the Job Submit form will not work. Note the Bright section in that document. Set up Run:ai Users Working with Users . Set up Projects for Researchers Working with Projects .","title":"Post-installation."},{"location":"admin/runai-setup/cluster-setup/dgx-bundle/#troubleshooting","text":"The cluster installer is a pod in Kubernetes. The pod is responsible for the installation preparation and prerequisite gathering phase. In case of an error during this pre-installation, you need to gather the pod's log. Once the Run:ai cluster installation has started, the behavior is identical to any Run:ai cluster installation flavor. See the troubleshooting page .","title":"Troubleshooting"},{"location":"admin/runai-setup/cluster-setup/install-k8s/","text":"Native Kubernetes Installation \u00b6 Kubernetes is composed of master(s) and workers. The instructions and script below are for creating a bare-bones installation of a single master and several workers for testing purposes. For a more complex, production-grade , Kubernetes installation, use tools such as Rancher Kubernetes Engine, or review Kubernetes documentation to learn how to customize the native installation. Prerequisites: \u00b6 The script below assumes all machines have Ubuntu 18.04 or later. For other Linux-based operating-systems see Kubernetes documentation . The script must be run with ROOT privileges. Inbound ports 6443,443,8080 must be allowed. The script support Kubernetes 1.24 or later. Install Kubernetes \u00b6 Install Kubernetes Master \u00b6 Get the script by running: wget https://raw.githubusercontent.com/run-ai/docs/master/install/kube-install.sh Run the script with ROOT privileges: sudo ./kube-install.sh When prompted, select the option to install Kubernetes master . Select the Kubernetes version you want or press Enter for the default script version. Select the CNI (networking) version or press Enter for the default. When the script finishes, it will prompt a join command_ to be run on all workers. Save the command for later use. Note The default token expires after 24 hours. If the token has expired, go to the master node and run sudo kubeadm token create --print-join-command . This will produce an up-to-date join command. Test that Kubernetes is up by running: kubectl get nodes Verify that the master node is ready Install Kubernetes Workers \u00b6 On each designated worker node: Get the script by running: wget https://raw.githubusercontent.com/run-ai/docs/master/install/kube-install.sh Run the script with ROOT privileges: sudo ./kube-install.sh When prompted, select the option to install Kubernetes worker . Select the Kubernetes version you want or press Enter for the default script version. The version should be the same as the one selected for the Kubernetes master. When the script finishes, run the join command saved above. To test that the worker has successfully joined, on the master node run: kubectl get nodes Verify that the new worker node is showing and ready (may take a couple of seconds). Avoiding Accidental Upgrades \u00b6 To avoid an accidental upgrade of Kubernetes binaries during Linux upgrades, it is recommended to hold the version. Run the following on all nodes: sudo apt-mark hold kubeadm kubelet kubectl Next Steps \u00b6 The administrative Kubernetes profile can be found in the master node under the .kube folder. Reset Nodes \u00b6 The same script also contains an option to completely remove Kubernetes from nodes (master or workers). To use, run: Get the script by running: wget https://raw.githubusercontent.com/run-ai/docs/master/install/kube-install.sh Run the script with ROOT privileges: sudo ./kube-install.sh When prompted, select the option to reset/delete kubernetes . Select yes when prompted to reset the cluster and remove Kubernetes packages.","title":"Kubernetes Install"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#native-kubernetes-installation","text":"Kubernetes is composed of master(s) and workers. The instructions and script below are for creating a bare-bones installation of a single master and several workers for testing purposes. For a more complex, production-grade , Kubernetes installation, use tools such as Rancher Kubernetes Engine, or review Kubernetes documentation to learn how to customize the native installation.","title":"Native Kubernetes Installation"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#prerequisites","text":"The script below assumes all machines have Ubuntu 18.04 or later. For other Linux-based operating-systems see Kubernetes documentation . The script must be run with ROOT privileges. Inbound ports 6443,443,8080 must be allowed. The script support Kubernetes 1.24 or later.","title":"Prerequisites:"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#install-kubernetes","text":"","title":"Install Kubernetes"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#install-kubernetes-master","text":"Get the script by running: wget https://raw.githubusercontent.com/run-ai/docs/master/install/kube-install.sh Run the script with ROOT privileges: sudo ./kube-install.sh When prompted, select the option to install Kubernetes master . Select the Kubernetes version you want or press Enter for the default script version. Select the CNI (networking) version or press Enter for the default. When the script finishes, it will prompt a join command_ to be run on all workers. Save the command for later use. Note The default token expires after 24 hours. If the token has expired, go to the master node and run sudo kubeadm token create --print-join-command . This will produce an up-to-date join command. Test that Kubernetes is up by running: kubectl get nodes Verify that the master node is ready","title":"Install Kubernetes Master"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#install-kubernetes-workers","text":"On each designated worker node: Get the script by running: wget https://raw.githubusercontent.com/run-ai/docs/master/install/kube-install.sh Run the script with ROOT privileges: sudo ./kube-install.sh When prompted, select the option to install Kubernetes worker . Select the Kubernetes version you want or press Enter for the default script version. The version should be the same as the one selected for the Kubernetes master. When the script finishes, run the join command saved above. To test that the worker has successfully joined, on the master node run: kubectl get nodes Verify that the new worker node is showing and ready (may take a couple of seconds).","title":"Install Kubernetes Workers"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#avoiding-accidental-upgrades","text":"To avoid an accidental upgrade of Kubernetes binaries during Linux upgrades, it is recommended to hold the version. Run the following on all nodes: sudo apt-mark hold kubeadm kubelet kubectl","title":"Avoiding Accidental Upgrades"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#next-steps","text":"The administrative Kubernetes profile can be found in the master node under the .kube folder.","title":"Next Steps"},{"location":"admin/runai-setup/cluster-setup/install-k8s/#reset-nodes","text":"The same script also contains an option to completely remove Kubernetes from nodes (master or workers). To use, run: Get the script by running: wget https://raw.githubusercontent.com/run-ai/docs/master/install/kube-install.sh Run the script with ROOT privileges: sudo ./kube-install.sh When prompted, select the option to reset/delete kubernetes . Select yes when prompted to reset the cluster and remove Kubernetes packages.","title":"Reset Nodes"},{"location":"admin/runai-setup/config/access-roles/","text":"Understand the Kubernetes Cluster Access provided to Run:ai \u00b6 Run:ai has configuration flags that control specific behavioral aspects of Run:ai. Specifically, those which require additional permissions. Such as automatic namespace/project creation, secret propagation, and more. The purpose of this document is to provide security officers with the ability to review what cluster-wide access Run:ai requires, and verify that it is in line with organizational policy, before installing the Run:ai cluster. Review Cluster Access Roles \u00b6 Run: helm repo add runai https://run-ai-charts.storage.googleapis.com helm repo update helm install runai-cluster runai/runai-cluster -n runai -f runai-<cluster-name>.yaml \\ --dry-run > cluster-all.yaml The file cluster-all.yaml can be then be reviewed. You can use the internal filenames (provided in comments within the file) to gain more understanding according to the table below: Folder File Purpose clusterroles base.yaml Mandatory Kubernetes Cluster Roles and Cluster Role Bindings clusterroles project-controller-ns-creation.yaml Automatic Project Creation and Maintenance. Provides Run:ai with the ability to create Kubernetes namespaces when the Run:ai administrator creates new Projects. Can be turned on/off via flag clusterroles project-controller-rb-creation.yaml Automatically assign Users to Projects. Can be turned on/off via flag clusterroles project-controller-cluster-wide-secrets.yaml Allow the propagation of Secrets. See Secrets in Jobs . Can be turned on/off via flag clusterroles project-controller-limit-range.yaml Disables the usage of the Kubernetes Limit Range feature. Can be turned on/off via flag ocp scc.yaml OpenShift-specific Security Contexts priorityclasses 4 files Folder contains a list of Priority Classes used by Run:ai","title":"Review Kubernetes Access provided to Run:ai"},{"location":"admin/runai-setup/config/access-roles/#understand-the-kubernetes-cluster-access-provided-to-runai","text":"Run:ai has configuration flags that control specific behavioral aspects of Run:ai. Specifically, those which require additional permissions. Such as automatic namespace/project creation, secret propagation, and more. The purpose of this document is to provide security officers with the ability to review what cluster-wide access Run:ai requires, and verify that it is in line with organizational policy, before installing the Run:ai cluster.","title":"Understand the Kubernetes Cluster Access provided to Run:ai"},{"location":"admin/runai-setup/config/access-roles/#review-cluster-access-roles","text":"Run: helm repo add runai https://run-ai-charts.storage.googleapis.com helm repo update helm install runai-cluster runai/runai-cluster -n runai -f runai-<cluster-name>.yaml \\ --dry-run > cluster-all.yaml The file cluster-all.yaml can be then be reviewed. You can use the internal filenames (provided in comments within the file) to gain more understanding according to the table below: Folder File Purpose clusterroles base.yaml Mandatory Kubernetes Cluster Roles and Cluster Role Bindings clusterroles project-controller-ns-creation.yaml Automatic Project Creation and Maintenance. Provides Run:ai with the ability to create Kubernetes namespaces when the Run:ai administrator creates new Projects. Can be turned on/off via flag clusterroles project-controller-rb-creation.yaml Automatically assign Users to Projects. Can be turned on/off via flag clusterroles project-controller-cluster-wide-secrets.yaml Allow the propagation of Secrets. See Secrets in Jobs . Can be turned on/off via flag clusterroles project-controller-limit-range.yaml Disables the usage of the Kubernetes Limit Range feature. Can be turned on/off via flag ocp scc.yaml OpenShift-specific Security Contexts priorityclasses 4 files Folder contains a list of Priority Classes used by Run:ai","title":"Review Cluster Access Roles"},{"location":"admin/runai-setup/config/allow-external-access-to-containers/","text":"Introduction \u00b6 Researchers working with containers. many times need to remotely access the container. Some examples: Using a Jupyter notebook that runs within the container Using PyCharm to run python commands remotely. Using TensorBoard to view machine learning visualizations This requires exposing container ports . When using docker, the way Researchers expose ports is by declaring them when starting the container. Run:ai has similar syntax. Run:ai is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers several options: Method Description Prerequisites Port Forwarding Simple port forwarding allows access to the container via local and/or remote port. None NodePort Exposes the service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service from outside the cluster by requesting <NODE-IP>:<NODE-PORT> regardless of which node the container actually resides in. None LoadBalancer Exposes the service externally using a cloud provider\u2019s load balancer. Only available with cloud providers See https://kubernetes.io/docs/concepts/services-networking/service for further details on these four options. Workspaces configuration \u00b6 Version 2.9 and up Version 2.9 introduces Workspaces which allow the Researcher to build AI models interactively. Workspaces allow the Researcher to launch tools such as Visual Studio code, TensorFlow, TensorBoard etc. These tools require access to the container. Access is provided via URLs. Run:ai uses the Cluster URL provided to dynamically create SSL-secured URLs for researchers\u2019 workspaces in the format of https://<CLUSTER_URL>/project-name/workspace-name . While this form of path-based routing conveniently works with applications like Jupyter Notebooks, it may often not be compatible with other applications. These applications assume running at the root file system, so hardcoded file paths and settings within the container may become invalid when running at a path other than the root. For instance, if the container is expecting to find a file at /etc/config.json but is running at /project-name/workspace-name , the file will not be found. This can cause the container to fail or not function as intended. To address this issue, Run:ai provides support for host-based routing . When enabled, Run:ai creates workspace URLs in a subdomain format ( https://project-name-workspace-name.<CLUSTER_URL>/ ), which allows all workspaces to run at the root path and function properly. To enable host-based routing you must perform the following steps: Create a second DNS entry *.<CLUSTER_URL> , pointing to the same IP as the original Cluster URL DNS. Obtain a star SSL certificate for this DNS. Add the certificate as a secret: SaaS Self hosted kubectl create secret tls runai-cluster-domain-star-tls-secret -n runai \\ --cert /path/to/fullchain.pem --key /path/to/private.pem kubectl create secret tls runai-cluster-domain-star-tls-secret -n runai-backend \\ --cert /path/to/fullchain.pem --key /path/to/private.pem Create an ingress rule to direct traffic: SaaS Self hosted kubectl patch ingress researcher-service-ingress -n runai --type json \\ --patch '[{ \"op\": \"add\", \"path\": \"/spec/tls/-\", \"value\": { \"hosts\": [ \"*.<CLUSTER_URL>\" ], \"secretName\": \"runai-cluster-domain-star-tls-secret\" } }]' kubectl patch ingress runai-backend-ingress -n runai-backend --type json \\ --patch '[{ \"op\": \"add\", \"path\": \"/spec/tls/-\", \"value\": { \"hosts\": [ \"*.<CLUSTER_URL>\" ], \"secretName\": \"runai-cluster-domain-star-tls-secret\" } }]' Edit Runaiconfig to generate the URLs correctly kubectl patch RunaiConfig runai -n runai --type=\"merge\" \\ -p '{\"spec\":{\"global\":{\"subdomainSupport\": true}}}' Once these requirements have been met, all workspaces will automatically be assigned a secured URL with a subdomain, ensuring full functionality for all researcher applications. See Also \u00b6 To learn how to use port forwarding see the Quickstart document: Launch an Interactive Build Workload with Connected Ports . See CLI command runai submit .","title":"External access to Containers"},{"location":"admin/runai-setup/config/allow-external-access-to-containers/#introduction","text":"Researchers working with containers. many times need to remotely access the container. Some examples: Using a Jupyter notebook that runs within the container Using PyCharm to run python commands remotely. Using TensorBoard to view machine learning visualizations This requires exposing container ports . When using docker, the way Researchers expose ports is by declaring them when starting the container. Run:ai has similar syntax. Run:ai is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers several options: Method Description Prerequisites Port Forwarding Simple port forwarding allows access to the container via local and/or remote port. None NodePort Exposes the service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service from outside the cluster by requesting <NODE-IP>:<NODE-PORT> regardless of which node the container actually resides in. None LoadBalancer Exposes the service externally using a cloud provider\u2019s load balancer. Only available with cloud providers See https://kubernetes.io/docs/concepts/services-networking/service for further details on these four options.","title":"Introduction"},{"location":"admin/runai-setup/config/allow-external-access-to-containers/#workspaces-configuration","text":"Version 2.9 and up Version 2.9 introduces Workspaces which allow the Researcher to build AI models interactively. Workspaces allow the Researcher to launch tools such as Visual Studio code, TensorFlow, TensorBoard etc. These tools require access to the container. Access is provided via URLs. Run:ai uses the Cluster URL provided to dynamically create SSL-secured URLs for researchers\u2019 workspaces in the format of https://<CLUSTER_URL>/project-name/workspace-name . While this form of path-based routing conveniently works with applications like Jupyter Notebooks, it may often not be compatible with other applications. These applications assume running at the root file system, so hardcoded file paths and settings within the container may become invalid when running at a path other than the root. For instance, if the container is expecting to find a file at /etc/config.json but is running at /project-name/workspace-name , the file will not be found. This can cause the container to fail or not function as intended. To address this issue, Run:ai provides support for host-based routing . When enabled, Run:ai creates workspace URLs in a subdomain format ( https://project-name-workspace-name.<CLUSTER_URL>/ ), which allows all workspaces to run at the root path and function properly. To enable host-based routing you must perform the following steps: Create a second DNS entry *.<CLUSTER_URL> , pointing to the same IP as the original Cluster URL DNS. Obtain a star SSL certificate for this DNS. Add the certificate as a secret: SaaS Self hosted kubectl create secret tls runai-cluster-domain-star-tls-secret -n runai \\ --cert /path/to/fullchain.pem --key /path/to/private.pem kubectl create secret tls runai-cluster-domain-star-tls-secret -n runai-backend \\ --cert /path/to/fullchain.pem --key /path/to/private.pem Create an ingress rule to direct traffic: SaaS Self hosted kubectl patch ingress researcher-service-ingress -n runai --type json \\ --patch '[{ \"op\": \"add\", \"path\": \"/spec/tls/-\", \"value\": { \"hosts\": [ \"*.<CLUSTER_URL>\" ], \"secretName\": \"runai-cluster-domain-star-tls-secret\" } }]' kubectl patch ingress runai-backend-ingress -n runai-backend --type json \\ --patch '[{ \"op\": \"add\", \"path\": \"/spec/tls/-\", \"value\": { \"hosts\": [ \"*.<CLUSTER_URL>\" ], \"secretName\": \"runai-cluster-domain-star-tls-secret\" } }]' Edit Runaiconfig to generate the URLs correctly kubectl patch RunaiConfig runai -n runai --type=\"merge\" \\ -p '{\"spec\":{\"global\":{\"subdomainSupport\": true}}}' Once these requirements have been met, all workspaces will automatically be assigned a secured URL with a subdomain, ensuring full functionality for all researcher applications.","title":"Workspaces configuration"},{"location":"admin/runai-setup/config/allow-external-access-to-containers/#see-also","text":"To learn how to use port forwarding see the Quickstart document: Launch an Interactive Build Workload with Connected Ports . See CLI command runai submit .","title":"See Also"},{"location":"admin/runai-setup/config/cli-admin-install/","text":"Install the Run:ai Administrator Command-line Interface \u00b6 The Run:ai Administrator Command-line Interface (Administrator CLI) allows performing administrative tasks on the Run:ai Cluster. The instructions below will guide you through the process of installing the Administrator CLI. Prerequisites \u00b6 Run:ai Administrator CLI runs on Mac and Linux. Kubectl (Kubernetes command-line interface) installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/ A Kubernetes configuration file obtained from a computer previously connected to the Kubernetes cluster Kubernetes Configuration \u00b6 The Run:ai Administrator CLI requires a Kubernetes profile with cluster administrative rights. Installation \u00b6 Download the Run:ai Administrator Command-line Interface by running: Mac Linux wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/darwin chmod +x runai-adm sudo mv runai-adm /usr/local/bin/runai-adm wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/linux chmod +x runai-adm sudo mv runai-adm /usr/local/bin/runai-adm To verify the installation run: runai-adm version Download a specific version \u00b6 To download a specific version of runai-adm add the version number to URL. For example: wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/v2.7.22/darwin Updating the Run:ai Administrator CLI \u00b6 To update the CLI to the latest version perform the same install process again. The command runai-adm update is no longer supported.","title":"Install Administrator CLI"},{"location":"admin/runai-setup/config/cli-admin-install/#install-the-runai-administrator-command-line-interface","text":"The Run:ai Administrator Command-line Interface (Administrator CLI) allows performing administrative tasks on the Run:ai Cluster. The instructions below will guide you through the process of installing the Administrator CLI.","title":"Install the Run:ai Administrator Command-line Interface"},{"location":"admin/runai-setup/config/cli-admin-install/#prerequisites","text":"Run:ai Administrator CLI runs on Mac and Linux. Kubectl (Kubernetes command-line interface) installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/ A Kubernetes configuration file obtained from a computer previously connected to the Kubernetes cluster","title":"Prerequisites"},{"location":"admin/runai-setup/config/cli-admin-install/#kubernetes-configuration","text":"The Run:ai Administrator CLI requires a Kubernetes profile with cluster administrative rights.","title":"Kubernetes Configuration"},{"location":"admin/runai-setup/config/cli-admin-install/#installation","text":"Download the Run:ai Administrator Command-line Interface by running: Mac Linux wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/darwin chmod +x runai-adm sudo mv runai-adm /usr/local/bin/runai-adm wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/linux chmod +x runai-adm sudo mv runai-adm /usr/local/bin/runai-adm To verify the installation run: runai-adm version","title":"Installation"},{"location":"admin/runai-setup/config/cli-admin-install/#download-a-specific-version","text":"To download a specific version of runai-adm add the version number to URL. For example: wget --content-disposition https://app.run.ai/v1/k8s/admin-cli/v2.7.22/darwin","title":"Download a specific version"},{"location":"admin/runai-setup/config/cli-admin-install/#updating-the-runai-administrator-cli","text":"To update the CLI to the latest version perform the same install process again. The command runai-adm update is no longer supported.","title":"Updating the Run:ai Administrator CLI"},{"location":"admin/runai-setup/config/dr/","text":"Planning for Disaster Recovery \u00b6 The SaaS version of Run:ai moves the bulk of the burden of disaster recovery to Run:ai. Backup of data is hence not an issue in such environments. With the self-hosted version, it is the responsibility of the IT organization to back up data for a possible disaster and to learn how to recover when needed. Backup \u00b6 Database \u00b6 Run:ai uses an internal PostgreSQL database. The database is stored on a Kubernetes Persistent Volume (PV). You must provide a backup solution for the database. Typically by backing up the persistent volume holding the database storage. Metrics \u00b6 Run:ai stores metric history using Thanos . Thanos is configured to store data on a persistent volume. The recommendation is to back up the PV. Additional Configuration \u00b6 During the installation of Run:ai you have created two value files: One for the Run:ai control plane. See Kubernetes or OpenShift , One for the cluster (see Kubernetes or OpenShift ). You will want to save these files or extract a current version of the file by using the upgrade script. Recovery \u00b6 To recover Run:ai Re-create the Kubernetes/OpenShift cluster. Recover the persistent volumes for metrics and database. Re-install the Run:ai control plane. Use the stored values file. If needed, modify the values file to connect to the restored PostgreSQL PV. Connect Prometheus to the stored metrics PV. Re-install the cluster. Use the stored values file or download a new file from the Administration UI. If the cluster is configured such that Projects do not create a namespace automatically, you will need to re-create namespaces and apply role bindings as discussed in Kubernetes or OpenShift .","title":"Disaster Recovery"},{"location":"admin/runai-setup/config/dr/#planning-for-disaster-recovery","text":"The SaaS version of Run:ai moves the bulk of the burden of disaster recovery to Run:ai. Backup of data is hence not an issue in such environments. With the self-hosted version, it is the responsibility of the IT organization to back up data for a possible disaster and to learn how to recover when needed.","title":"Planning for Disaster Recovery"},{"location":"admin/runai-setup/config/dr/#backup","text":"","title":"Backup"},{"location":"admin/runai-setup/config/dr/#database","text":"Run:ai uses an internal PostgreSQL database. The database is stored on a Kubernetes Persistent Volume (PV). You must provide a backup solution for the database. Typically by backing up the persistent volume holding the database storage.","title":"Database"},{"location":"admin/runai-setup/config/dr/#metrics","text":"Run:ai stores metric history using Thanos . Thanos is configured to store data on a persistent volume. The recommendation is to back up the PV.","title":"Metrics"},{"location":"admin/runai-setup/config/dr/#additional-configuration","text":"During the installation of Run:ai you have created two value files: One for the Run:ai control plane. See Kubernetes or OpenShift , One for the cluster (see Kubernetes or OpenShift ). You will want to save these files or extract a current version of the file by using the upgrade script.","title":"Additional Configuration"},{"location":"admin/runai-setup/config/dr/#recovery","text":"To recover Run:ai Re-create the Kubernetes/OpenShift cluster. Recover the persistent volumes for metrics and database. Re-install the Run:ai control plane. Use the stored values file. If needed, modify the values file to connect to the restored PostgreSQL PV. Connect Prometheus to the stored metrics PV. Re-install the cluster. Use the stored values file or download a new file from the Administration UI. If the cluster is configured such that Projects do not create a namespace automatically, you will need to re-create namespaces and apply role bindings as discussed in Kubernetes or OpenShift .","title":"Recovery"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/","text":"Node affinity with cloud node pools \u00b6 Run:Ai allows for node affinity . Node affinity is the ability to assign a Project to run on specific nodes. To use the node affinity feature, You will need to label the target nodes with the label run.ai/node-type . Most cloud clusters allow configuring node labels for the node pools in the cluster. This guide shows how to apply this configuration to different cloud providers. To make the node affinity work with node pools on various cloud providers, we need to make sure the node pools are configured with the appropriate Kubernetes label ( run.ai/type=<TYPE_VALUE> ). Setting node labels while creating a new cluster \u00b6 You can configure node-pool labels at cluster creation time GKE AKS EKS At the first creation screen, you will see a menu on the left side named node-pools . Expand the node pool you want to label. Click on Metadata . Near the bottom, you will find the Kubernetes label section. Add the key run.ai/type and the value <TYPE_VALUE> . When creating AKS cluster at the node-pools page click on create new node-pool. Go to the labels section and add key run.ai/type and the value <TYPE_VALUE> . Create a regular EKS cluster. Click on compute . Click on Add node group . In the Kubernetes labels section click on Add label . Add the key run.ai/type and the value <TYPE_VALUE> . Setting node labels for a new node pool \u00b6 GKE AKS EKS At the node pool creation screen, go to the metadata section. Near the bottom, you will find the Kubernetes label section. Add the key run.ai/type and the value <TYPE_VALUE> . Go to your AKS page at Azure. On the left menu click the node-pools button. Click on Add Node Pool . In the new Node Pool page go to Optional settings . In the Kubernetes labels section click on Add label . Add the key run.ai/type and the value <TYPE_VALUE> . Go to Add node group screen. In the Kubernetes labels section click on Add label . Add the key run.ai/type and the value <TYPE_VALUE> . Editing node labels for an existing node pool \u00b6 GKE AKS EKS Go to the Google Kubernetes Engine page in the Google Cloud console. Go to Google Kubernetes Engine . In the cluster list, click the name of the cluster you want to modify. Click the Nodes tab Under Node Pools , click the name of the node pool you want to modify, then click Edit . Near the bottom, you will find the Kubernetes label section. Add the key run.ai/type and the value <TYPE_VALUE> . To update an existing node pool label you must use the azure cli . Run the following command: az aks nodepool update \\ --resource-group [RESOURCE GROUP] \\ --cluster-name [CLUSTER NAME] \\ --name labelnp \\ --labels run.ai/type=[TYPE_VALUE] \\ --no-wait Go to the node group page and click on Edit . In the Kubernetes labels section click on Add label . Add the key run.ai/type and the value <TYPE_VALUE> .","title":"Node Affinity with Cloud Node Pools"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/#node-affinity-with-cloud-node-pools","text":"Run:Ai allows for node affinity . Node affinity is the ability to assign a Project to run on specific nodes. To use the node affinity feature, You will need to label the target nodes with the label run.ai/node-type . Most cloud clusters allow configuring node labels for the node pools in the cluster. This guide shows how to apply this configuration to different cloud providers. To make the node affinity work with node pools on various cloud providers, we need to make sure the node pools are configured with the appropriate Kubernetes label ( run.ai/type=<TYPE_VALUE> ).","title":"Node affinity with cloud node pools"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/#setting-node-labels-while-creating-a-new-cluster","text":"You can configure node-pool labels at cluster creation time GKE AKS EKS At the first creation screen, you will see a menu on the left side named node-pools . Expand the node pool you want to label. Click on Metadata . Near the bottom, you will find the Kubernetes label section. Add the key run.ai/type and the value <TYPE_VALUE> . When creating AKS cluster at the node-pools page click on create new node-pool. Go to the labels section and add key run.ai/type and the value <TYPE_VALUE> . Create a regular EKS cluster. Click on compute . Click on Add node group . In the Kubernetes labels section click on Add label . Add the key run.ai/type and the value <TYPE_VALUE> .","title":"Setting node labels while creating a new cluster"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/#setting-node-labels-for-a-new-node-pool","text":"GKE AKS EKS At the node pool creation screen, go to the metadata section. Near the bottom, you will find the Kubernetes label section. Add the key run.ai/type and the value <TYPE_VALUE> . Go to your AKS page at Azure. On the left menu click the node-pools button. Click on Add Node Pool . In the new Node Pool page go to Optional settings . In the Kubernetes labels section click on Add label . Add the key run.ai/type and the value <TYPE_VALUE> . Go to Add node group screen. In the Kubernetes labels section click on Add label . Add the key run.ai/type and the value <TYPE_VALUE> .","title":"Setting node labels for a new node pool"},{"location":"admin/runai-setup/config/node-affinity-with-cloud-node-pools/#editing-node-labels-for-an-existing-node-pool","text":"GKE AKS EKS Go to the Google Kubernetes Engine page in the Google Cloud console. Go to Google Kubernetes Engine . In the cluster list, click the name of the cluster you want to modify. Click the Nodes tab Under Node Pools , click the name of the node pool you want to modify, then click Edit . Near the bottom, you will find the Kubernetes label section. Add the key run.ai/type and the value <TYPE_VALUE> . To update an existing node pool label you must use the azure cli . Run the following command: az aks nodepool update \\ --resource-group [RESOURCE GROUP] \\ --cluster-name [CLUSTER NAME] \\ --name labelnp \\ --labels run.ai/type=[TYPE_VALUE] \\ --no-wait Go to the node group page and click on Edit . In the Kubernetes labels section click on Add label . Add the key run.ai/type and the value <TYPE_VALUE> .","title":"Editing node labels for an existing node pool"},{"location":"admin/runai-setup/config/node-roles/","text":"Designating Specific Role Nodes \u00b6 When installing a production cluster you may want to: Set one or more Run:ai system nodes. These are nodes dedicated to Run:ai software. Machine learning frequently requires jobs that require CPU but not GPU . You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. Limit Run:ai monitoring and scheduling to specific nodes in the cluster. To perform these tasks you will need the Run:ai Administrator CLI. See Installing the Run:ai Administrator Command-line Interface . Dedicated Run:ai System Nodes \u00b6 Find out the names of the nodes designated for the Run:ai system by running kubectl get nodes . For each such node run: runai-adm set node-role --runai-system-worker <node-name> If you re-run kubectl get nodes you will see the node role of these nodes changed to runai-system To remove the runai-system node role run: runai-adm remove node-role --runai-system-worker <node-name> Warning Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443). Dedicated GPU & CPU Nodes \u00b6 Separate nodes into those that: Run GPU workloads Run CPU workloads Do not run Run:ai at all. these jobs will not be monitored using the Run:ai Administration User interface. Review nodes names using kubectl get nodes . For each such node run: runai-adm set node-role --gpu-worker <node-name> or runai-adm set node-role --cpu-worker <node-name> Nodes not marked as GPU worker or CPU worker will not run Run:ai at all. To set all workers not running runai-system as GPU workers run: runai-adm set node-role --all <node-name> To remove the CPU or GPU worker node role run: runai-adm remove node-role --cpu-worker <node-name> or runai-adm remove node-role --gpu-worker <node-name>","title":"Set Node Roles"},{"location":"admin/runai-setup/config/node-roles/#designating-specific-role-nodes","text":"When installing a production cluster you may want to: Set one or more Run:ai system nodes. These are nodes dedicated to Run:ai software. Machine learning frequently requires jobs that require CPU but not GPU . You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. Limit Run:ai monitoring and scheduling to specific nodes in the cluster. To perform these tasks you will need the Run:ai Administrator CLI. See Installing the Run:ai Administrator Command-line Interface .","title":"Designating Specific Role Nodes"},{"location":"admin/runai-setup/config/node-roles/#dedicated-runai-system-nodes","text":"Find out the names of the nodes designated for the Run:ai system by running kubectl get nodes . For each such node run: runai-adm set node-role --runai-system-worker <node-name> If you re-run kubectl get nodes you will see the node role of these nodes changed to runai-system To remove the runai-system node role run: runai-adm remove node-role --runai-system-worker <node-name> Warning Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).","title":"Dedicated Run:ai System Nodes"},{"location":"admin/runai-setup/config/node-roles/#dedicated-gpu-cpu-nodes","text":"Separate nodes into those that: Run GPU workloads Run CPU workloads Do not run Run:ai at all. these jobs will not be monitored using the Run:ai Administration User interface. Review nodes names using kubectl get nodes . For each such node run: runai-adm set node-role --gpu-worker <node-name> or runai-adm set node-role --cpu-worker <node-name> Nodes not marked as GPU worker or CPU worker will not run Run:ai at all. To set all workers not running runai-system as GPU workers run: runai-adm set node-role --all <node-name> To remove the CPU or GPU worker node role run: runai-adm remove node-role --cpu-worker <node-name> or runai-adm remove node-role --gpu-worker <node-name>","title":"Dedicated GPU &amp; CPU Nodes"},{"location":"admin/runai-setup/config/non-root-containers/","text":"User Identity in Container \u00b6 The identity of the user in the container determines its access to resources. For example, network file storage solutions typically use this identity to determine the container's access to network volumes. This document explains multiple ways for propagating the user identity into the container. The Default: Root Access \u00b6 In docker, as well as in Kubernetes, the default for running containers is running as root . The implication of running as root is that processes running within the container have enough permissions to change anything in the container, and if propagated to network resources - can have permissions outside the container as well. This gives a lot of power to the Researcher but does not sit well with modern security standards of enterprise security. By default, if you run: runai submit -i ubuntu --attach --interactive -- bash then run id , you will see the root user. Use Run:ai flags to limit root access \u00b6 There are two runai submit flags which control user identity at the Researcher level: The flag --run-as-user starts the container with a specific user. The user is the current Linux user (see below for other behaviors if used in conjunction with Single sign-on). The flag --prevent-privilege-escalation prevents the container from elevating its own privileges into root (e.g. running sudo or changing system files.). Equivalent flags exist in the Researcher User Interface. Run as Current User \u00b6 From a Linux/Mac box, run: runai submit -i ubuntu --attach --interactive --run-as-user -- bash then run id , you will see the users and groups of the box you have been using to launch the Job. Prevent Escalation \u00b6 From a Linux/Mac box, run: runai submit -i ubuntu --attach --interactive --run-as-user \\ --prevent-privilege-escalation -- bash then verify that you cannot run su to become root within the container. Setting a Cluster-Wide Default \u00b6 The two flags are voluntary. They are not enforced by the system. It is however possible to enforce them using Policies . Polices allow an Administrator to force compliance on both the User Interface and Command-line interface. Passing user identity \u00b6 Passing user identity from Identity Provider \u00b6 A best practice is to store the user identifier (UID) and the group identifier (GID) in the organization's directory. Run:ai allows you to pass these values to the container and use them as the container identity. To perform this, you must: Set up single sign-on . Perform the steps for UID/GID integration. Run: runai login and enter your credentials Use the flag --run-as-user Running id should show the identifier from the directory. Passing user identity explicitly via the Researcher UI \u00b6 Via the Researcher User Interface, it is possible to explicitly provide the user id and group id: Using OpenShift or Gatekeeper to provide Cluster Level Controls \u00b6 Run:ai supports OpenShift as a Kubernetes platform. In OpenShift the system will provide a random UID to containers. The flags --run-as-user and --prevent-privilege-escalation are disabled on OpenShift. It is possible to achieve a similar effect on Kubernetes systems that are not OpenShift. A leading tool is Gatekeeper . Gatekeeper similarly enforces non-root on containers at the system level. Creating a Temporary Home Directory \u00b6 When containers run as a specific user, the user needs to have a pre-created home directory within the image. Otherwise, when running a shell, you will not have a home directory: runai submit -i ubuntu --attach --interactive --run-as-user -- bash The job 'job-0' has been submitted successfully You can run ` runai describe job job-0 -p team-a ` to check the job status Waiting for pod to start running... INFO [ 0007 ] Job started Connecting to pod job-0-0-0 If you don ' t see a command prompt, try pressing enter. I have no name!@job-0-0-0:/$ Adding home directories to an image per user is not a viable solution. To overcome this, Run:ai provides an additional flag --create-home-dir . Adding this flag creates a temporary home directory for the user within the container. Notes Data saved in this directory will not be saved when the container exits. This flag is set by default to true when the --run-as-user flag is used, and false if not.","title":"User Identity in Container"},{"location":"admin/runai-setup/config/non-root-containers/#user-identity-in-container","text":"The identity of the user in the container determines its access to resources. For example, network file storage solutions typically use this identity to determine the container's access to network volumes. This document explains multiple ways for propagating the user identity into the container.","title":"User Identity in Container"},{"location":"admin/runai-setup/config/non-root-containers/#the-default-root-access","text":"In docker, as well as in Kubernetes, the default for running containers is running as root . The implication of running as root is that processes running within the container have enough permissions to change anything in the container, and if propagated to network resources - can have permissions outside the container as well. This gives a lot of power to the Researcher but does not sit well with modern security standards of enterprise security. By default, if you run: runai submit -i ubuntu --attach --interactive -- bash then run id , you will see the root user.","title":"The Default: Root Access"},{"location":"admin/runai-setup/config/non-root-containers/#use-runai-flags-to-limit-root-access","text":"There are two runai submit flags which control user identity at the Researcher level: The flag --run-as-user starts the container with a specific user. The user is the current Linux user (see below for other behaviors if used in conjunction with Single sign-on). The flag --prevent-privilege-escalation prevents the container from elevating its own privileges into root (e.g. running sudo or changing system files.). Equivalent flags exist in the Researcher User Interface.","title":"Use Run:ai flags to limit root access"},{"location":"admin/runai-setup/config/non-root-containers/#run-as-current-user","text":"From a Linux/Mac box, run: runai submit -i ubuntu --attach --interactive --run-as-user -- bash then run id , you will see the users and groups of the box you have been using to launch the Job.","title":"Run as Current User"},{"location":"admin/runai-setup/config/non-root-containers/#prevent-escalation","text":"From a Linux/Mac box, run: runai submit -i ubuntu --attach --interactive --run-as-user \\ --prevent-privilege-escalation -- bash then verify that you cannot run su to become root within the container.","title":"Prevent Escalation"},{"location":"admin/runai-setup/config/non-root-containers/#setting-a-cluster-wide-default","text":"The two flags are voluntary. They are not enforced by the system. It is however possible to enforce them using Policies . Polices allow an Administrator to force compliance on both the User Interface and Command-line interface.","title":"Setting a Cluster-Wide Default"},{"location":"admin/runai-setup/config/non-root-containers/#passing-user-identity","text":"","title":"Passing user identity"},{"location":"admin/runai-setup/config/non-root-containers/#passing-user-identity-from-identity-provider","text":"A best practice is to store the user identifier (UID) and the group identifier (GID) in the organization's directory. Run:ai allows you to pass these values to the container and use them as the container identity. To perform this, you must: Set up single sign-on . Perform the steps for UID/GID integration. Run: runai login and enter your credentials Use the flag --run-as-user Running id should show the identifier from the directory.","title":"Passing user identity from Identity Provider"},{"location":"admin/runai-setup/config/non-root-containers/#passing-user-identity-explicitly-via-the-researcher-ui","text":"Via the Researcher User Interface, it is possible to explicitly provide the user id and group id:","title":"Passing user identity explicitly via the Researcher UI"},{"location":"admin/runai-setup/config/non-root-containers/#using-openshift-or-gatekeeper-to-provide-cluster-level-controls","text":"Run:ai supports OpenShift as a Kubernetes platform. In OpenShift the system will provide a random UID to containers. The flags --run-as-user and --prevent-privilege-escalation are disabled on OpenShift. It is possible to achieve a similar effect on Kubernetes systems that are not OpenShift. A leading tool is Gatekeeper . Gatekeeper similarly enforces non-root on containers at the system level.","title":"Using OpenShift or Gatekeeper to provide Cluster Level Controls"},{"location":"admin/runai-setup/config/non-root-containers/#creating-a-temporary-home-directory","text":"When containers run as a specific user, the user needs to have a pre-created home directory within the image. Otherwise, when running a shell, you will not have a home directory: runai submit -i ubuntu --attach --interactive --run-as-user -- bash The job 'job-0' has been submitted successfully You can run ` runai describe job job-0 -p team-a ` to check the job status Waiting for pod to start running... INFO [ 0007 ] Job started Connecting to pod job-0-0-0 If you don ' t see a command prompt, try pressing enter. I have no name!@job-0-0-0:/$ Adding home directories to an image per user is not a viable solution. To overcome this, Run:ai provides an additional flag --create-home-dir . Adding this flag creates a temporary home directory for the user within the container. Notes Data saved in this directory will not be saved when the container exits. This flag is set by default to true when the --run-as-user flag is used, and false if not.","title":"Creating a Temporary Home Directory"},{"location":"admin/runai-setup/config/overview/","text":"Run:ai Configuration Articles \u00b6 This section provides a list of installation-related articles dealing with a wide range of subjects: Article Purpose Designating Specific Role Nodes Set one or more designated Run:ai system nodes or limit Run:ai monitoring and scheduling to specific nodes in the cluster. Setup Project-based Researcher Access Control Enable Run:ai access control is at the Project level. Single sign-on Integrate with the organization's Identity Provider to provide single sign-on for Run:ai Review Kubernetes Access provided to Run:ai In Restrictive Kubernetes environments such as when using OpenShift, understand and control what Kubernetes roles are provided to Run:ai External access to Containers Understand the available options for Researchers to access containers from the outside User Identity in Container The identity of the user in the container determines its access to cluster resources. The document explains multiple way on how to propagate the user identity into the container. Install the Run:ai Administrator Command-line Interface The Administrator command-line is useful in a variety of flows such as cluster upgrade, node setup etc.","title":"Overview"},{"location":"admin/runai-setup/config/overview/#runai-configuration-articles","text":"This section provides a list of installation-related articles dealing with a wide range of subjects: Article Purpose Designating Specific Role Nodes Set one or more designated Run:ai system nodes or limit Run:ai monitoring and scheduling to specific nodes in the cluster. Setup Project-based Researcher Access Control Enable Run:ai access control is at the Project level. Single sign-on Integrate with the organization's Identity Provider to provide single sign-on for Run:ai Review Kubernetes Access provided to Run:ai In Restrictive Kubernetes environments such as when using OpenShift, understand and control what Kubernetes roles are provided to Run:ai External access to Containers Understand the available options for Researchers to access containers from the outside User Identity in Container The identity of the user in the container determines its access to cluster resources. The document explains multiple way on how to propagate the user identity into the container. Install the Run:ai Administrator Command-line Interface The Administrator command-line is useful in a variety of flows such as cluster upgrade, node setup etc.","title":"Run:ai Configuration Articles"},{"location":"admin/runai-setup/maintenance/audit-log/","text":"Introduction \u00b6 The Run:ai control plane provides audit log API and audit log user interface table. Both reflect the same information: All changes to business objects All logins to the control plane. Event History - Audit Log User Interface \u00b6 The Administrators of the system can view the audit log using the user interface. The audit log screen is under the 'Event History' section: Event History (audit log) information fields \u00b6 The Administrator can choose what information fields to view within the audit log table, this is done by clicking the 'Columns' button and checking the required fields to be presented: Here's the list of available information fields in the Event History (audit log) table: Field Type Description User/App user id The identity of the User or Application that executed this operation. Data & Time date The exact timestamp at which the event occured. Format dd/mm/yyyy for date and hh:mm am/pm for time. Event event type The type of the logged operation. Possible values: Create , Update , Delete , Login . Event ID integer Sequanicialy incrmental number of the logged operation, lower number means older event, higher means newer event. Status string The outcome of the logged operation. Possible values: Succeeded , Failed . Entity type string The type of the logged business object. Possible values: Project , Department , User , Group , Login , Settings , Applications , Node Pool . Entity name string The name of logged business object. Entity ID string The system's internal id of the logged business object. Cluster Name string The name of the cluster that the loged operation relates to. If the operation is not cluster specific - cluster name remains empty. Cluster ID string The system internal identifier of the cluster that the loged operation relates to. If the operation is not cluster specific - cluster id remains empty. Event History - Date Selector \u00b6 The Event History table saves logged operations for the last 90 days. However, the table itself presents up to the last 30 days of information due to the potentially very high number of operations that might be logged during this period. To view older logged operations, or if you wish to refine your search and get more specific results or fewer results, you should use the time selector and change the period you search for. You can also refine your search by using filters as explained below. Event History - Filters \u00b6 The administrator can choose to filter the table using a list of predefined filters. The filter's value is a free text keyword entered by the administrator and must be fully matched to the requested field's actual value, otherwise, the filter will not find the requested keyword. Multiple filters can be set in parallel. Event History - Download the Audit Log file \u00b6 The event history table allows you to download the logged information in text form formatted as CSV or JSON files. The scope of the downloaded information is set by the scope of the table filters, i.e. if no filters or date selectors are used, the downloaded file includes the full scope of the information that the table holds - i.e. up to 30 days of logged information. To view older logged information (up to 90 days older, but no more than 30 days at a time), shorter periods, or narrower (filtered) scopes - use the date selector and filters. Audit log API \u00b6 Since the amount of data is not trivial, the API is based on paging in the sense that it will retrieve a specified number of items for each API call. You can get more data by using subsequent calls. Retrieve Audit Log data via API \u00b6 To retrieve the Audit log you need to call an API. You can do this via code or by using the Audit function via a user interface for calling APIs . Retrieve via Code \u00b6 Create an Application and generate a bearer token by following the API Authentication document. To get the first 40 records of the audit log starting January 1st, 2022, run: curl -X 'GET' \\ 'https://<COMPANY-URL>/v1/k8s/audit?start=2022-1-1' \\ # (1) -H 'accept: application/json' \\ -H 'Authorization: Bearer <ACCESS-TOKEN>' # (2) <COMPANY-URL> is app.run.ai for SaaS installations (not <company>.run.ai ) or the Run:ai user interface URL for Self-hosted installations. To obtain a Bearer token see API authentication . Sample result: [ { \"id\" : 3 , \"tenantId\" : 1 , \"happenedAt\" : \"2022-07-07T09:45:32.069Z\" , \"action\" : \"Update\" , \"version\" : \"1.0\" , \"entityId\" : \"1\" , \"entityType\" : \"Project\" , \"entityName\" : \"team-a\" , \"sourceType\" : \"User\" , \"sourceId\" : \"a79500fb-c452-471f-adc0-b65c972bd5c2\" , \"sourceName\" : \"test@run.ai\" , \"context\" : { \"user_agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\" , \"ip_address\" : \"10.244.0.0\" } }, { \"id\" : 2 , \"tenantId\" : 1 , \"happenedAt\" : \"2022-07-07T08:27:39.649Z\" , \"action\" : \"Create\" , \"version\" : \"1.0\" , \"entityId\" : \"fdc90aab-b183-4856-8337-14039063b876\" , \"entityType\" : \"App\" , \"entityName\" : \"admin\" , \"sourceType\" : \"User\" , \"sourceId\" : \"a79500fb-c452-471f-adc0-b65c972bd5c2\" , \"sourceName\" : \"test@run.ai\" , \"context\" : { \"user_agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\" , \"ip_address\" : \"10.244.0.0\" } }, ... ] Paging \u00b6 Use the limit and offset properties to retrieve all audit log entries. Additional filter \u00b6 You can add additional filters to the query as follows: Field Type Description start date Start date for audit logs retrieval. Format yyyy-MM-dd for date or yyyy-MM-ddThh:mm:ss for date-time. end date End date for audit logs retrieval. Format yyyy-MM-dd for date or yyyy-MM-ddThh:mm:ss for date-time. action string The action of the logged operation. Possible values: Create , Update , Delete , Login source_type string The initiator of the action (user or machine to machine key). Possible values: User , Application source_id string The id of the source of the action. For User , this is the internal user id. For an Application , this is the internal id of the Application source_name string The name of the source of the action. For a User , this is the user's email, for an Application , this is the Application name. entity_type string The type of business object. Possible values: Project , Department , User , Group , Login , Settings , Applications entity_id string The id of the business object limit integer Paging: the number of records to fetch at once (default is 40 record) offset integer Paging: The offset from which to start fetching records. success string enter true for successful audit log records and false for failures (default is all records) download string enter true to download the logs into a file","title":"Audit Log"},{"location":"admin/runai-setup/maintenance/audit-log/#introduction","text":"The Run:ai control plane provides audit log API and audit log user interface table. Both reflect the same information: All changes to business objects All logins to the control plane.","title":"Introduction"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-audit-log-user-interface","text":"The Administrators of the system can view the audit log using the user interface. The audit log screen is under the 'Event History' section:","title":"Event History - Audit Log User Interface"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-audit-log-information-fields","text":"The Administrator can choose what information fields to view within the audit log table, this is done by clicking the 'Columns' button and checking the required fields to be presented: Here's the list of available information fields in the Event History (audit log) table: Field Type Description User/App user id The identity of the User or Application that executed this operation. Data & Time date The exact timestamp at which the event occured. Format dd/mm/yyyy for date and hh:mm am/pm for time. Event event type The type of the logged operation. Possible values: Create , Update , Delete , Login . Event ID integer Sequanicialy incrmental number of the logged operation, lower number means older event, higher means newer event. Status string The outcome of the logged operation. Possible values: Succeeded , Failed . Entity type string The type of the logged business object. Possible values: Project , Department , User , Group , Login , Settings , Applications , Node Pool . Entity name string The name of logged business object. Entity ID string The system's internal id of the logged business object. Cluster Name string The name of the cluster that the loged operation relates to. If the operation is not cluster specific - cluster name remains empty. Cluster ID string The system internal identifier of the cluster that the loged operation relates to. If the operation is not cluster specific - cluster id remains empty.","title":"Event History (audit log) information fields"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-date-selector","text":"The Event History table saves logged operations for the last 90 days. However, the table itself presents up to the last 30 days of information due to the potentially very high number of operations that might be logged during this period. To view older logged operations, or if you wish to refine your search and get more specific results or fewer results, you should use the time selector and change the period you search for. You can also refine your search by using filters as explained below.","title":"Event History - Date Selector"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-filters","text":"The administrator can choose to filter the table using a list of predefined filters. The filter's value is a free text keyword entered by the administrator and must be fully matched to the requested field's actual value, otherwise, the filter will not find the requested keyword. Multiple filters can be set in parallel.","title":"Event History - Filters"},{"location":"admin/runai-setup/maintenance/audit-log/#event-history-download-the-audit-log-file","text":"The event history table allows you to download the logged information in text form formatted as CSV or JSON files. The scope of the downloaded information is set by the scope of the table filters, i.e. if no filters or date selectors are used, the downloaded file includes the full scope of the information that the table holds - i.e. up to 30 days of logged information. To view older logged information (up to 90 days older, but no more than 30 days at a time), shorter periods, or narrower (filtered) scopes - use the date selector and filters.","title":"Event History - Download the Audit Log file"},{"location":"admin/runai-setup/maintenance/audit-log/#audit-log-api","text":"Since the amount of data is not trivial, the API is based on paging in the sense that it will retrieve a specified number of items for each API call. You can get more data by using subsequent calls.","title":"Audit log API"},{"location":"admin/runai-setup/maintenance/audit-log/#retrieve-audit-log-data-via-api","text":"To retrieve the Audit log you need to call an API. You can do this via code or by using the Audit function via a user interface for calling APIs .","title":"Retrieve Audit Log data via API"},{"location":"admin/runai-setup/maintenance/audit-log/#retrieve-via-code","text":"Create an Application and generate a bearer token by following the API Authentication document. To get the first 40 records of the audit log starting January 1st, 2022, run: curl -X 'GET' \\ 'https://<COMPANY-URL>/v1/k8s/audit?start=2022-1-1' \\ # (1) -H 'accept: application/json' \\ -H 'Authorization: Bearer <ACCESS-TOKEN>' # (2) <COMPANY-URL> is app.run.ai for SaaS installations (not <company>.run.ai ) or the Run:ai user interface URL for Self-hosted installations. To obtain a Bearer token see API authentication . Sample result: [ { \"id\" : 3 , \"tenantId\" : 1 , \"happenedAt\" : \"2022-07-07T09:45:32.069Z\" , \"action\" : \"Update\" , \"version\" : \"1.0\" , \"entityId\" : \"1\" , \"entityType\" : \"Project\" , \"entityName\" : \"team-a\" , \"sourceType\" : \"User\" , \"sourceId\" : \"a79500fb-c452-471f-adc0-b65c972bd5c2\" , \"sourceName\" : \"test@run.ai\" , \"context\" : { \"user_agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\" , \"ip_address\" : \"10.244.0.0\" } }, { \"id\" : 2 , \"tenantId\" : 1 , \"happenedAt\" : \"2022-07-07T08:27:39.649Z\" , \"action\" : \"Create\" , \"version\" : \"1.0\" , \"entityId\" : \"fdc90aab-b183-4856-8337-14039063b876\" , \"entityType\" : \"App\" , \"entityName\" : \"admin\" , \"sourceType\" : \"User\" , \"sourceId\" : \"a79500fb-c452-471f-adc0-b65c972bd5c2\" , \"sourceName\" : \"test@run.ai\" , \"context\" : { \"user_agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\" , \"ip_address\" : \"10.244.0.0\" } }, ... ]","title":"Retrieve via Code"},{"location":"admin/runai-setup/maintenance/audit-log/#paging","text":"Use the limit and offset properties to retrieve all audit log entries.","title":"Paging"},{"location":"admin/runai-setup/maintenance/audit-log/#additional-filter","text":"You can add additional filters to the query as follows: Field Type Description start date Start date for audit logs retrieval. Format yyyy-MM-dd for date or yyyy-MM-ddThh:mm:ss for date-time. end date End date for audit logs retrieval. Format yyyy-MM-dd for date or yyyy-MM-ddThh:mm:ss for date-time. action string The action of the logged operation. Possible values: Create , Update , Delete , Login source_type string The initiator of the action (user or machine to machine key). Possible values: User , Application source_id string The id of the source of the action. For User , this is the internal user id. For an Application , this is the internal id of the Application source_name string The name of the source of the action. For a User , this is the user's email, for an Application , this is the Application name. entity_type string The type of business object. Possible values: Project , Department , User , Group , Login , Settings , Applications entity_id string The id of the business object limit integer Paging: the number of records to fetch at once (default is 40 record) offset integer Paging: The offset from which to start fetching records. success string enter true for successful audit log records and false for failures (default is all records) download string enter true to download the logs into a file","title":"Additional filter"},{"location":"admin/runai-setup/maintenance/monitoring/","text":"Cluster Monitoring \u00b6 Introduction \u00b6 Organizations typically want to automatically highlight critical issues and escalate issues to IT/DevOps personnel. The standard practice is to install an alert management tool and connect it to critical systems. Run:ai is comprised of two parts: A control plane part, typically resides in the cloud. The health of the cloud portion of Run:ai can be viewed at status.run.ai . In Self-hosted installations of Run:ai is installed on-prem. One or more GPU Clusters . The purpose of this document is to configure the Run:ai to emit health alerts and to connect these alerts to alert-management systems within the organization. Alerts are emitted for Run:ai Clusters as well as the Run:ai control plane on Self-hosted installation where the control plane resides on the same Kubernetes cluster as one of the Run:ai clusters. Alert Infrastructure \u00b6 Run:ai uses Prometheus for externalizing metrics. The Run:ai cluster installation installs Prometheus or can connect to an existing Prometheus instance used in the organization. Run:ai cluster alerts are based on the Prometheus Alert Manager . The Prometheus Alert Manager is enabled by default. This document explains, how to: Configure alert destinations. Triggered alerts will send data to destinations. Understand the out-of-the-box cluster alerts. Advanced: add additional custom alerts. Configure Alert Destinations \u00b6 Prometheus Alert Manager provides a structured way to connect to alert-management systems. Configuration details are here . There are built-in plugins for popular systems such as PagerDuty and OpsGenie, including a generic webhook. Following is an example showing how to integrate Run:ai to a webhook: Use https://webhook.site/ . Get the Unique URL . When installing the Run:ai cluster, edit the values file to add the following. kube-prometheus-stack : ... alertmanager : enabled : true config : global : resolve_timeout : 5m receivers : - name : \"null\" - name : webhook-notifications webhook_configs : - url : <WEB-HOOK-URL> send_resolved : true route : group_by : - alertname group_interval : 5m group_wait : 30s receiver : 'null' repeat_interval : 10m routes : - receiver : webhook-notifications (Replace <WEB-HOOK-URL> with the URL above). On an existing installation, use the upgrade cluster instructions to modify the values file. Verify that you have received alerts at https://webhook.site/ . Out-of-the-box Alerts \u00b6 A Run:ai cluster comes with several built-in alerts. Each alert tests a specific aspect of the Run:ai functionality. In addition, there is a single, inclusive alert, which aggregates all component-based alerts into a single cluster health test . The aggregated alert is named RunaiCriticalProblem . It is categorized as \"critical\". Add a custom alert \u00b6 You can add additional alerts from Run:ai. Alerts are triggered by using the Promtheus query language with any Run:ai metric . To add new alert: When installing the Run:ai cluster, edit the values file . On an existing installation, use the upgrade cluster instructions to modify the values file. Add an alert according to the structure specified below. Add more alerts with the following structure: kube-prometheus-stack : additionalPrometheusRulesMap : custom-runai : groups : - name : custom-runai-rules rules : - alert : <ALERT-NAME> annotations : summary : <ALERT-SUMMARY-TEXT> expr : <PROMQL-EXPRESSION> for : <optional : duration s/m/h> labels : severity : <critical/warning> You can find an example in the Prometheus documentation here .","title":"Monitoring Cluster Health"},{"location":"admin/runai-setup/maintenance/monitoring/#cluster-monitoring","text":"","title":"Cluster Monitoring"},{"location":"admin/runai-setup/maintenance/monitoring/#introduction","text":"Organizations typically want to automatically highlight critical issues and escalate issues to IT/DevOps personnel. The standard practice is to install an alert management tool and connect it to critical systems. Run:ai is comprised of two parts: A control plane part, typically resides in the cloud. The health of the cloud portion of Run:ai can be viewed at status.run.ai . In Self-hosted installations of Run:ai is installed on-prem. One or more GPU Clusters . The purpose of this document is to configure the Run:ai to emit health alerts and to connect these alerts to alert-management systems within the organization. Alerts are emitted for Run:ai Clusters as well as the Run:ai control plane on Self-hosted installation where the control plane resides on the same Kubernetes cluster as one of the Run:ai clusters.","title":"Introduction"},{"location":"admin/runai-setup/maintenance/monitoring/#alert-infrastructure","text":"Run:ai uses Prometheus for externalizing metrics. The Run:ai cluster installation installs Prometheus or can connect to an existing Prometheus instance used in the organization. Run:ai cluster alerts are based on the Prometheus Alert Manager . The Prometheus Alert Manager is enabled by default. This document explains, how to: Configure alert destinations. Triggered alerts will send data to destinations. Understand the out-of-the-box cluster alerts. Advanced: add additional custom alerts.","title":"Alert Infrastructure"},{"location":"admin/runai-setup/maintenance/monitoring/#configure-alert-destinations","text":"Prometheus Alert Manager provides a structured way to connect to alert-management systems. Configuration details are here . There are built-in plugins for popular systems such as PagerDuty and OpsGenie, including a generic webhook. Following is an example showing how to integrate Run:ai to a webhook: Use https://webhook.site/ . Get the Unique URL . When installing the Run:ai cluster, edit the values file to add the following. kube-prometheus-stack : ... alertmanager : enabled : true config : global : resolve_timeout : 5m receivers : - name : \"null\" - name : webhook-notifications webhook_configs : - url : <WEB-HOOK-URL> send_resolved : true route : group_by : - alertname group_interval : 5m group_wait : 30s receiver : 'null' repeat_interval : 10m routes : - receiver : webhook-notifications (Replace <WEB-HOOK-URL> with the URL above). On an existing installation, use the upgrade cluster instructions to modify the values file. Verify that you have received alerts at https://webhook.site/ .","title":"Configure Alert Destinations"},{"location":"admin/runai-setup/maintenance/monitoring/#out-of-the-box-alerts","text":"A Run:ai cluster comes with several built-in alerts. Each alert tests a specific aspect of the Run:ai functionality. In addition, there is a single, inclusive alert, which aggregates all component-based alerts into a single cluster health test . The aggregated alert is named RunaiCriticalProblem . It is categorized as \"critical\".","title":"Out-of-the-box Alerts"},{"location":"admin/runai-setup/maintenance/monitoring/#add-a-custom-alert","text":"You can add additional alerts from Run:ai. Alerts are triggered by using the Promtheus query language with any Run:ai metric . To add new alert: When installing the Run:ai cluster, edit the values file . On an existing installation, use the upgrade cluster instructions to modify the values file. Add an alert according to the structure specified below. Add more alerts with the following structure: kube-prometheus-stack : additionalPrometheusRulesMap : custom-runai : groups : - name : custom-runai-rules rules : - alert : <ALERT-NAME> annotations : summary : <ALERT-SUMMARY-TEXT> expr : <PROMQL-EXPRESSION> for : <optional : duration s/m/h> labels : severity : <critical/warning> You can find an example in the Prometheus documentation here .","title":"Add a custom alert"},{"location":"admin/runai-setup/maintenance/node-downtime/","text":"Planned and Unplanned Node Downtime \u00b6 Introduction \u00b6 Nodes (Machines) that are part of the cluster are susceptible to occasional downtime. This can be either as part of planned maintenance where we bring down the node for a specified time in an orderly fashion or an unplanned downtime where the machine abruptly stops due to a software or hardware issue. The purpose of this document is to provide a process for retaining the Run:ai service and Researcher workloads during and after the downtime. Self-hosted installation \u00b6 The self-hosted installation differs from the Classic (SaaS) installation of Run:ai in that it includes the Run:ai control-plane. The control plane contains data that must be preserved during downtime. As such, you must first follow the disaster recovery planning process. Node Types \u00b6 The document differentiates between Run:ai System Worker Nodes and GPU Worker Nodes : Worker Nodes - are where Machine Learning workloads run. Run:ai System Nodes - In a production installation Run:ai software runs on one or more Run:ai System Nodes on which the Run:ai software runs. Worker Nodes \u00b6 Worker Nodes are where machine learning workloads run. Ideally, when a node is down, whether for planned maintenance or due to an abrupt downtime, these workloads should migrate to other available nodes or wait in the queue to be started when possible. Training vs. Interactive \u00b6 Run:ai differentiates between Training and Interactive workloads. The key difference at node downtime is that Training workloads will automatically move to a new node while Interactive workloads require a manual process. The manual process is recommended for Training workloads as well, as it hastens the process -- it takes time for Kubernetes to identify that a node is down. Planned Maintenance \u00b6 Before stopping a Worker node, perform the following: Stop the Kubernetes scheduler from starting new workloads on the node and drain node from all existing workloads. Workloads will move to other nodes or await on queue for renewed execution: kubectl taint nodes <node-name> runai=drain:NoExecute Shut down the node and perform the required maintenance. When done, start the node and then run: kubectl taint nodes <node-name> runai=drain:NoExecute- Unplanned Downtime \u00b6 If a node has failed and has immediately restarted, all services will automatically start. If a node is to remain down for some time, you will want to drain the node so that workloads will migrate to another node: kubectl taint nodes <node-name> runai=drain:NoExecute When the node is up again, run: kubectl taint nodes <node-name> runai=drain:NoExecute- If the node is to be permanently shut down, you can remove it completely from Kubernetes. Run: kubectl delete node <node-name> However, if you plan to bring back the node, you will need to rejoin the node into the cluster. See Rejoin . Run:ai System Nodes \u00b6 In a production installation, Run:ai software runs on one or more Run:ai system nodes. As a best practice, it's best to have more than one such node so that during planned maintenance or unplanned downtime of a single node, the other node will take over. If a second node does not exist, you will have to designate an arbitrary node on the cluster as a Run:ai system node to complete the process below. Protocols for planned maintenance and unplanned downtime are identical to Worker Nodes. See the section above. Rejoin a Node into the Kubernetes Cluster \u00b6 To rejoin a node to the cluster follow the following steps: On the master node, run: kubeadm token create --print-join-command * This would output a kubeadm join command. Run the command on the worker node for it to re-join the Kubernetes cluster. * Verify that the node is joined by running: kubectl get nodes When the machine is up you will need to re-label nodes according to their role","title":"Node Downtime"},{"location":"admin/runai-setup/maintenance/node-downtime/#planned-and-unplanned-node-downtime","text":"","title":"Planned and Unplanned Node Downtime"},{"location":"admin/runai-setup/maintenance/node-downtime/#introduction","text":"Nodes (Machines) that are part of the cluster are susceptible to occasional downtime. This can be either as part of planned maintenance where we bring down the node for a specified time in an orderly fashion or an unplanned downtime where the machine abruptly stops due to a software or hardware issue. The purpose of this document is to provide a process for retaining the Run:ai service and Researcher workloads during and after the downtime.","title":"Introduction"},{"location":"admin/runai-setup/maintenance/node-downtime/#self-hosted-installation","text":"The self-hosted installation differs from the Classic (SaaS) installation of Run:ai in that it includes the Run:ai control-plane. The control plane contains data that must be preserved during downtime. As such, you must first follow the disaster recovery planning process.","title":"Self-hosted installation"},{"location":"admin/runai-setup/maintenance/node-downtime/#node-types","text":"The document differentiates between Run:ai System Worker Nodes and GPU Worker Nodes : Worker Nodes - are where Machine Learning workloads run. Run:ai System Nodes - In a production installation Run:ai software runs on one or more Run:ai System Nodes on which the Run:ai software runs.","title":"Node Types"},{"location":"admin/runai-setup/maintenance/node-downtime/#worker-nodes","text":"Worker Nodes are where machine learning workloads run. Ideally, when a node is down, whether for planned maintenance or due to an abrupt downtime, these workloads should migrate to other available nodes or wait in the queue to be started when possible.","title":"Worker Nodes"},{"location":"admin/runai-setup/maintenance/node-downtime/#training-vs-interactive","text":"Run:ai differentiates between Training and Interactive workloads. The key difference at node downtime is that Training workloads will automatically move to a new node while Interactive workloads require a manual process. The manual process is recommended for Training workloads as well, as it hastens the process -- it takes time for Kubernetes to identify that a node is down.","title":"Training vs. Interactive"},{"location":"admin/runai-setup/maintenance/node-downtime/#planned-maintenance","text":"Before stopping a Worker node, perform the following: Stop the Kubernetes scheduler from starting new workloads on the node and drain node from all existing workloads. Workloads will move to other nodes or await on queue for renewed execution: kubectl taint nodes <node-name> runai=drain:NoExecute Shut down the node and perform the required maintenance. When done, start the node and then run: kubectl taint nodes <node-name> runai=drain:NoExecute-","title":"Planned Maintenance"},{"location":"admin/runai-setup/maintenance/node-downtime/#unplanned-downtime","text":"If a node has failed and has immediately restarted, all services will automatically start. If a node is to remain down for some time, you will want to drain the node so that workloads will migrate to another node: kubectl taint nodes <node-name> runai=drain:NoExecute When the node is up again, run: kubectl taint nodes <node-name> runai=drain:NoExecute- If the node is to be permanently shut down, you can remove it completely from Kubernetes. Run: kubectl delete node <node-name> However, if you plan to bring back the node, you will need to rejoin the node into the cluster. See Rejoin .","title":"Unplanned Downtime"},{"location":"admin/runai-setup/maintenance/node-downtime/#runai-system-nodes","text":"In a production installation, Run:ai software runs on one or more Run:ai system nodes. As a best practice, it's best to have more than one such node so that during planned maintenance or unplanned downtime of a single node, the other node will take over. If a second node does not exist, you will have to designate an arbitrary node on the cluster as a Run:ai system node to complete the process below. Protocols for planned maintenance and unplanned downtime are identical to Worker Nodes. See the section above.","title":"Run:ai System Nodes"},{"location":"admin/runai-setup/maintenance/node-downtime/#rejoin-a-node-into-the-kubernetes-cluster","text":"To rejoin a node to the cluster follow the following steps: On the master node, run: kubeadm token create --print-join-command * This would output a kubeadm join command. Run the command on the worker node for it to re-join the Kubernetes cluster. * Verify that the node is joined by running: kubectl get nodes When the machine is up you will need to re-label nodes according to their role","title":"Rejoin a Node into the Kubernetes Cluster"},{"location":"admin/runai-setup/self-hosted/overview/","text":"Self Hosted Run:ai Installation \u00b6 The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns. Run:ai self-hosting comes with two variants: Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet The self-hosted installation is priced differently. For further information please talk to Run:ai sales. Self-hosting with Kubernetes vs OpenShift \u00b6 Kubernetes has many Certified Kubernetes Providers . Run:ai has been installed with a number of those such as Rancher, OpenShift, HPE Ezmeral, and Native Kubernetes. The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections: OpenShift-based installation. See Run:ai OpenShift installation . The Run:ai operator for OpenShift is certified by Red Hat. Kubernetes-based installation. See Run:ai Kubernetes installation .","title":"Overview"},{"location":"admin/runai-setup/self-hosted/overview/#self-hosted-runai-installation","text":"The self-hosted option is for organizations that cannot use a SaaS solution due to data leakage concerns. Run:ai self-hosting comes with two variants: Self-hosting Type Description Connected The organization can freely download from the internet (though upload is not allowed) Air-gapped The organization has no connection to the internet The self-hosted installation is priced differently. For further information please talk to Run:ai sales.","title":"Self Hosted Run:ai Installation"},{"location":"admin/runai-setup/self-hosted/overview/#self-hosting-with-kubernetes-vs-openshift","text":"Kubernetes has many Certified Kubernetes Providers . Run:ai has been installed with a number of those such as Rancher, OpenShift, HPE Ezmeral, and Native Kubernetes. The OpenShift installation is different from the rest. As such, the Run:ai self-hosted installation instructions are divided into two separate sections: OpenShift-based installation. See Run:ai OpenShift installation . The Run:ai operator for OpenShift is certified by Red Hat. Kubernetes-based installation. See Run:ai Kubernetes installation .","title":"Self-hosting with Kubernetes vs OpenShift"},{"location":"admin/runai-setup/self-hosted/k8s/additional-clusters/","text":"Installing additional Clusters \u00b6 The first Run:ai cluster is typically installed on the same Kubernetes cluster as the Run:ai control plane. Run:ai supports multiple clusters per single control plane. This document is about installing additional clusters on different Kubernetes clusters . The instructions are for Run:ai version 2.8 and up. Installation \u00b6 Follow the Run:ai SaaS installation network instructions as describe here . Specifically: The Cluster should have a dedicated URL with a trusted certificate. Install NGINX. Create a secret in the Run:ai namespace containing the details of a trusted certificate. Create a new cluster and download a values file. Perform the following changes in the file: Under: runai-operator.config.global set clusterDomain to the domain name of the new cluster. Under runai-operator.config.researcher-service set ingress to true .","title":"Install additional Clusters"},{"location":"admin/runai-setup/self-hosted/k8s/additional-clusters/#installing-additional-clusters","text":"The first Run:ai cluster is typically installed on the same Kubernetes cluster as the Run:ai control plane. Run:ai supports multiple clusters per single control plane. This document is about installing additional clusters on different Kubernetes clusters . The instructions are for Run:ai version 2.8 and up.","title":"Installing additional Clusters"},{"location":"admin/runai-setup/self-hosted/k8s/additional-clusters/#installation","text":"Follow the Run:ai SaaS installation network instructions as describe here . Specifically: The Cluster should have a dedicated URL with a trusted certificate. Install NGINX. Create a secret in the Run:ai namespace containing the details of a trusted certificate. Create a new cluster and download a values file. Perform the following changes in the file: Under: runai-operator.config.global set clusterDomain to the domain name of the new cluster. Under runai-operator.config.researcher-service set ingress to true .","title":"Installation"},{"location":"admin/runai-setup/self-hosted/k8s/backend/","text":"Install the Run:ai Control Plane \u00b6 Create a Control Plane Configuration \u00b6 Create a configuration file to install the Run:ai control plane: Connected Airgapped Run:ai 2.7 Airgapped Run:ai 2.8 and above Generate a values file by running: runai-adm generate-values \\ --external-ips <ip> \\ # (1) --domain <dns-record> \\ # (2) --tls-cert <file-name> --tls-key <file-name> \\ # (3) --nfs-server <nfs-server-address> --nfs-path <path-in-nfs> # (4) An available, IP Address that is accessible from Run:ai Users' machines. Typically (but not always) the IP of one of the nodes. DNS A record such as runai.<company-name> or similar. The A record should point to the IP address above. TLS certificate and private key for the above domain. NFS server location where Run:ai can create files. For using alternative storage mechanisms see optional values below Note In cloud environments, the flag --external-ips should contain both the internal and external IPs (comma separated) A file called runai-backend-values.yaml will be created. Generate a values file by running the following under the deploy folder : runai-adm generate-values \\ --external-ips <ip> \\ # (1) --domain <dns-record> \\ # (2) --tls-cert <file-name> --tls-key <file-name> \\ # (3) --nfs-server <nfs-server-address> --nfs-path <path-in-nfs> \\ # (4) --airgapped An available, IP Address that is accessible from Run:ai Users' machines. Typically (but not always) the IP of one of the nodes. DNS A record such as runai.<company-name> or similar. The A record should point to the IP address above. TLS certificate and private key for the above domain. NFS server location where Run:ai can create files. For using alternative storage mechanisms see optional values below Ignore the message about a downloaded file. Generate a values file by running the following under the deploy folder : runai-adm generate-values \\ --external-ips <ip> \\ # (1) --domain <dns-record> \\ # (2) --tls-cert <file-name> --tls-key <file-name> \\ # (3) --nfs-server <nfs-server-address> --nfs-path <path-in-nfs> \\ # (4) --registry <docker-registry-address> #(5) An available, IP Address that is accessible from Run:ai Users' machines. Typically (but not always) the IP of one of the nodes. DNS A record such as runai.<company-name> or similar. The A record should point to the IP address above. TLS certificate and private key for the above domain. NFS server location where Run:ai can create files. For using alternative storage mechanisms see optional values below Docker Registry address in the form of NAME:PORT (do not add https ): Ignore the message about a downloaded file. (Optional) Edit Configuration File \u00b6 There may be cases where you need to change properties in the values file as follows: Key Change Description backend.initTenant.promProxy and grafana.datasources.datasources.yaml.datasources.url When using an existing Prometheus service, replace this URL with the URL of the existing Prometheus service (obtain by running kubectl get svc on the Prometheus namespace) Internal URL to Prometheus server postgresql.persistence PostgreSQL permanent storage via a Persistent Volume You can either use storageClassName to create a PV automatically or set nfs.server and nfs.path to provide the network file storage for the PV. The folder in the path should be pre-created and have full access rights. This key is now covered under the runai-adm flags above nginx-ingress.controller.externalIPs <RUNAI_IP_ADDRESS> IP address allocated for Run:ai. This key is now covered under the runai-adm flags above backend.https Replace key and crt with public and private keys for runai.<company-name> . This key is now covered under the runai-adm flags above thanos.receive.persistence Permanent storage for Run:ai metrics See postgresql.persistence above. Can use the same location. This key is now covered under the runai-adm flags above backend.initTenant.admin Change password for admin@run.ai This user is the master Control Plane administrator Upload images (Airgapped only) \u00b6 Upload images to a local Docker Registry. Set the Docker Registry address in the form of NAME:PORT (do not add https ): export REGISTRY_URL=<Docker Registry address> Run the following script (you must have at least 20GB of free disk space to run): sudo -E ./prepare_installation.sh If Docker is configured to run as non-root then sudo is not required. Install the Control Plane \u00b6 Run the helm command below: Connected Airgapped helm repo add runai-backend https://backend-charts.storage.googleapis.com helm repo update helm install runai-backend -n runai-backend runai-backend/runai-backend \\ -f runai-backend-values.yaml Info To install a specific version, add --version <version> to the install command. You can find available versions by running helm search repo -l runai-backend . helm install runai-backend runai-backend-<version>.tgz -n \\ runai-backend -f runai-backend-values.yaml (replace <version> with the Run:ai control plane version) Tip Use the --dry-run flag to gain an understanding of what is being installed before the actual installation. Connect to Run:ai User Interface \u00b6 Go to: runai.<company-name> . Log in using the default credentials: User: test@run.ai , Password: Abcd!234 . Go to the Users area and change the password. (Optional) Enable \"Forgot password\" \u00b6 To support the \u201cForgot password\u201d functionality, follow the steps below. Go to runai.<company-name>/auth and Log in. Under Realm settings , select the Login tab and enable the Forgot password feature. Under the Email tab, define an SMTP server, as explained here Next Steps \u00b6 Continue with installing a Run:ai Cluster .","title":"Install Control Plane"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#install-the-runai-control-plane","text":"","title":"Install the Run:ai Control Plane"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#create-a-control-plane-configuration","text":"Create a configuration file to install the Run:ai control plane: Connected Airgapped Run:ai 2.7 Airgapped Run:ai 2.8 and above Generate a values file by running: runai-adm generate-values \\ --external-ips <ip> \\ # (1) --domain <dns-record> \\ # (2) --tls-cert <file-name> --tls-key <file-name> \\ # (3) --nfs-server <nfs-server-address> --nfs-path <path-in-nfs> # (4) An available, IP Address that is accessible from Run:ai Users' machines. Typically (but not always) the IP of one of the nodes. DNS A record such as runai.<company-name> or similar. The A record should point to the IP address above. TLS certificate and private key for the above domain. NFS server location where Run:ai can create files. For using alternative storage mechanisms see optional values below Note In cloud environments, the flag --external-ips should contain both the internal and external IPs (comma separated) A file called runai-backend-values.yaml will be created. Generate a values file by running the following under the deploy folder : runai-adm generate-values \\ --external-ips <ip> \\ # (1) --domain <dns-record> \\ # (2) --tls-cert <file-name> --tls-key <file-name> \\ # (3) --nfs-server <nfs-server-address> --nfs-path <path-in-nfs> \\ # (4) --airgapped An available, IP Address that is accessible from Run:ai Users' machines. Typically (but not always) the IP of one of the nodes. DNS A record such as runai.<company-name> or similar. The A record should point to the IP address above. TLS certificate and private key for the above domain. NFS server location where Run:ai can create files. For using alternative storage mechanisms see optional values below Ignore the message about a downloaded file. Generate a values file by running the following under the deploy folder : runai-adm generate-values \\ --external-ips <ip> \\ # (1) --domain <dns-record> \\ # (2) --tls-cert <file-name> --tls-key <file-name> \\ # (3) --nfs-server <nfs-server-address> --nfs-path <path-in-nfs> \\ # (4) --registry <docker-registry-address> #(5) An available, IP Address that is accessible from Run:ai Users' machines. Typically (but not always) the IP of one of the nodes. DNS A record such as runai.<company-name> or similar. The A record should point to the IP address above. TLS certificate and private key for the above domain. NFS server location where Run:ai can create files. For using alternative storage mechanisms see optional values below Docker Registry address in the form of NAME:PORT (do not add https ): Ignore the message about a downloaded file.","title":"Create a Control Plane Configuration"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#optional-edit-configuration-file","text":"There may be cases where you need to change properties in the values file as follows: Key Change Description backend.initTenant.promProxy and grafana.datasources.datasources.yaml.datasources.url When using an existing Prometheus service, replace this URL with the URL of the existing Prometheus service (obtain by running kubectl get svc on the Prometheus namespace) Internal URL to Prometheus server postgresql.persistence PostgreSQL permanent storage via a Persistent Volume You can either use storageClassName to create a PV automatically or set nfs.server and nfs.path to provide the network file storage for the PV. The folder in the path should be pre-created and have full access rights. This key is now covered under the runai-adm flags above nginx-ingress.controller.externalIPs <RUNAI_IP_ADDRESS> IP address allocated for Run:ai. This key is now covered under the runai-adm flags above backend.https Replace key and crt with public and private keys for runai.<company-name> . This key is now covered under the runai-adm flags above thanos.receive.persistence Permanent storage for Run:ai metrics See postgresql.persistence above. Can use the same location. This key is now covered under the runai-adm flags above backend.initTenant.admin Change password for admin@run.ai This user is the master Control Plane administrator","title":"(Optional) Edit Configuration File"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#upload-images-airgapped-only","text":"Upload images to a local Docker Registry. Set the Docker Registry address in the form of NAME:PORT (do not add https ): export REGISTRY_URL=<Docker Registry address> Run the following script (you must have at least 20GB of free disk space to run): sudo -E ./prepare_installation.sh If Docker is configured to run as non-root then sudo is not required.","title":"Upload images (Airgapped only)"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#install-the-control-plane","text":"Run the helm command below: Connected Airgapped helm repo add runai-backend https://backend-charts.storage.googleapis.com helm repo update helm install runai-backend -n runai-backend runai-backend/runai-backend \\ -f runai-backend-values.yaml Info To install a specific version, add --version <version> to the install command. You can find available versions by running helm search repo -l runai-backend . helm install runai-backend runai-backend-<version>.tgz -n \\ runai-backend -f runai-backend-values.yaml (replace <version> with the Run:ai control plane version) Tip Use the --dry-run flag to gain an understanding of what is being installed before the actual installation.","title":"Install the Control Plane"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#connect-to-runai-user-interface","text":"Go to: runai.<company-name> . Log in using the default credentials: User: test@run.ai , Password: Abcd!234 . Go to the Users area and change the password.","title":"Connect to Run:ai User Interface"},{"location":"admin/runai-setup/self-hosted/k8s/backend/#optional-enable-forgot-password","text":"To support the \u201cForgot password\u201d functionality, follow the steps below. Go to runai.<company-name>/auth and Log in. Under Realm settings , select the Login tab and enable the Forgot password feature. Under the Email tab, define an SMTP server, as explained here","title":"(Optional) Enable \"Forgot password\""},{"location":"admin/runai-setup/self-hosted/k8s/backend/#next-steps","text":"Continue with installing a Run:ai Cluster .","title":"Next Steps"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/","text":"Prerequisites \u00b6 Install prerequisites as per cluster prerequisites document. Version 2.8 or lower The Run:ai Cluster installation installs Prometheus by default. If your Kubernetes cluster already has Prometheus installed, set the flag kube-prometheus-stack.enabled to false . When using an existing Prometheus installation, you will need to add additional rules to your Prometheus configuration. The rules can be found under deploy/runai-prometheus-rules.yaml . Customize Installation \u00b6 Perform the cluster installation instructions explained here . (Optional) make the following changes to the configuration file you have downloaded: Key Default Description runai-operator.config.project-controller.createNamespaces true Set to false if unwilling to provide Run:ai the ability to create namespaces, or would want to create namespaces manually rather than use the Run:ai convention of runai-<PROJECT-NAME> . When set to false , will require an additional manual step when creating new Run:ai Projects. runai-operator.config.project-controller.clusterWideSecret true Set to false if unwilling to provide Run:ai the ability to create Kubernetes Secrets. When not enabled, automatic secret propagation will not be available runai-operator.config.mps-server.enabled false Allow the use of NVIDIA MPS . MPS is useful with Inference workloads. Requires extra cluster permissions runai-operator.config.runai-container-toolkit.enabled true Controls the usage of Fractions . Requires extra cluster permissions runai-operator.config.global.runtime docker Defines the container runtime of the cluster (supports docker and containerd ). Set to containerd when using Tanzu runai-operator.config.runaiBackend.password Default password already set admin@run.ai password. Need to change only if you have changed the password here runai-operator.config.global.prometheusService.address The address of the default Prometheus Service If you installed your own custom Prometheus Service, add this field with the address kube-prometheus-stack.enabled true (Version 2.8 or lower) Install Prometheus. Set to false if Prometheus is already installed in cluster Install Cluster \u00b6 Run: Connected Airgapped helm repo add runai https://run-ai-charts.storage.googleapis.com helm repo update helm install runai-cluster runai/runai-cluster -n runai \\ -f runai-<cluster-name>.yaml --create-namespace Info To install a specific version, add --version <version> to the install command. You can find available versions by running helm search repo -l runai-cluster . helm install runai-cluster -n runai \\ runai-cluster-<version>.tgz -f runai-<cluster-name>.yaml --create-namespace Tip Use the --dry-run flag to gain an understanding of what is being installed before the actual installation. For more details see Understanding cluster access roles .","title":"Install a Cluster"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#prerequisites","text":"Install prerequisites as per cluster prerequisites document. Version 2.8 or lower The Run:ai Cluster installation installs Prometheus by default. If your Kubernetes cluster already has Prometheus installed, set the flag kube-prometheus-stack.enabled to false . When using an existing Prometheus installation, you will need to add additional rules to your Prometheus configuration. The rules can be found under deploy/runai-prometheus-rules.yaml .","title":"Prerequisites"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#customize-installation","text":"Perform the cluster installation instructions explained here . (Optional) make the following changes to the configuration file you have downloaded: Key Default Description runai-operator.config.project-controller.createNamespaces true Set to false if unwilling to provide Run:ai the ability to create namespaces, or would want to create namespaces manually rather than use the Run:ai convention of runai-<PROJECT-NAME> . When set to false , will require an additional manual step when creating new Run:ai Projects. runai-operator.config.project-controller.clusterWideSecret true Set to false if unwilling to provide Run:ai the ability to create Kubernetes Secrets. When not enabled, automatic secret propagation will not be available runai-operator.config.mps-server.enabled false Allow the use of NVIDIA MPS . MPS is useful with Inference workloads. Requires extra cluster permissions runai-operator.config.runai-container-toolkit.enabled true Controls the usage of Fractions . Requires extra cluster permissions runai-operator.config.global.runtime docker Defines the container runtime of the cluster (supports docker and containerd ). Set to containerd when using Tanzu runai-operator.config.runaiBackend.password Default password already set admin@run.ai password. Need to change only if you have changed the password here runai-operator.config.global.prometheusService.address The address of the default Prometheus Service If you installed your own custom Prometheus Service, add this field with the address kube-prometheus-stack.enabled true (Version 2.8 or lower) Install Prometheus. Set to false if Prometheus is already installed in cluster","title":"Customize Installation"},{"location":"admin/runai-setup/self-hosted/k8s/cluster/#install-cluster","text":"Run: Connected Airgapped helm repo add runai https://run-ai-charts.storage.googleapis.com helm repo update helm install runai-cluster runai/runai-cluster -n runai \\ -f runai-<cluster-name>.yaml --create-namespace Info To install a specific version, add --version <version> to the install command. You can find available versions by running helm search repo -l runai-cluster . helm install runai-cluster -n runai \\ runai-cluster-<version>.tgz -f runai-<cluster-name>.yaml --create-namespace Tip Use the --dry-run flag to gain an understanding of what is being installed before the actual installation. For more details see Understanding cluster access roles .","title":"Install Cluster"},{"location":"admin/runai-setup/self-hosted/k8s/next-steps/","text":"Next Steps \u00b6 Create additional I Users . Set up Project-based Researcher Access Control . Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users. Review advanced setup and maintenace scenarios.","title":"Next Steps"},{"location":"admin/runai-setup/self-hosted/k8s/next-steps/#next-steps","text":"Create additional I Users . Set up Project-based Researcher Access Control . Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users. Review advanced setup and maintenace scenarios.","title":"Next Steps"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/","text":"Prerequisites \u00b6 See Prerequisites section above . Prepare Installation Artifacts \u00b6 Run:ai Software Files \u00b6 SSH into a node with kubectl access to the cluster and Docker installed. Connected Airgapped Run the following to enable image download from the Run:ai Container Registry on Google cloud: kubectl create namespace runai-backend kubectl apply -f runai-gcr-secret.yaml To extract Run:ai files, replace <VERSION> in the command below and run: tar xvf runai-<version>.tar.gz cd deploy kubectl create namespace runai-backend Run:ai Administration CLI \u00b6 Connected Airgapped Install the Run:ai Administrator Command-line Interface by following the steps here . Use the Run:ai Administrator Command-line Interface located in the deploy folder. To allow running the binary, run: chmod +x runai-adm Install Helm \u00b6 If helm v3 does not yet exist on the machine, install it now: Connected Airgapped See https://helm.sh/docs/intro/install/ on how to install Helm. Run:ai works with Helm version 3 only (not helm 2). The Helm installation image is under the deploy directory. Run: tar xvf helm-<version>-linux-amd64.tar.gz sudo mv linux-amd64/helm /usr/local/bin/ Mark Run:ai System Workers \u00b6 The Run:ai control plane should be installed on a set of dedicated Run:ai system worker nodes rather than GPU worker nodes. To set system worker nodes run: kubectl label node <NODE-NAME> node-role.kubernetes.io/runai-system=true To avoid single-point-of-failure issues, we recommend assigning more than one node in production environments. Warning Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443). Additional Permissions \u00b6 As part of the installation you will be required to install the Run:ai Control Plane and Cluster Helm Charts . The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the --dry-run on both helm charts. Next Steps \u00b6 Continue with installing the Run:ai Control Plane .","title":"Preparations"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#prerequisites","text":"See Prerequisites section above .","title":"Prerequisites"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#prepare-installation-artifacts","text":"","title":"Prepare Installation Artifacts"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#runai-software-files","text":"SSH into a node with kubectl access to the cluster and Docker installed. Connected Airgapped Run the following to enable image download from the Run:ai Container Registry on Google cloud: kubectl create namespace runai-backend kubectl apply -f runai-gcr-secret.yaml To extract Run:ai files, replace <VERSION> in the command below and run: tar xvf runai-<version>.tar.gz cd deploy kubectl create namespace runai-backend","title":"Run:ai Software Files"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#runai-administration-cli","text":"Connected Airgapped Install the Run:ai Administrator Command-line Interface by following the steps here . Use the Run:ai Administrator Command-line Interface located in the deploy folder. To allow running the binary, run: chmod +x runai-adm","title":"Run:ai Administration CLI"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#install-helm","text":"If helm v3 does not yet exist on the machine, install it now: Connected Airgapped See https://helm.sh/docs/intro/install/ on how to install Helm. Run:ai works with Helm version 3 only (not helm 2). The Helm installation image is under the deploy directory. Run: tar xvf helm-<version>-linux-amd64.tar.gz sudo mv linux-amd64/helm /usr/local/bin/","title":"Install Helm"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#mark-runai-system-workers","text":"The Run:ai control plane should be installed on a set of dedicated Run:ai system worker nodes rather than GPU worker nodes. To set system worker nodes run: kubectl label node <NODE-NAME> node-role.kubernetes.io/runai-system=true To avoid single-point-of-failure issues, we recommend assigning more than one node in production environments. Warning Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).","title":"Mark Run:ai System Workers"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#additional-permissions","text":"As part of the installation you will be required to install the Run:ai Control Plane and Cluster Helm Charts . The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the --dry-run on both helm charts.","title":"Additional Permissions"},{"location":"admin/runai-setup/self-hosted/k8s/preparations/#next-steps","text":"Continue with installing the Run:ai Control Plane .","title":"Next Steps"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/","text":"Before proceeding with this document, please review the installation types documentation to understand the difference between air-gapped and connected installations. Control-plane and clusters \u00b6 As part of the installation process you will install: A control-plane managing cluster One or more clusters Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must. Hardware Requirements \u00b6 See Cluster prerequisites hardware requirements. In addition, the control plane installation of Run:ai requires the configuration of Kubernetes Persistent Volumes of a total size of 110GB. Run:ai Software \u00b6 Connected Airgapped You should receive a file: runai-gcr-secret.yaml from Run:ai Customer Support. The file provides access to the Run:ai Container registry. You should receive a single file runai-<version>.tar from Run:ai customer support Run:ai Software Prerequisites \u00b6 Operating System \u00b6 See Run:ai Cluster prerequisites operating system requirements. The Run:ai control plane operating system prerequisites are identical. Kubernetes \u00b6 See Run:ai Cluster prerequisites Kubernetes requirements. The Run:ai control plane operating system prerequisites are identical. NVIDIA Prerequisites \u00b6 See Run:ai Cluster prerequisites NVIDIA requirements. The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites. Prometheus Prerequisites \u00b6 See Run:ai Cluster prerequisites Prometheus requirements. The Run:ai control plane, when installed without a Run:ai cluster, does not require the Prometheus prerequisites. (Optional) Inference Prerequisites \u00b6 See Run:ai Cluster prerequisites Inference requirements. The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites. Network Requirements \u00b6 Ingress Controller \u00b6 Version 2.9 Version 2.8 or lower The Run:ai control plane installation assumes an existing installation of NGINX as the ingress controller. You can follow the Run:ai Cluster prerequisites ingress controller installation. The Run:ai controller installs NGINX. Thus, in the typical scenario where the Run:ai control plane is installed together with the first cluster, NGINX need not be installed. If the Run:ai cluster is installed on a separate Kubernetes cluster, follow the Run:ai Cluster prerequisites ingress controller requirements. Domain name \u00b6 The Run:ai control plane requires a domain name (FQDN). You must supply a domain name as well as a trusted certificate for that domain. When installing the first Run:ai cluster on the same Kubernetes cluster as the control plane, the Run:ai cluster URL will be the same as the control-plane URL. When installing the Run:ai cluster on a separate Kubernetes cluster, follow the Run:ai domain name requirements. Installer Machine \u00b6 The machine running the installation script (typically the Kubernetes master) must have: At least 50GB of free space. Docker installed. Other \u00b6 (Airgapped installation only) Private Docker Registry . Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <REGISTRY_URL> ). (Optional) SAML Integration as described under single sign-on . Pre-install Script \u00b6 Once you believe that the Run:ai prerequisites are met, we highly recommend installing and running the Run:ai pre-install diagnostics script . The tool: Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking. Looks at additional components installed and analyze their relevance to a successful Run:ai installation. To use the script download the latest version of the script and run: chmod +x preinstall-diagnostics-<platform> ./preinstall-diagnostics-<platform> --domain <dns-entry> If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file runai-preinstall-diagnostics.txt in the current directory and send it to Run:ai technical support. For more information on the script including additional command-line flags, see here .","title":"Prerequisites"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#control-plane-and-clusters","text":"As part of the installation process you will install: A control-plane managing cluster One or more clusters Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must.","title":"Control-plane and clusters"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#hardware-requirements","text":"See Cluster prerequisites hardware requirements. In addition, the control plane installation of Run:ai requires the configuration of Kubernetes Persistent Volumes of a total size of 110GB.","title":"Hardware Requirements"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#runai-software","text":"Connected Airgapped You should receive a file: runai-gcr-secret.yaml from Run:ai Customer Support. The file provides access to the Run:ai Container registry. You should receive a single file runai-<version>.tar from Run:ai customer support","title":"Run:ai Software"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#runai-software-prerequisites","text":"","title":"Run:ai Software Prerequisites"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#operating-system","text":"See Run:ai Cluster prerequisites operating system requirements. The Run:ai control plane operating system prerequisites are identical.","title":"Operating System"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#kubernetes","text":"See Run:ai Cluster prerequisites Kubernetes requirements. The Run:ai control plane operating system prerequisites are identical.","title":"Kubernetes"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#nvidia-prerequisites","text":"See Run:ai Cluster prerequisites NVIDIA requirements. The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites.","title":"NVIDIA Prerequisites"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#prometheus-prerequisites","text":"See Run:ai Cluster prerequisites Prometheus requirements. The Run:ai control plane, when installed without a Run:ai cluster, does not require the Prometheus prerequisites.","title":"Prometheus Prerequisites"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#optional-inference-prerequisites","text":"See Run:ai Cluster prerequisites Inference requirements. The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites.","title":"(Optional) Inference Prerequisites"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#network-requirements","text":"","title":"Network Requirements"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#ingress-controller","text":"Version 2.9 Version 2.8 or lower The Run:ai control plane installation assumes an existing installation of NGINX as the ingress controller. You can follow the Run:ai Cluster prerequisites ingress controller installation. The Run:ai controller installs NGINX. Thus, in the typical scenario where the Run:ai control plane is installed together with the first cluster, NGINX need not be installed. If the Run:ai cluster is installed on a separate Kubernetes cluster, follow the Run:ai Cluster prerequisites ingress controller requirements.","title":"Ingress Controller"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#domain-name","text":"The Run:ai control plane requires a domain name (FQDN). You must supply a domain name as well as a trusted certificate for that domain. When installing the first Run:ai cluster on the same Kubernetes cluster as the control plane, the Run:ai cluster URL will be the same as the control-plane URL. When installing the Run:ai cluster on a separate Kubernetes cluster, follow the Run:ai domain name requirements.","title":"Domain name"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#installer-machine","text":"The machine running the installation script (typically the Kubernetes master) must have: At least 50GB of free space. Docker installed.","title":"Installer Machine"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#other","text":"(Airgapped installation only) Private Docker Registry . Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <REGISTRY_URL> ). (Optional) SAML Integration as described under single sign-on .","title":"Other"},{"location":"admin/runai-setup/self-hosted/k8s/prerequisites/#pre-install-script","text":"Once you believe that the Run:ai prerequisites are met, we highly recommend installing and running the Run:ai pre-install diagnostics script . The tool: Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking. Looks at additional components installed and analyze their relevance to a successful Run:ai installation. To use the script download the latest version of the script and run: chmod +x preinstall-diagnostics-<platform> ./preinstall-diagnostics-<platform> --domain <dns-entry> If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file runai-preinstall-diagnostics.txt in the current directory and send it to Run:ai technical support. For more information on the script including additional command-line flags, see here .","title":"Pre-install Script"},{"location":"admin/runai-setup/self-hosted/k8s/project-management/","text":"Introduction \u00b6 The Administrator creates Run:ai Projects via the Run:ai user interface . When enabling Researcher Authentication you also assign users to Projects. Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically: Creates a namespace by the name of runai-<PROJECT-NAME> . Labels the namespace as managed by Run:ai . Provides access to the namespace for Run:ai services. Associates users with the namespace. This process may need to be altered if, Researchers already have existing Kubernetes namespaces The organization's Kubernetes namespace naming convention does not allow the runai- prefix. The organization's policy does not allow the automatic creation of namespaces. Process \u00b6 Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace: When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag createNamespaces to false . Using the Run:ai User Interface, create a new Project <PROJECT-NAME> Assuming an existing namespace <NAMESPACE> , associate it with the Run:ai project by running: kubectl label ns <NAMESPACE> runai/queue=<PROJECT_NAME> Caution Setting the createNamespaces flag to false moves the responsibility of creating namespaces to match Run:ai Projects to the administrator.","title":"Manually Create Projects"},{"location":"admin/runai-setup/self-hosted/k8s/project-management/#introduction","text":"The Administrator creates Run:ai Projects via the Run:ai user interface . When enabling Researcher Authentication you also assign users to Projects. Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically: Creates a namespace by the name of runai-<PROJECT-NAME> . Labels the namespace as managed by Run:ai . Provides access to the namespace for Run:ai services. Associates users with the namespace. This process may need to be altered if, Researchers already have existing Kubernetes namespaces The organization's Kubernetes namespace naming convention does not allow the runai- prefix. The organization's policy does not allow the automatic creation of namespaces.","title":"Introduction"},{"location":"admin/runai-setup/self-hosted/k8s/project-management/#process","text":"Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace: When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag createNamespaces to false . Using the Run:ai User Interface, create a new Project <PROJECT-NAME> Assuming an existing namespace <NAMESPACE> , associate it with the Run:ai project by running: kubectl label ns <NAMESPACE> runai/queue=<PROJECT_NAME> Caution Setting the createNamespaces flag to false moves the responsibility of creating namespaces to match Run:ai Projects to the administrator.","title":"Process"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/","text":"Uninstall Run:ai \u00b6 Uninstall a Run:ai Cluster \u00b6 To uninstall the cluster see: cluster delete Uninstall the Run:ai Control Plane \u00b6 To delete the control plane, run: helm delete runai-backend -n runai-backend","title":"Uninstall"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/#uninstall-runai","text":"","title":"Uninstall Run:ai"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/#uninstall-a-runai-cluster","text":"To uninstall the cluster see: cluster delete","title":"Uninstall a Run:ai Cluster"},{"location":"admin/runai-setup/self-hosted/k8s/uninstall/#uninstall-the-runai-control-plane","text":"To delete the control plane, run: helm delete runai-backend -n runai-backend","title":"Uninstall the Run:ai Control Plane"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/","text":"Upgrade Run:ai \u00b6 Preparations \u00b6 Connected Airgapped No preparation required. Ask for a tar file runai-air-gapped-<new-version>.tar from Run:ai customer support. The file contains the new version you want to upgrade to. new-version is the updated version of the Run:ai control plane. Prepare the installation artifact as described here (untar the file and run the script to upload it to the local container registry). Upgrading to Version 2.9 \u00b6 Before upgrading the control plane, run: kubectl delete --namespace runai-backend --all \\ deployments,statefulset,svc,ing,ServiceAccount,secrets Prior to version 2.9, the Run:ai installation, by default, has also installed NGINX. It was possible to disable this installation. if NGINX is disabled in your current installation then do not run the following 2 lines. kubectl delete ValidatingWebhookConfiguration runai-backend-nginx-ingress-admission kubectl delete ingressclass nginx Next, install NGINX as described here Then upgrade the control plane as described in the next section. Upgrade Control Plane \u00b6 Run the helm command below. Connected Airgapped helm repo add runai-backend https://backend-charts.storage.googleapis.com helm repo update helm get values runai-backend -n runai-backend > be-values.yaml helm upgrade runai-backend -n runai-backend runai-backend/runai-backend -f be-values.yaml helm get values runai-backend -n runai-backend > be-values.yaml helm upgrade runai-backend runai-backend/runai-backend-<version>.tgz -n \\ runai-backend -f be-values.yaml (replace <version> with the control plane version) Upgrade Cluster \u00b6 Connected Airgapped To upgrade the cluster follow the instructions here . kubectl apply -f runai-crds.yaml helm get values runai-cluster -n runai > values.yaml helm upgrade runai-cluster -n runai runai-cluster-<version>.tgz -f values.yaml (replace <version> with the cluster version)","title":"Upgrade"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-runai","text":"","title":"Upgrade Run:ai"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#preparations","text":"Connected Airgapped No preparation required. Ask for a tar file runai-air-gapped-<new-version>.tar from Run:ai customer support. The file contains the new version you want to upgrade to. new-version is the updated version of the Run:ai control plane. Prepare the installation artifact as described here (untar the file and run the script to upload it to the local container registry).","title":"Preparations"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrading-to-version-29","text":"Before upgrading the control plane, run: kubectl delete --namespace runai-backend --all \\ deployments,statefulset,svc,ing,ServiceAccount,secrets Prior to version 2.9, the Run:ai installation, by default, has also installed NGINX. It was possible to disable this installation. if NGINX is disabled in your current installation then do not run the following 2 lines. kubectl delete ValidatingWebhookConfiguration runai-backend-nginx-ingress-admission kubectl delete ingressclass nginx Next, install NGINX as described here Then upgrade the control plane as described in the next section.","title":"Upgrading to Version 2.9"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-control-plane","text":"Run the helm command below. Connected Airgapped helm repo add runai-backend https://backend-charts.storage.googleapis.com helm repo update helm get values runai-backend -n runai-backend > be-values.yaml helm upgrade runai-backend -n runai-backend runai-backend/runai-backend -f be-values.yaml helm get values runai-backend -n runai-backend > be-values.yaml helm upgrade runai-backend runai-backend/runai-backend-<version>.tgz -n \\ runai-backend -f be-values.yaml (replace <version> with the control plane version)","title":"Upgrade Control Plane"},{"location":"admin/runai-setup/self-hosted/k8s/upgrade/#upgrade-cluster","text":"Connected Airgapped To upgrade the cluster follow the instructions here . kubectl apply -f runai-crds.yaml helm get values runai-cluster -n runai > values.yaml helm upgrade runai-cluster -n runai runai-cluster-<version>.tgz -f values.yaml (replace <version> with the cluster version)","title":"Upgrade Cluster"},{"location":"admin/runai-setup/self-hosted/ocp/backend/","text":"Install the Run:ai Control Plane \u00b6 Create a Control Plane Configuration \u00b6 Note The control-plane installation on Openshift assumes that an identity provider has been configured in OpenShift. Run: oc login . Then create a configuration file to install the Run:ai control plane: Connected Airgapped 2.7 Airgapped 2.8 and above Generate a values file by running: runai-adm generate-values --openshift \\ --first-admin <FIRST_ADMIN_USER_OF_RUNAI> # (1) Name of the administrator user in the company directory Generate a values file by running the following under the deploy folder : runai-adm generate-values --openshift \\ --first-admin <FIRST_ADMIN_USER_OF_RUNAI> \\ # (1) --airgapped Name of the administrator user in the company directory Generate a values file by running the following under the deploy folder : runai-adm generate-values --openshift \\ --first-admin <FIRST_ADMIN_USER_OF_RUNAI> \\ # (1) --registry <docker-registry-address> # (2) Name of the administrator user in the company directory Docker Registry address in the form of NAME:PORT (do not add https ): A file called runai-backend-values.yaml will be created. Upload images (Airgapped only) \u00b6 Upload images to a local Docker Registry. Set the Docker Registry address in the form of NAME:PORT (do not add https ): export REGISTRY_URL=<Docker Registry address> Run the following script (you must have at least 20GB of free disk space to run): sudo -E ./prepare_installation.sh (If docker is configured to run as non-root then sudo is not required). Install the Control Plane \u00b6 Run the helm command below: Connected Airgapped helm repo add runai-backend https://backend-charts.storage.googleapis.com helm repo update helm install runai-backend -n runai-backend runai-backend/runai-backend \\ -f runai-backend-values.yaml Info To install a specific version, add --version <version> to the install command. You can find available versions by running helm search repo -l runai-backend . helm install runai-backend runai-backend/runai-backend-<version>.tgz -n \\ runai-backend -f runai-backend-values.yaml (replace <version> with the control plane version) Tip Use the --dry-run flag to gain an understanding of what is being installed before the actual installation. Connect to Run:ai User Interface \u00b6 Run: oc get routes -n runai-backend to find the Run:ai Administration User Interface URL. Log in using the default credentials: User: test@run.ai , Password: Abcd!234 . Go to the Users area and change the password. Next Steps \u00b6 Continue with installing a Run:ai Cluster .","title":"Install Control Plane"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#install-the-runai-control-plane","text":"","title":"Install the Run:ai Control Plane"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#create-a-control-plane-configuration","text":"Note The control-plane installation on Openshift assumes that an identity provider has been configured in OpenShift. Run: oc login . Then create a configuration file to install the Run:ai control plane: Connected Airgapped 2.7 Airgapped 2.8 and above Generate a values file by running: runai-adm generate-values --openshift \\ --first-admin <FIRST_ADMIN_USER_OF_RUNAI> # (1) Name of the administrator user in the company directory Generate a values file by running the following under the deploy folder : runai-adm generate-values --openshift \\ --first-admin <FIRST_ADMIN_USER_OF_RUNAI> \\ # (1) --airgapped Name of the administrator user in the company directory Generate a values file by running the following under the deploy folder : runai-adm generate-values --openshift \\ --first-admin <FIRST_ADMIN_USER_OF_RUNAI> \\ # (1) --registry <docker-registry-address> # (2) Name of the administrator user in the company directory Docker Registry address in the form of NAME:PORT (do not add https ): A file called runai-backend-values.yaml will be created.","title":"Create a Control Plane Configuration"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#upload-images-airgapped-only","text":"Upload images to a local Docker Registry. Set the Docker Registry address in the form of NAME:PORT (do not add https ): export REGISTRY_URL=<Docker Registry address> Run the following script (you must have at least 20GB of free disk space to run): sudo -E ./prepare_installation.sh (If docker is configured to run as non-root then sudo is not required).","title":"Upload images (Airgapped only)"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#install-the-control-plane","text":"Run the helm command below: Connected Airgapped helm repo add runai-backend https://backend-charts.storage.googleapis.com helm repo update helm install runai-backend -n runai-backend runai-backend/runai-backend \\ -f runai-backend-values.yaml Info To install a specific version, add --version <version> to the install command. You can find available versions by running helm search repo -l runai-backend . helm install runai-backend runai-backend/runai-backend-<version>.tgz -n \\ runai-backend -f runai-backend-values.yaml (replace <version> with the control plane version) Tip Use the --dry-run flag to gain an understanding of what is being installed before the actual installation.","title":"Install the Control Plane"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#connect-to-runai-user-interface","text":"Run: oc get routes -n runai-backend to find the Run:ai Administration User Interface URL. Log in using the default credentials: User: test@run.ai , Password: Abcd!234 . Go to the Users area and change the password.","title":"Connect to Run:ai User Interface"},{"location":"admin/runai-setup/self-hosted/ocp/backend/#next-steps","text":"Continue with installing a Run:ai Cluster .","title":"Next Steps"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/","text":"Install NVIDIA Dependencies \u00b6 Note You must have Cluster Administrator rights to install these dependencies. Before installing Run:ai, you must install NVIDIA software on your OpenShift cluster to enable GPUs. NVIDIA has provided detailed documentation . Follow the instructions to install the two operators Node Feature Discovery and NVIDIA GPU Operator from the OpenShift web console. When done, verify that the GPU Operator is installed by running: oc get pods -n nvidia-gpu-operator (the GPU Operator namespace may differ in different operator versions). Create OpenShift Projects \u00b6 Run:ai cluster installation uses several namespaces (or projects in OpenShift terminology). The installation will automatically create the namespaces, but if your organization requires manual creation of namespaces, you must create them before installing: oc new-project runai oc new-project runai-reservation oc new-project runai-scale-adjust The last namespace ( runai-scale-adjust ) is only required if the cluster is a cloud cluster and is configured for auto-scaling. Monitoring Pre-check \u00b6 Version 2.9 Version 2.8 or lower Not required Run:ai uses the OpenShift monitoring stack. As such, it requires creating or changing the OpenShift monitoring configuration. Check if a configmap already exists: oc get configmap cluster-monitoring-config -n openshift-monitoring If it does, To the cluster values file, add the flag createOpenshiftMonitoringConfig as described under Cluster Installation below. Post-installation, edit the configmap by running: oc edit configmap cluster-monitoring-config -n openshift-monitoring . Add the following: apiVersion : v1 kind : ConfigMap metadata : name : cluster-monitoring-config namespace : openshift-monitoring data : config.yaml : | prometheusK8s: scrapeInterval: \"10s\" evaluationInterval: \"10s\" externalLabels: clusterId: <CLUSTER_ID> prometheus: \"\" prometheus_replica: \"\" For <CLUSTER_ID> use the Cluster UUID field as shown in the Run:ai user interface under the Clusters area. Cluster Installation \u00b6 Perform the cluster installation instructions explained here . When creating a new cluster, select the OpenShift target platform. Attention The cluster wizard shows extra commands which are unique to OpenShift. Remember to run them all. Optional configuration \u00b6 Make the following changes to the configuration file you have downloaded: Key Change Description createOpenshiftMonitoringConfig false see Monitoring Pre-check above. runai-operator.config.project-controller.createNamespaces true Set to false if unwilling to provide Run:ai the ability to create namespaces, or would want to create namespaces manually rather than use the Run:ai convention of runai-<PROJECT-NAME> . When set to false , will require an additional manual step when creating new Run:ai Projects. runai-operator.config.mps-server.enabled Default is false Allow the use of NVIDIA MPS . MPS is useful with Inference workloads. Requires extra permissions runai-operator.config.runai-container-toolkit.enabled Default is true Controls the usage of Fractions . Requires extra permissions runai-operator.config.runaiBackend.password Default password already set admin@run.ai password. Need to change only if you have changed the password here runai-operator.config.global.prometheusService.address The address of the default Prometheus Service If you installed your own custom Prometheus Service, change to its' address Run: Connected Airgapped Follow the instructions on the Cluster Wizard Info To install a specific version, add --version <version> to the install command. You can find available versions by running helm search repo -l runai-cluster . oc label ns runai openshift.io/cluster-monitoring=true oc -n openshift-ingress-operator patch ingresscontroller/default --patch '{\"spec\":{\"routeAdmission\":{\"namespaceOwnership\":\"InterNamespaceAllowed\"}}}' --type=merge helm install runai-cluster -n runai \\ runai-cluster-<version>.tgz -f runai-<cluster-name>.yaml Tip Use the --dry-run flag to gain an understanding of what is being installed before the actual installation. For more details see understanding cluster access roles . Connect Run:ai to GPU Operator \u00b6 Version 2.9 Version 2.8 or lower Not required Locate the name of the GPU operator namespace and run: kubectl patch RunaiConfig runai -n runai -p '{\"spec\": {\"global\": {\"nvidiaDcgmExporter\": {\"namespace\": \"INSERT_NAMESPACE_HERE\"}}}}' --type=\"merge\" (Optional) Prometheus Adapter for Inference \u00b6 The Prometheus adapter is required if you are using Inference workloads and require a custom metric for autoscaling. The following additional steps are required for it to work: Copy prometheus-adapter-prometheus-config and serving-certs-ca-bundle ConfigMaps from openshift-monitoring namespace to the monitoring namespace kubectl get cm prometheus-adapter-prometheus-config --namespace=openshift-monitoring -o yaml \\ | sed 's/namespace: openshift-monitoring/namespace: monitoring/' \\ | kubectl create -f - kubectl get cm serving-certs-ca-bundle --namespace=openshift-monitoring -o yaml \\ | sed 's/namespace: openshift-monitoring/namespace: monitoring/' \\ | kubectl create -f - Allow Prometheus Adapter serviceaccount to create a SecurityContext with RunAsUser 10001: oc adm policy add-scc-to-user anyuid system:serviceaccount:monitoring:runai-cluster-prometheus-adapter Next Steps \u00b6 Continue to create Run:ai Projects .","title":"Install a Cluster"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#install-nvidia-dependencies","text":"Note You must have Cluster Administrator rights to install these dependencies. Before installing Run:ai, you must install NVIDIA software on your OpenShift cluster to enable GPUs. NVIDIA has provided detailed documentation . Follow the instructions to install the two operators Node Feature Discovery and NVIDIA GPU Operator from the OpenShift web console. When done, verify that the GPU Operator is installed by running: oc get pods -n nvidia-gpu-operator (the GPU Operator namespace may differ in different operator versions).","title":"Install NVIDIA Dependencies"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#create-openshift-projects","text":"Run:ai cluster installation uses several namespaces (or projects in OpenShift terminology). The installation will automatically create the namespaces, but if your organization requires manual creation of namespaces, you must create them before installing: oc new-project runai oc new-project runai-reservation oc new-project runai-scale-adjust The last namespace ( runai-scale-adjust ) is only required if the cluster is a cloud cluster and is configured for auto-scaling.","title":"Create OpenShift Projects"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#monitoring-pre-check","text":"Version 2.9 Version 2.8 or lower Not required Run:ai uses the OpenShift monitoring stack. As such, it requires creating or changing the OpenShift monitoring configuration. Check if a configmap already exists: oc get configmap cluster-monitoring-config -n openshift-monitoring If it does, To the cluster values file, add the flag createOpenshiftMonitoringConfig as described under Cluster Installation below. Post-installation, edit the configmap by running: oc edit configmap cluster-monitoring-config -n openshift-monitoring . Add the following: apiVersion : v1 kind : ConfigMap metadata : name : cluster-monitoring-config namespace : openshift-monitoring data : config.yaml : | prometheusK8s: scrapeInterval: \"10s\" evaluationInterval: \"10s\" externalLabels: clusterId: <CLUSTER_ID> prometheus: \"\" prometheus_replica: \"\" For <CLUSTER_ID> use the Cluster UUID field as shown in the Run:ai user interface under the Clusters area.","title":"Monitoring Pre-check"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#cluster-installation","text":"Perform the cluster installation instructions explained here . When creating a new cluster, select the OpenShift target platform. Attention The cluster wizard shows extra commands which are unique to OpenShift. Remember to run them all.","title":"Cluster Installation"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#optional-configuration","text":"Make the following changes to the configuration file you have downloaded: Key Change Description createOpenshiftMonitoringConfig false see Monitoring Pre-check above. runai-operator.config.project-controller.createNamespaces true Set to false if unwilling to provide Run:ai the ability to create namespaces, or would want to create namespaces manually rather than use the Run:ai convention of runai-<PROJECT-NAME> . When set to false , will require an additional manual step when creating new Run:ai Projects. runai-operator.config.mps-server.enabled Default is false Allow the use of NVIDIA MPS . MPS is useful with Inference workloads. Requires extra permissions runai-operator.config.runai-container-toolkit.enabled Default is true Controls the usage of Fractions . Requires extra permissions runai-operator.config.runaiBackend.password Default password already set admin@run.ai password. Need to change only if you have changed the password here runai-operator.config.global.prometheusService.address The address of the default Prometheus Service If you installed your own custom Prometheus Service, change to its' address Run: Connected Airgapped Follow the instructions on the Cluster Wizard Info To install a specific version, add --version <version> to the install command. You can find available versions by running helm search repo -l runai-cluster . oc label ns runai openshift.io/cluster-monitoring=true oc -n openshift-ingress-operator patch ingresscontroller/default --patch '{\"spec\":{\"routeAdmission\":{\"namespaceOwnership\":\"InterNamespaceAllowed\"}}}' --type=merge helm install runai-cluster -n runai \\ runai-cluster-<version>.tgz -f runai-<cluster-name>.yaml Tip Use the --dry-run flag to gain an understanding of what is being installed before the actual installation. For more details see understanding cluster access roles .","title":"Optional configuration"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#connect-runai-to-gpu-operator","text":"Version 2.9 Version 2.8 or lower Not required Locate the name of the GPU operator namespace and run: kubectl patch RunaiConfig runai -n runai -p '{\"spec\": {\"global\": {\"nvidiaDcgmExporter\": {\"namespace\": \"INSERT_NAMESPACE_HERE\"}}}}' --type=\"merge\"","title":"Connect Run:ai to GPU Operator"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#optional-prometheus-adapter-for-inference","text":"The Prometheus adapter is required if you are using Inference workloads and require a custom metric for autoscaling. The following additional steps are required for it to work: Copy prometheus-adapter-prometheus-config and serving-certs-ca-bundle ConfigMaps from openshift-monitoring namespace to the monitoring namespace kubectl get cm prometheus-adapter-prometheus-config --namespace=openshift-monitoring -o yaml \\ | sed 's/namespace: openshift-monitoring/namespace: monitoring/' \\ | kubectl create -f - kubectl get cm serving-certs-ca-bundle --namespace=openshift-monitoring -o yaml \\ | sed 's/namespace: openshift-monitoring/namespace: monitoring/' \\ | kubectl create -f - Allow Prometheus Adapter serviceaccount to create a SecurityContext with RunAsUser 10001: oc adm policy add-scc-to-user anyuid system:serviceaccount:monitoring:runai-cluster-prometheus-adapter","title":"(Optional) Prometheus Adapter for Inference"},{"location":"admin/runai-setup/self-hosted/ocp/cluster/#next-steps","text":"Continue to create Run:ai Projects .","title":"Next Steps"},{"location":"admin/runai-setup/self-hosted/ocp/next-steps/","text":"Next Steps \u00b6 Create additional Run:ai Users . Set up Project-based Researcher Access Control . Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users. Review advanced setup and maintenace scenarios.","title":"Next Steps"},{"location":"admin/runai-setup/self-hosted/ocp/next-steps/#next-steps","text":"Create additional Run:ai Users . Set up Project-based Researcher Access Control . Set up Researchers to work with the Run:ai Command-line interface (CLI). See Installing the Run:ai Command-line Interface on how to install the CLI for users. Review advanced setup and maintenace scenarios.","title":"Next Steps"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/","text":"Preparing for a Run:ai OpenShift Installation \u00b6 The following section provides IT with the information needed to prepare for a Run:ai installation. This includes third-party dependencies which must be met as well as access control that must be granted for Run:ai components. Create OpenShift Projects \u00b6 Run:ai control plane uses a namespace runai-backend (or project in OpenShift terminology). The installation will automatically create the namespace, but if your organization requires manual creation of namespaces, you must create it before installing: oc new-project runai-backend Prepare Run:ai Installation Artifacts \u00b6 Run:ai Software Files \u00b6 SSH into a node with oc access ( oc is the OpenShift command line) to the cluster and Docker installed. Connected Airgapped Run the following to enable image download from the Run:ai Container Registry on Google cloud: oc apply -f runai-gcr-secret.yaml -n runai-backend To extract Run:ai files, replace <VERSION> in the command below and run: tar xvf runai-<version>.tar.gz cd deploy Run:ai Administration CLI \u00b6 Connected Airgapped Install the Run:ai Administrator Command-line Interface by following the steps here . Install the Run:ai Administrator Command-line Interface by following the steps here . Use the image under deploy/runai-admin-cli-<version>-linux-amd64.tar.gz Install Helm \u00b6 If helm v3 does not yet exist on the machine, install it now: Connected Airgapped See https://helm.sh/docs/intro/install/ on how to install Helm. Run:ai works with Helm version 3 only (not helm 2). tar xvf helm-<version>-linux-amd64.tar.gz sudo mv linux-amd64/helm /usr/local/bin/ Mark Run:ai System Workers \u00b6 The Run:ai Control plane should be installed on a set of dedicated Run:ai system worker nodes rather than GPU worker nodes. To set system worker nodes run: oc label node <NODE-NAME> node-role.kubernetes.io/runai-system=true To avoid single-point-of-failure issues, we recommend assigning more than one node in production environments. Warning Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443). Additional Permissions \u00b6 As part of the installation, you will be required to install the Control plane and Cluster Helm Charts . The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the --dry-run on both helm charts. Next Steps \u00b6 Continue with installing the Run:ai Control Plane .","title":"Preparations"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#preparing-for-a-runai-openshift-installation","text":"The following section provides IT with the information needed to prepare for a Run:ai installation. This includes third-party dependencies which must be met as well as access control that must be granted for Run:ai components.","title":"Preparing for a Run:ai OpenShift Installation"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#create-openshift-projects","text":"Run:ai control plane uses a namespace runai-backend (or project in OpenShift terminology). The installation will automatically create the namespace, but if your organization requires manual creation of namespaces, you must create it before installing: oc new-project runai-backend","title":"Create OpenShift Projects"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#prepare-runai-installation-artifacts","text":"","title":"Prepare Run:ai Installation Artifacts"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#runai-software-files","text":"SSH into a node with oc access ( oc is the OpenShift command line) to the cluster and Docker installed. Connected Airgapped Run the following to enable image download from the Run:ai Container Registry on Google cloud: oc apply -f runai-gcr-secret.yaml -n runai-backend To extract Run:ai files, replace <VERSION> in the command below and run: tar xvf runai-<version>.tar.gz cd deploy","title":"Run:ai Software Files"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#runai-administration-cli","text":"Connected Airgapped Install the Run:ai Administrator Command-line Interface by following the steps here . Install the Run:ai Administrator Command-line Interface by following the steps here . Use the image under deploy/runai-admin-cli-<version>-linux-amd64.tar.gz","title":"Run:ai Administration CLI"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#install-helm","text":"If helm v3 does not yet exist on the machine, install it now: Connected Airgapped See https://helm.sh/docs/intro/install/ on how to install Helm. Run:ai works with Helm version 3 only (not helm 2). tar xvf helm-<version>-linux-amd64.tar.gz sudo mv linux-amd64/helm /usr/local/bin/","title":"Install Helm"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#mark-runai-system-workers","text":"The Run:ai Control plane should be installed on a set of dedicated Run:ai system worker nodes rather than GPU worker nodes. To set system worker nodes run: oc label node <NODE-NAME> node-role.kubernetes.io/runai-system=true To avoid single-point-of-failure issues, we recommend assigning more than one node in production environments. Warning Do not select the Kubernetes master as a runai-system node. This may cause Kubernetes to stop working (specifically if Kubernetes API Server is configured on 443 instead of the default 6443).","title":"Mark Run:ai System Workers"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#additional-permissions","text":"As part of the installation, you will be required to install the Control plane and Cluster Helm Charts . The Helm Charts require Kubernetes administrator permissions. You can review the exact permissions provided by using the --dry-run on both helm charts.","title":"Additional Permissions"},{"location":"admin/runai-setup/self-hosted/ocp/preparations/#next-steps","text":"Continue with installing the Run:ai Control Plane .","title":"Next Steps"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/","text":"Before proceeding with this document, please review the installation types documentation to understand the difference between air-gapped and connected installations. Control-plane and clusters \u00b6 As part of the installation process you will install: A control-plane managing cluster One or more clusters Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must. Important In Openshift environments, adding a cluster connecting to a remote control plane currently requires the assistance of customer support. Hardware Requirements \u00b6 See Cluster prerequisites hardware requirements. Run:ai Software \u00b6 Connected Airgapped You should receive a file: runai-gcr-secret.yaml from Run:ai Customer Support. The file provides access to the Run:ai Container registry. You should receive a single file runai-<version>.tar from Run:ai customer support Run:ai Software Prerequisites \u00b6 Operating System \u00b6 OpenShift has specific operating system requirements that can be found in the RedHat documentation. OpenShift \u00b6 Run:ai supports OpenShift. Supported versions are 4.8 through 4.11. OpenShift must be configured with a trusted certificate. OpenShift must have a configured identity provider . OpenShift must have Entitlement . Entitlement is the RedHat OpenShift licensing mechanism. Without entitlement, you will not be able to install the NVIDIA drivers used by the GPU Operator. For further information see here , or the equivalent NVIDIA documentation . Entitlement is not required anymore if you are using OpenShift 4.9.9 or above NVIDIA Prerequisites \u00b6 See Run:ai Cluster prerequisites installing NVIDIA dependencies in OpenShift . The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites. Information on how to download the GPU Operator for air-gapped installation can be found in the NVIDIA GPU Operator pre-requisites . (Optional) Inference Prerequisites \u00b6 See Run:ai Cluster prerequisites Inference requirements. The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites. Installer Machine \u00b6 The machine running the installation script (typically the Kubernetes master) must have: At least 50GB of free space. Docker installed. Other \u00b6 (Airgapped installation only) Private Docker Registry . Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <REGISTRY_URL> ). Pre-install Script \u00b6 Once you believe that the Run:ai prerequisites are met, we highly recommend installing and running the Run:ai pre-install diagnostics script . The tool: Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking. Looks at additional components installed and analyzes their relevancy to a successful Run:ai installation. To use the script download the latest version of the script and run: chmod +x preinstall-diagnostics-<platform> ./preinstall-diagnostics-<platform> If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file runai-preinstall-diagnostics.txt in the current directory and send it to Run:ai technical support. For more information on the script including additional command-line flags, see here .","title":"Prerequisites"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#control-plane-and-clusters","text":"As part of the installation process you will install: A control-plane managing cluster One or more clusters Both the control plane and clusters require Kubernetes. Typically the control plane and first cluster are installed on the same Kubernetes cluster but this is not a must. Important In Openshift environments, adding a cluster connecting to a remote control plane currently requires the assistance of customer support.","title":"Control-plane and clusters"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#hardware-requirements","text":"See Cluster prerequisites hardware requirements.","title":"Hardware Requirements"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#runai-software","text":"Connected Airgapped You should receive a file: runai-gcr-secret.yaml from Run:ai Customer Support. The file provides access to the Run:ai Container registry. You should receive a single file runai-<version>.tar from Run:ai customer support","title":"Run:ai Software"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#runai-software-prerequisites","text":"","title":"Run:ai Software Prerequisites"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#operating-system","text":"OpenShift has specific operating system requirements that can be found in the RedHat documentation.","title":"Operating System"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#openshift","text":"Run:ai supports OpenShift. Supported versions are 4.8 through 4.11. OpenShift must be configured with a trusted certificate. OpenShift must have a configured identity provider . OpenShift must have Entitlement . Entitlement is the RedHat OpenShift licensing mechanism. Without entitlement, you will not be able to install the NVIDIA drivers used by the GPU Operator. For further information see here , or the equivalent NVIDIA documentation . Entitlement is not required anymore if you are using OpenShift 4.9.9 or above","title":"OpenShift"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#nvidia-prerequisites","text":"See Run:ai Cluster prerequisites installing NVIDIA dependencies in OpenShift . The Run:ai control plane, when installed without a Run:ai cluster, does not require the NVIDIA prerequisites. Information on how to download the GPU Operator for air-gapped installation can be found in the NVIDIA GPU Operator pre-requisites .","title":"NVIDIA Prerequisites"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#optional-inference-prerequisites","text":"See Run:ai Cluster prerequisites Inference requirements. The Run:ai control plane, when installed without a Run:ai cluster, does not require the Inference prerequisites.","title":"(Optional) Inference Prerequisites"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#installer-machine","text":"The machine running the installation script (typically the Kubernetes master) must have: At least 50GB of free space. Docker installed.","title":"Installer Machine"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#other","text":"(Airgapped installation only) Private Docker Registry . Run:ai assumes the existence of a Docker registry for images. Most likely installed within the organization. The installation requires the network address and port for the registry (referenced below as <REGISTRY_URL> ).","title":"Other"},{"location":"admin/runai-setup/self-hosted/ocp/prerequisites/#pre-install-script","text":"Once you believe that the Run:ai prerequisites are met, we highly recommend installing and running the Run:ai pre-install diagnostics script . The tool: Tests the below requirements as well as additional failure points related to Kubernetes, NVIDIA, storage, and networking. Looks at additional components installed and analyzes their relevancy to a successful Run:ai installation. To use the script download the latest version of the script and run: chmod +x preinstall-diagnostics-<platform> ./preinstall-diagnostics-<platform> If the script fails, or if the script succeeds but the Kubernetes system contains components other than Run:ai, locate the file runai-preinstall-diagnostics.txt in the current directory and send it to Run:ai technical support. For more information on the script including additional command-line flags, see here .","title":"Pre-install Script"},{"location":"admin/runai-setup/self-hosted/ocp/project-management/","text":"Introduction \u00b6 The Administrator creates Run:ai Projects via the Run:ai User Interface . When enabling Researcher Authentication you also assign users to Projects. Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically: Creates a namespace by the name of runai-<PROJECT-NAME> . Labels the namespace as managed by Run:ai . Provides access to the namespace for Run:ai services. Associates users with the namespace. This process may need to be altered if, Researchers already have existing Kubernetes namespaces The organization's Kubernetes namespace naming convention does not allow the runai- prefix. The organization's policy does not allow the automatic creation of namespaces Process \u00b6 Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace: When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag createNamespaces to false . Using the Run:ai User Interface, create a new Project <PROJECT-NAME> Assuming an existing namespace <NAMESPACE> , associate it with the Run:ai project by running: oc label ns <NAMESPACE> runai/queue=<PROJECT_NAME> Caution Setting the createNamespaces flag to false moves the responsibility of creating namespaces to match Run:ai Projects to the administrator.","title":"Manually Create Projects"},{"location":"admin/runai-setup/self-hosted/ocp/project-management/#introduction","text":"The Administrator creates Run:ai Projects via the Run:ai User Interface . When enabling Researcher Authentication you also assign users to Projects. Run:ai Projects are implemented as Kubernetes namespaces. When creating a new Run:ai Project, Run:ai does the following automatically: Creates a namespace by the name of runai-<PROJECT-NAME> . Labels the namespace as managed by Run:ai . Provides access to the namespace for Run:ai services. Associates users with the namespace. This process may need to be altered if, Researchers already have existing Kubernetes namespaces The organization's Kubernetes namespace naming convention does not allow the runai- prefix. The organization's policy does not allow the automatic creation of namespaces","title":"Introduction"},{"location":"admin/runai-setup/self-hosted/ocp/project-management/#process","text":"Run:ai allows the association of a Run:ai Project with any existing Kubernetes namespace: When setting up a Run:ai cluster, Disable namespace creation by setting the cluster flag createNamespaces to false . Using the Run:ai User Interface, create a new Project <PROJECT-NAME> Assuming an existing namespace <NAMESPACE> , associate it with the Run:ai project by running: oc label ns <NAMESPACE> runai/queue=<PROJECT_NAME> Caution Setting the createNamespaces flag to false moves the responsibility of creating namespaces to match Run:ai Projects to the administrator.","title":"Process"},{"location":"admin/runai-setup/self-hosted/ocp/uninstall/","text":"Uninstall Run:ai \u00b6 See uninstall section here","title":"Uninstall"},{"location":"admin/runai-setup/self-hosted/ocp/uninstall/#uninstall-runai","text":"See uninstall section here","title":"Uninstall Run:ai"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/","text":"Upgrade Run:ai \u00b6 See the upgrade section here","title":"Upgrade"},{"location":"admin/runai-setup/self-hosted/ocp/upgrade/#upgrade-runai","text":"See the upgrade section here","title":"Upgrade Run:ai"},{"location":"admin/troubleshooting/cluster-health-check/","text":"Verifying Cluster Health \u00b6 Following is a set of tests that determine the Run:ai cluster health: Verify that data is sent to the cloud \u00b6 Log in to <company-name>.run.ai/dashboards/now . Verify that all metrics in the overview dashboard are showing. Verify that all metrics are showing in the Nodes view. Go to Projects and create a new Project. Find the new Project using the CLI command: runai list projects Verify that the Run:ai services are running \u00b6 Run: kubectl get pods -n runai kubectl get pods -n monitoring kibectl get pods -n cert-manager Verify that all pods are in Running status and a ready state (1/1 or similar) Run: kubectl get deployments -n runai Check that all deployments are in a ready state (1/1) Run: kubectl get daemonset -n runai A Daemonset runs on every node. Some of the Run:ai daemon-sets run on all nodes. Others run only on nodes that contain GPUs. Verify that for all daemonsets the desired number is equal to current and to ready . Submit a Job via the command-line interface \u00b6 Submitting a Job will allow you to verify that the Run:ai scheduling service is in order. Make sure that the Project you have created has a quota of at least 1 GPU Run: runai config project <project-name> runai submit -i gcr.io/run-ai-demo/quickstart -g 1 Verify that the Job is in a Running state by running: runai list jobs Verify that the Job is showing in the Jobs area at <company-name>.run.ai/jobs . Submit a Job via the user interface \u00b6 Log into the Run:ai user interface, and verify that you have a Researcher or Research Manager role. Go to the Jobs area. On the top right, press the button to create a Job. Once the form opens -- submit a Job.","title":"Cluster Health"},{"location":"admin/troubleshooting/cluster-health-check/#verifying-cluster-health","text":"Following is a set of tests that determine the Run:ai cluster health:","title":"Verifying Cluster Health"},{"location":"admin/troubleshooting/cluster-health-check/#verify-that-data-is-sent-to-the-cloud","text":"Log in to <company-name>.run.ai/dashboards/now . Verify that all metrics in the overview dashboard are showing. Verify that all metrics are showing in the Nodes view. Go to Projects and create a new Project. Find the new Project using the CLI command: runai list projects","title":"Verify that data is sent to the cloud"},{"location":"admin/troubleshooting/cluster-health-check/#verify-that-the-runai-services-are-running","text":"Run: kubectl get pods -n runai kubectl get pods -n monitoring kibectl get pods -n cert-manager Verify that all pods are in Running status and a ready state (1/1 or similar) Run: kubectl get deployments -n runai Check that all deployments are in a ready state (1/1) Run: kubectl get daemonset -n runai A Daemonset runs on every node. Some of the Run:ai daemon-sets run on all nodes. Others run only on nodes that contain GPUs. Verify that for all daemonsets the desired number is equal to current and to ready .","title":"Verify that the Run:ai services are running"},{"location":"admin/troubleshooting/cluster-health-check/#submit-a-job-via-the-command-line-interface","text":"Submitting a Job will allow you to verify that the Run:ai scheduling service is in order. Make sure that the Project you have created has a quota of at least 1 GPU Run: runai config project <project-name> runai submit -i gcr.io/run-ai-demo/quickstart -g 1 Verify that the Job is in a Running state by running: runai list jobs Verify that the Job is showing in the Jobs area at <company-name>.run.ai/jobs .","title":"Submit a Job via the command-line interface"},{"location":"admin/troubleshooting/cluster-health-check/#submit-a-job-via-the-user-interface","text":"Log into the Run:ai user interface, and verify that you have a Researcher or Research Manager role. Go to the Jobs area. On the top right, press the button to create a Job. Once the form opens -- submit a Job.","title":"Submit a Job via the user interface"},{"location":"admin/troubleshooting/diagnostics/","text":"Diagnostic Tools \u00b6 Add Verbosity to the Database container \u00b6 Run:ai Self-hosted installation contains an internal database. To diagnose database issues, you can run the database in debug mode. In the runai-backend-values, search for postgresql . Add: postgresql : image : debug : true Re-install the Run:ai control-plane and then review the database logs by running: kubectl logs -n runai-backend runai-postgresql-0 Internal Networking Issues \u00b6 Run:ai is based on Kubernetes. Kubernetes runs its own internal subnet with a separate DNS service. If you see in the logs that services have trouble connecting, the problem may reside there. You can find further information on how to debug Kubernetes DNS here . Specifically, it is useful to start a pod with networking utilities and use it for network resolution: kubectl run -i --tty netutils --image=dersimn/netutils -- bash Add Verbosity to Prometheus \u00b6 Add verbosity to Prometheus by editing RunaiConfig: kubectl edit runaiconfig runai -n runai Add a debug log level: prometheus-operator : prometheus : prometheusSpec : logLevel : debug To view logs, run: kubectl logs prometheus-runai-prometheus-operator-prometheus-0 prometheus \\ -n monitoring -f --tail 100 Add Verbosity to Scheduler \u00b6 To view extended logs run: kubectl edit ruaiconfig runai -n runai Then under the scheduler section add: runai-scheduler: args: verbosity: 6 Warning Verbose scheduler logs consume a significant amount of disk space.","title":"Diagnostics"},{"location":"admin/troubleshooting/diagnostics/#diagnostic-tools","text":"","title":"Diagnostic Tools"},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-the-database-container","text":"Run:ai Self-hosted installation contains an internal database. To diagnose database issues, you can run the database in debug mode. In the runai-backend-values, search for postgresql . Add: postgresql : image : debug : true Re-install the Run:ai control-plane and then review the database logs by running: kubectl logs -n runai-backend runai-postgresql-0","title":"Add Verbosity to the Database container"},{"location":"admin/troubleshooting/diagnostics/#internal-networking-issues","text":"Run:ai is based on Kubernetes. Kubernetes runs its own internal subnet with a separate DNS service. If you see in the logs that services have trouble connecting, the problem may reside there. You can find further information on how to debug Kubernetes DNS here . Specifically, it is useful to start a pod with networking utilities and use it for network resolution: kubectl run -i --tty netutils --image=dersimn/netutils -- bash","title":"Internal Networking Issues"},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-prometheus","text":"Add verbosity to Prometheus by editing RunaiConfig: kubectl edit runaiconfig runai -n runai Add a debug log level: prometheus-operator : prometheus : prometheusSpec : logLevel : debug To view logs, run: kubectl logs prometheus-runai-prometheus-operator-prometheus-0 prometheus \\ -n monitoring -f --tail 100","title":"Add Verbosity to Prometheus"},{"location":"admin/troubleshooting/diagnostics/#add-verbosity-to-scheduler","text":"To view extended logs run: kubectl edit ruaiconfig runai -n runai Then under the scheduler section add: runai-scheduler: args: verbosity: 6 Warning Verbose scheduler logs consume a significant amount of disk space.","title":"Add Verbosity to Scheduler"},{"location":"admin/troubleshooting/troubleshooting/","text":"Troubleshooting Run:ai \u00b6 Installation \u00b6 init-ca pod crashing Symptom: After installing, when running kubectl get pods -n runai you see a pod named init-ca which has crashed. Root cause: Run:ai installs Cert Manager , but there is an existing cert-manager on the cluster. Review the logs for init-ca . If the logs say: Failed to sign certificate: Operation cannot be fulfilled on certificatesigningrequests.certificates.k8s.io \u201crunai-admission-controller.runai\u201d: the object has been modified; please apply your changes to the latest version and try again Resolution: Call Run:ai customer support to understand how to modify the Run:ai installation not to install cert-manager. Upgrade fails with \"Ingress already exists\" Symptom: The installation fails with error: Error: rendered manifests contain a resource that already exists. Unable to continue with install: IngressClass \"nginx\" in namespace \"\" exists Root cause: Run:ai installs NGINX , but there is an existing NGINX on the cluster. Resolution: In the Run:ai cluster YAML file, disable the installation of NGINX by setting: ingress-nginx: enabled: false Dashboard Issues \u00b6 No Metrics are showing on Dashboard Symptom: No metrics are showing on dashboards at https://<company-name>.run.ai/dashboards/now Typical root causes: Firewall-related issues. Internal clock is not synced. Prometheus pods are not running. Firewall issues Add verbosity to Prometheus as describe here .Verify that there are no errors. If there are connectivity-related errors you may need to: Check your firewall for outbound connections. See the required permitted URL list in Network requirements . If you need to set up an internet proxy or certificate, please contact Run:ai customer support. Machine Clocks are not synced Run: date on cluster nodes and verify that date/time is correct. If not: Set the Linux time service (NTP). Restart Run:ai services. Depending on the previous time gap between servers, you may need to reinstall the Run:ai cluster Prometheus pods are not running Run: kubectl get pods -n monitoring -o wide Verify that all pods are running. The default Prometheus installation is not built for high availability. If a node is down, the Prometheus pod may not recover by itself unless manually deleted. Delete the pod to see it start on a different node and consider adding a second replica to Prometheus. GPU Relates metrics not showing Symptom: GPU-related metrics such as GPU Nodes and Total GPUs are showing zero but other metrics, such as Cluster load are shown. Root cause: An installation issue related to the NVIDIA stack. Resolution: Need to run through the NVIDIA stack and find the issue. The current NVIDIA stack looks as follows: NVIDIA Drivers (at the OS level, on every node) NVIDIA Docker (extension to Docker, on every node) Kubernetes Node feature discovery (mark node properties) NVIDIA GPU Feature discovery (mark nodes as \u201chaving GPUs\u201d) NVIDIA Device plug-in (Exposes GPUs to Kubernetes) NVIDIA DCGM Exporter (Exposes metrics from GPUs in Kubernetes) Run:ai requires the installation of the NVIDIA GPU Operator which installs the entire stack above. However, there are two alternative methods for using the operator: Use the default operator values to install 1 through 6. If NVIDIA Drivers (#1 above) are already installed on all nodes, use the operator with a flag that disables drivers install. For more information see Cluster prerequisites . NVIDIA GPU Operator Run: kubectl get pods -n gpu-operator | grep nvidia and verify that all pods are running. Node and GPU feature discovery Kubernetes Node feature discovery identifies and annotates nodes. NVIDIA GPU Feature Discovery identifies and annotates nodes with GPU properties. See that: All such pods are up. The GPU feature discovery pod is available for every node with a GPU. And finally, when describing nodes, they show an active gpu/nvidia resource. NVIDIA Drivers If NVIDIA drivers have been installed on the nodes themselves, ssh into each node and run nvidia-smi . Run sudo systemctl status docker and verify that docker is running. Run nvidia-docker and verify that it is installed and working. Linux software upgrades may require a node restart. If NVIDIA drivers are installed by the Operator, verify that the NVIDIA driver daemonset has created a pod for each node and that all nodes are running. Review the logs of all such pods. A typical problem may be the driver version which is too advanced for the GPU hardware. You can set the driver version via operator flags. NVIDIA DCGM Exporter View the logs of the DCGM exporter pod and verify that no errors are prohibiting the sending of metrics. To validate that the dcgm-exporter exposes metrics, find one of the DCGM Exporter pods and run: kubectl port-forward <dcgm-exporter-pod-name> 9400:9400 Then browse to http://localhost:9400/metrics and verify that the metrics have reached the DCGM exporter. The next step after the DCGM Exporter is Prometheus . To validate that metrics from the DCGM Exporter reach Prometheus, run: kubectl port-forward svc/runai-cluster-kube-prometh-prometheus -n monitoring 9090:9090 Then browse to localhost:9090 . In the UI, type DCGM_FI_DEV_GPU_UTIL as the metric name, and verify that the metric has reached Prometheus. If the DCGM Exporter is running correctly and exposing metrics, but this metric does not appear in Prometheus, there may be a connectivity issue between these components. Allocation-related metrics not showing Symptom: GPU Allocation-related metrics such as Allocated GPUs are showing zero but other metrics, such as Cluster load are shown. Root cause: The origin of such metrics is the scheduler. Resolution: Run: kubectl get pods -n runai | grep scheduler . Verify that the pod is running. Review the scheduler logs and look for errors. If such errors exist, contact Run:ai customer support. All metrics are showing \"No Data\" Symptom: All data on all dashboards is showing the text \"No Data\". Root cause: Internal issue with metrics infrastructure. Resolution: Please contact Run:ai customer support. Authentication Issues \u00b6 After a successful login, you are redirected to the same login page For a self-hosted installation, check Linux clock synchronization as described above. Use the Run:ai pre-install script to test this automatically. Single-sign-on issues For single-sign-on issues, see the troubleshooting section in the single-sign-on configuration document. User Interface Submit Job Issues \u00b6 New Job button is grayed out Symptom: The New Job button on the top right of the Job list is grayed out. Root Cause: This can happen due to multiple configuration issues: Open Chrome developer tools and refresh the screen. Under Network locate a network call error. Search for the HTTP error code. Resolution for 401 HTTP Error The Cluster certificate provided as part of the installation is valid and trusted (not self-signed). Researcher Authentication has not been properly configured. Try running runai login from the Command-line interface. Alternatively, run: kubectl get pods -n kube-system , identify the api-server pod and review its logs. Resolution for 403 HTTP Error Run: kubectl get pods -n runai , identify the agent pod, see that it's running, and review its logs. New Job button is not showing Symptom: The New Job button on the top right of the Job list does not show. Root Causes: (multiple) You do not have Researcher or Research Manager permissions. Under Settings | General , verify that Unified UI is on. Submit form is distorted Symptom: Submit form is showing vertical lines. Root Cause: The control plane does not know the cluster URL. Using the Run:ai user interface, go to the Clusters list. See that there is no cluster URL next to your cluster. Resolution: Cluster must be re-installed. Submit form is not showing after pressing Create Job button Symptom: (SaaS only) Submit form now showing and under Chrome developer tools you see that all network calls with /workload return an error. Root Cause: Multiple network-related issues. Resolution: Incorrect cluster IP Cluster certificate has not been created Run: kubectl get certificate -n runai . Verify that all 3 entries are of status Ready . Run: kubectl get pods -n cert-manager and verify that all pods are Running. Run: kubectl get crd | grep cert Verify that there are at least the following 6 entries: certificaterequests.cert-manager.io certificates.cert-manager.io challenges.acme.cert-manager.io clusterissuers.cert-manager.io issuers.cert-manager.io orders.acme.cert-manager.io Run: kubectl get challenges -A verify that all challenges are in running state. If not, run: kubectl describe challenge <challlenge-name> -n runai . Possible issues are: Networking issues: most likely a firewall problem. Waiting for dns propagation : This is normal. Need to wait. Submit form does not show the list of Projects Symptom: When connected with Single-sign-on, in the Submit form, the list of Projects is empty. Root Cause: SSO is on and researcher authentication is not properly configured as such. Resolution: Verify API Server settings as described in Researcher Authentication configuration . Job form is not opening on OpenShift Symptom: When clicking on \"New Job\" the Job forms does not load. Network shows 405 Root Cause: An installation step has been missed. Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a patch command at the end of the instruction set. Run it. Networking Issues \u00b6 'admission controller' connectivity issue Symptoms: Pods are failing with 'admission controller' connectivity errors. The command-line runai submit fails with an 'admission controller' connectivity error. Agent or cluster sync pods are crashing in self-hosted installation. Root cause: Connectivity issues between different nodes in the cluster. Resolution: Run the preinstall script and search for networking errors. Run: kubectl get pods -n kube-system -o wide . Verify that all networking pods are running. Run: kubectl get nodes . Check that all nodes are ready and connected. Run: kubectl get pods -o wide -A to see which pods are Pending or in Error and which nodes they belong to. See if pods from different nodes have trouble communicating with each other. Advanced, run: kubectl exec <pod-name> -it /bin/sh from a pod in one node and ping a pod from another. Projects are not syncing Symptom: Create a Project on the Run:ai user interface, then run: runai list projects . The new Project does not appear. Root cause: The Run:ai agent is not syncing properly. This may be due to firewall issues. Resolution Run: runai pods -n runai | grep agent . See that the agent is in Running state. Select the agent's full name and run: kubectl logs -n runai runai-agent-<id> . Verify that there are no errors. If there are connectivity-related errors you may need to check your firewall for outbound connections. See the required permitted URL list in Network requirements . If you need to set up an internet proxy or certificate, please contact Run:ai customer support. Jobs are not syncing Symptom: A Job on the cluster ( runai list jobs ) does not show in the Run:ai user interface Job list. Root cause: The Run:ai cluster-sync pod is not syncing properly. Resolution : Search the cluster-sync pod for errors. Job-related Issues \u00b6 Jobs fail with ContainerCannotRun status Symptom: When running runai list jobs , your Job has a status of ContainerCannotRun . Root Cause: The issue may be caused due to an unattended upgrade of the NVIDIA driver. To verify, run: runai describe job <job-name> , and search for an error driver/library version mismatch . Resolution: Reboot the node on which the Job attempted to run. Going forward, we recommend blacklisting NVIDIA driver from unattended-upgrades. You can do that by editing /etc/apt/apt.conf.d/50unattended-upgrades , and adding nvidia-driver- to the Unattended-Upgrade::Package-Blacklist section. It should look something like that: Unattended-Upgrade::Package-Blacklist { // The following matches all packages starting with linux- // \"linux-\"; \"nvidia-driver-\"; Kubernetes-specific Issues \u00b6 Cluster Installation failed on RKE or EKS Symptom: Cluster is not installed. When running kubectl get pods -n runai you see that pod init-ca has not started. Resolution: Perform the required cert-manager steps here . Inference Issues \u00b6 New Deployment button is grayed out Symptoms: The New Deployment button on the top right of the Deployment list is grayed out. Cannot create a deployment via Inference API . Root Cause: Run:ai Inference prerequisites have not been met. Resolution: Review inference prerequisites and install accordingly. New Deployment button is not showing Symptom: The New Deployment button on the top right of the Deployments list does not show. Root Cause: You do not have ML Engineer permissions. Submitted Deployment remains in Pending state Symptom: A submitted deployment is not running. Root Cause: The patch statement to add the runai-scheduler has not been performed. Some Autoscaling metrics are not working Symptom: Deployments do not autoscale when using metrics other than requests-per-second or concurrency . Root Cause: The horizontal pod autoscaler prerequisite has not been installed. Deployment status is \"Failed\" Symptom: Deployment status is always Failed . Root Cause: (multiple) Not enough resources in the cluster. Server model command is misconfigured (i.e sleep infinity). Server port is misconfigured. Deployment does not scale up from zero Symptom: In the Deployment form, when \"Auto-scaling\" is enabled, and \"Minimum Replicas\" is set to zero, the deployment cannot scale up from zero. Root Cause: Clients are not sending requests. Clients are not using the same port/protocol as the server model. Server model command is misconfigured (i.e sleep infinity). Command-line interface Issues \u00b6 Unable to install CLI due to certificate errors Symptom: The curl command and download button to download the CLI is not working. Root Cause: The cluster is not accessible from the download location Resolution: Use an alternate method for downloading the CLI. Run: kubectl port-forward -n runai svc/researcher-service 4180 In another shell, run: wget --content-disposition http://localhost:4180/cli/linux When running the CLI you get an error: open .../.kube/config.lock: permission denied Symptom: When running any CLI command you get a permission denied error. Root Cause: The user running the CLI does not have read permissions to the .kube directory. Resolution: Change permissions for the directory. When running 'runai logs', the logs are delayed Symptom: Printout from the container is not immediately shown in the log. Root Cause: By default, Python buffers stdout, and stderr, which are not flushed in real-time. This may cause logs to appear sometimes minutes after being buffered. Resolution: Set the env var PYTHONUNBUFFERED to any non-empty string or pass -u to Python. e.g. python -u main.py . CLI does not download properly on OpenShift Symptom: When trying to download the CLI on OpenShift, the wget statement downloads a text file named darwin or linux rather than the binary runai . Root Cause: An installation step has been missed. Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a patch command at the end of the instruction set. Run it.","title":"Troubleshooting"},{"location":"admin/troubleshooting/troubleshooting/#troubleshooting-runai","text":"","title":"Troubleshooting Run:ai"},{"location":"admin/troubleshooting/troubleshooting/#installation","text":"init-ca pod crashing Symptom: After installing, when running kubectl get pods -n runai you see a pod named init-ca which has crashed. Root cause: Run:ai installs Cert Manager , but there is an existing cert-manager on the cluster. Review the logs for init-ca . If the logs say: Failed to sign certificate: Operation cannot be fulfilled on certificatesigningrequests.certificates.k8s.io \u201crunai-admission-controller.runai\u201d: the object has been modified; please apply your changes to the latest version and try again Resolution: Call Run:ai customer support to understand how to modify the Run:ai installation not to install cert-manager. Upgrade fails with \"Ingress already exists\" Symptom: The installation fails with error: Error: rendered manifests contain a resource that already exists. Unable to continue with install: IngressClass \"nginx\" in namespace \"\" exists Root cause: Run:ai installs NGINX , but there is an existing NGINX on the cluster. Resolution: In the Run:ai cluster YAML file, disable the installation of NGINX by setting: ingress-nginx: enabled: false","title":"Installation"},{"location":"admin/troubleshooting/troubleshooting/#dashboard-issues","text":"No Metrics are showing on Dashboard Symptom: No metrics are showing on dashboards at https://<company-name>.run.ai/dashboards/now Typical root causes: Firewall-related issues. Internal clock is not synced. Prometheus pods are not running. Firewall issues Add verbosity to Prometheus as describe here .Verify that there are no errors. If there are connectivity-related errors you may need to: Check your firewall for outbound connections. See the required permitted URL list in Network requirements . If you need to set up an internet proxy or certificate, please contact Run:ai customer support. Machine Clocks are not synced Run: date on cluster nodes and verify that date/time is correct. If not: Set the Linux time service (NTP). Restart Run:ai services. Depending on the previous time gap between servers, you may need to reinstall the Run:ai cluster Prometheus pods are not running Run: kubectl get pods -n monitoring -o wide Verify that all pods are running. The default Prometheus installation is not built for high availability. If a node is down, the Prometheus pod may not recover by itself unless manually deleted. Delete the pod to see it start on a different node and consider adding a second replica to Prometheus. GPU Relates metrics not showing Symptom: GPU-related metrics such as GPU Nodes and Total GPUs are showing zero but other metrics, such as Cluster load are shown. Root cause: An installation issue related to the NVIDIA stack. Resolution: Need to run through the NVIDIA stack and find the issue. The current NVIDIA stack looks as follows: NVIDIA Drivers (at the OS level, on every node) NVIDIA Docker (extension to Docker, on every node) Kubernetes Node feature discovery (mark node properties) NVIDIA GPU Feature discovery (mark nodes as \u201chaving GPUs\u201d) NVIDIA Device plug-in (Exposes GPUs to Kubernetes) NVIDIA DCGM Exporter (Exposes metrics from GPUs in Kubernetes) Run:ai requires the installation of the NVIDIA GPU Operator which installs the entire stack above. However, there are two alternative methods for using the operator: Use the default operator values to install 1 through 6. If NVIDIA Drivers (#1 above) are already installed on all nodes, use the operator with a flag that disables drivers install. For more information see Cluster prerequisites . NVIDIA GPU Operator Run: kubectl get pods -n gpu-operator | grep nvidia and verify that all pods are running. Node and GPU feature discovery Kubernetes Node feature discovery identifies and annotates nodes. NVIDIA GPU Feature Discovery identifies and annotates nodes with GPU properties. See that: All such pods are up. The GPU feature discovery pod is available for every node with a GPU. And finally, when describing nodes, they show an active gpu/nvidia resource. NVIDIA Drivers If NVIDIA drivers have been installed on the nodes themselves, ssh into each node and run nvidia-smi . Run sudo systemctl status docker and verify that docker is running. Run nvidia-docker and verify that it is installed and working. Linux software upgrades may require a node restart. If NVIDIA drivers are installed by the Operator, verify that the NVIDIA driver daemonset has created a pod for each node and that all nodes are running. Review the logs of all such pods. A typical problem may be the driver version which is too advanced for the GPU hardware. You can set the driver version via operator flags. NVIDIA DCGM Exporter View the logs of the DCGM exporter pod and verify that no errors are prohibiting the sending of metrics. To validate that the dcgm-exporter exposes metrics, find one of the DCGM Exporter pods and run: kubectl port-forward <dcgm-exporter-pod-name> 9400:9400 Then browse to http://localhost:9400/metrics and verify that the metrics have reached the DCGM exporter. The next step after the DCGM Exporter is Prometheus . To validate that metrics from the DCGM Exporter reach Prometheus, run: kubectl port-forward svc/runai-cluster-kube-prometh-prometheus -n monitoring 9090:9090 Then browse to localhost:9090 . In the UI, type DCGM_FI_DEV_GPU_UTIL as the metric name, and verify that the metric has reached Prometheus. If the DCGM Exporter is running correctly and exposing metrics, but this metric does not appear in Prometheus, there may be a connectivity issue between these components. Allocation-related metrics not showing Symptom: GPU Allocation-related metrics such as Allocated GPUs are showing zero but other metrics, such as Cluster load are shown. Root cause: The origin of such metrics is the scheduler. Resolution: Run: kubectl get pods -n runai | grep scheduler . Verify that the pod is running. Review the scheduler logs and look for errors. If such errors exist, contact Run:ai customer support. All metrics are showing \"No Data\" Symptom: All data on all dashboards is showing the text \"No Data\". Root cause: Internal issue with metrics infrastructure. Resolution: Please contact Run:ai customer support.","title":"Dashboard Issues"},{"location":"admin/troubleshooting/troubleshooting/#authentication-issues","text":"After a successful login, you are redirected to the same login page For a self-hosted installation, check Linux clock synchronization as described above. Use the Run:ai pre-install script to test this automatically. Single-sign-on issues For single-sign-on issues, see the troubleshooting section in the single-sign-on configuration document.","title":"Authentication Issues"},{"location":"admin/troubleshooting/troubleshooting/#user-interface-submit-job-issues","text":"New Job button is grayed out Symptom: The New Job button on the top right of the Job list is grayed out. Root Cause: This can happen due to multiple configuration issues: Open Chrome developer tools and refresh the screen. Under Network locate a network call error. Search for the HTTP error code. Resolution for 401 HTTP Error The Cluster certificate provided as part of the installation is valid and trusted (not self-signed). Researcher Authentication has not been properly configured. Try running runai login from the Command-line interface. Alternatively, run: kubectl get pods -n kube-system , identify the api-server pod and review its logs. Resolution for 403 HTTP Error Run: kubectl get pods -n runai , identify the agent pod, see that it's running, and review its logs. New Job button is not showing Symptom: The New Job button on the top right of the Job list does not show. Root Causes: (multiple) You do not have Researcher or Research Manager permissions. Under Settings | General , verify that Unified UI is on. Submit form is distorted Symptom: Submit form is showing vertical lines. Root Cause: The control plane does not know the cluster URL. Using the Run:ai user interface, go to the Clusters list. See that there is no cluster URL next to your cluster. Resolution: Cluster must be re-installed. Submit form is not showing after pressing Create Job button Symptom: (SaaS only) Submit form now showing and under Chrome developer tools you see that all network calls with /workload return an error. Root Cause: Multiple network-related issues. Resolution: Incorrect cluster IP Cluster certificate has not been created Run: kubectl get certificate -n runai . Verify that all 3 entries are of status Ready . Run: kubectl get pods -n cert-manager and verify that all pods are Running. Run: kubectl get crd | grep cert Verify that there are at least the following 6 entries: certificaterequests.cert-manager.io certificates.cert-manager.io challenges.acme.cert-manager.io clusterissuers.cert-manager.io issuers.cert-manager.io orders.acme.cert-manager.io Run: kubectl get challenges -A verify that all challenges are in running state. If not, run: kubectl describe challenge <challlenge-name> -n runai . Possible issues are: Networking issues: most likely a firewall problem. Waiting for dns propagation : This is normal. Need to wait. Submit form does not show the list of Projects Symptom: When connected with Single-sign-on, in the Submit form, the list of Projects is empty. Root Cause: SSO is on and researcher authentication is not properly configured as such. Resolution: Verify API Server settings as described in Researcher Authentication configuration . Job form is not opening on OpenShift Symptom: When clicking on \"New Job\" the Job forms does not load. Network shows 405 Root Cause: An installation step has been missed. Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a patch command at the end of the instruction set. Run it.","title":"User Interface Submit Job Issues"},{"location":"admin/troubleshooting/troubleshooting/#networking-issues","text":"'admission controller' connectivity issue Symptoms: Pods are failing with 'admission controller' connectivity errors. The command-line runai submit fails with an 'admission controller' connectivity error. Agent or cluster sync pods are crashing in self-hosted installation. Root cause: Connectivity issues between different nodes in the cluster. Resolution: Run the preinstall script and search for networking errors. Run: kubectl get pods -n kube-system -o wide . Verify that all networking pods are running. Run: kubectl get nodes . Check that all nodes are ready and connected. Run: kubectl get pods -o wide -A to see which pods are Pending or in Error and which nodes they belong to. See if pods from different nodes have trouble communicating with each other. Advanced, run: kubectl exec <pod-name> -it /bin/sh from a pod in one node and ping a pod from another. Projects are not syncing Symptom: Create a Project on the Run:ai user interface, then run: runai list projects . The new Project does not appear. Root cause: The Run:ai agent is not syncing properly. This may be due to firewall issues. Resolution Run: runai pods -n runai | grep agent . See that the agent is in Running state. Select the agent's full name and run: kubectl logs -n runai runai-agent-<id> . Verify that there are no errors. If there are connectivity-related errors you may need to check your firewall for outbound connections. See the required permitted URL list in Network requirements . If you need to set up an internet proxy or certificate, please contact Run:ai customer support. Jobs are not syncing Symptom: A Job on the cluster ( runai list jobs ) does not show in the Run:ai user interface Job list. Root cause: The Run:ai cluster-sync pod is not syncing properly. Resolution : Search the cluster-sync pod for errors.","title":"Networking Issues"},{"location":"admin/troubleshooting/troubleshooting/#job-related-issues","text":"Jobs fail with ContainerCannotRun status Symptom: When running runai list jobs , your Job has a status of ContainerCannotRun . Root Cause: The issue may be caused due to an unattended upgrade of the NVIDIA driver. To verify, run: runai describe job <job-name> , and search for an error driver/library version mismatch . Resolution: Reboot the node on which the Job attempted to run. Going forward, we recommend blacklisting NVIDIA driver from unattended-upgrades. You can do that by editing /etc/apt/apt.conf.d/50unattended-upgrades , and adding nvidia-driver- to the Unattended-Upgrade::Package-Blacklist section. It should look something like that: Unattended-Upgrade::Package-Blacklist { // The following matches all packages starting with linux- // \"linux-\"; \"nvidia-driver-\";","title":"Job-related Issues"},{"location":"admin/troubleshooting/troubleshooting/#kubernetes-specific-issues","text":"Cluster Installation failed on RKE or EKS Symptom: Cluster is not installed. When running kubectl get pods -n runai you see that pod init-ca has not started. Resolution: Perform the required cert-manager steps here .","title":"Kubernetes-specific Issues"},{"location":"admin/troubleshooting/troubleshooting/#inference-issues","text":"New Deployment button is grayed out Symptoms: The New Deployment button on the top right of the Deployment list is grayed out. Cannot create a deployment via Inference API . Root Cause: Run:ai Inference prerequisites have not been met. Resolution: Review inference prerequisites and install accordingly. New Deployment button is not showing Symptom: The New Deployment button on the top right of the Deployments list does not show. Root Cause: You do not have ML Engineer permissions. Submitted Deployment remains in Pending state Symptom: A submitted deployment is not running. Root Cause: The patch statement to add the runai-scheduler has not been performed. Some Autoscaling metrics are not working Symptom: Deployments do not autoscale when using metrics other than requests-per-second or concurrency . Root Cause: The horizontal pod autoscaler prerequisite has not been installed. Deployment status is \"Failed\" Symptom: Deployment status is always Failed . Root Cause: (multiple) Not enough resources in the cluster. Server model command is misconfigured (i.e sleep infinity). Server port is misconfigured. Deployment does not scale up from zero Symptom: In the Deployment form, when \"Auto-scaling\" is enabled, and \"Minimum Replicas\" is set to zero, the deployment cannot scale up from zero. Root Cause: Clients are not sending requests. Clients are not using the same port/protocol as the server model. Server model command is misconfigured (i.e sleep infinity).","title":"Inference Issues"},{"location":"admin/troubleshooting/troubleshooting/#command-line-interface-issues","text":"Unable to install CLI due to certificate errors Symptom: The curl command and download button to download the CLI is not working. Root Cause: The cluster is not accessible from the download location Resolution: Use an alternate method for downloading the CLI. Run: kubectl port-forward -n runai svc/researcher-service 4180 In another shell, run: wget --content-disposition http://localhost:4180/cli/linux When running the CLI you get an error: open .../.kube/config.lock: permission denied Symptom: When running any CLI command you get a permission denied error. Root Cause: The user running the CLI does not have read permissions to the .kube directory. Resolution: Change permissions for the directory. When running 'runai logs', the logs are delayed Symptom: Printout from the container is not immediately shown in the log. Root Cause: By default, Python buffers stdout, and stderr, which are not flushed in real-time. This may cause logs to appear sometimes minutes after being buffered. Resolution: Set the env var PYTHONUNBUFFERED to any non-empty string or pass -u to Python. e.g. python -u main.py . CLI does not download properly on OpenShift Symptom: When trying to download the CLI on OpenShift, the wget statement downloads a text file named darwin or linux rather than the binary runai . Root Cause: An installation step has been missed. Resolution: Open the Cluster list and open the cluster installation wizard again. After selecting OpenShift, you will see a patch command at the end of the instruction set. Run it.","title":"Command-line interface Issues"},{"location":"admin/workloads/inference-overview/","text":"What is Inference \u00b6 Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output. With Inference, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time. Inference and GPUs \u00b6 The Inference process is a subset of the original Training algorithm on a single datum (e.g. one sentence or one image), or a small batch. As such, GPU memory requirements are typically smaller than a full-blown Training process. Given that, Inference lends itself nicely to the usage of Run:ai Fractions. You can, for example, run 4 instances of an Inference server on a single GPU, each employing a fourth of the memory. Inference @Run:ai \u00b6 Run:ai provides Inference services as an equal part together with the other two Workload types: Train and Build . Inference is considered a high-priority workload as it is customer-facing. Running an Inference workload (within the Project's quota) will preempt any Run:ai Workload marked as Training . Inference workloads will receive priority over Train and Build workloads during scheduling. Inference is implemented as a Kubernetes Deployment object with a defined number of replicas. The replicas are load-balanced by Kubernetes so adding more replicas will improve the overall throughput of the system. Multiple replicas will appear in Run:ai as a single Inference workload. The workload will appear in all Run:ai dashboards and views as well as the Command-line interface. Inference workloads can be submitted via Run:ai user interface as well as Run:ai API . Internally, spawning an Inference workload also creates a Kubernetes Service . The service is an end-point to which clients can connect. Auto Scaling \u00b6 To withstand SLA, Inference workloads are typically set with auto scaling . Auto-scaling is the ability to add more computing power (Kubernetes pods) when the load increases and shrink allocated resources when the system is idle. There are a number of ways to trigger auto-scaling. Run:ai supports the following: Metric Units Run:ai name GPU Utilization % gpu-utilization CPU Utilization % cpu-utilization Latency milliseconds latency Throughput requests/second throughput Concurrency concurrency Custom metric custom The Minimum and Maximum number of replicas can be configured as part of the autoscaling configuration. Auto Scaling also supports a scale to zero policy with Throughput and Concurrency metrics, meaning that given enough time under the target threshold, the number of replicas will be scaled down to 0. This has the benefit of conserving resources at the risk of a delay from \"cold starting\" the model when traffic resumes. See Also \u00b6 To set up Inference, see Cluster installation prerequisites . For running Inference see Inference quick-start . To run Inference from the user interface see Deployments . To run Inference using API see Workload overview .","title":"Inference"},{"location":"admin/workloads/inference-overview/#what-is-inference","text":"Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output. With Inference, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time.","title":"What is Inference"},{"location":"admin/workloads/inference-overview/#inference-and-gpus","text":"The Inference process is a subset of the original Training algorithm on a single datum (e.g. one sentence or one image), or a small batch. As such, GPU memory requirements are typically smaller than a full-blown Training process. Given that, Inference lends itself nicely to the usage of Run:ai Fractions. You can, for example, run 4 instances of an Inference server on a single GPU, each employing a fourth of the memory.","title":"Inference and GPUs"},{"location":"admin/workloads/inference-overview/#inference-runai","text":"Run:ai provides Inference services as an equal part together with the other two Workload types: Train and Build . Inference is considered a high-priority workload as it is customer-facing. Running an Inference workload (within the Project's quota) will preempt any Run:ai Workload marked as Training . Inference workloads will receive priority over Train and Build workloads during scheduling. Inference is implemented as a Kubernetes Deployment object with a defined number of replicas. The replicas are load-balanced by Kubernetes so adding more replicas will improve the overall throughput of the system. Multiple replicas will appear in Run:ai as a single Inference workload. The workload will appear in all Run:ai dashboards and views as well as the Command-line interface. Inference workloads can be submitted via Run:ai user interface as well as Run:ai API . Internally, spawning an Inference workload also creates a Kubernetes Service . The service is an end-point to which clients can connect.","title":"Inference @Run:ai"},{"location":"admin/workloads/inference-overview/#auto-scaling","text":"To withstand SLA, Inference workloads are typically set with auto scaling . Auto-scaling is the ability to add more computing power (Kubernetes pods) when the load increases and shrink allocated resources when the system is idle. There are a number of ways to trigger auto-scaling. Run:ai supports the following: Metric Units Run:ai name GPU Utilization % gpu-utilization CPU Utilization % cpu-utilization Latency milliseconds latency Throughput requests/second throughput Concurrency concurrency Custom metric custom The Minimum and Maximum number of replicas can be configured as part of the autoscaling configuration. Auto Scaling also supports a scale to zero policy with Throughput and Concurrency metrics, meaning that given enough time under the target threshold, the number of replicas will be scaled down to 0. This has the benefit of conserving resources at the risk of a delay from \"cold starting\" the model when traffic resumes.","title":"Auto Scaling"},{"location":"admin/workloads/inference-overview/#see-also","text":"To set up Inference, see Cluster installation prerequisites . For running Inference see Inference quick-start . To run Inference from the user interface see Deployments . To run Inference using API see Workload overview .","title":"See Also"},{"location":"admin/workloads/policies/","text":"Configure Policies \u00b6 What are Policies? \u00b6 Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For example: Restrict researchers from requesting more than 2 GPUs, or less than 1GB of memory for an interactive workload. Set the default memory of each training job to 1GB, or mount a default volume to be used by any submitted Workload. Policies are stored as Kubernetes custom resources . Policies are specific to Workload type as such there are several kinds of Policies: Workload Type Kubernetes Workload Name Kubernetes Policy Name Interactive InteractiveWorkload InteractivePolicy Training TrainingWorkload TrainingPolicy Inference InferenceWorkload InferencePolicy A Policy can be created per Run:ai Project (Kubernetes namespace). Additionally, a Policy resource can be created in the runai namespace. This special Policy will take effect when there is no project-specific Policy for the relevant workload kind. Creating a Policy \u00b6 Creating your First Policy \u00b6 To create a sample InteractivePolicy , prepare a file (e.g. policy.yaml ) containing the following YAML: gpupolicy.yaml apiVersion : run.ai/v2alpha1 kind : InteractivePolicy metadata : name : interactive-policy1 namespace : runai-team-a # (1) spec : gpu : rules : required : true min : \"1\" # (2) max : \"4\" value : \"1\" Set the Project namespace here. GPU values are quoted as they can contain non-integer values. The policy places a default and limit on the available values for GPU allocation. To apply this policy, run: kubectl apply -f gpupolicy.yaml Now, try the following command: runai submit --gpu 5 --interactive -p team-a The following message will appear: gpu: must be no greater than 4 A similar message will appear in the New Job form of the Run:ai user interface, when attempting to enter the number of GPUs, which is out of range for an Interactive tab. Read-only values \u00b6 When you do not want the user to be able to change a value, you can force the corresponding user interface control to become read-only by using the canEdit key. For example, runasuserpolicy.yaml apiVersion : run.ai/v2alpha1 kind : TrainingPolicy metadata : name : train-policy1 namespace : runai-team-a # (1) spec : runAsUser : rules : required : true # (2) canEdit : false # (3) value : true # (4) Set the Project namespace here. The field is required. The field will be shown as read-only in the user interface. The field value is true. Complex Values \u00b6 The example above illustrated rules for parameters of \"primitive\" types, such as GPU allocation , CPU memory , working directory , etc. These parameters contain a single value. Other workload parameters, such as ports or volumes , are \"complex\", in the sense that they may contain multiple values: a workload may contain multiple ports and multiple volumes. Following is an example of a policy containing the value ports , which is complex: The ports flag typically contains two values: The external port that is mapped to an internal container port. One can have multiple port tuples defined for a single Workload: apiVersion : run.ai/v2alpha1 kind : InteractivePolicy metadata : name : interactive-policy namespace : runai spec : ports : rules : canAdd : true itemRules : container : min : 30000 max : 32767 external : max : 32767 items : admin-port-a : rules : canRemove : false canEdit : false value : container : 30100 external : 8080 admin-port-b : value : container : 30101 external : 8081 A policy for a complex field is composed of three parts: Rules : Rules apply to the ports parameter as a whole. In this example, the administrator specifies canAdd rule with true value, indicating that a researcher submitting an interactive job can add additional ports to the ports listed by the policy ( true is the default for canAdd , so it actually could have been omitted from the policy above). When canAdd is set to false , the researcher will not be able to add any additional port except those already specified by the policy. itemRules : itemRules impose restrictions on the data members of each item, in this case - container and external . In the above example, the administrator has limited the value of container to 30000-32767, and the value of external to a maximum of 32767. Items : Specifies a list of default ports. Each port is an item in the ports list and given a label (e.g. admin-port-b ). The administrator can also specify whether a researcher can change/delete ports from the submitted workload. In the above example, admin-port-a is hardwired and cannot be changed or deleted, while admin-port-b can be changed or deleted by the researcher when submitting the Workload. Syntax \u00b6 The complete syntax of the policy YAML can be obtained using the explain command of kubectl. For example: kubectl explain trainingpolicy.spec Should provide the list of all possible fields in the spec of training policies: KIND : TrainingPolicy VERSION : run.ai/v2alpha1 RESOURCE : spec <Object> DESCRIPTION : The specifications of this TrainingPolicy FIELDS : annotations <Object> Specifies annotations to be set in the container running the created workload. arguments <Object> If set, the arguments are sent along with the command which overrides the image's entry point of the created workload. command <Object> If set, overrides the image's entry point with the supplied command. ... You can further drill down to get the syntax for ports by running: kubectl explain trainingpolicy.spec.ports KIND : TrainingPolicy VERSION : run.ai/v2alpha1 RESOURCE : ports <Object> DESCRIPTION : Specify the set of ports exposed from the container running the created workload. Used together with --service-type. FIELDS : itemRules <Object> items <map[string]Object> rules <Object> these rules apply to a value of type map (=non primitive) as a whole additionally there are rules which apply for specific items of the map Drill down into the ports.rules object by running: kubectl explain trainingpolicy.spec.ports.rules KIND : TrainingPolicy VERSION : run.ai/ RESOURCE : rules <Object> DESCRIPTION : these rules apply to a value of type map (=non primitive) as a whole additionally there are rules which apply for specific items of the map FIELDS : canAdd <boolean> is it allowed for a workload to add items to this map required <boolean> if the map as a whole is required Note that each kind of policy has a slightly different set of parameters. For example, an InteractivePolicy has a jupyter parameter that is not available under TrainingPolicy . Using Secrets for Environment Variables \u00b6 It is possible to add values from Kubernetes secrets as the value of environment variables included in the policy. The secret will be extracted from the secret object when the Job is created. For example: environment : items : MYPASSWORD : value : \"SECRET:my-secret,password\" When submitting a workload that is affected by this policy, the created container will have an environment variable called MYPASSWORD whose value is the key password residing in Kubernetes secret my-secret which has been pre-created in the namespace where the workload runs. Note Run:ai provides a secret propagation mechanism from the runai namespace to all project namespaces. For further information see secret propagation Modifying/Deleting Policies \u00b6 Use the standard kubectl get/apply/delete commands to modify and delete policies. For example, to view the global interactive policy: kubectl get interactivepolicies -n runai Should return the following: NAME AGE interactive-policy 2d3h ```` To delete this policy: ``` bash kubectl delete InteractivePolicy interactive-policy -n runai To access project-specific policies, replace the -n runai parameter with the namespace of the relevant project. See Also \u00b6 For creating workloads based on policies, see the Run:ai submitting workloads","title":"Policies"},{"location":"admin/workloads/policies/#configure-policies","text":"","title":"Configure Policies"},{"location":"admin/workloads/policies/#what-are-policies","text":"Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For example: Restrict researchers from requesting more than 2 GPUs, or less than 1GB of memory for an interactive workload. Set the default memory of each training job to 1GB, or mount a default volume to be used by any submitted Workload. Policies are stored as Kubernetes custom resources . Policies are specific to Workload type as such there are several kinds of Policies: Workload Type Kubernetes Workload Name Kubernetes Policy Name Interactive InteractiveWorkload InteractivePolicy Training TrainingWorkload TrainingPolicy Inference InferenceWorkload InferencePolicy A Policy can be created per Run:ai Project (Kubernetes namespace). Additionally, a Policy resource can be created in the runai namespace. This special Policy will take effect when there is no project-specific Policy for the relevant workload kind.","title":"What are Policies?"},{"location":"admin/workloads/policies/#creating-a-policy","text":"","title":"Creating a Policy"},{"location":"admin/workloads/policies/#creating-your-first-policy","text":"To create a sample InteractivePolicy , prepare a file (e.g. policy.yaml ) containing the following YAML: gpupolicy.yaml apiVersion : run.ai/v2alpha1 kind : InteractivePolicy metadata : name : interactive-policy1 namespace : runai-team-a # (1) spec : gpu : rules : required : true min : \"1\" # (2) max : \"4\" value : \"1\" Set the Project namespace here. GPU values are quoted as they can contain non-integer values. The policy places a default and limit on the available values for GPU allocation. To apply this policy, run: kubectl apply -f gpupolicy.yaml Now, try the following command: runai submit --gpu 5 --interactive -p team-a The following message will appear: gpu: must be no greater than 4 A similar message will appear in the New Job form of the Run:ai user interface, when attempting to enter the number of GPUs, which is out of range for an Interactive tab.","title":"Creating your First Policy"},{"location":"admin/workloads/policies/#read-only-values","text":"When you do not want the user to be able to change a value, you can force the corresponding user interface control to become read-only by using the canEdit key. For example, runasuserpolicy.yaml apiVersion : run.ai/v2alpha1 kind : TrainingPolicy metadata : name : train-policy1 namespace : runai-team-a # (1) spec : runAsUser : rules : required : true # (2) canEdit : false # (3) value : true # (4) Set the Project namespace here. The field is required. The field will be shown as read-only in the user interface. The field value is true.","title":"Read-only values"},{"location":"admin/workloads/policies/#complex-values","text":"The example above illustrated rules for parameters of \"primitive\" types, such as GPU allocation , CPU memory , working directory , etc. These parameters contain a single value. Other workload parameters, such as ports or volumes , are \"complex\", in the sense that they may contain multiple values: a workload may contain multiple ports and multiple volumes. Following is an example of a policy containing the value ports , which is complex: The ports flag typically contains two values: The external port that is mapped to an internal container port. One can have multiple port tuples defined for a single Workload: apiVersion : run.ai/v2alpha1 kind : InteractivePolicy metadata : name : interactive-policy namespace : runai spec : ports : rules : canAdd : true itemRules : container : min : 30000 max : 32767 external : max : 32767 items : admin-port-a : rules : canRemove : false canEdit : false value : container : 30100 external : 8080 admin-port-b : value : container : 30101 external : 8081 A policy for a complex field is composed of three parts: Rules : Rules apply to the ports parameter as a whole. In this example, the administrator specifies canAdd rule with true value, indicating that a researcher submitting an interactive job can add additional ports to the ports listed by the policy ( true is the default for canAdd , so it actually could have been omitted from the policy above). When canAdd is set to false , the researcher will not be able to add any additional port except those already specified by the policy. itemRules : itemRules impose restrictions on the data members of each item, in this case - container and external . In the above example, the administrator has limited the value of container to 30000-32767, and the value of external to a maximum of 32767. Items : Specifies a list of default ports. Each port is an item in the ports list and given a label (e.g. admin-port-b ). The administrator can also specify whether a researcher can change/delete ports from the submitted workload. In the above example, admin-port-a is hardwired and cannot be changed or deleted, while admin-port-b can be changed or deleted by the researcher when submitting the Workload.","title":"Complex Values"},{"location":"admin/workloads/policies/#syntax","text":"The complete syntax of the policy YAML can be obtained using the explain command of kubectl. For example: kubectl explain trainingpolicy.spec Should provide the list of all possible fields in the spec of training policies: KIND : TrainingPolicy VERSION : run.ai/v2alpha1 RESOURCE : spec <Object> DESCRIPTION : The specifications of this TrainingPolicy FIELDS : annotations <Object> Specifies annotations to be set in the container running the created workload. arguments <Object> If set, the arguments are sent along with the command which overrides the image's entry point of the created workload. command <Object> If set, overrides the image's entry point with the supplied command. ... You can further drill down to get the syntax for ports by running: kubectl explain trainingpolicy.spec.ports KIND : TrainingPolicy VERSION : run.ai/v2alpha1 RESOURCE : ports <Object> DESCRIPTION : Specify the set of ports exposed from the container running the created workload. Used together with --service-type. FIELDS : itemRules <Object> items <map[string]Object> rules <Object> these rules apply to a value of type map (=non primitive) as a whole additionally there are rules which apply for specific items of the map Drill down into the ports.rules object by running: kubectl explain trainingpolicy.spec.ports.rules KIND : TrainingPolicy VERSION : run.ai/ RESOURCE : rules <Object> DESCRIPTION : these rules apply to a value of type map (=non primitive) as a whole additionally there are rules which apply for specific items of the map FIELDS : canAdd <boolean> is it allowed for a workload to add items to this map required <boolean> if the map as a whole is required Note that each kind of policy has a slightly different set of parameters. For example, an InteractivePolicy has a jupyter parameter that is not available under TrainingPolicy .","title":"Syntax"},{"location":"admin/workloads/policies/#using-secrets-for-environment-variables","text":"It is possible to add values from Kubernetes secrets as the value of environment variables included in the policy. The secret will be extracted from the secret object when the Job is created. For example: environment : items : MYPASSWORD : value : \"SECRET:my-secret,password\" When submitting a workload that is affected by this policy, the created container will have an environment variable called MYPASSWORD whose value is the key password residing in Kubernetes secret my-secret which has been pre-created in the namespace where the workload runs. Note Run:ai provides a secret propagation mechanism from the runai namespace to all project namespaces. For further information see secret propagation","title":"Using Secrets for Environment Variables"},{"location":"admin/workloads/policies/#modifyingdeleting-policies","text":"Use the standard kubectl get/apply/delete commands to modify and delete policies. For example, to view the global interactive policy: kubectl get interactivepolicies -n runai Should return the following: NAME AGE interactive-policy 2d3h ```` To delete this policy: ``` bash kubectl delete InteractivePolicy interactive-policy -n runai To access project-specific policies, replace the -n runai parameter with the namespace of the relevant project.","title":"Modifying/Deleting Policies"},{"location":"admin/workloads/policies/#see-also","text":"For creating workloads based on policies, see the Run:ai submitting workloads","title":"See Also"},{"location":"admin/workloads/secrets/","text":"Secrets in Workloads \u00b6 Kubernetes Secrets \u00b6 Sometimes you want to use sensitive information within your code. For example passwords, OAuth tokens, or ssh keys. The best practice for saving such information in Kubernetes is via Kubernetes Secrets . Kubernetes Secrets let you store and manage sensitive information. Access to secrets is limited via configuration. A Kubernetes secret may hold multiple key - value pairs. Using Secrets in Run:ai Workloads \u00b6 Our goal is to provide Run:ai Workloads with secrets as input in a secure way. Using the Run:ai command line, you will be able to pass a reference to a secret that already exists in Kubernetes. Creating a secret \u00b6 For details on how to create a Kubernetes secret see: https://kubernetes.io/docs/concepts/configuration/secret/ . Here is an example: apiVersion : v1 kind : Secret metadata : name : my-secret namespace : runai-<project-name> data : username : am9obgo= password : bXktcGFzc3dvcmQK Then run: kubectl apply -f <file-name> Notes Secrets are base64 encoded Secrets are stored in the scope of a namespace and will not be accessible from other namespaces. Hence the reference to the Run:ai Project name above. Run:ai provides the ability to propagate secrets throughout all Run:ai Projects. See below. Attaching a secret to a Workload on Submit \u00b6 When you submit a new Workload, you will want to connect the secret to the new Workload. To do that, run: runai submit -e <ENV-VARIABLE>=SECRET:<secret-name>,<secret-key> .... For example: runai submit -i ubuntu -e MYUSERNAME=SECRET:my-secret,username Secrets and Projects \u00b6 As per the note above, secrets are namespace-specific. If your secret relates to all Run:ai Projects, do the following to propagate the secret to all Projects: Create a secret within the runai namespace. Run the following once to allow Run:ai to propagate the secret to all Run:ai Projects: runai-adm set secret <secret name> --cluster-wide Reminder The Run:ai Administrator CLI can be obtained here . To delete a secret from all Run:ai Projects, run: runai-adm remove secret <secret name> --cluster-wide Secrets and Policies \u00b6 A Secret can be set at the policy level. For additional information see policies guide","title":"Secrets"},{"location":"admin/workloads/secrets/#secrets-in-workloads","text":"","title":"Secrets in Workloads"},{"location":"admin/workloads/secrets/#kubernetes-secrets","text":"Sometimes you want to use sensitive information within your code. For example passwords, OAuth tokens, or ssh keys. The best practice for saving such information in Kubernetes is via Kubernetes Secrets . Kubernetes Secrets let you store and manage sensitive information. Access to secrets is limited via configuration. A Kubernetes secret may hold multiple key - value pairs.","title":"Kubernetes Secrets"},{"location":"admin/workloads/secrets/#using-secrets-in-runai-workloads","text":"Our goal is to provide Run:ai Workloads with secrets as input in a secure way. Using the Run:ai command line, you will be able to pass a reference to a secret that already exists in Kubernetes.","title":"Using Secrets in Run:ai Workloads"},{"location":"admin/workloads/secrets/#creating-a-secret","text":"For details on how to create a Kubernetes secret see: https://kubernetes.io/docs/concepts/configuration/secret/ . Here is an example: apiVersion : v1 kind : Secret metadata : name : my-secret namespace : runai-<project-name> data : username : am9obgo= password : bXktcGFzc3dvcmQK Then run: kubectl apply -f <file-name> Notes Secrets are base64 encoded Secrets are stored in the scope of a namespace and will not be accessible from other namespaces. Hence the reference to the Run:ai Project name above. Run:ai provides the ability to propagate secrets throughout all Run:ai Projects. See below.","title":"Creating a secret"},{"location":"admin/workloads/secrets/#attaching-a-secret-to-a-workload-on-submit","text":"When you submit a new Workload, you will want to connect the secret to the new Workload. To do that, run: runai submit -e <ENV-VARIABLE>=SECRET:<secret-name>,<secret-key> .... For example: runai submit -i ubuntu -e MYUSERNAME=SECRET:my-secret,username","title":"Attaching a secret to a Workload on Submit"},{"location":"admin/workloads/secrets/#secrets-and-projects","text":"As per the note above, secrets are namespace-specific. If your secret relates to all Run:ai Projects, do the following to propagate the secret to all Projects: Create a secret within the runai namespace. Run the following once to allow Run:ai to propagate the secret to all Run:ai Projects: runai-adm set secret <secret name> --cluster-wide Reminder The Run:ai Administrator CLI can be obtained here . To delete a secret from all Run:ai Projects, run: runai-adm remove secret <secret name> --cluster-wide","title":"Secrets and Projects"},{"location":"admin/workloads/secrets/#secrets-and-policies","text":"A Secret can be set at the policy level. For additional information see policies guide","title":"Secrets and Policies"},{"location":"admin/workloads/workload-overview-admin/","text":"Workloads Overview \u00b6 Workloads \u00b6 Run:ai schedules Workloads . Run:ai workloads are comprised of: The Kubernetes object (Job, Deployment, etc) which is used to launch the container, inside which the data science code runs. A set of additional resources that are required to run the Workload. Examples: a service entry point that allows access to the Job, a persistent volume claim to access data on the network, and more. All of these components are created together and deleted together when the Workload ends. Run:ai currently supports the following Workloads types: Workload Type Kubernetes Name Description Interactive InteractiveWorkload Submit an interactive workload Training TrainingWorkload Submit a training workload Inference InferenceWorkload Submit an inference workload Values \u00b6 A Workload will typically have a list of values (sometimes called flags ), such as name, image, and resources. A full list of values is available in the runai-submit Command-line reference. How to Submit \u00b6 A Workload can be submitted via various channels: The Run:ai user interface . The Run:ai command-line interface, via the runai submit command. The Run:ai Cluster API . Workload Policies \u00b6 As an administrator, you can set Policies on Workloads. Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For more information see Workload Policies .","title":"Overview"},{"location":"admin/workloads/workload-overview-admin/#workloads-overview","text":"","title":"Workloads Overview"},{"location":"admin/workloads/workload-overview-admin/#workloads","text":"Run:ai schedules Workloads . Run:ai workloads are comprised of: The Kubernetes object (Job, Deployment, etc) which is used to launch the container, inside which the data science code runs. A set of additional resources that are required to run the Workload. Examples: a service entry point that allows access to the Job, a persistent volume claim to access data on the network, and more. All of these components are created together and deleted together when the Workload ends. Run:ai currently supports the following Workloads types: Workload Type Kubernetes Name Description Interactive InteractiveWorkload Submit an interactive workload Training TrainingWorkload Submit a training workload Inference InferenceWorkload Submit an inference workload","title":"Workloads"},{"location":"admin/workloads/workload-overview-admin/#values","text":"A Workload will typically have a list of values (sometimes called flags ), such as name, image, and resources. A full list of values is available in the runai-submit Command-line reference.","title":"Values"},{"location":"admin/workloads/workload-overview-admin/#how-to-submit","text":"A Workload can be submitted via various channels: The Run:ai user interface . The Run:ai command-line interface, via the runai submit command. The Run:ai Cluster API .","title":"How to Submit"},{"location":"admin/workloads/workload-overview-admin/#workload-policies","text":"As an administrator, you can set Policies on Workloads. Policies allow administrators to impose restrictions and set default values for Researcher Workloads. For more information see Workload Policies .","title":"Workload Policies"},{"location":"developer/overview-developer/","text":"Overview: Developer Documentation \u00b6 Developers can access Run:ai through various programmatic interfaces. API Architecture \u00b6 Run:ai is composed of a single, multi-tenant control plane. Each tenant can be connected to one or more GPU clusters. See Run:ai system components for detailed information. Below is a diagram of the Run:ai API Architecture. A developer may: Access the control plane via the Administrator API . Access any one of the GPU clusters via Cluster API . Access cluster metrics via the Metrics API . Administrator API \u00b6 Add, delete, modify and list Run:ai meta-data objects such as Projects, Departments, Users, and more. The API is provided as REST and is accessible via the control plane endpoint. For more information see Administrator REST API . Cluster API \u00b6 Submit and delete Workloads. The API is provided as Kubernetes API . Cluster API is accessible via the GPU cluster itself. As such, multiple clusters may have multiple endpoints . Note The same functionality is also available via the Run:ai Command-line interface . The CLI provides an alternative for automating with shell scripts. Metrics API \u00b6 Retrieve metrics from multiple GPU clusters. See the Metrics API document. API Authentication \u00b6 See API Authentication for information on how to gain authenticated access to Run:ai APIs.","title":"Overview"},{"location":"developer/overview-developer/#overview-developer-documentation","text":"Developers can access Run:ai through various programmatic interfaces.","title":"Overview: Developer Documentation"},{"location":"developer/overview-developer/#api-architecture","text":"Run:ai is composed of a single, multi-tenant control plane. Each tenant can be connected to one or more GPU clusters. See Run:ai system components for detailed information. Below is a diagram of the Run:ai API Architecture. A developer may: Access the control plane via the Administrator API . Access any one of the GPU clusters via Cluster API . Access cluster metrics via the Metrics API .","title":"API Architecture"},{"location":"developer/overview-developer/#administrator-api","text":"Add, delete, modify and list Run:ai meta-data objects such as Projects, Departments, Users, and more. The API is provided as REST and is accessible via the control plane endpoint. For more information see Administrator REST API .","title":"Administrator API"},{"location":"developer/overview-developer/#cluster-api","text":"Submit and delete Workloads. The API is provided as Kubernetes API . Cluster API is accessible via the GPU cluster itself. As such, multiple clusters may have multiple endpoints . Note The same functionality is also available via the Run:ai Command-line interface . The CLI provides an alternative for automating with shell scripts.","title":"Cluster API"},{"location":"developer/overview-developer/#metrics-api","text":"Retrieve metrics from multiple GPU clusters. See the Metrics API document.","title":"Metrics API"},{"location":"developer/overview-developer/#api-authentication","text":"See API Authentication for information on how to gain authenticated access to Run:ai APIs.","title":"API Authentication"},{"location":"developer/rest-auth/","text":"API Authentication \u00b6 The following document explains how to authenticate with Run:ai APIs. Run:ai APIs are accessed using bearer tokens . A token can be obtained in several ways: When logging into the Run:ai user interface, you enter an email and password (or authenticated via single sign-on) which are used to obtain a token. When using the Run:ai command-line, you use a Kubernetes profile and are authenticated by pre-running runai login (or oc login with OpenShift). The command attachs a token to the profile and allows you access to Run:ai functionality. When using Run:ai APIs, you need to create an Application through the Run:ai user interface. The Application is created with specific roles and contains a secret . Using the secret you can obtain a token and use it within subsequent API calls. Create a Client Application \u00b6 Open the Run:ai Run:ai User Interface. Go to Settings | Application and create a new Application. Set the required roles: Select Researcher to manipulate Jobs using the Cluster API . To provide access to a specific project, you will also need to go to Application | Projects and provide the Application with access to specific projects. Select Editor to manipulate Projects and Departments using the Administrator REST API . Select Administrator to manipulate Users , Tenant Settings and Clusters using the Administrator REST API . Copy the <APPLICATION-NAME> and <CLIENT-SECRET> to be used below Go to Settings | General , under Researcher Authentication copy <REALM> . Important Creating Client Application tokens is only available with SaaS installations where the tenant has been created post-January 2022 or any Self-hosted installation. If you are an administrator but do not see the Settings | Application area, please contact Run:ai customer support. Request an API Token \u00b6 Use the above parameters to get a temporary token to access Run:ai as follows. Example command to get an API token \u00b6 Replace <COMPANY-URL> below with app.run.ai for SaaS installations (not <company>.run.ai ) or the Run:ai user interface URL for Self-hosted installations. cURL Python curl -X POST 'https://<COMPANY-URL>/auth/realms/<REALM>/protocol/openid-connect/token' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'grant_type=client_credentials' \\ --data-urlencode 'scope=openid' \\ --data-urlencode 'response_type=id_token' \\ --data-urlencode 'client_id=<APPLICATION-NAME>' \\ --data-urlencode 'client_secret=<CLIENT-SECRET>' import http.client conn = http . client . HTTPSConnection ( \"\" ) payload = \"grant_type=client_credentials&client_id=<APPLICATION-NAME>&client_secret=<CLIENT_SECRET>\" headers = { 'content-type' : \"application/x-www-form-urlencoded\" } conn . request ( \"POST\" , \"/<COMPANY-URL>/auth/realms/<REALM>/protocol/openid-connect/token\" , payload , headers ) res = conn . getresponse () data = res . read () print ( data . decode ( \"utf-8\" )) Response \u00b6 The API response will look as follows: API Response { \"access_token\" : \"...\" , // (1) \"expires_in\" : 36000 , .... \"token_type\" : \"bearer\" } Use the access_token as the Bearer token below. To call APIs, the application must pass the retrieved access_token as a Bearer token in the Authorization header of your HTTP request. To retrieve and manipulate Workloads, use the Cluster API . Researcher API works at the cluster level and you will have different endpoints for different clusters. To retrieve and manipulate other metadata objects, use the Administrator REST API . Administrator API works at the control-plane level and you have a single endpoint for all clusters.","title":"API Authentication"},{"location":"developer/rest-auth/#api-authentication","text":"The following document explains how to authenticate with Run:ai APIs. Run:ai APIs are accessed using bearer tokens . A token can be obtained in several ways: When logging into the Run:ai user interface, you enter an email and password (or authenticated via single sign-on) which are used to obtain a token. When using the Run:ai command-line, you use a Kubernetes profile and are authenticated by pre-running runai login (or oc login with OpenShift). The command attachs a token to the profile and allows you access to Run:ai functionality. When using Run:ai APIs, you need to create an Application through the Run:ai user interface. The Application is created with specific roles and contains a secret . Using the secret you can obtain a token and use it within subsequent API calls.","title":"API Authentication"},{"location":"developer/rest-auth/#create-a-client-application","text":"Open the Run:ai Run:ai User Interface. Go to Settings | Application and create a new Application. Set the required roles: Select Researcher to manipulate Jobs using the Cluster API . To provide access to a specific project, you will also need to go to Application | Projects and provide the Application with access to specific projects. Select Editor to manipulate Projects and Departments using the Administrator REST API . Select Administrator to manipulate Users , Tenant Settings and Clusters using the Administrator REST API . Copy the <APPLICATION-NAME> and <CLIENT-SECRET> to be used below Go to Settings | General , under Researcher Authentication copy <REALM> . Important Creating Client Application tokens is only available with SaaS installations where the tenant has been created post-January 2022 or any Self-hosted installation. If you are an administrator but do not see the Settings | Application area, please contact Run:ai customer support.","title":"Create a Client Application"},{"location":"developer/rest-auth/#request-an-api-token","text":"Use the above parameters to get a temporary token to access Run:ai as follows.","title":"Request an API Token"},{"location":"developer/rest-auth/#example-command-to-get-an-api-token","text":"Replace <COMPANY-URL> below with app.run.ai for SaaS installations (not <company>.run.ai ) or the Run:ai user interface URL for Self-hosted installations. cURL Python curl -X POST 'https://<COMPANY-URL>/auth/realms/<REALM>/protocol/openid-connect/token' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'grant_type=client_credentials' \\ --data-urlencode 'scope=openid' \\ --data-urlencode 'response_type=id_token' \\ --data-urlencode 'client_id=<APPLICATION-NAME>' \\ --data-urlencode 'client_secret=<CLIENT-SECRET>' import http.client conn = http . client . HTTPSConnection ( \"\" ) payload = \"grant_type=client_credentials&client_id=<APPLICATION-NAME>&client_secret=<CLIENT_SECRET>\" headers = { 'content-type' : \"application/x-www-form-urlencoded\" } conn . request ( \"POST\" , \"/<COMPANY-URL>/auth/realms/<REALM>/protocol/openid-connect/token\" , payload , headers ) res = conn . getresponse () data = res . read () print ( data . decode ( \"utf-8\" ))","title":"Example command to get an API token"},{"location":"developer/rest-auth/#response","text":"The API response will look as follows: API Response { \"access_token\" : \"...\" , // (1) \"expires_in\" : 36000 , .... \"token_type\" : \"bearer\" } Use the access_token as the Bearer token below. To call APIs, the application must pass the retrieved access_token as a Bearer token in the Authorization header of your HTTP request. To retrieve and manipulate Workloads, use the Cluster API . Researcher API works at the cluster level and you will have different endpoints for different clusters. To retrieve and manipulate other metadata objects, use the Administrator REST API . Administrator API works at the control-plane level and you have a single endpoint for all clusters.","title":"Response"},{"location":"developer/admin-rest-api/overview/","text":"Administrator REST API \u00b6 The purpose of the Administrator REST API is to provide an easy-to-use programming interface for administrative tasks. Endpoint URL for API \u00b6 The domain used for Administrator REST APIs is the same domain used to browse for the Run:ai User Interface. Either <company>.run.ai , or app.run.ai for older tenants or a custom URL used for Self-hosted installations. Authentication \u00b6 Create a Client Application to make API requests. Use the client application and secret, to obtain a time-bound bearer token ( <ACCESS-TOKEN> ). For details, see Calling REST APIs . Use the token for subsequent API calls. Example Usage \u00b6 For example, if you have an Administrator role, you can get a list of clusters by running: cURL Python curl 'https://<COMPANY-URL>/v1/k8s/clusters' \\ --header 'Accept: application/json' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Bearer <ACCESS-TOKEN>' import http.client conn = http . client . HTTPSConnection ( \"https://<COMPANY-URL>\" ) headers = { 'content-type' : \"application/json\" , 'authorization' : \"Bearer <ACCESS-TOKEN>\" } conn . request ( \"GET\" , \"/v1/k8s/clusters\" , headers = headers ) res = conn . getresponse () data = res . read () print ( data . decode ( \"utf-8\" )) (replace <ACCESS-TOKEN> with the bearer token from above). For an additional example, see the following code. It is an example of how to use the Run:ai Administrator REST API to create a User and a Project and set the User to the Project. Administrator API Documentation \u00b6 The Administrator API provides the developer interfaces for getting and manipulating the Run:ai metadata objects such as Projects, Departments, Clusters, and Users. Detailed API documentation can be found under https://app.run.ai/api/docs . The document uses the Open API specification to describe the API. You can test the API within the document after creating a token. Administrator API Documentation","title":"Administrator API"},{"location":"developer/admin-rest-api/overview/#administrator-rest-api","text":"The purpose of the Administrator REST API is to provide an easy-to-use programming interface for administrative tasks.","title":"Administrator REST API"},{"location":"developer/admin-rest-api/overview/#endpoint-url-for-api","text":"The domain used for Administrator REST APIs is the same domain used to browse for the Run:ai User Interface. Either <company>.run.ai , or app.run.ai for older tenants or a custom URL used for Self-hosted installations.","title":"Endpoint URL for API"},{"location":"developer/admin-rest-api/overview/#authentication","text":"Create a Client Application to make API requests. Use the client application and secret, to obtain a time-bound bearer token ( <ACCESS-TOKEN> ). For details, see Calling REST APIs . Use the token for subsequent API calls.","title":"Authentication"},{"location":"developer/admin-rest-api/overview/#example-usage","text":"For example, if you have an Administrator role, you can get a list of clusters by running: cURL Python curl 'https://<COMPANY-URL>/v1/k8s/clusters' \\ --header 'Accept: application/json' \\ --header 'Content-Type: application/json' \\ --header 'Authorization: Bearer <ACCESS-TOKEN>' import http.client conn = http . client . HTTPSConnection ( \"https://<COMPANY-URL>\" ) headers = { 'content-type' : \"application/json\" , 'authorization' : \"Bearer <ACCESS-TOKEN>\" } conn . request ( \"GET\" , \"/v1/k8s/clusters\" , headers = headers ) res = conn . getresponse () data = res . read () print ( data . decode ( \"utf-8\" )) (replace <ACCESS-TOKEN> with the bearer token from above). For an additional example, see the following code. It is an example of how to use the Run:ai Administrator REST API to create a User and a Project and set the User to the Project.","title":"Example Usage"},{"location":"developer/admin-rest-api/overview/#administrator-api-documentation","text":"The Administrator API provides the developer interfaces for getting and manipulating the Run:ai metadata objects such as Projects, Departments, Clusters, and Users. Detailed API documentation can be found under https://app.run.ai/api/docs . The document uses the Open API specification to describe the API. You can test the API within the document after creating a token. Administrator API Documentation","title":"Administrator API Documentation"},{"location":"developer/cluster-api/other-resources/","text":"Support for other Kubernetes Applications \u00b6 Introduction \u00b6 Kubernetes has several built-in resources that encapsulate running Pods . These are called Kubernetes Workloads and should not be confused with Run:ai Workloads . Examples of such resources are a Deployment that manages a stateless application, or a Job that runs tasks to completion. Run:ai natively runs Run:ai Workloads . A Run:ai workload encapsulates all the resources needed to run, creates them, and deletes them together. However, Run:ai, being an open platform allows the scheduling of any Kubernetes Workflow. How To \u00b6 To run Kubernetes Workloads with Run:ai you must add the following to the YAML: A namespace that is associated with a Run:ai Project. A scheduler name: runai-scheduler . When using Fractions, use a specific syntax for the nvidia/gpu limit. Example: Job \u00b6 job1.yaml apiVersion : batch/v1 kind : Job # (1) metadata : name : job1 namespace : runai-team-a # (2) spec : template : spec : containers : - name : job1-container image : gcr.io/run-ai-demo/quickstart resources : limits : nvidia.com/gpu : 1 # (4) restartPolicy : Never schedulerName : runai-scheduler # (3) This is a Kubernetes Job . Namespace: Replace runai-team-a with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). The job to be scheduled with the Run:ai scheduler. To run with half a GPU replace 1 with \"0.5\" (with apostrophes). To submit the Job run: kubectl apply -f job1.yaml You will be able to see the Job in the Run:ai User interface, including all metrics and lists Example: Deployment \u00b6 deployment1.yaml apiVersion : apps/v1 kind : Deployment # (1) metadata : name : inference-1 namespace : runai-team-a # (2) spec : replicas : 1 selector : matchLabels : app : inference-1 template : metadata : labels : app : inference-1 spec : containers : - resources : limits : nvidia.com/gpu : 1 # (4) image : runai/example-marian-server imagePullPolicy : Always name : inference-1 ports : - containerPort : 8888 schedulerName : runai-scheduler # (3) --- apiVersion : v1 kind : Service # (5) metadata : labels : app : inference-1 name : inference-1 spec : type : ClusterIP ports : - port : 8888 targetPort : 8888 selector : app : inference-1 This is a Kubernetes Deployment . Namespace: Replace runai-team-a with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). The job to be scheduled with the Run:ai scheduler. To run with half a GPU replace 1 with \"0.5\" (with apostrophes). This example also contains the creation of a service to connect to the deployment. It is not mandatory. To submit the Deployment run: kubectl apply -f deployment1.yaml Limitations \u00b6 The Run:ai command line interface provides limited support for Kubernetes Workloads. See Also \u00b6 Run:ai has specific integrations with additional third-party tools such as KubeFlow , MLFlow , and more. These integrations use the same instructions as described above.","title":"Kubernetes Workloads"},{"location":"developer/cluster-api/other-resources/#support-for-other-kubernetes-applications","text":"","title":"Support for other Kubernetes Applications"},{"location":"developer/cluster-api/other-resources/#introduction","text":"Kubernetes has several built-in resources that encapsulate running Pods . These are called Kubernetes Workloads and should not be confused with Run:ai Workloads . Examples of such resources are a Deployment that manages a stateless application, or a Job that runs tasks to completion. Run:ai natively runs Run:ai Workloads . A Run:ai workload encapsulates all the resources needed to run, creates them, and deletes them together. However, Run:ai, being an open platform allows the scheduling of any Kubernetes Workflow.","title":"Introduction"},{"location":"developer/cluster-api/other-resources/#how-to","text":"To run Kubernetes Workloads with Run:ai you must add the following to the YAML: A namespace that is associated with a Run:ai Project. A scheduler name: runai-scheduler . When using Fractions, use a specific syntax for the nvidia/gpu limit.","title":"How To"},{"location":"developer/cluster-api/other-resources/#example-job","text":"job1.yaml apiVersion : batch/v1 kind : Job # (1) metadata : name : job1 namespace : runai-team-a # (2) spec : template : spec : containers : - name : job1-container image : gcr.io/run-ai-demo/quickstart resources : limits : nvidia.com/gpu : 1 # (4) restartPolicy : Never schedulerName : runai-scheduler # (3) This is a Kubernetes Job . Namespace: Replace runai-team-a with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). The job to be scheduled with the Run:ai scheduler. To run with half a GPU replace 1 with \"0.5\" (with apostrophes). To submit the Job run: kubectl apply -f job1.yaml You will be able to see the Job in the Run:ai User interface, including all metrics and lists","title":"Example: Job"},{"location":"developer/cluster-api/other-resources/#example-deployment","text":"deployment1.yaml apiVersion : apps/v1 kind : Deployment # (1) metadata : name : inference-1 namespace : runai-team-a # (2) spec : replicas : 1 selector : matchLabels : app : inference-1 template : metadata : labels : app : inference-1 spec : containers : - resources : limits : nvidia.com/gpu : 1 # (4) image : runai/example-marian-server imagePullPolicy : Always name : inference-1 ports : - containerPort : 8888 schedulerName : runai-scheduler # (3) --- apiVersion : v1 kind : Service # (5) metadata : labels : app : inference-1 name : inference-1 spec : type : ClusterIP ports : - port : 8888 targetPort : 8888 selector : app : inference-1 This is a Kubernetes Deployment . Namespace: Replace runai-team-a with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). The job to be scheduled with the Run:ai scheduler. To run with half a GPU replace 1 with \"0.5\" (with apostrophes). This example also contains the creation of a service to connect to the deployment. It is not mandatory. To submit the Deployment run: kubectl apply -f deployment1.yaml","title":"Example: Deployment"},{"location":"developer/cluster-api/other-resources/#limitations","text":"The Run:ai command line interface provides limited support for Kubernetes Workloads.","title":"Limitations"},{"location":"developer/cluster-api/other-resources/#see-also","text":"Run:ai has specific integrations with additional third-party tools such as KubeFlow , MLFlow , and more. These integrations use the same instructions as described above.","title":"See Also"},{"location":"developer/cluster-api/submit-rest/","text":"Submitting Workloads via HTTP/REST \u00b6 You can submit Workloads via HTTP calls, using the Kubernetes REST API. Submit Workload Example \u00b6 To submit a workload via HTTP, run the following: curl -X POST \\ # (1) 'https://<IP>:6443/apis/run.ai/v2alpha1/namespaces/<PROJECT>/trainingworkloads' \\ --header 'Content-Type: application/yaml' \\ --header 'Authorization: Bearer <BEARER>' \\ # (2) --data-raw ' apiVersion: run.ai/v2alpha1 kind: TrainingWorkload # (3) metadata: name: job-1 spec: gpu: value: \"1\" image: value: gcr.io/run-ai-demo/quickstart name: value: job-1 Replace <IP> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile). Replace <PROJECT> with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). Replace trainingworkloads with interactiveworkloads or inferenceworkloads according to type. Add Bearer token. To obtain a Bearer token see API authentication . See Submitting a Workload via YAML for an explanation of the YAML-based workload. Run: runai list jobs to see the new Workload. Delete Workload Example \u00b6 To delete a workload run: curl -X DELETE \\ # (1) 'https://<IP>:6443/apis/run.ai/v2alpha1/namespaces/<PROJECT>/trainingworkloads/<JOB-NAME>' \\ --header 'Content-Type: application/yaml' \\ --header 'Authorization: Bearer <BEARER>' # (2) Replace <IP> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile). Replace <PROJECT> with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). Replace trainingworkloads with interactiveworkloads or inferenceworkloads according to type. Replace <JOB-NAME> with the name of the Job. Add Bearer token. To obtain a Bearer token see API authentication . Using other Programming Languages \u00b6 You can use any Kubernetes client library together with the YAML documentation above to submit workloads via other programming languages. For more information see Kubernetes client libraries . Python example \u00b6 Create the following file and run it via python: create-train.py import json import requests # (1) url = \"https://<IP>:6443/apis/run.ai/v2alpha1/namespaces/<PROJECT>/trainingworkloads\" payload = json . dumps ({ \"apiVersion\" : \"run.ai/v2alpha1\" , \"kind\" : \"TrainingWorkload\" , \"metadata\" : { \"name\" : \"train1\" , \"namespace\" : \"runai-team-a\" }, \"spec\" : { \"image\" : { \"value\" : \"gcr.io/run-ai-demo/quickstart\" }, \"name\" : { \"value\" : \"train1\" }, \"gpu\" : { \"value\" : \"1\" } } }) headers = { 'Content-Type' : 'application/json' , 'Authorization' : 'Bearer <TOKEN>' #(2) } response = requests . request ( \"POST\" , url , headers = headers , data = payload ) # (3) print ( json . dumps ( json . loads ( response . text ), indent = 4 )) Replace <IP> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile). Replace <PROJECT> with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). Replace trainingworkloads with interactiveworkloads or inferenceworkloads according to type. Add Bearer token. To obtain a Bearer token see API authentication . if you do not have a valid certificate, you can add the flag verify=False .","title":"Submit Workload via HTTP/REST"},{"location":"developer/cluster-api/submit-rest/#submitting-workloads-via-httprest","text":"You can submit Workloads via HTTP calls, using the Kubernetes REST API.","title":"Submitting Workloads via HTTP/REST"},{"location":"developer/cluster-api/submit-rest/#submit-workload-example","text":"To submit a workload via HTTP, run the following: curl -X POST \\ # (1) 'https://<IP>:6443/apis/run.ai/v2alpha1/namespaces/<PROJECT>/trainingworkloads' \\ --header 'Content-Type: application/yaml' \\ --header 'Authorization: Bearer <BEARER>' \\ # (2) --data-raw ' apiVersion: run.ai/v2alpha1 kind: TrainingWorkload # (3) metadata: name: job-1 spec: gpu: value: \"1\" image: value: gcr.io/run-ai-demo/quickstart name: value: job-1 Replace <IP> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile). Replace <PROJECT> with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). Replace trainingworkloads with interactiveworkloads or inferenceworkloads according to type. Add Bearer token. To obtain a Bearer token see API authentication . See Submitting a Workload via YAML for an explanation of the YAML-based workload. Run: runai list jobs to see the new Workload.","title":"Submit Workload Example"},{"location":"developer/cluster-api/submit-rest/#delete-workload-example","text":"To delete a workload run: curl -X DELETE \\ # (1) 'https://<IP>:6443/apis/run.ai/v2alpha1/namespaces/<PROJECT>/trainingworkloads/<JOB-NAME>' \\ --header 'Content-Type: application/yaml' \\ --header 'Authorization: Bearer <BEARER>' # (2) Replace <IP> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile). Replace <PROJECT> with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). Replace trainingworkloads with interactiveworkloads or inferenceworkloads according to type. Replace <JOB-NAME> with the name of the Job. Add Bearer token. To obtain a Bearer token see API authentication .","title":"Delete Workload Example"},{"location":"developer/cluster-api/submit-rest/#using-other-programming-languages","text":"You can use any Kubernetes client library together with the YAML documentation above to submit workloads via other programming languages. For more information see Kubernetes client libraries .","title":"Using other Programming Languages"},{"location":"developer/cluster-api/submit-rest/#python-example","text":"Create the following file and run it via python: create-train.py import json import requests # (1) url = \"https://<IP>:6443/apis/run.ai/v2alpha1/namespaces/<PROJECT>/trainingworkloads\" payload = json . dumps ({ \"apiVersion\" : \"run.ai/v2alpha1\" , \"kind\" : \"TrainingWorkload\" , \"metadata\" : { \"name\" : \"train1\" , \"namespace\" : \"runai-team-a\" }, \"spec\" : { \"image\" : { \"value\" : \"gcr.io/run-ai-demo/quickstart\" }, \"name\" : { \"value\" : \"train1\" }, \"gpu\" : { \"value\" : \"1\" } } }) headers = { 'Content-Type' : 'application/json' , 'Authorization' : 'Bearer <TOKEN>' #(2) } response = requests . request ( \"POST\" , url , headers = headers , data = payload ) # (3) print ( json . dumps ( json . loads ( response . text ), indent = 4 )) Replace <IP> with the Kubernetes control-plane endpoint (can be found in kubeconfig profile). Replace <PROJECT> with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). Replace trainingworkloads with interactiveworkloads or inferenceworkloads according to type. Add Bearer token. To obtain a Bearer token see API authentication . if you do not have a valid certificate, you can add the flag verify=False .","title":"Python example"},{"location":"developer/cluster-api/submit-yaml/","text":"Submitting Workloads via YAML \u00b6 You can use YAML to submit Workloads directly to Run:ai. Below are examples of how to create training, interactive and inference workloads via YAML. Submit Workload Example \u00b6 Create a file named training1.yaml with the following text: training1.yaml apiVersion : run.ai/v2alpha1 kind : TrainingWorkload # (1) metadata : name : job-1 # (2) namespace : runai-team-a # (3) spec : gpu : value : \"1\" image : value : gcr.io/run-ai-demo/quickstart name : value : job-1 # (4) This is a Training workload. Kubernetes object name. Mandatory, but has no functional significance. Namespace: Replace runai-team-a with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). Job name as appears in Run:ai. Can provide name, or create automatically if name prefix is configured. Change the namespace and run: kubectl apply -f training1.yaml Run: runai list jobs to see the new Workload. Delete Workload Example \u00b6 Run: kubectl delete -f training1.yaml to delete the Workload. Creating a YAML syntax from a CLI command \u00b6 An easy way to create a YAML for a workload is to generate it from the runai submit command by using the --dry-run flag. For example, run: runai submit build1 -i ubuntu -g 1 --interactive --dry-run \\ -- sleep infinity The result will be the following Kubernetes object declaration: apiVersion : run.ai/v2alpha1 kind : InteractiveWorkload # (1) metadata : creationTimestamp : null labels : PreviousJob : \"true\" name : job-0-2022-05-02t08-50-57 namespace : runai-team-a spec : command : value : sleep infinity gpu : value : \"1\" image : value : ubuntu imagePullPolicy : value : Always name : value : job-0 ... Additional internal and status properties... This is an Interactive workload. Inference Workload Example \u00b6 Creating an inference workload is similar to the above two examples. apiVersion : run.ai/v2alpha1 kind : InferenceWorkload metadata : name : inference1 namespace : runai-team-a spec : name : value : inference1 gpu : value : \"0.5\" image : value : \"gcr.io/run-ai-demo/example-triton-server\" minScale : value : 1 maxScale : value : 2 metric : value : concurrency # (1) target : value : 80 # (2) ports : items : port1 : value : container : 8000 Possible metrics can be cpu-utilization , latency , throughput , concurrency , gpu-utilization , custom . Different metrics may require additional installations at the cluster level. Inference requires a port to receive requests. See Also \u00b6 To understand how to connect to the inference workload, see Inference Quickstart . To learn more about Inference and Run:ai see Inference overview .","title":"Submit Workload via YAML"},{"location":"developer/cluster-api/submit-yaml/#submitting-workloads-via-yaml","text":"You can use YAML to submit Workloads directly to Run:ai. Below are examples of how to create training, interactive and inference workloads via YAML.","title":"Submitting Workloads via YAML"},{"location":"developer/cluster-api/submit-yaml/#submit-workload-example","text":"Create a file named training1.yaml with the following text: training1.yaml apiVersion : run.ai/v2alpha1 kind : TrainingWorkload # (1) metadata : name : job-1 # (2) namespace : runai-team-a # (3) spec : gpu : value : \"1\" image : value : gcr.io/run-ai-demo/quickstart name : value : job-1 # (4) This is a Training workload. Kubernetes object name. Mandatory, but has no functional significance. Namespace: Replace runai-team-a with the name of the Run:ai namespace for the specific Project (typically runai-<Project-Name> ). Job name as appears in Run:ai. Can provide name, or create automatically if name prefix is configured. Change the namespace and run: kubectl apply -f training1.yaml Run: runai list jobs to see the new Workload.","title":"Submit Workload Example"},{"location":"developer/cluster-api/submit-yaml/#delete-workload-example","text":"Run: kubectl delete -f training1.yaml to delete the Workload.","title":"Delete Workload Example"},{"location":"developer/cluster-api/submit-yaml/#creating-a-yaml-syntax-from-a-cli-command","text":"An easy way to create a YAML for a workload is to generate it from the runai submit command by using the --dry-run flag. For example, run: runai submit build1 -i ubuntu -g 1 --interactive --dry-run \\ -- sleep infinity The result will be the following Kubernetes object declaration: apiVersion : run.ai/v2alpha1 kind : InteractiveWorkload # (1) metadata : creationTimestamp : null labels : PreviousJob : \"true\" name : job-0-2022-05-02t08-50-57 namespace : runai-team-a spec : command : value : sleep infinity gpu : value : \"1\" image : value : ubuntu imagePullPolicy : value : Always name : value : job-0 ... Additional internal and status properties... This is an Interactive workload.","title":"Creating a YAML syntax from a CLI command"},{"location":"developer/cluster-api/submit-yaml/#inference-workload-example","text":"Creating an inference workload is similar to the above two examples. apiVersion : run.ai/v2alpha1 kind : InferenceWorkload metadata : name : inference1 namespace : runai-team-a spec : name : value : inference1 gpu : value : \"0.5\" image : value : \"gcr.io/run-ai-demo/example-triton-server\" minScale : value : 1 maxScale : value : 2 metric : value : concurrency # (1) target : value : 80 # (2) ports : items : port1 : value : container : 8000 Possible metrics can be cpu-utilization , latency , throughput , concurrency , gpu-utilization , custom . Different metrics may require additional installations at the cluster level. Inference requires a port to receive requests.","title":"Inference Workload Example"},{"location":"developer/cluster-api/submit-yaml/#see-also","text":"To understand how to connect to the inference workload, see Inference Quickstart . To learn more about Inference and Run:ai see Inference overview .","title":"See Also"},{"location":"developer/cluster-api/workload-overview-dev/","text":"Workloads Overview \u00b6 Workloads \u00b6 Run:ai schedules Workloads . Run:ai workloads contain: The Kubernetes resource (Job, Deployment, etc) that is used to launch the container inside which the data science code runs. A set of additional resources that is required to run the Workload. Examples: a service entry point that allows access to the Job, a persistent volume claim to access data on the network and more. Run:ai supports the following Workloads types: Workload Type Kubernetes Name Description Interactive InteractiveWorkload Submit an interactive workload Training TrainingWorkload Submit a training workload Inference InferenceWorkload Submit an inference workload Values \u00b6 A Workload will typically have a list of values , such as name, image, and resources. A full list of values is available in the runai-submit Command-line reference. You can also find the exact YAML syntax run: kubectl explain TrainingWorkload.spec (and similarly for other Workload types). To get information on a specific value (e.g. node type ), you can also run: kubectl explain TrainingWorkload.spec.nodeType Result: KIND: TrainingWorkload VERSION: run.ai/v2alpha1 RESOURCE: nodeType <Object> DESCRIPTION: Specifies nodes (machines) or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information consult the Projects guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. FIELDS: value <string> How to Submit \u00b6 A Workload can be submitted via various channels: The Run:ai user interface . The Run:ai command-line interface, via the runai submit command. The Run:ai Cluster API . Policies \u00b6 An Administrator can set Policies for Workload submission. Policies serve two purposes: To constrain the values a researcher can specify. To provide default values. For example, an administrator can, Set a maximum of 5 GPUs per Workload. Provide a default value of 1 GPU for each container. Each workload type has a matching kind of workload policy. For example, an InteractiveWorkload has a matching InteractivePolicy A Policy of each type can be defined per-project . There is also a global policy that applies to any project that does not have a per-project policy. For further details on policies, see Policies .","title":"Workloads Overview"},{"location":"developer/cluster-api/workload-overview-dev/#workloads-overview","text":"","title":"Workloads Overview"},{"location":"developer/cluster-api/workload-overview-dev/#workloads","text":"Run:ai schedules Workloads . Run:ai workloads contain: The Kubernetes resource (Job, Deployment, etc) that is used to launch the container inside which the data science code runs. A set of additional resources that is required to run the Workload. Examples: a service entry point that allows access to the Job, a persistent volume claim to access data on the network and more. Run:ai supports the following Workloads types: Workload Type Kubernetes Name Description Interactive InteractiveWorkload Submit an interactive workload Training TrainingWorkload Submit a training workload Inference InferenceWorkload Submit an inference workload","title":"Workloads"},{"location":"developer/cluster-api/workload-overview-dev/#values","text":"A Workload will typically have a list of values , such as name, image, and resources. A full list of values is available in the runai-submit Command-line reference. You can also find the exact YAML syntax run: kubectl explain TrainingWorkload.spec (and similarly for other Workload types). To get information on a specific value (e.g. node type ), you can also run: kubectl explain TrainingWorkload.spec.nodeType Result: KIND: TrainingWorkload VERSION: run.ai/v2alpha1 RESOURCE: nodeType <Object> DESCRIPTION: Specifies nodes (machines) or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information consult the Projects guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. FIELDS: value <string>","title":"Values"},{"location":"developer/cluster-api/workload-overview-dev/#how-to-submit","text":"A Workload can be submitted via various channels: The Run:ai user interface . The Run:ai command-line interface, via the runai submit command. The Run:ai Cluster API .","title":"How to Submit"},{"location":"developer/cluster-api/workload-overview-dev/#policies","text":"An Administrator can set Policies for Workload submission. Policies serve two purposes: To constrain the values a researcher can specify. To provide default values. For example, an administrator can, Set a maximum of 5 GPUs per Workload. Provide a default value of 1 GPU for each container. Each workload type has a matching kind of workload policy. For example, an InteractiveWorkload has a matching InteractivePolicy A Policy of each type can be defined per-project . There is also a global policy that applies to any project that does not have a per-project policy. For further details on policies, see Policies .","title":"Policies"},{"location":"developer/cluster-api/reference/inference/","text":"Inference Workload Parameters \u00b6 Following is a full list of all inference workload parameters. The text below is equivalent to running kubectl explain inferenceworkload.spec . You can also run kubectl explain inferenceworkload.spec.<parameter-name> to see the description of a specific parameter. KIND : InferenceWorkload VERSION : run.ai/v2alpha1 RESOURCE : spec <Object> DESCRIPTION : The specifications of this workload FIELDS : annotations <Object> Specifies annotations to be set in the container that is running the created workload. arguments <Object> When set,contains the arguments sent along with the command. These override the entry point of the image in the created workload. baseWorkload <string> Reference to an another workload. When set, this workload inherits its values from the base workload. Base workload can either reside on the same namespace of this workload (referred to as \"user\" template) or can reside in the runai namespace (referred to as a \"global\" template) capabilities <Object> The capabilities field allows adding a set of unix capabilities to the container running the workload. Linux capabilities are distinct privileges traditionally associated with superuser which can be independently enabled and disabled. For more information see https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container class <Object> The autoscaler class for knative to use command <Object> If set, overrides the image's entry point with the supplied command. cpu <Object> Specifies CPU units to allocate for the created workload (0.5, 1, .etc). The workload will receive at least this amount of CPU. Note that the workload will not be scheduled unless the system can guarantee this amount of CPUs to the workload. cpuLimit <Object> Specifies a limit on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload will not be able to consume more than this amount of CPUs. createHomeDir <Object> Instructs the system to create a temporary home directory for the user within the container. Data stored in this directory will not be saved when the container exits. When the runAsUser flag is set to true, this flag will default to true as well. environment <Object> Specifies environment variables to be set in the container running the created workload. exposedUrls <Object> Specifies a set of exported url (e.g. ingress) from the container running the created workload. extendedResources <Object> Specifies values for extended resources. Extended resources are third-party devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that you want to allocate to your Job. For more information see : https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/ gitSync <Object> Specifies git repositories to mount into the container running the workload. gpu <Object> Specifies the number on the number of GPUs to allocate for the created workload. The default is no allocated GPUs. The GPU value can be an integer or a fraction between 0 and 1. gpuLimit <Object> Specifies a limit on the GPUs to allocate for this workload (1G, 20M, .etc). Intended to use for Opportunistic jobs (with the smart node-scheduler). gpuMemory <Object> Specifies GPU memory to allocate for the created workload. The workload will receive this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of GPU memory to the workload. hostIpc <Object> Specifies that the created workload will use the host's ipc namespace. hostNetwork <Object> Specifies that the created workload will use the host's network stack inside its container. For more information see the Docker Run Reference at https://docs.docker.com/engine/reference/run/ image <Object> Specifies the image to use when creating the container running the workload. imagePullPolicy <Object> Specifies the pull policy of the image when starting a container running the created workload. Options are : always, ifNotPresent, or never. For more information see : https://kubernetes.io/docs/concepts/containers/images ingressUrl <Object> This field is for internal use only. isPrivateServiceUrl <Object> Configure the inference service to be available only on the cluster-local network, and not on the public internet labels <Object> Specifies labels to be set in the container running the created workload. largeShm <Object> Specifies a large /dev/shm device to mount into a container running the created workload. An shm is a shared file system mounted on RAM. maxScale <Object> The maximum number of replicas to run memory <Object> Specifies the amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload will receive at least this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of memory to the workload memoryLimit <Object> Specifies a limit on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload will not be able to consume more than this amount of memory. The workload will receive an error when trying to allocate more memory than this limit. metric <Object> The predefined metric to use for autoscaling. Possible values are : cpu-utilization, latency, throughput, concurrency, gpu-utilization, custom. metricName <Object> The exact metric name to use for autoscaling (overrides Metric field) migProfile <Object> Specifies the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology. minScale <Object> The minimum number of replicas to run mountPropagation <Object> Allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. The volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories. name <Object> The specific name of the created resource. Either name of namePrefix should be provided, but not both. namePrefix <Object> A prefix used for assigning a name to the created resource. Either name of namePrefix should be provided, but not both. nodePool <Object> Specifies a group of nodes (machines) on which the workload will run. To use this feature, your Administrator will need to label nodes and create a node-pool, as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. Administrator can assign quota for each node-pool in any of the Projects and Departments that the system contains. This flag can optionally be used in conjunction with NodeType and Project-based affinity. In this case, the combination of both flags is used to refine the list of allowable nodes from a node-pool which the workload can use to run. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. nodeType <Object> Specifies nodes (machines) or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. ports <Object> Specifies a set of ports exposed from the container running the created workload. Used together with --service-type. pvcs <Object> Specifies persistent volume claims to mount into a container running the created workload. runAsGid <Object> Specifies the Unix group id with which the container should run. Will be used only if runAsUser is set to true. runAsUid <Object> Specifies the Unix user id with which the container running the created workload should run. Will be used only if runAsUser is set to true. runAsUser <Object> Limits the container running the created workload to run in the context of a specific non-root user. The user id is provided by the runAsUid field. This would manifest itself in access to operating system resources, in the ownership of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see the User Identity guide at https://docs.run.ai/admin/runai-setup/config/non-root-containers/ s3 <Object> Specifies S3 buckets to mount into the container running the workload serviceType <Object> Specifies the default service exposure method for ports. The default shall be used for ports which do not specify service type. Options are : LoadBalancer, NodePort or ClusterIP. For more information see the External Access to Containers guide on https://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/ stdin <Object> Instructs the system to keep stdin open for the container(s) running the created workload, even if nothing is attached. supplementalGroups <Object> ';' separated list of supplemental group IDs. Will be added to the security context of the container running the created workload. target <Object> The target value for the autoscaling metric tolerations <Object> Toleration rules which apply to the pods running the workload. Toleration rules guide (but do not require) the system to which node each pod can be scheduled to or evicted from, based on matching between those rules and the set of taints defined for each Kubernetes node. tty <Object> Instructs the system to allocate a pseudo-TTY for the created workload. usage <string> The intended usage of this workload. possible values are \"Template\" : this workload is used as the base for other workloads. \"Submit\" : this workload is used for submitting a job and/or other Kubernetes resources. username <Object> Display-only field describing the user who owns the workload. The data is not used for authentication or authorization purposes. volumes <Object> Specifies volumes to mount into a container running the created workload. workingDir <Object> Specifies a directory that will be used as the current directory when the container running the created workload starts.","title":"Inference Workloads"},{"location":"developer/cluster-api/reference/inference/#inference-workload-parameters","text":"Following is a full list of all inference workload parameters. The text below is equivalent to running kubectl explain inferenceworkload.spec . You can also run kubectl explain inferenceworkload.spec.<parameter-name> to see the description of a specific parameter. KIND : InferenceWorkload VERSION : run.ai/v2alpha1 RESOURCE : spec <Object> DESCRIPTION : The specifications of this workload FIELDS : annotations <Object> Specifies annotations to be set in the container that is running the created workload. arguments <Object> When set,contains the arguments sent along with the command. These override the entry point of the image in the created workload. baseWorkload <string> Reference to an another workload. When set, this workload inherits its values from the base workload. Base workload can either reside on the same namespace of this workload (referred to as \"user\" template) or can reside in the runai namespace (referred to as a \"global\" template) capabilities <Object> The capabilities field allows adding a set of unix capabilities to the container running the workload. Linux capabilities are distinct privileges traditionally associated with superuser which can be independently enabled and disabled. For more information see https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container class <Object> The autoscaler class for knative to use command <Object> If set, overrides the image's entry point with the supplied command. cpu <Object> Specifies CPU units to allocate for the created workload (0.5, 1, .etc). The workload will receive at least this amount of CPU. Note that the workload will not be scheduled unless the system can guarantee this amount of CPUs to the workload. cpuLimit <Object> Specifies a limit on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload will not be able to consume more than this amount of CPUs. createHomeDir <Object> Instructs the system to create a temporary home directory for the user within the container. Data stored in this directory will not be saved when the container exits. When the runAsUser flag is set to true, this flag will default to true as well. environment <Object> Specifies environment variables to be set in the container running the created workload. exposedUrls <Object> Specifies a set of exported url (e.g. ingress) from the container running the created workload. extendedResources <Object> Specifies values for extended resources. Extended resources are third-party devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that you want to allocate to your Job. For more information see : https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/ gitSync <Object> Specifies git repositories to mount into the container running the workload. gpu <Object> Specifies the number on the number of GPUs to allocate for the created workload. The default is no allocated GPUs. The GPU value can be an integer or a fraction between 0 and 1. gpuLimit <Object> Specifies a limit on the GPUs to allocate for this workload (1G, 20M, .etc). Intended to use for Opportunistic jobs (with the smart node-scheduler). gpuMemory <Object> Specifies GPU memory to allocate for the created workload. The workload will receive this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of GPU memory to the workload. hostIpc <Object> Specifies that the created workload will use the host's ipc namespace. hostNetwork <Object> Specifies that the created workload will use the host's network stack inside its container. For more information see the Docker Run Reference at https://docs.docker.com/engine/reference/run/ image <Object> Specifies the image to use when creating the container running the workload. imagePullPolicy <Object> Specifies the pull policy of the image when starting a container running the created workload. Options are : always, ifNotPresent, or never. For more information see : https://kubernetes.io/docs/concepts/containers/images ingressUrl <Object> This field is for internal use only. isPrivateServiceUrl <Object> Configure the inference service to be available only on the cluster-local network, and not on the public internet labels <Object> Specifies labels to be set in the container running the created workload. largeShm <Object> Specifies a large /dev/shm device to mount into a container running the created workload. An shm is a shared file system mounted on RAM. maxScale <Object> The maximum number of replicas to run memory <Object> Specifies the amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload will receive at least this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of memory to the workload memoryLimit <Object> Specifies a limit on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload will not be able to consume more than this amount of memory. The workload will receive an error when trying to allocate more memory than this limit. metric <Object> The predefined metric to use for autoscaling. Possible values are : cpu-utilization, latency, throughput, concurrency, gpu-utilization, custom. metricName <Object> The exact metric name to use for autoscaling (overrides Metric field) migProfile <Object> Specifies the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology. minScale <Object> The minimum number of replicas to run mountPropagation <Object> Allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. The volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories. name <Object> The specific name of the created resource. Either name of namePrefix should be provided, but not both. namePrefix <Object> A prefix used for assigning a name to the created resource. Either name of namePrefix should be provided, but not both. nodePool <Object> Specifies a group of nodes (machines) on which the workload will run. To use this feature, your Administrator will need to label nodes and create a node-pool, as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. Administrator can assign quota for each node-pool in any of the Projects and Departments that the system contains. This flag can optionally be used in conjunction with NodeType and Project-based affinity. In this case, the combination of both flags is used to refine the list of allowable nodes from a node-pool which the workload can use to run. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. nodeType <Object> Specifies nodes (machines) or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. ports <Object> Specifies a set of ports exposed from the container running the created workload. Used together with --service-type. pvcs <Object> Specifies persistent volume claims to mount into a container running the created workload. runAsGid <Object> Specifies the Unix group id with which the container should run. Will be used only if runAsUser is set to true. runAsUid <Object> Specifies the Unix user id with which the container running the created workload should run. Will be used only if runAsUser is set to true. runAsUser <Object> Limits the container running the created workload to run in the context of a specific non-root user. The user id is provided by the runAsUid field. This would manifest itself in access to operating system resources, in the ownership of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see the User Identity guide at https://docs.run.ai/admin/runai-setup/config/non-root-containers/ s3 <Object> Specifies S3 buckets to mount into the container running the workload serviceType <Object> Specifies the default service exposure method for ports. The default shall be used for ports which do not specify service type. Options are : LoadBalancer, NodePort or ClusterIP. For more information see the External Access to Containers guide on https://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/ stdin <Object> Instructs the system to keep stdin open for the container(s) running the created workload, even if nothing is attached. supplementalGroups <Object> ';' separated list of supplemental group IDs. Will be added to the security context of the container running the created workload. target <Object> The target value for the autoscaling metric tolerations <Object> Toleration rules which apply to the pods running the workload. Toleration rules guide (but do not require) the system to which node each pod can be scheduled to or evicted from, based on matching between those rules and the set of taints defined for each Kubernetes node. tty <Object> Instructs the system to allocate a pseudo-TTY for the created workload. usage <string> The intended usage of this workload. possible values are \"Template\" : this workload is used as the base for other workloads. \"Submit\" : this workload is used for submitting a job and/or other Kubernetes resources. username <Object> Display-only field describing the user who owns the workload. The data is not used for authentication or authorization purposes. volumes <Object> Specifies volumes to mount into a container running the created workload. workingDir <Object> Specifies a directory that will be used as the current directory when the container running the created workload starts.","title":"Inference Workload Parameters"},{"location":"developer/cluster-api/reference/interactive/","text":"Interactive Workload Parameters \u00b6 Following is a full list of all interactive workload parameters. The text below is equivalent to running kubectl explain interactiveworkload.spec . You can also run kubectl explain interactiveworkload.spec.<parameter-name> to see the description of a specific parameter. KIND : InteractiveWorkload VERSION : run.ai/v2alpha1 RESOURCE : spec <Object> DESCRIPTION : The specifications of this InteractiveWorkload FIELDS : allowPrivilegeEscalation <Object> Allow the container running the workload and all launched processes to gain additional privileges after the workload starts. For more information see the \"User Identity in Container\" guide at https://docs.run.ai/admin/runai-setup/config/non-root-containers/ annotations <Object> Specifies annotations to be set in the container that is running the created workload. arguments <Object> When set,contains the arguments sent along with the command. These override the entry point of the image in the created workload. baseWorkload <string> Reference to an another workload. When set, this workload inherits its values from the base workload. Base workload can either reside on the same namespace of this workload (referred to as \"user\" template) or can reside in the runai namespace (referred to as a \"global\" template) capabilities <Object> The capabilities field allows adding a set of unix capabilities to the container running the workload. Linux capabilities are distinct privileges traditionally associated with superuser which can be independently enabled and disabled. For more information see https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container command <Object> If set, overrides the image's entry point with the supplied command. cpu <Object> Specifies CPU units to allocate for the created workload (0.5, 1, .etc). The workload will receive at least this amount of CPU. Note that the workload will not be scheduled unless the system can guarantee this amount of CPUs to the workload. cpuLimit <Object> Specifies a limit on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload will not be able to consume more than this amount of CPUs. createHomeDir <Object> Instructs the system to create a temporary home directory for the user within the container. Data stored in this directory will not be saved when the container exits. When the runAsUser flag is set to true, this flag will default to true as well. environment <Object> Specifies environment variables to be set in the container running the created workload. exposedUrls <Object> Specifies a set of exported url (e.g. ingress) from the container running the created workload. extendedResources <Object> Specifies values for extended resources. Extended resources are third-party devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that you want to allocate to your Job. For more information see : https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/ gitSync <Object> Specifies git repositories to mount into the container running the workload. gpu <Object> Specifies the number on the number of GPUs to allocate for the created workload. The default is no allocated GPUs. The GPU value can be an integer or a fraction between 0 and 1. gpuLimit <Object> Specifies a limit on the GPUs to allocate for this workload (1G, 20M, .etc). Intended to use for Opportunistic jobs (with the smart node-scheduler). gpuMemory <Object> Specifies GPU memory to allocate for the created workload. The workload will receive this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of GPU memory to the workload. hostIpc <Object> Specifies that the created workload will use the host's ipc namespace. hostNetwork <Object> Specifies that the created workload will use the host's network stack inside its container. For more information see the Docker Run Reference at https://docs.docker.com/engine/reference/run/ image <Object> Specifies the image to use when creating the container running the workload. imagePullPolicy <Object> Specifies the pull policy of the image when starting a container running the created workload. Options are : always, ifNotPresent, or never. For more information see : https://kubernetes.io/docs/concepts/containers/images ingressUrl <Object> This field is for internal use only. jupyter <Object> Indication if an interactive workload should run jupyter notebook labels <Object> Specifies labels to be set in the container running the created workload. largeShm <Object> Specifies a large /dev/shm device to mount into a container running the created workload. An shm is a shared file system mounted on RAM. memory <Object> Specifies the amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload will receive at least this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of memory to the workload memoryLimit <Object> Specifies a limit on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload will not be able to consume more than this amount of memory. The workload will receive an error when trying to allocate more memory than this limit. migProfile <Object> Specifies the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology. mountPropagation <Object> Allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. The volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories. mpi <Object> This workload produces mpijob name <Object> The specific name of the created resource. Either name of namePrefix should be provided, but not both. namePrefix <Object> A prefix used for assigning a name to the created resource. Either name of namePrefix should be provided, but not both. nodePool <Object> Specifies a group of nodes (machines) on which the workload will run. To use this feature, your Administrator will need to label nodes and create a node-pool, as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. Administrator can assign quota for each node-pool in any of the Projects and Departments that the system contains. This flag can optionally be used in conjunction with NodeType and Project-based affinity. In this case, the combination of both flags is used to refine the list of allowable nodes from a node-pool which the workload can use to run. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. nodeType <Object> Specifies nodes (machines) or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. notebookToken <Object> ports <Object> Specifies a set of ports exposed from the container running the created workload. Used together with --service-type. preemptible <Object> Specifies that the created workload will be preemptible. Interactive preemptible workloads can be scheduled above the guaranteed quota but may be reclaimed at any time. processes <Object> Number of distributed training processes that will be allocated for the created mpijob. pvcs <Object> Specifies persistent volume claims to mount into a container running the created workload. runAsGid <Object> Specifies the Unix group id with which the container should run. Will be used only if runAsUser is set to true. runAsUid <Object> Specifies the Unix user id with which the container running the created workload should run. Will be used only if runAsUser is set to true. runAsUser <Object> Limits the container running the created workload to run in the context of a specific non-root user. The user id is provided by the runAsUid field. This would manifest itself in access to operating system resources, in the ownership of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see the User Identity guide at https://docs.run.ai/admin/runai-setup/config/non-root-containers/ s3 <Object> Specifies S3 buckets to mount into the container running the workload serviceType <Object> Specifies the default service exposure method for ports. The default shall be used for ports which do not specify service type. Options are : LoadBalancer, NodePort or ClusterIP. For more information see the External Access to Containers guide on https://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/ slotsPerWorker <Object> Number of slots to allocate per worker in the created mpijob. stdin <Object> Instructs the system to keep stdin open for the container(s) running the created workload, even if nothing is attached. supplementalGroups <Object> ';' separated list of supplemental group IDs. Will be added to the security context of the container running the created workload. tensorboard <Object> Indicates that this interactive workload should also run a TensorBoard dashboard tensorboardLogdir <Object> The TensorBoard Logs directory tolerations <Object> Toleration rules which apply to the pods running the workload. Toleration rules guide (but do not require) the system to which node each pod can be scheduled to or evicted from, based on matching between those rules and the set of taints defined for each Kubernetes node. tty <Object> Instructs the system to allocate a pseudo-TTY for the created workload. usage <string> The intended usage of this workload. possible values are \"Template\" : this workload is used as the base for other workloads. \"Submit\" : this workload is used for submitting a job and/or other Kubernetes resources. username <Object> Display-only field describing the user who owns the workload. The data is not used for authentication or authorization purposes. volumes <Object> Specifies volumes to mount into a container running the created workload. workingDir <Object> Specifies a directory that will be used as the current directory when the container running the created workload starts.","title":"Interactive Workloads"},{"location":"developer/cluster-api/reference/interactive/#interactive-workload-parameters","text":"Following is a full list of all interactive workload parameters. The text below is equivalent to running kubectl explain interactiveworkload.spec . You can also run kubectl explain interactiveworkload.spec.<parameter-name> to see the description of a specific parameter. KIND : InteractiveWorkload VERSION : run.ai/v2alpha1 RESOURCE : spec <Object> DESCRIPTION : The specifications of this InteractiveWorkload FIELDS : allowPrivilegeEscalation <Object> Allow the container running the workload and all launched processes to gain additional privileges after the workload starts. For more information see the \"User Identity in Container\" guide at https://docs.run.ai/admin/runai-setup/config/non-root-containers/ annotations <Object> Specifies annotations to be set in the container that is running the created workload. arguments <Object> When set,contains the arguments sent along with the command. These override the entry point of the image in the created workload. baseWorkload <string> Reference to an another workload. When set, this workload inherits its values from the base workload. Base workload can either reside on the same namespace of this workload (referred to as \"user\" template) or can reside in the runai namespace (referred to as a \"global\" template) capabilities <Object> The capabilities field allows adding a set of unix capabilities to the container running the workload. Linux capabilities are distinct privileges traditionally associated with superuser which can be independently enabled and disabled. For more information see https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container command <Object> If set, overrides the image's entry point with the supplied command. cpu <Object> Specifies CPU units to allocate for the created workload (0.5, 1, .etc). The workload will receive at least this amount of CPU. Note that the workload will not be scheduled unless the system can guarantee this amount of CPUs to the workload. cpuLimit <Object> Specifies a limit on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload will not be able to consume more than this amount of CPUs. createHomeDir <Object> Instructs the system to create a temporary home directory for the user within the container. Data stored in this directory will not be saved when the container exits. When the runAsUser flag is set to true, this flag will default to true as well. environment <Object> Specifies environment variables to be set in the container running the created workload. exposedUrls <Object> Specifies a set of exported url (e.g. ingress) from the container running the created workload. extendedResources <Object> Specifies values for extended resources. Extended resources are third-party devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that you want to allocate to your Job. For more information see : https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/ gitSync <Object> Specifies git repositories to mount into the container running the workload. gpu <Object> Specifies the number on the number of GPUs to allocate for the created workload. The default is no allocated GPUs. The GPU value can be an integer or a fraction between 0 and 1. gpuLimit <Object> Specifies a limit on the GPUs to allocate for this workload (1G, 20M, .etc). Intended to use for Opportunistic jobs (with the smart node-scheduler). gpuMemory <Object> Specifies GPU memory to allocate for the created workload. The workload will receive this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of GPU memory to the workload. hostIpc <Object> Specifies that the created workload will use the host's ipc namespace. hostNetwork <Object> Specifies that the created workload will use the host's network stack inside its container. For more information see the Docker Run Reference at https://docs.docker.com/engine/reference/run/ image <Object> Specifies the image to use when creating the container running the workload. imagePullPolicy <Object> Specifies the pull policy of the image when starting a container running the created workload. Options are : always, ifNotPresent, or never. For more information see : https://kubernetes.io/docs/concepts/containers/images ingressUrl <Object> This field is for internal use only. jupyter <Object> Indication if an interactive workload should run jupyter notebook labels <Object> Specifies labels to be set in the container running the created workload. largeShm <Object> Specifies a large /dev/shm device to mount into a container running the created workload. An shm is a shared file system mounted on RAM. memory <Object> Specifies the amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload will receive at least this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of memory to the workload memoryLimit <Object> Specifies a limit on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload will not be able to consume more than this amount of memory. The workload will receive an error when trying to allocate more memory than this limit. migProfile <Object> Specifies the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology. mountPropagation <Object> Allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. The volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories. mpi <Object> This workload produces mpijob name <Object> The specific name of the created resource. Either name of namePrefix should be provided, but not both. namePrefix <Object> A prefix used for assigning a name to the created resource. Either name of namePrefix should be provided, but not both. nodePool <Object> Specifies a group of nodes (machines) on which the workload will run. To use this feature, your Administrator will need to label nodes and create a node-pool, as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. Administrator can assign quota for each node-pool in any of the Projects and Departments that the system contains. This flag can optionally be used in conjunction with NodeType and Project-based affinity. In this case, the combination of both flags is used to refine the list of allowable nodes from a node-pool which the workload can use to run. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. nodeType <Object> Specifies nodes (machines) or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. notebookToken <Object> ports <Object> Specifies a set of ports exposed from the container running the created workload. Used together with --service-type. preemptible <Object> Specifies that the created workload will be preemptible. Interactive preemptible workloads can be scheduled above the guaranteed quota but may be reclaimed at any time. processes <Object> Number of distributed training processes that will be allocated for the created mpijob. pvcs <Object> Specifies persistent volume claims to mount into a container running the created workload. runAsGid <Object> Specifies the Unix group id with which the container should run. Will be used only if runAsUser is set to true. runAsUid <Object> Specifies the Unix user id with which the container running the created workload should run. Will be used only if runAsUser is set to true. runAsUser <Object> Limits the container running the created workload to run in the context of a specific non-root user. The user id is provided by the runAsUid field. This would manifest itself in access to operating system resources, in the ownership of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see the User Identity guide at https://docs.run.ai/admin/runai-setup/config/non-root-containers/ s3 <Object> Specifies S3 buckets to mount into the container running the workload serviceType <Object> Specifies the default service exposure method for ports. The default shall be used for ports which do not specify service type. Options are : LoadBalancer, NodePort or ClusterIP. For more information see the External Access to Containers guide on https://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/ slotsPerWorker <Object> Number of slots to allocate per worker in the created mpijob. stdin <Object> Instructs the system to keep stdin open for the container(s) running the created workload, even if nothing is attached. supplementalGroups <Object> ';' separated list of supplemental group IDs. Will be added to the security context of the container running the created workload. tensorboard <Object> Indicates that this interactive workload should also run a TensorBoard dashboard tensorboardLogdir <Object> The TensorBoard Logs directory tolerations <Object> Toleration rules which apply to the pods running the workload. Toleration rules guide (but do not require) the system to which node each pod can be scheduled to or evicted from, based on matching between those rules and the set of taints defined for each Kubernetes node. tty <Object> Instructs the system to allocate a pseudo-TTY for the created workload. usage <string> The intended usage of this workload. possible values are \"Template\" : this workload is used as the base for other workloads. \"Submit\" : this workload is used for submitting a job and/or other Kubernetes resources. username <Object> Display-only field describing the user who owns the workload. The data is not used for authentication or authorization purposes. volumes <Object> Specifies volumes to mount into a container running the created workload. workingDir <Object> Specifies a directory that will be used as the current directory when the container running the created workload starts.","title":"Interactive Workload Parameters"},{"location":"developer/cluster-api/reference/training/","text":"Training Workload Parameters \u00b6 Following is a full list of all training workload parameters. The text below is equivalent to running kubectl explain trainingworkload.spec . You can also run kubectl explain trainingworkload.spec.<parameter-name> to see the description of a specific parameter. KIND : TrainingWorkload VERSION : run.ai/v2alpha1 RESOURCE : spec <Object> DESCRIPTION : The specifications of this TrainingWorkload FIELDS : allowPrivilegeEscalation <Object> Allow the container running the workload and all launched processes to gain additional privileges after the workload starts. For more information see the \"User Identity in Container\" guide at https://docs.run.ai/admin/runai-setup/config/non-root-containers/ annotations <Object> Specifies annotations to be set in the container that is running the created workload. arguments <Object> When set,contains the arguments sent along with the command. These override the entry point of the image in the created workload. backoffLimit <Object> Specifies the number of retries before marking a workload as failed. Defaults to 6 baseWorkload <string> Reference to an another workload. When set, this workload inherits its values from the base workload. Base workload can either reside on the same namespace of this workload (referred to as \"user\" template) or can reside in the runai namespace (referred to as a \"global\" template) capabilities <Object> The capabilities field allows adding a set of unix capabilities to the container running the workload. Linux capabilities are distinct privileges traditionally associated with superuser which can be independently enabled and disabled. For more information see https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container command <Object> If set, overrides the image's entry point with the supplied command. completions <Object> Used with Hyperparameter Optimization. Specifies the number of successful pods the job should reach to be completed. The Job will be marked as successful once the specified amount of pods has succeeded. The default value for 'completions' is 1. The 'parallelism' flag should be smaller or equal to 'completions' For more information see : https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ cpu <Object> Specifies CPU units to allocate for the created workload (0.5, 1, .etc). The workload will receive at least this amount of CPU. Note that the workload will not be scheduled unless the system can guarantee this amount of CPUs to the workload. cpuLimit <Object> Specifies a limit on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload will not be able to consume more than this amount of CPUs. createHomeDir <Object> Instructs the system to create a temporary home directory for the user within the container. Data stored in this directory will not be saved when the container exits. When the runAsUser flag is set to true, this flag will default to true as well. environment <Object> Specifies environment variables to be set in the container running the created workload. exposedUrls <Object> Specifies a set of exported url (e.g. ingress) from the container running the created workload. extendedResources <Object> Specifies values for extended resources. Extended resources are third-party devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that you want to allocate to your Job. For more information see : https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/ gitSync <Object> Specifies git repositories to mount into the container running the workload. gpu <Object> Specifies the number on the number of GPUs to allocate for the created workload. The default is no allocated GPUs. The GPU value can be an integer or a fraction between 0 and 1. gpuLimit <Object> Specifies a limit on the GPUs to allocate for this workload (1G, 20M, .etc). Intended to use for Opportunistic jobs (with the smart node-scheduler). gpuMemory <Object> Specifies GPU memory to allocate for the created workload. The workload will receive this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of GPU memory to the workload. hostIpc <Object> Specifies that the created workload will use the host's ipc namespace. hostNetwork <Object> Specifies that the created workload will use the host's network stack inside its container. For more information see the Docker Run Reference at https://docs.docker.com/engine/reference/run/ image <Object> Specifies the image to use when creating the container running the workload. imagePullPolicy <Object> Specifies the pull policy of the image when starting a container running the created workload. Options are : always, ifNotPresent, or never. For more information see : https://kubernetes.io/docs/concepts/containers/images ingressUrl <Object> This field is for internal use only. labels <Object> Specifies labels to be set in the container running the created workload. largeShm <Object> Specifies a large /dev/shm device to mount into a container running the created workload. An shm is a shared file system mounted on RAM. memory <Object> Specifies the amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload will receive at least this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of memory to the workload memoryLimit <Object> Specifies a limit on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload will not be able to consume more than this amount of memory. The workload will receive an error when trying to allocate more memory than this limit. migProfile <Object> Specifies the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology. mountPropagation <Object> Allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. The volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories. mpi <Object> This workload produces mpijob name <Object> The specific name of the created resource. Either name of namePrefix should be provided, but not both. namePrefix <Object> A prefix used for assigning a name to the created resource. Either name of namePrefix should be provided, but not both. nodePool <Object> Specifies a group of nodes (machines) on which the workload will run. To use this feature, your Administrator will need to label nodes and create a node-pool, as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. Administrator can assign quota for each node-pool in any of the Projects and Departments that the system contains. This flag can optionally be used in conjunction with NodeType and Project-based affinity. In this case, the combination of both flags is used to refine the list of allowable nodes from a node-pool which the workload can use to run. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. nodeType <Object> Specifies nodes (machines) or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. parallelism <Object> Specifies the maximum desired number of pods the workload should run at any given time. The actual number of pods running in a steady state will be less than this number when ((.spec.completions - .status.successful) < .spec.parallelism), i.e. when the work left to do is less than max parallelism. For more information, see : https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ ports <Object> Specifies a set of ports exposed from the container running the created workload. Used together with --service-type. processes <Object> Number of distributed training processes that will be allocated for the created mpijob. pvcs <Object> Specifies persistent volume claims to mount into a container running the created workload. runAsGid <Object> Specifies the Unix group id with which the container should run. Will be used only if runAsUser is set to true. runAsUid <Object> Specifies the Unix user id with which the container running the created workload should run. Will be used only if runAsUser is set to true. runAsUser <Object> Limits the container running the created workload to run in the context of a specific non-root user. The user id is provided by the runAsUid field. This would manifest itself in access to operating system resources, in the ownership of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see the User Identity guide at https://docs.run.ai/admin/runai-setup/config/non-root-containers/ s3 <Object> Specifies S3 buckets to mount into the container running the workload serviceType <Object> Specifies the default service exposure method for ports. The default shall be used for ports which do not specify service type. Options are : LoadBalancer, NodePort or ClusterIP. For more information see the External Access to Containers guide on https://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/ slotsPerWorker <Object> Number of slots to allocate per worker in the created mpijob. stdin <Object> Instructs the system to keep stdin open for the container(s) running the created workload, even if nothing is attached. supplementalGroups <Object> ';' separated list of supplemental group IDs. Will be added to the security context of the container running the created workload. tolerations <Object> Toleration rules which apply to the pods running the workload. Toleration rules guide (but do not require) the system to which node each pod can be scheduled to or evicted from, based on matching between those rules and the set of taints defined for each Kubernetes node. ttlAfterFinish <Object> Specifies the duration after which it is possible for a finished workload to be automatically deleted. When the workload is being deleted, its lifecycle guarantees (e.g. finalizers) will be honored. If this field is unset, the workload won't be automatically deleted. If this field is set to zero, the workload becomes eligible to be deleted immediately after it finishes. This field is alpha-level and is only honored by servers that enable the TTLAfterFinished feature. tty <Object> Instructs the system to allocate a pseudo-TTY for the created workload. usage <string> The intended usage of this workload. possible values are \"Template\" : this workload is used as the base for other workloads. \"Submit\" : this workload is used for submitting a job and/or other Kubernetes resources. username <Object> Display-only field describing the user who owns the workload. The data is not used for authentication or authorization purposes. volumes <Object> Specifies volumes to mount into a container running the created workload. workingDir <Object> Specifies a directory that will be used as the current directory when the container running the created workload starts.","title":"Training Workloads"},{"location":"developer/cluster-api/reference/training/#training-workload-parameters","text":"Following is a full list of all training workload parameters. The text below is equivalent to running kubectl explain trainingworkload.spec . You can also run kubectl explain trainingworkload.spec.<parameter-name> to see the description of a specific parameter. KIND : TrainingWorkload VERSION : run.ai/v2alpha1 RESOURCE : spec <Object> DESCRIPTION : The specifications of this TrainingWorkload FIELDS : allowPrivilegeEscalation <Object> Allow the container running the workload and all launched processes to gain additional privileges after the workload starts. For more information see the \"User Identity in Container\" guide at https://docs.run.ai/admin/runai-setup/config/non-root-containers/ annotations <Object> Specifies annotations to be set in the container that is running the created workload. arguments <Object> When set,contains the arguments sent along with the command. These override the entry point of the image in the created workload. backoffLimit <Object> Specifies the number of retries before marking a workload as failed. Defaults to 6 baseWorkload <string> Reference to an another workload. When set, this workload inherits its values from the base workload. Base workload can either reside on the same namespace of this workload (referred to as \"user\" template) or can reside in the runai namespace (referred to as a \"global\" template) capabilities <Object> The capabilities field allows adding a set of unix capabilities to the container running the workload. Linux capabilities are distinct privileges traditionally associated with superuser which can be independently enabled and disabled. For more information see https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container command <Object> If set, overrides the image's entry point with the supplied command. completions <Object> Used with Hyperparameter Optimization. Specifies the number of successful pods the job should reach to be completed. The Job will be marked as successful once the specified amount of pods has succeeded. The default value for 'completions' is 1. The 'parallelism' flag should be smaller or equal to 'completions' For more information see : https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ cpu <Object> Specifies CPU units to allocate for the created workload (0.5, 1, .etc). The workload will receive at least this amount of CPU. Note that the workload will not be scheduled unless the system can guarantee this amount of CPUs to the workload. cpuLimit <Object> Specifies a limit on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload will not be able to consume more than this amount of CPUs. createHomeDir <Object> Instructs the system to create a temporary home directory for the user within the container. Data stored in this directory will not be saved when the container exits. When the runAsUser flag is set to true, this flag will default to true as well. environment <Object> Specifies environment variables to be set in the container running the created workload. exposedUrls <Object> Specifies a set of exported url (e.g. ingress) from the container running the created workload. extendedResources <Object> Specifies values for extended resources. Extended resources are third-party devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that you want to allocate to your Job. For more information see : https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/ gitSync <Object> Specifies git repositories to mount into the container running the workload. gpu <Object> Specifies the number on the number of GPUs to allocate for the created workload. The default is no allocated GPUs. The GPU value can be an integer or a fraction between 0 and 1. gpuLimit <Object> Specifies a limit on the GPUs to allocate for this workload (1G, 20M, .etc). Intended to use for Opportunistic jobs (with the smart node-scheduler). gpuMemory <Object> Specifies GPU memory to allocate for the created workload. The workload will receive this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of GPU memory to the workload. hostIpc <Object> Specifies that the created workload will use the host's ipc namespace. hostNetwork <Object> Specifies that the created workload will use the host's network stack inside its container. For more information see the Docker Run Reference at https://docs.docker.com/engine/reference/run/ image <Object> Specifies the image to use when creating the container running the workload. imagePullPolicy <Object> Specifies the pull policy of the image when starting a container running the created workload. Options are : always, ifNotPresent, or never. For more information see : https://kubernetes.io/docs/concepts/containers/images ingressUrl <Object> This field is for internal use only. labels <Object> Specifies labels to be set in the container running the created workload. largeShm <Object> Specifies a large /dev/shm device to mount into a container running the created workload. An shm is a shared file system mounted on RAM. memory <Object> Specifies the amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload will receive at least this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of memory to the workload memoryLimit <Object> Specifies a limit on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload will not be able to consume more than this amount of memory. The workload will receive an error when trying to allocate more memory than this limit. migProfile <Object> Specifies the memory profile to be used for workload running on NVIDIA Multi-Instance GPU (MIG) technology. mountPropagation <Object> Allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node. The volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories. mpi <Object> This workload produces mpijob name <Object> The specific name of the created resource. Either name of namePrefix should be provided, but not both. namePrefix <Object> A prefix used for assigning a name to the created resource. Either name of namePrefix should be provided, but not both. nodePool <Object> Specifies a group of nodes (machines) on which the workload will run. To use this feature, your Administrator will need to label nodes and create a node-pool, as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. Administrator can assign quota for each node-pool in any of the Projects and Departments that the system contains. This flag can optionally be used in conjunction with NodeType and Project-based affinity. In this case, the combination of both flags is used to refine the list of allowable nodes from a node-pool which the workload can use to run. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. nodeType <Object> Specifies nodes (machines) or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes as explained in the Group Nodes guide at https://docs.run.ai/admin/researcher-setup/limit-to-node-group. This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see the Projects setup guide at https://docs.run.ai/admin/admin-ui-setup/project-setup. parallelism <Object> Specifies the maximum desired number of pods the workload should run at any given time. The actual number of pods running in a steady state will be less than this number when ((.spec.completions - .status.successful) < .spec.parallelism), i.e. when the work left to do is less than max parallelism. For more information, see : https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/ ports <Object> Specifies a set of ports exposed from the container running the created workload. Used together with --service-type. processes <Object> Number of distributed training processes that will be allocated for the created mpijob. pvcs <Object> Specifies persistent volume claims to mount into a container running the created workload. runAsGid <Object> Specifies the Unix group id with which the container should run. Will be used only if runAsUser is set to true. runAsUid <Object> Specifies the Unix user id with which the container running the created workload should run. Will be used only if runAsUser is set to true. runAsUser <Object> Limits the container running the created workload to run in the context of a specific non-root user. The user id is provided by the runAsUid field. This would manifest itself in access to operating system resources, in the ownership of new folders created under shared directories, etc. Alternatively, if your cluster is connected to Run:ai via SAML, you can map the container to use the Linux UID/GID which is stored in the organization's directory. For more information see the User Identity guide at https://docs.run.ai/admin/runai-setup/config/non-root-containers/ s3 <Object> Specifies S3 buckets to mount into the container running the workload serviceType <Object> Specifies the default service exposure method for ports. The default shall be used for ports which do not specify service type. Options are : LoadBalancer, NodePort or ClusterIP. For more information see the External Access to Containers guide on https://docs.run.ai/admin/runai-setup/config/allow-external-access-to-containers/ slotsPerWorker <Object> Number of slots to allocate per worker in the created mpijob. stdin <Object> Instructs the system to keep stdin open for the container(s) running the created workload, even if nothing is attached. supplementalGroups <Object> ';' separated list of supplemental group IDs. Will be added to the security context of the container running the created workload. tolerations <Object> Toleration rules which apply to the pods running the workload. Toleration rules guide (but do not require) the system to which node each pod can be scheduled to or evicted from, based on matching between those rules and the set of taints defined for each Kubernetes node. ttlAfterFinish <Object> Specifies the duration after which it is possible for a finished workload to be automatically deleted. When the workload is being deleted, its lifecycle guarantees (e.g. finalizers) will be honored. If this field is unset, the workload won't be automatically deleted. If this field is set to zero, the workload becomes eligible to be deleted immediately after it finishes. This field is alpha-level and is only honored by servers that enable the TTLAfterFinished feature. tty <Object> Instructs the system to allocate a pseudo-TTY for the created workload. usage <string> The intended usage of this workload. possible values are \"Template\" : this workload is used as the base for other workloads. \"Submit\" : this workload is used for submitting a job and/or other Kubernetes resources. username <Object> Display-only field describing the user who owns the workload. The data is not used for authentication or authorization purposes. volumes <Object> Specifies volumes to mount into a container running the created workload. workingDir <Object> Specifies a directory that will be used as the current directory when the container running the created workload starts.","title":"Training Workload Parameters"},{"location":"developer/deprecated/inference/overview/","text":"Warning Inference API is deprecated. See Cluster API for its replacement. To read more about Inference see the new Inference Overview . What is Inference \u00b6 Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output. With Inference, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time. Inference and GPUs \u00b6 The Inference process is a subset of the original Training algorithm on a single datum (e.g. one sentence or one image), or a small batch. As such, GPU memory requirements are typically smaller than a full-blown Training process. Given that, Inference lends itself nicely to the usage of Run:ai Fractions. You can, for example, run 4 instances of an Inference server on a single GPU, each employing a fourth of the memory. Inference @Run:ai \u00b6 Run:ai provides Inference services as an equal part together with the other two Workload types: Train and Build . Inference is considered a high-priority workload as it is customer-facing. Running an Inference workload (within the Project's quota) will preempt any Run:ai Workload marked as Training . Inference is implemented as a Kubernetes Deployment with a defined number of replicas. The replicas are load-balanced by Kubernetes so that adding more replicas will improve the overall throughput of the system. Multiple replicas will appear in Run:ai as a single Inference workload. The workload will appear in all Run:ai dashboards and views as well as the Command-line interface. Inference workloads can be submitted via Run:ai Command-line interface as well as Kubernetes API/YAML. Internally, spawning an Inference workload also creates a Kubernetes Service . The service is an end-point to which clients can connect. See Also \u00b6 To setup Inference, see Inference Setup For running Inference see Inference quick-start","title":"Overview"},{"location":"developer/deprecated/inference/overview/#what-is-inference","text":"Machine learning (ML) inference is the process of running live data points into a machine-learning algorithm to calculate an output. With Inference, you are taking a trained Model and deploying it into a production environment. The deployment must align with the organization's production standards such as average and 95% response time as well as up-time.","title":"What is Inference"},{"location":"developer/deprecated/inference/overview/#inference-and-gpus","text":"The Inference process is a subset of the original Training algorithm on a single datum (e.g. one sentence or one image), or a small batch. As such, GPU memory requirements are typically smaller than a full-blown Training process. Given that, Inference lends itself nicely to the usage of Run:ai Fractions. You can, for example, run 4 instances of an Inference server on a single GPU, each employing a fourth of the memory.","title":"Inference and GPUs"},{"location":"developer/deprecated/inference/overview/#inference-runai","text":"Run:ai provides Inference services as an equal part together with the other two Workload types: Train and Build . Inference is considered a high-priority workload as it is customer-facing. Running an Inference workload (within the Project's quota) will preempt any Run:ai Workload marked as Training . Inference is implemented as a Kubernetes Deployment with a defined number of replicas. The replicas are load-balanced by Kubernetes so that adding more replicas will improve the overall throughput of the system. Multiple replicas will appear in Run:ai as a single Inference workload. The workload will appear in all Run:ai dashboards and views as well as the Command-line interface. Inference workloads can be submitted via Run:ai Command-line interface as well as Kubernetes API/YAML. Internally, spawning an Inference workload also creates a Kubernetes Service . The service is an end-point to which clients can connect.","title":"Inference @Run:ai"},{"location":"developer/deprecated/inference/overview/#see-also","text":"To setup Inference, see Inference Setup For running Inference see Inference quick-start","title":"See Also"},{"location":"developer/deprecated/inference/setup/","text":"Inference Setup \u00b6 Warning Inference API is deprecated. See Cluster API for its replacement. Inference Jobs are an integral part of Run:ai and do not require setting up per se. However, Running multiple production-grade processes on a single GPU is best performed with an NVIDIA technology called Multi-Process Service or MPS By default, MPS is not enabled on GPU nodes. Enable MPS \u00b6 To enable the MPS server on all nodes, you must edit the cluster installation values file: When installing the Run:ai cluster, edit the values file . On an existing installation, use the upgrade cluster instructions to modify the values file. Use: runai-operator : config : mps-server : enabled : true Wait for the MPS server to start running: kubectl get pods -n runai When the MPS server pod has started to run, restart the nvidia-device-plugin pods: kubectl rollout restart ds/nvidia-device-plugin-daemonset -n gpu-operator To enable the MPS server on selected nodes, please contact Run:ai customer support. Verify MPS is Enabled \u00b6 Run: kubectl get pods -n runai --selector=app=runai-mps-server -o wide Verify that all mps-server pods are in Running state. Submit a workload with MPS enabled using the --mps flag. Then run: runai list Identify the node on which the workload is running. In the get pods command above find the pod running on the same node and then run: kubectl logs -n runai runai-mps-server-<name> -f You should see activity in the log","title":"Setup"},{"location":"developer/deprecated/inference/setup/#inference-setup","text":"Warning Inference API is deprecated. See Cluster API for its replacement. Inference Jobs are an integral part of Run:ai and do not require setting up per se. However, Running multiple production-grade processes on a single GPU is best performed with an NVIDIA technology called Multi-Process Service or MPS By default, MPS is not enabled on GPU nodes.","title":"Inference Setup"},{"location":"developer/deprecated/inference/setup/#enable-mps","text":"To enable the MPS server on all nodes, you must edit the cluster installation values file: When installing the Run:ai cluster, edit the values file . On an existing installation, use the upgrade cluster instructions to modify the values file. Use: runai-operator : config : mps-server : enabled : true Wait for the MPS server to start running: kubectl get pods -n runai When the MPS server pod has started to run, restart the nvidia-device-plugin pods: kubectl rollout restart ds/nvidia-device-plugin-daemonset -n gpu-operator To enable the MPS server on selected nodes, please contact Run:ai customer support.","title":"Enable MPS"},{"location":"developer/deprecated/inference/setup/#verify-mps-is-enabled","text":"Run: kubectl get pods -n runai --selector=app=runai-mps-server -o wide Verify that all mps-server pods are in Running state. Submit a workload with MPS enabled using the --mps flag. Then run: runai list Identify the node on which the workload is running. In the get pods command above find the pod running on the same node and then run: kubectl logs -n runai runai-mps-server-<name> -f You should see activity in the log","title":"Verify MPS is Enabled"},{"location":"developer/deprecated/inference/submit-via-cli/","text":"Submit an inference Workload \u00b6 Warning Inference API is deprecated. See Cluster API for its replacement. The easiest way to submit a new Inference workload is using the Run:ai Command-line interface. For additional information see the Inference Quickstart documentation.","title":"Submit via CLI"},{"location":"developer/deprecated/inference/submit-via-cli/#submit-an-inference-workload","text":"Warning Inference API is deprecated. See Cluster API for its replacement. The easiest way to submit a new Inference workload is using the Run:ai Command-line interface. For additional information see the Inference Quickstart documentation.","title":"Submit an inference Workload"},{"location":"developer/deprecated/k8s-api/launch-job-via-kubernetes-api/","text":"Submit a Run:ai Job via Kubernetes API \u00b6 Warning Researcher Kubernetes API is deprecated. See Cluster API for its replacement. This article is a complementary article to the article on launching jobs via YAML . It shows how to use a programming language and Kubernetes API to submit jobs. The article uses Python, though Kubernetes API is available in several other programming languages. Submit a Run:ai Job \u00b6 from __future__ import print_function import kubernetes from kubernetes import client , config from pprint import pprint import json config . load_kube_config () with client . ApiClient () as api_client : namespace = 'runai-team-a' # Run:ai project name is prefixed by runai- jobname = 'my-job' username = 'john' # used in un-authenticated systems only gpus = 1 body = client . V1Job ( api_version = \"run.ai/v1\" , kind = \"RunaiJob\" ) body . metadata = client . V1ObjectMeta ( namespace = namespace , name = jobname ) template = client . V1PodTemplate () template . template = client . V1PodTemplateSpec () template . template . metadata = client . V1ObjectMeta ( labels = { 'user' : username }) resource = client . V1ResourceRequirements ( limits = { 'nvidia.com/gpu' : gpus }) container = client . V1Container ( name = jobname , image = 'gcr.io/run-ai-demo/quickstart' , resources = resource ) template . template . spec = client . V1PodSpec ( containers = [ container ], restart_policy = 'Never' , scheduler_name = 'runai-scheduler' ) body . spec = client . V1JobSpec ( template = template . template ) pprint ( body ) try : api_instance = client . CustomObjectsApi ( api_client ) api_response = api_instance . create_namespaced_custom_object ( \"run.ai\" , \"v1\" , namespace , \"runaijobs\" , body ) pprint ( api_response ) except client . rest . ApiException as e : print ( \"Exception when calling AppsV1Api->create_namespaced_job: %s \\n \" % e )","title":"Submit a Job via Kubernetes API"},{"location":"developer/deprecated/k8s-api/launch-job-via-kubernetes-api/#submit-a-runai-job-via-kubernetes-api","text":"Warning Researcher Kubernetes API is deprecated. See Cluster API for its replacement. This article is a complementary article to the article on launching jobs via YAML . It shows how to use a programming language and Kubernetes API to submit jobs. The article uses Python, though Kubernetes API is available in several other programming languages.","title":"Submit a Run:ai Job via Kubernetes API"},{"location":"developer/deprecated/k8s-api/launch-job-via-kubernetes-api/#submit-a-runai-job","text":"from __future__ import print_function import kubernetes from kubernetes import client , config from pprint import pprint import json config . load_kube_config () with client . ApiClient () as api_client : namespace = 'runai-team-a' # Run:ai project name is prefixed by runai- jobname = 'my-job' username = 'john' # used in un-authenticated systems only gpus = 1 body = client . V1Job ( api_version = \"run.ai/v1\" , kind = \"RunaiJob\" ) body . metadata = client . V1ObjectMeta ( namespace = namespace , name = jobname ) template = client . V1PodTemplate () template . template = client . V1PodTemplateSpec () template . template . metadata = client . V1ObjectMeta ( labels = { 'user' : username }) resource = client . V1ResourceRequirements ( limits = { 'nvidia.com/gpu' : gpus }) container = client . V1Container ( name = jobname , image = 'gcr.io/run-ai-demo/quickstart' , resources = resource ) template . template . spec = client . V1PodSpec ( containers = [ container ], restart_policy = 'Never' , scheduler_name = 'runai-scheduler' ) body . spec = client . V1JobSpec ( template = template . template ) pprint ( body ) try : api_instance = client . CustomObjectsApi ( api_client ) api_response = api_instance . create_namespaced_custom_object ( \"run.ai\" , \"v1\" , namespace , \"runaijobs\" , body ) pprint ( api_response ) except client . rest . ApiException as e : print ( \"Exception when calling AppsV1Api->create_namespaced_job: %s \\n \" % e )","title":"Submit a Run:ai Job"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/","text":"Submit a Run:ai Job via YAML \u00b6 Warning Researcher Kubernetes API is deprecated. See Cluster API for its replacement. You can use YAML files to submit jobs directly to Kubernetes. A frequent scenario for using the Kubernetes YAML syntax to submit Jobs is integrations . Researchers may already be working with an existing system that submits Jobs, and want to continue working with the same system. Terminology \u00b6 We differentiate between three types of Workloads: Train workloads. Train workloads are characterized by a deep learning session that has a start and an end. A Training session can take anywhere from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. Training workloads typically utilize large percentages of GPU computing power and memory. Build workloads. Build workloads are interactive. They are used by data scientists to write machine learning code and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. Inference workloads. Inference workloads are used for serving models in production. For details on how to submit Inference workloads via YAML see here . The internal Kubernetes implementation of a Run:ai Job is a CRD (Customer Resource) named RunaiJob which is similar to a Kubernetes Job . Run:ai extends the Kubernetes Scheduler . A Kubernetes Scheduler is the software that determines which workload to start on which node. Run:ai provides a custom scheduler named runai-scheduler . The Run:ai scheduler schedules computing resources by associating Workloads with Run:ai Projects : A Project is assigned with a GPU quota through the Run:ai Run:ai User Interface. A workload must be associated with a Project name and will receive resources according to the defined quota for the Project and the currently running Workloads Internally, Run:ai Projects are implemented as Kubernetes namespaces. The scripts below assume that the code is being run after the relevant namespace has been set. Submit Workloads \u00b6 <JOB-NAME> . The name of the Job. <IMAGE-NAME> . The name of the docker image to use. Example: gcr.io/run-ai-demo/quickstart . <USER-NAME> . The name of the user submitting the Job. The name is used for display purposes only when Run:ai is installed in an unauthenticated mode . <REQUESTED-GPUs> . An integer number of GPUs you request to be allocated for the Job. Examples: 1, 2. <NAMESAPCE> . The name of the Project's namespace. This is usually runai-<PROJECT-NAME> . Regular Jobs \u00b6 Copy the following into a file and change the parameters: apiVersion : run.ai/v1 kind : RunaiJob (* see note below) metadata : name : <JOB-NAME> namespace : <NAMESPACE> labels : priorityClassName : \"build\" (* see note below) spec : template : metadata : labels : user : <USER-NAME> spec : containers : - name : <JOB-NAME> image : <IMAGE-NAME> resources : limits : nvidia.com/gpu : <REQUESTED-GPUs> restartPolicy : Never schedulerName : runai-scheduler To submit the job, run: kubectl apply -f <FILE-NAME> Note You can use either a regular Job or RunaiJob . The latter is a Run:ai object which solves various Kubernetes Bugs and provides a better naming for multiple pods in Hyper-Parameter Optimization scenarios Using build in the priorityClassName field is equivalent to running a job via the CLI with a '--interactive' flag. To run a Train job, delete this line. The runai submit CLI command includes many more flags. These flags can be correlated with Kubernetes API functions and added to the YAML above. Using Fractional GPUs \u00b6 To submit a Job with fractions of a GPU, replace <REQUESTED-GPUs> with a fraction in quotes. e.g. limits : nvidia.com/gpu : \"0.5\" where \"0.5\" is the requested GPU fraction. Mapping Additional Flags \u00b6 Run:ai Command-Line runai submit has a significant number of flags. The easiest way to find out the mapping from a flag to the correct YAML attribute is to use the --dry-run flag. For example, to find the location of the --large-shm flag, run: > runai submit -i ubuntu --large-shm --dry-run Template YAML file can be found at: /var/folders/xb/rnf9b1bx2jg45c7jprv71d9m0000gn/T/job.yaml185826190 Delete Workloads \u00b6 To delete a Run:ai workload, delete the Job: kubectl delete runaijob <JOB-NAME> See Also \u00b6 See how to use the above YAML syntax with Kubernetes API Use the Researcher REST API to submit, list and delete Jobs.","title":"Submit a Job via YAML"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#submit-a-runai-job-via-yaml","text":"Warning Researcher Kubernetes API is deprecated. See Cluster API for its replacement. You can use YAML files to submit jobs directly to Kubernetes. A frequent scenario for using the Kubernetes YAML syntax to submit Jobs is integrations . Researchers may already be working with an existing system that submits Jobs, and want to continue working with the same system.","title":"Submit a Run:ai Job via YAML"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#terminology","text":"We differentiate between three types of Workloads: Train workloads. Train workloads are characterized by a deep learning session that has a start and an end. A Training session can take anywhere from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. Training workloads typically utilize large percentages of GPU computing power and memory. Build workloads. Build workloads are interactive. They are used by data scientists to write machine learning code and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. Inference workloads. Inference workloads are used for serving models in production. For details on how to submit Inference workloads via YAML see here . The internal Kubernetes implementation of a Run:ai Job is a CRD (Customer Resource) named RunaiJob which is similar to a Kubernetes Job . Run:ai extends the Kubernetes Scheduler . A Kubernetes Scheduler is the software that determines which workload to start on which node. Run:ai provides a custom scheduler named runai-scheduler . The Run:ai scheduler schedules computing resources by associating Workloads with Run:ai Projects : A Project is assigned with a GPU quota through the Run:ai Run:ai User Interface. A workload must be associated with a Project name and will receive resources according to the defined quota for the Project and the currently running Workloads Internally, Run:ai Projects are implemented as Kubernetes namespaces. The scripts below assume that the code is being run after the relevant namespace has been set.","title":"Terminology"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#submit-workloads","text":"<JOB-NAME> . The name of the Job. <IMAGE-NAME> . The name of the docker image to use. Example: gcr.io/run-ai-demo/quickstart . <USER-NAME> . The name of the user submitting the Job. The name is used for display purposes only when Run:ai is installed in an unauthenticated mode . <REQUESTED-GPUs> . An integer number of GPUs you request to be allocated for the Job. Examples: 1, 2. <NAMESAPCE> . The name of the Project's namespace. This is usually runai-<PROJECT-NAME> .","title":"Submit Workloads"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#regular-jobs","text":"Copy the following into a file and change the parameters: apiVersion : run.ai/v1 kind : RunaiJob (* see note below) metadata : name : <JOB-NAME> namespace : <NAMESPACE> labels : priorityClassName : \"build\" (* see note below) spec : template : metadata : labels : user : <USER-NAME> spec : containers : - name : <JOB-NAME> image : <IMAGE-NAME> resources : limits : nvidia.com/gpu : <REQUESTED-GPUs> restartPolicy : Never schedulerName : runai-scheduler To submit the job, run: kubectl apply -f <FILE-NAME> Note You can use either a regular Job or RunaiJob . The latter is a Run:ai object which solves various Kubernetes Bugs and provides a better naming for multiple pods in Hyper-Parameter Optimization scenarios Using build in the priorityClassName field is equivalent to running a job via the CLI with a '--interactive' flag. To run a Train job, delete this line. The runai submit CLI command includes many more flags. These flags can be correlated with Kubernetes API functions and added to the YAML above.","title":"Regular Jobs"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#using-fractional-gpus","text":"To submit a Job with fractions of a GPU, replace <REQUESTED-GPUs> with a fraction in quotes. e.g. limits : nvidia.com/gpu : \"0.5\" where \"0.5\" is the requested GPU fraction.","title":"Using Fractional GPUs"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#mapping-additional-flags","text":"Run:ai Command-Line runai submit has a significant number of flags. The easiest way to find out the mapping from a flag to the correct YAML attribute is to use the --dry-run flag. For example, to find the location of the --large-shm flag, run: > runai submit -i ubuntu --large-shm --dry-run Template YAML file can be found at: /var/folders/xb/rnf9b1bx2jg45c7jprv71d9m0000gn/T/job.yaml185826190","title":"Mapping Additional Flags"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#delete-workloads","text":"To delete a Run:ai workload, delete the Job: kubectl delete runaijob <JOB-NAME>","title":"Delete Workloads"},{"location":"developer/deprecated/k8s-api/launch-job-via-yaml/#see-also","text":"See how to use the above YAML syntax with Kubernetes API Use the Researcher REST API to submit, list and delete Jobs.","title":"See Also"},{"location":"developer/deprecated/k8s-api/overview/","text":"Overview: Launch a Job via Kubernetes API \u00b6 Warning Researcher Kubernetes API is deprecated. See Cluster API for its replacement. You can create, submit, list or delete jobs using the Command-line interface or the Run:ai User Interface. To do the same programmatically you can use the Run:ai Researcher REST API . You can also communicate directly with the underlying Kubernetes infrastructure by: Using YAML files or, By using a variety of programming languages to send requests to Kubernetes. See Submit a Run:ai Job via Kubernetes API for a python sample.","title":"Overview"},{"location":"developer/deprecated/k8s-api/overview/#overview-launch-a-job-via-kubernetes-api","text":"Warning Researcher Kubernetes API is deprecated. See Cluster API for its replacement. You can create, submit, list or delete jobs using the Command-line interface or the Run:ai User Interface. To do the same programmatically you can use the Run:ai Researcher REST API . You can also communicate directly with the underlying Kubernetes infrastructure by: Using YAML files or, By using a variety of programming languages to send requests to Kubernetes. See Submit a Run:ai Job via Kubernetes API for a python sample.","title":"Overview: Launch a Job via Kubernetes API"},{"location":"developer/deprecated/researcher-rest-api/overview/","text":"Researcher REST API \u00b6 Warning Researcher Kubernetes API is deprecated. See Cluster API for its replacement. The purpose of the Researcher REST API is to provide an easy-to-use programming interface for submitting, listing, and deleting Jobs. There are other APIs that provide the same functionality. Specifically: If your code is script-based, you may consider using the Run:ai command-line interface . You can communicate directly with the underlying Kubernetes infrastructure by sending YAML files or by using a variety of programming languages to send requests to Kubernetes. See Submit a Run:ai Job via Kubernetes API . The Researcher REST API is cluster-specific in the sense that if you have multiple GPU clusters, you will have a separate URL per cluster. This <CLUSTER-ENDPOINT> can be found in the Run:ai User Interface, under Clusters . Each cluster will have a separate URL. Authentication \u00b6 By default, researcher APIs are unauthenticated. To protect researcher API, you must configure researcher authentication . Once configured, you must create a Client Application to make API requests. Use the client application and secret, to obtain a time-bound bearer token ( <ACCESS-TOKEN> ). For details, see Calling REST APIs . Use the token for subsequent API calls. Example \u00b6 Get all the jobs for a project named team-a : curl 'https://<CLUSTER-ENDPOINT>/researcher/api/v1/jobs/team-a' \\ -H 'accept: application/json' \\ --header 'Authorization: Bearer <ACCESS-TOKEN>' Researcher API Scope \u00b6 The Researcher API provides the following functionality: Submit a new Job List jobs for specific Projects. Delete an existing Job Get a list of Projects for which you have access to Researcher API Documentation \u00b6 To review API documentation: Open the Run:ai user interface Go to Clusters Locate your cluster and browse to https://<cluster-url>/researcher/api/docs . When using the Authenticate button, add Bearer <ACCESS TOKEN> (simply adding the access token will not work). The document uses the Open API specification to describe the API. You can test the API within the document after creating and saving a token.","title":"REST API"},{"location":"developer/deprecated/researcher-rest-api/overview/#researcher-rest-api","text":"Warning Researcher Kubernetes API is deprecated. See Cluster API for its replacement. The purpose of the Researcher REST API is to provide an easy-to-use programming interface for submitting, listing, and deleting Jobs. There are other APIs that provide the same functionality. Specifically: If your code is script-based, you may consider using the Run:ai command-line interface . You can communicate directly with the underlying Kubernetes infrastructure by sending YAML files or by using a variety of programming languages to send requests to Kubernetes. See Submit a Run:ai Job via Kubernetes API . The Researcher REST API is cluster-specific in the sense that if you have multiple GPU clusters, you will have a separate URL per cluster. This <CLUSTER-ENDPOINT> can be found in the Run:ai User Interface, under Clusters . Each cluster will have a separate URL.","title":"Researcher REST API"},{"location":"developer/deprecated/researcher-rest-api/overview/#authentication","text":"By default, researcher APIs are unauthenticated. To protect researcher API, you must configure researcher authentication . Once configured, you must create a Client Application to make API requests. Use the client application and secret, to obtain a time-bound bearer token ( <ACCESS-TOKEN> ). For details, see Calling REST APIs . Use the token for subsequent API calls.","title":"Authentication"},{"location":"developer/deprecated/researcher-rest-api/overview/#example","text":"Get all the jobs for a project named team-a : curl 'https://<CLUSTER-ENDPOINT>/researcher/api/v1/jobs/team-a' \\ -H 'accept: application/json' \\ --header 'Authorization: Bearer <ACCESS-TOKEN>'","title":"Example"},{"location":"developer/deprecated/researcher-rest-api/overview/#researcher-api-scope","text":"The Researcher API provides the following functionality: Submit a new Job List jobs for specific Projects. Delete an existing Job Get a list of Projects for which you have access to","title":"Researcher API Scope"},{"location":"developer/deprecated/researcher-rest-api/overview/#researcher-api-documentation","text":"To review API documentation: Open the Run:ai user interface Go to Clusters Locate your cluster and browse to https://<cluster-url>/researcher/api/docs . When using the Authenticate button, add Bearer <ACCESS TOKEN> (simply adding the access token will not work). The document uses the Open API specification to describe the API. You can test the API within the document after creating and saving a token.","title":"Researcher API Documentation"},{"location":"developer/metrics/metrics/","text":"What are Metrics \u00b6 Metrics are numeric measurements recorded over time that are emitted from the Run:ai cluster. Typical metrics involve utilization, allocation, time measurements and so on. Metrics are used in Run:ai dashboards as well as in the Run:ai administration user interface. The purpose of this document is to detail the structure and purpose of metrics emitted by Run:ai to enable customers to create custom dashboards or integrate metric data into other monitoring systems. Run:ai uses Prometheus for collecting and querying metrics. Published Run:ai Metrics \u00b6 Following is the list of published Run:ai metrics: Metric name Labels Measurement Description runai_active_job_cpu_requested_cores {clusterId, job_name, job_uuid} CPU Cores Job's requested CPU cores runai_active_job_memory_requested_bytes {clusterId, job_name, job_uuid} Bytes Job's requested CPU memory runai_cluster_cpu_utilization {clusterId} 0 to 1 CPU utilization of the entire cluster runai_cluster_memory_used_bytes {clusterId} Bytes Used CPU memory of the entire cluster runai_cluster_memory_utilization {clusterId} 0 to 1 CPU memory utilization of the entire cluster runai_gpu_is_allocated {gpu, clusterId, node} 0/1 Is a GPU hosting a pod runai_gpu_is_running_fractional_job {gpu, clusterId, node} 0/1 Is GPU hosting Fractional GPU jobs runai_gpu_last_active_time {gpu, clusterId, node} Unix time Last time GPU was not idle runai_gpu_utilization_non_ fractional_jobs {job_uuid, job_name, clusterId, gpu, node} 0 to 100 Utilization per GPU for jobs running on a full GPU runai_gpu_utilization_with_pod_info {pod_namespace, pod_name, clusterId, gpu, node} 0 to 100 GPU utilization per GPU runai_job_allocated_gpus {job_type, job_uuid, job_name, clusterId, project} Double GPUs allocated to Jobs runai_job_gpu_utilization {job_uuid, clusterId, job_name, project} 0 to 100 Average GPU utilization per job runai_job_image {image, job_uuid, job_name, clusterId} N/A Image name per job runai_job_requested_gpu_memory {job_type, job_uuid, job_name, clusterId, project} Bytes Requested GPU memory per job (0 if not specified by the user) runai_job_requested_gpus {job_type, job_uuid, job_name, clusterId, project} Double Number of requested GPU per job runai_job_status_with_info {user, job_type, status, job_name, clusterId, node, project} N/A Additional information on jobs runai_job_total_runtime {clusterId, job_uuid} Seconds Total run time per job runai_job_total_wait_time {clusterId, job_uuid} Seconds Total wait time per job runai_job_used_gpu_memory_bytes {clusterId, job_uuid} Bytes Used GPU memory per job runai_job_used_gpu_memory_bytes_ with_gpu_node {job_uuid, job_name, clusterId, gpu, node} Bytes Used GPU memory per job, per GPU on which the job is running runai_node_cpu_requested_cores {clusterId, node} Double Sum of the requested CPU cores of all jobs running in a node runai_node_cpu_utilization {clusterId, node} 0 to 1 CPU utilization per node runai_node_gpu_used_memory_bytes {clusterId, node} Bytes Used GPU memory per node runai_node_memory_utilization {clusterId, node} 0 to 1 CPU memory utilization per node runai_node_requested_memory_bytes {clusterId, node} Bytes Sum of the requested CPU memory of all jobs running in a node runai_node_total_memory_bytes {clusterId, node} Bytes Total GPU memory per node runai_node_used_memory_bytes {clusterId, node} Bytes Used CPU memory per node runai_project_guaranteed_gpus {clusterId, project} Double Guaranteed GPU quota per project runai_project_info {memory_quota, cpu_quota, gpu_guaranteed_quota, clusterId, project, department_name} N/A Information on CPU, CPU memory, GPU quota per project runai_running_job_cpu_limit_cores {clusterId, job_name , job_uuid} Double Jobs CPU limit (in number of cores). See link runai_running_job_cpu_requested_cores {clusterId, job_name, job_uuid} Double Jobs requested CPU cores. See link runai_running_job_cpu_used_cores {job_uuid, clusterId, job_name, project} Double Jobs CPU usage (in number of cores) runai_running_job_memory_limit_bytes {clusterId, job_name, job_uuid} Bytes Jobs CPU memory limit. See link runai_running_job_memory_requested_bytes {clusterId, job_name, job_uuid} Bytes Jobs requested CPU memory. See link runai_running_job_memory_used_bytes {job_uuid, clusterId, job_name, project} Bytes Jobs used CPU memory runai_mig_mode_gpu_count {clusterId, node} Double Number of GPUs on MIG nodes runai_job_swap_memory_used_bytes {clusterId, job_uuid, job_name, project, node} Bytes Used Swap CPU memory for the job runai_deployment_request_rate {clusterId, namespace_name, deployment_name} Number Rate of received HTTP requests per second runai_deployment_request_latencies {clusterId, namespace_name, deployment_name, le} Number Histogram of response time (bins are in milliseconds) Following is a list of labels appearing in Run:ai metrics: Label Description clusterId Cluster Identifier department_name Name of Run:ai Department cpu_quota CPU limit per project gpu GPU index gpu_guaranteed_quota Guaranteed GPU quota per project image Name of Docker image namespace_name Namespace deployment_name Deployment name job_name Job name job_type Job type: training, interactive or inference job_uuid Job identifier pod_name Pod name. A Job can contain many pods. pod_namespace Pod namespace memory_quota CPU memory limit per project node Node name project Name of Run:ai Project status Job status: Running, Pending, etc. For more information on Job statuses see document user User identifier Other Metrics \u00b6 Run:ai exports other metrics emitted by NVIDIA and Kubernetes packages, as follows: Metric name Description dcgm_gpu_utilization GPU utilization kube_node_status_allocatable Resources (CPU, memory, GPU etc) are allocatble (available for scheduling) kube_node_status_capacity The capacity for different resources of a node kube_node_status_condition The condition of a cluster node kube_pod_container_resource_requests The number of requested resources by a container kube_pod_container_resource_requests_cpu_cores The number of CPU cores requested by container kube_pod_container_resource_requests_memory_bytes Bytes of memory requested by a container kube_pod_info Information about pod kube_pod_status_phase The current phase of the pod For additional information, see Kubernetes kube-state-metrics and NVIDIA dcgm exporter . How to Query Metrics \u00b6 SaaS Self Hosted Run:ai customer support should provide <BASE-METRICS-URL> , <DATASOURCE-ID> and <GRAFANA-API-KEY> . Browse to <RUNAI-URL>/grafana ( <BASE-METRICS-URL> ) and log in as administrator Under Keys , generate a viewer key ( <GRAFANA-API-KEY> ) Under Data sources , locate a numeric data source ID ( <DATASOURCE-ID> ) Use the Run:ai metrics documentation above together with Prometheus API syntax to access data. Example: curl \"https://<BASE-METRICS-URL>/api/datasources/proxy/<DATASOURCE-ID>/api/v1/query?query=runai_job_total_runtime\" \\ --header 'Accept: application/json' \\ --header 'Authorization: Bearer <GRAFANA-API_KEY>'","title":"Metrics"},{"location":"developer/metrics/metrics/#what-are-metrics","text":"Metrics are numeric measurements recorded over time that are emitted from the Run:ai cluster. Typical metrics involve utilization, allocation, time measurements and so on. Metrics are used in Run:ai dashboards as well as in the Run:ai administration user interface. The purpose of this document is to detail the structure and purpose of metrics emitted by Run:ai to enable customers to create custom dashboards or integrate metric data into other monitoring systems. Run:ai uses Prometheus for collecting and querying metrics.","title":"What are Metrics"},{"location":"developer/metrics/metrics/#published-runai-metrics","text":"Following is the list of published Run:ai metrics: Metric name Labels Measurement Description runai_active_job_cpu_requested_cores {clusterId, job_name, job_uuid} CPU Cores Job's requested CPU cores runai_active_job_memory_requested_bytes {clusterId, job_name, job_uuid} Bytes Job's requested CPU memory runai_cluster_cpu_utilization {clusterId} 0 to 1 CPU utilization of the entire cluster runai_cluster_memory_used_bytes {clusterId} Bytes Used CPU memory of the entire cluster runai_cluster_memory_utilization {clusterId} 0 to 1 CPU memory utilization of the entire cluster runai_gpu_is_allocated {gpu, clusterId, node} 0/1 Is a GPU hosting a pod runai_gpu_is_running_fractional_job {gpu, clusterId, node} 0/1 Is GPU hosting Fractional GPU jobs runai_gpu_last_active_time {gpu, clusterId, node} Unix time Last time GPU was not idle runai_gpu_utilization_non_ fractional_jobs {job_uuid, job_name, clusterId, gpu, node} 0 to 100 Utilization per GPU for jobs running on a full GPU runai_gpu_utilization_with_pod_info {pod_namespace, pod_name, clusterId, gpu, node} 0 to 100 GPU utilization per GPU runai_job_allocated_gpus {job_type, job_uuid, job_name, clusterId, project} Double GPUs allocated to Jobs runai_job_gpu_utilization {job_uuid, clusterId, job_name, project} 0 to 100 Average GPU utilization per job runai_job_image {image, job_uuid, job_name, clusterId} N/A Image name per job runai_job_requested_gpu_memory {job_type, job_uuid, job_name, clusterId, project} Bytes Requested GPU memory per job (0 if not specified by the user) runai_job_requested_gpus {job_type, job_uuid, job_name, clusterId, project} Double Number of requested GPU per job runai_job_status_with_info {user, job_type, status, job_name, clusterId, node, project} N/A Additional information on jobs runai_job_total_runtime {clusterId, job_uuid} Seconds Total run time per job runai_job_total_wait_time {clusterId, job_uuid} Seconds Total wait time per job runai_job_used_gpu_memory_bytes {clusterId, job_uuid} Bytes Used GPU memory per job runai_job_used_gpu_memory_bytes_ with_gpu_node {job_uuid, job_name, clusterId, gpu, node} Bytes Used GPU memory per job, per GPU on which the job is running runai_node_cpu_requested_cores {clusterId, node} Double Sum of the requested CPU cores of all jobs running in a node runai_node_cpu_utilization {clusterId, node} 0 to 1 CPU utilization per node runai_node_gpu_used_memory_bytes {clusterId, node} Bytes Used GPU memory per node runai_node_memory_utilization {clusterId, node} 0 to 1 CPU memory utilization per node runai_node_requested_memory_bytes {clusterId, node} Bytes Sum of the requested CPU memory of all jobs running in a node runai_node_total_memory_bytes {clusterId, node} Bytes Total GPU memory per node runai_node_used_memory_bytes {clusterId, node} Bytes Used CPU memory per node runai_project_guaranteed_gpus {clusterId, project} Double Guaranteed GPU quota per project runai_project_info {memory_quota, cpu_quota, gpu_guaranteed_quota, clusterId, project, department_name} N/A Information on CPU, CPU memory, GPU quota per project runai_running_job_cpu_limit_cores {clusterId, job_name , job_uuid} Double Jobs CPU limit (in number of cores). See link runai_running_job_cpu_requested_cores {clusterId, job_name, job_uuid} Double Jobs requested CPU cores. See link runai_running_job_cpu_used_cores {job_uuid, clusterId, job_name, project} Double Jobs CPU usage (in number of cores) runai_running_job_memory_limit_bytes {clusterId, job_name, job_uuid} Bytes Jobs CPU memory limit. See link runai_running_job_memory_requested_bytes {clusterId, job_name, job_uuid} Bytes Jobs requested CPU memory. See link runai_running_job_memory_used_bytes {job_uuid, clusterId, job_name, project} Bytes Jobs used CPU memory runai_mig_mode_gpu_count {clusterId, node} Double Number of GPUs on MIG nodes runai_job_swap_memory_used_bytes {clusterId, job_uuid, job_name, project, node} Bytes Used Swap CPU memory for the job runai_deployment_request_rate {clusterId, namespace_name, deployment_name} Number Rate of received HTTP requests per second runai_deployment_request_latencies {clusterId, namespace_name, deployment_name, le} Number Histogram of response time (bins are in milliseconds) Following is a list of labels appearing in Run:ai metrics: Label Description clusterId Cluster Identifier department_name Name of Run:ai Department cpu_quota CPU limit per project gpu GPU index gpu_guaranteed_quota Guaranteed GPU quota per project image Name of Docker image namespace_name Namespace deployment_name Deployment name job_name Job name job_type Job type: training, interactive or inference job_uuid Job identifier pod_name Pod name. A Job can contain many pods. pod_namespace Pod namespace memory_quota CPU memory limit per project node Node name project Name of Run:ai Project status Job status: Running, Pending, etc. For more information on Job statuses see document user User identifier","title":"Published Run:ai Metrics"},{"location":"developer/metrics/metrics/#other-metrics","text":"Run:ai exports other metrics emitted by NVIDIA and Kubernetes packages, as follows: Metric name Description dcgm_gpu_utilization GPU utilization kube_node_status_allocatable Resources (CPU, memory, GPU etc) are allocatble (available for scheduling) kube_node_status_capacity The capacity for different resources of a node kube_node_status_condition The condition of a cluster node kube_pod_container_resource_requests The number of requested resources by a container kube_pod_container_resource_requests_cpu_cores The number of CPU cores requested by container kube_pod_container_resource_requests_memory_bytes Bytes of memory requested by a container kube_pod_info Information about pod kube_pod_status_phase The current phase of the pod For additional information, see Kubernetes kube-state-metrics and NVIDIA dcgm exporter .","title":"Other Metrics"},{"location":"developer/metrics/metrics/#how-to-query-metrics","text":"SaaS Self Hosted Run:ai customer support should provide <BASE-METRICS-URL> , <DATASOURCE-ID> and <GRAFANA-API-KEY> . Browse to <RUNAI-URL>/grafana ( <BASE-METRICS-URL> ) and log in as administrator Under Keys , generate a viewer key ( <GRAFANA-API-KEY> ) Under Data sources , locate a numeric data source ID ( <DATASOURCE-ID> ) Use the Run:ai metrics documentation above together with Prometheus API syntax to access data. Example: curl \"https://<BASE-METRICS-URL>/api/datasources/proxy/<DATASOURCE-ID>/api/v1/query?query=runai_job_total_runtime\" \\ --header 'Accept: application/json' \\ --header 'Authorization: Bearer <GRAFANA-API_KEY>'","title":"How to Query Metrics"},{"location":"home/components/","text":"Run:ai System Components \u00b6 Components \u00b6 Run:ai is installed over a Kubernetes Cluster Researchers submit Machine Learning workloads via the Run:ai Command-Line Interface (CLI), or directly by sending YAML files to Kubernetes. Administrators monitor and set priorities via the Run:ai User Interface The Run:ai Cluster \u00b6 The Run:ai Cluster contains: The Run:ai Scheduler which extends the Kubernetes scheduler. It uses business rules to schedule workloads sent by Researchers. Fractional GPU management. Responsible for the Run:ai technology which allows Researchers to allocate parts of a GPU rather than a whole GPU The Run:ai agent. Responsible for sending Monitoring data to the Run:ai Cloud. Clusters require outbound network connectivity to the Run:ai Cloud. Kubernetes-Related Details \u00b6 The Run:ai cluster is installed as a Kubernetes Operator Run:ai is installed in its own Kubernetes namespace named runai Workloads are run in the context of Projects . Each Project is a Kubernetes namespace with its own settings and access control. The Run:ai Control Plane \u00b6 The Run:ai control plane is the basis of the Run:ai User Interface. The Run:ai cloud aggregates monitoring information from multiple tenants (customers). Each customer may manage multiple Run:ai clusters. The Run:ai control plane resides on the cloud but can also be locally installed. To understand the various installation options see the installation types document.","title":"System Components"},{"location":"home/components/#runai-system-components","text":"","title":"Run:ai System Components"},{"location":"home/components/#components","text":"Run:ai is installed over a Kubernetes Cluster Researchers submit Machine Learning workloads via the Run:ai Command-Line Interface (CLI), or directly by sending YAML files to Kubernetes. Administrators monitor and set priorities via the Run:ai User Interface","title":"Components"},{"location":"home/components/#the-runai-cluster","text":"The Run:ai Cluster contains: The Run:ai Scheduler which extends the Kubernetes scheduler. It uses business rules to schedule workloads sent by Researchers. Fractional GPU management. Responsible for the Run:ai technology which allows Researchers to allocate parts of a GPU rather than a whole GPU The Run:ai agent. Responsible for sending Monitoring data to the Run:ai Cloud. Clusters require outbound network connectivity to the Run:ai Cloud.","title":"The Run:ai Cluster"},{"location":"home/components/#kubernetes-related-details","text":"The Run:ai cluster is installed as a Kubernetes Operator Run:ai is installed in its own Kubernetes namespace named runai Workloads are run in the context of Projects . Each Project is a Kubernetes namespace with its own settings and access control.","title":"Kubernetes-Related Details"},{"location":"home/components/#the-runai-control-plane","text":"The Run:ai control plane is the basis of the Run:ai User Interface. The Run:ai cloud aggregates monitoring information from multiple tenants (customers). Each customer may manage multiple Run:ai clusters. The Run:ai control plane resides on the cloud but can also be locally installed. To understand the various installation options see the installation types document.","title":"The Run:ai Control Plane"},{"location":"home/data-privacy-details/","text":"Data Privacy \u00b6 Run:ai SaaS Cluster installation uses the Run:ai cloud as its control plane. The cluster sends information to the cloud for the purpose of control as well as dashboards. The document below is a run-down of the data that is being sent to the Run:ai cloud. Note If the data detailed below is not in line with your organization's policy, you can choose to install the Run:ai self-hosted version. The self-hosted installation includes the Run:ai control-plane and will not communicate with the cloud. The self-hosted installation has different pricing. Data \u00b6 Following is a list of platform data items that are sent to the Run:ai cloud. Asset Data Details Job Metrics Job names, CPU, GPU, and Memory metrics, parameters sent using the runai submit command Node Metrics Node names and IPs, CPU, GPU, and Memory metrics Cluster Metrics Cluster names, CPU, GPU, and Memory metrics Projects & Departments Names, quota information Users User Run:ai roles, emails and passwords (when single-sign on not used) Run:ai does not send deep-learning artifacts to the cloud. As such any Code, images, container logs, training data, models, checkpoints and the like, stay behind corporate firewalls. See Also \u00b6 The Run:ai privacy policy .","title":"Data Privacy"},{"location":"home/data-privacy-details/#data-privacy","text":"Run:ai SaaS Cluster installation uses the Run:ai cloud as its control plane. The cluster sends information to the cloud for the purpose of control as well as dashboards. The document below is a run-down of the data that is being sent to the Run:ai cloud. Note If the data detailed below is not in line with your organization's policy, you can choose to install the Run:ai self-hosted version. The self-hosted installation includes the Run:ai control-plane and will not communicate with the cloud. The self-hosted installation has different pricing.","title":"Data Privacy"},{"location":"home/data-privacy-details/#data","text":"Following is a list of platform data items that are sent to the Run:ai cloud. Asset Data Details Job Metrics Job names, CPU, GPU, and Memory metrics, parameters sent using the runai submit command Node Metrics Node names and IPs, CPU, GPU, and Memory metrics Cluster Metrics Cluster names, CPU, GPU, and Memory metrics Projects & Departments Names, quota information Users User Run:ai roles, emails and passwords (when single-sign on not used) Run:ai does not send deep-learning artifacts to the cloud. As such any Code, images, container logs, training data, models, checkpoints and the like, stay behind corporate firewalls.","title":"Data"},{"location":"home/data-privacy-details/#see-also","text":"The Run:ai privacy policy .","title":"See Also"},{"location":"home/whats-new-2-8/","text":"Run:ai Version 2.8 \u00b6 Release Date \u00b6 November 2022 Release Content \u00b6 Node Pools \u00b6 Node Pools is a new method for managing GPU and CPU resources by grouping the resources into distinct pools. With node pools: The administrator allocates Project and Department resources from these pools to be used by Workloads. The administrator controls which workloads can use which resources, allowing an optimized utilization of resources according to customer's specific mode of operation. User Interface Enhancements \u00b6 The Departments screen has been revamped and new functionality added, including a new and clean look and feel, and improved search and filtering capabilities. The Jobs screen has been split into 2 tabs for ease of use: Current : (the default tab) consists of all the jobs that currently exist in the cluster. History : consists of all the jobs that have been deleted from the cluster. Deleting Jobs also deletes their Log (no change). Installation improvements \u00b6 The Run:ai user interface requires a URL address to the Kubernetes cluster. The requirement is relevant for SaaS installation only. In previous versions of Run:ai the administrator should provide an IP address and Run:ai would automatically create a DNS entry for it and a matching trusted certificate. In version 2.8, the default is for the Run:ai administrator to provide a DNS and a trusted certificate . The older option still exists but is being deprecated due to complexity. Inference \u00b6 The Deployment details page now contains the URL for the Inference service Hyperparameter Optimization (HPO) \u00b6 HPO Jobs are now presented as a single line in the Job List rather than a separate line per experiment. Known Issues \u00b6 Internal ID Description Workaround RUN-6236 The Run:ai access control system prevents setting a role of researcher together with ML engineer or researcher manager at the same time. However, using the UI you can select these two roles by clicking the text near the check None RUN-6218 When installing Run:ai on openshift a second time, oauth client secret is incorrect/not updated. As a result, login is not possible Can be performed via manual configuration. Please contact Run:ai support. RUN-6216 In the multi cluster overview, the allocated GPU in the table of each cluster is wrong. The correct number is in the overview dashboard. None RUN-6190 When deleting a cluster, there are leftover pods that are not deleted. No side effects on functionality. Delete the pods manually. RUN-5855 (SaaS version only) The new control plane, versioned 2.8 does not allow the creation of a new deployment on a cluster whose version is lower than 2.8. Upgrade your cluster to 2.8 RUN-5780 It is possible to change runai/node-pool label of a running pod. This is a wrong usage of the system and may cause unexpected behavior. None. RUN-5527 Idle allocated GPU metric is not displayed for MIG workloads in openshift. None RUN-5519 When selecting a Job, the GPU memory utilization metrics is not displayed on the right-hand side. This is an NVIDIA DCGM known bug (see: https://github.com/NVIDIA/dcgm-exporter/issues/103 ) which has been fixed in a later version but was not yet included in the latest NVIDIA GPU Operator Install the suggested version as described by NVIDIA. RUN-5478 Dashboard panels of GPU Allocation/project and Allocated jobs per project metrics: In rare cases, some metrics reflect the wrong number of GPUs None RUN-5444 Dynamic MIG feature does not work with A-100 with 80GB of memory. None RUN-5424 When a workload is selected in the job list, the GPU tab in the right panel, shows the details of the whole GPUs in the node, instead of the details of the GPUs used by the workload. None RUN-5226 In rare occasions, when there is more than 1 NVIDIA MIG workload, nvidia-smi command to one of the workloads will result with no devices. None RUN-6359 In rare cases, when using fractions and the kubelet service on the scheduled node is down (Kubernetes not running on node)the pending workload will never run, even when the IT problem is solved. Delete the job and re-submit the workload. RUN-6399 Requested GPUs are sometimes displayed in the Job list as 0 for distributed workloads. None. This is a display-only issue RUN-6400 On EKS (Amazon Kubernetes Server), when using runai CLI, every command response starts with an error. No functionality harm. None. The CLI functions as expected. Fixed Issues \u00b6 Internal ID Description RUN-5676 When Interactive Jupyter notebook workloads that contain passwords are cloned, the password is exposed in the displayed CLI command. RUN-5457 When using the Home environment variable in conjunction with the ran-as-user option in the CLI, the Home environment variable is overwritten with the user's home directory. RUN-5370 It is possible to submit two jobs with the same node-port. RUN-5314 When you apply an inference deployment via a file, the allocated GPUs are displayed as 0 in the deployments list. RUN-5284 When workloads are deleted while the cluster synchronization is down, there might be a non-existent Job shown in the user interface. The Job cannot be deleted. RUN-5160 In some situations, when a Job is deleted, there may be leftover Kubernetes configMaps in the system RUN-5154 In some cases, an error \"failed to load data\" can be seen in the graphs showing on the Job sidebar. RUN-5145 The default Kubernetes \"priority Class\" for deployments is the same as the priority class for interactive jobs. RUN-5039 In some scenarios, Dashboards may show \"found duplicate series for the match group\" error RUN-4941 The scheduler is wrongly trying to schedule jobs on a node, where there are allocated GPU jobs at an \"ImagePullBackoff\" state. This causes an error of \"UnexpectedAdmissionError\" RUN-4574 The role \"Researcher Manager\" is not displayed in the access control list of projects. RUN-4554 Users are trying to login with single-sign-on get a \"review profile\" page. RUN-4464 Single HPO (hyperparameter optimization) workload is displayed in the Job list user interfgace as multiple jobs (one for every pod).","title":"Version 2.8"},{"location":"home/whats-new-2-8/#runai-version-28","text":"","title":"Run:ai Version 2.8"},{"location":"home/whats-new-2-8/#release-date","text":"November 2022","title":"Release Date"},{"location":"home/whats-new-2-8/#release-content","text":"","title":"Release Content"},{"location":"home/whats-new-2-8/#node-pools","text":"Node Pools is a new method for managing GPU and CPU resources by grouping the resources into distinct pools. With node pools: The administrator allocates Project and Department resources from these pools to be used by Workloads. The administrator controls which workloads can use which resources, allowing an optimized utilization of resources according to customer's specific mode of operation.","title":"Node Pools"},{"location":"home/whats-new-2-8/#user-interface-enhancements","text":"The Departments screen has been revamped and new functionality added, including a new and clean look and feel, and improved search and filtering capabilities. The Jobs screen has been split into 2 tabs for ease of use: Current : (the default tab) consists of all the jobs that currently exist in the cluster. History : consists of all the jobs that have been deleted from the cluster. Deleting Jobs also deletes their Log (no change).","title":"User Interface Enhancements"},{"location":"home/whats-new-2-8/#installation-improvements","text":"The Run:ai user interface requires a URL address to the Kubernetes cluster. The requirement is relevant for SaaS installation only. In previous versions of Run:ai the administrator should provide an IP address and Run:ai would automatically create a DNS entry for it and a matching trusted certificate. In version 2.8, the default is for the Run:ai administrator to provide a DNS and a trusted certificate . The older option still exists but is being deprecated due to complexity.","title":"Installation improvements"},{"location":"home/whats-new-2-8/#inference","text":"The Deployment details page now contains the URL for the Inference service","title":"Inference"},{"location":"home/whats-new-2-8/#hyperparameter-optimization-hpo","text":"HPO Jobs are now presented as a single line in the Job List rather than a separate line per experiment.","title":"Hyperparameter Optimization (HPO)"},{"location":"home/whats-new-2-8/#known-issues","text":"Internal ID Description Workaround RUN-6236 The Run:ai access control system prevents setting a role of researcher together with ML engineer or researcher manager at the same time. However, using the UI you can select these two roles by clicking the text near the check None RUN-6218 When installing Run:ai on openshift a second time, oauth client secret is incorrect/not updated. As a result, login is not possible Can be performed via manual configuration. Please contact Run:ai support. RUN-6216 In the multi cluster overview, the allocated GPU in the table of each cluster is wrong. The correct number is in the overview dashboard. None RUN-6190 When deleting a cluster, there are leftover pods that are not deleted. No side effects on functionality. Delete the pods manually. RUN-5855 (SaaS version only) The new control plane, versioned 2.8 does not allow the creation of a new deployment on a cluster whose version is lower than 2.8. Upgrade your cluster to 2.8 RUN-5780 It is possible to change runai/node-pool label of a running pod. This is a wrong usage of the system and may cause unexpected behavior. None. RUN-5527 Idle allocated GPU metric is not displayed for MIG workloads in openshift. None RUN-5519 When selecting a Job, the GPU memory utilization metrics is not displayed on the right-hand side. This is an NVIDIA DCGM known bug (see: https://github.com/NVIDIA/dcgm-exporter/issues/103 ) which has been fixed in a later version but was not yet included in the latest NVIDIA GPU Operator Install the suggested version as described by NVIDIA. RUN-5478 Dashboard panels of GPU Allocation/project and Allocated jobs per project metrics: In rare cases, some metrics reflect the wrong number of GPUs None RUN-5444 Dynamic MIG feature does not work with A-100 with 80GB of memory. None RUN-5424 When a workload is selected in the job list, the GPU tab in the right panel, shows the details of the whole GPUs in the node, instead of the details of the GPUs used by the workload. None RUN-5226 In rare occasions, when there is more than 1 NVIDIA MIG workload, nvidia-smi command to one of the workloads will result with no devices. None RUN-6359 In rare cases, when using fractions and the kubelet service on the scheduled node is down (Kubernetes not running on node)the pending workload will never run, even when the IT problem is solved. Delete the job and re-submit the workload. RUN-6399 Requested GPUs are sometimes displayed in the Job list as 0 for distributed workloads. None. This is a display-only issue RUN-6400 On EKS (Amazon Kubernetes Server), when using runai CLI, every command response starts with an error. No functionality harm. None. The CLI functions as expected.","title":"Known Issues"},{"location":"home/whats-new-2-8/#fixed-issues","text":"Internal ID Description RUN-5676 When Interactive Jupyter notebook workloads that contain passwords are cloned, the password is exposed in the displayed CLI command. RUN-5457 When using the Home environment variable in conjunction with the ran-as-user option in the CLI, the Home environment variable is overwritten with the user's home directory. RUN-5370 It is possible to submit two jobs with the same node-port. RUN-5314 When you apply an inference deployment via a file, the allocated GPUs are displayed as 0 in the deployments list. RUN-5284 When workloads are deleted while the cluster synchronization is down, there might be a non-existent Job shown in the user interface. The Job cannot be deleted. RUN-5160 In some situations, when a Job is deleted, there may be leftover Kubernetes configMaps in the system RUN-5154 In some cases, an error \"failed to load data\" can be seen in the graphs showing on the Job sidebar. RUN-5145 The default Kubernetes \"priority Class\" for deployments is the same as the priority class for interactive jobs. RUN-5039 In some scenarios, Dashboards may show \"found duplicate series for the match group\" error RUN-4941 The scheduler is wrongly trying to schedule jobs on a node, where there are allocated GPU jobs at an \"ImagePullBackoff\" state. This causes an error of \"UnexpectedAdmissionError\" RUN-4574 The role \"Researcher Manager\" is not displayed in the access control list of projects. RUN-4554 Users are trying to login with single-sign-on get a \"review profile\" page. RUN-4464 Single HPO (hyperparameter optimization) workload is displayed in the Job list user interfgace as multiple jobs (one for every pod).","title":"Fixed Issues"},{"location":"home/whats-new-2-9/","text":"Run:ai Version 2.9 \u00b6 Release Date \u00b6 February 2023 Release Content \u00b6 Authentication \u00b6 Openshift groups Ability to manage access control through IDP groups declaration - groups are managed from the Openshift platform and integrated into Run:ai platform, as opposed to group management in vanilla k8s with SSO. Openshift doesn\u2019t need any additional configuration as this comes built-in with regular installation or the upgrade option. UID/GID for SSO users When running a workload through the UI the Run:ai platform now automatically injects the UID and GID into the created container. This has changed from previous versions where the user would enter data in these fields manually. This is designed for environments where UIDs and GIDs are managed in an SSO server, and Run:ai is configured with SSO. SSO: block access to Run:ai When configuring SSO in the Run:ai platform all users are assigned a new default role. It means an SSO user will not have any access to the Run:ai platform unless a manager explicitly assigns additional roles via the user or group management areas. Run CPU over-quota workloads Added support for CPU workloads to support over-quota - CPU resources fairness was added to the Run:ai scheduler in addition to the GPU fairness that is already supported. The updated fairness algorithm takes into account all resource types (GPU, CPU compute and CPU memory) and is supported regardless of node pool configuration. Run:ai Workspaces \u00b6 A Run:ai workspace is a simplified, efficient tool for researchers to conduct their experiments, build AI models, access standard MLOps tools, and collaborate with their peers. Run:ai workspaces abstract complex concepts related to running containerized workloads in a Kubernetes environment, such as networking, storage, and secrets, and are built from predefined abstracted setups, that ease and streamline the researcher AI models development. A workspace consists of container images, data sets, resource requests, and all the required tools for the research. They are quickly created with the workspace wizard. For more information see Workspaces . New supported tools for researchers \u00b6 As part of the introduction of Run:ai workspaces a few new development and research tools were added. The new supported tools are: RStudio, Visual Studio Code, Matlab and Weights and Biases (see full details). This is an addition to adding already supported tools, such as JupyterNotebook and TensorBoard to Run:ai workspaces. Weight and Biases Weights and Biases is a commercial tool that provides experiment tracking, model visualization, and collaboration for machine learning projects. It helps researchers and developers keep track of their experiments, visualize their results, and compare different models to make informed decisions. This integration provides data researchers with connectivity between the running Workspace in Run:ai and the relevant project for experiment tracking. For more information, see Weights and Biases Node pools enhancements Added additional support to multi-node pools. This new capability allows the researcher to specify a prioritized list of node pools for the Run:ai scheduler to use. Researchers now gain the flexibility to use multiple resource types and maximize the utilization of the system\u2019s GPU and CPU resources. Administrators now have the option to set a default Project (namespace) level with a prioritized list of node pools that a workload will use if the researcher did not set its own priorities. New nodes and node pools Screens Run:ai has revised the nodes table, adding new information fields and graphs. It is now easier to assess how resources are allocated and utilized. Run:ai has also added a new \u2018node pools\u2019 table where Administrators can add a new node pool, update, and delete an existing node pool. In addition, the node pools table presents a large number of metrics and details about each of the node pools. A set of graphs reflect the node pools\u2019 resource status over time according to different criteria. Consumption Dashboard Added a \u201cConsumption\u201d dashboard. When enabled by the \u201cShow Consumption Dashboard\u201d alpha flag under \u201cSettings\u201d, this dashboard allows the admin to review consumption patterns for GPUs, CPUs and RAM over time. You can segregate consumption by over or in-quota allocation in the project or department level. Event History (Audit Log UI) Added the option for Administrators to view the system\u2019s Audit Log via the Run:ai user interface. Configuration changes and other administrative operations (login/logout etc) are saved in an Audit Log facility. Administrators can browse through the Admin Log (Event History), download as a JSON or CSV, filter specific date periods, set multiple criteria filters, and decide which information fields to view. Idle jobs timeout policy Added an option \u2018Editor\u2019 so that Administrators can terminate idle workloads by setting the criteria of \u2018idle time\u2019 per project so that the editor can identify and terminate idle Training and Interactive (build) workloads. This is used for maximizing and maintaining system sanitation. Installation Enhancements \u00b6 Cluster Upgrade \u00b6 Cluster upgrade to 2.9 requires uninstalling and then installing. No data is lost during the process. For more information see cluster upgrade . Using an IP address for a cluster URL is no longer available in this version. You must use a domain name . Cluster Prerequisites \u00b6 Prometheus is no longer installed together with Run:ai. You must install the Prometheus stack before installing Run:ai. This is designed for organizations that already have Prometheus installed in the cluster. The Run:ai installation configures the existing Prometheus with a custom set of rules designed to extract metrics from the cluster. NGINX is no longer installed together with Run:ai. You must install an Ingress controller before installing Run:ai. This is designed for organizations that already have an ingress controller installed. The Run:ai installation creates NGINX rules to work with the controller. List of Run:ai installation Prerequisites can be found here . The Run:ai installation now performs a series of checks to verify the installation's validity. When the installation is complete, verify by reviewing the following in the log file: Are all mandatory prerequisites met? Are optional prerequisites met? Does the cluster have connectivity to the Run:ai control plane? Does Run:ai support the underlying Kubernetes version? Control Plane Upgrade \u00b6 A special process is required to upgrade the control-plane to version 2.9. Control plane Prerequisites \u00b6 Run:ai control plane installation no longer installs NGINX. You must pre-install an ingress controller. The default persistent storage is now a default storage class preconfigured in Kubernetes rather than the older NFS assumptions. NFS flags in runai-adm generate-values still exist for backward compatibility. Other \u00b6 Cluster Wizard has been simplified for environments with multiple clusters in a self-hosted configuration. Clusters are now easier to configure. Choose a cluster location: Same as Control Plane Remote to Control Plane. New Supported Software \u00b6 Run:ai now supports Kubernetes 1.25 and 1.26. Run:ai now supports Openshift 4.11 Run:ai now supports Dynamic MIG with NVIDIA H100 hardware The Run:ai command-line interface now supports Microsoft Windows. See Install the Run:ai Command-line Interface . Known Issues \u00b6 Internal ID Description Workaround RUN-7874 When a project is not connected to a namespace - new job returns \"malformed URL\" None RUN-7617 Cannot delete Node affinity from project after it was created Remove it using the API. Fixed Issues \u00b6 Internal ID Description RUN-7776 user does not exist in the UI due to pagination limitation RUN-6995 Group Mapping from SSO Group to Researcher Manager Role no working RUN-6460 S3 Fail (read/write in Jupyter notebook) RUN-6445 Project can be created with deleted node pool RUN-6400 EKS - Every command response in runai CLI starts with an error. No functionality harm RUN-6399 Requested GPU is always 0 for MPI jobs, making also other metrics wrong RUN-6359 Job gets UnexpectedAdmissionError race condition with Kubelet RUN-6272 runai pod which owner is not RunaiJob - Do not allow deletion, suspension, cloning RUN-6218 When installing Run:ai on openshift a second time, oauth client secret is incorrect/not updated RUN-6216 Multi cluster: allocated GPU is wrong as a result of metric not counting jobs in error RUN-6029 CLI Submit git sync severe bug RUN-6027 [Security Issue] Job submitted with github sync -- Password is displayed in the UI RUN-5822 Environment Variables in the UI do not honor the \"canRemove:false\" attribute in Policy RUN-5676 Security issue with \"Clone Job\" functionality RUN-5527 Metrics (MIG - OCP): GPU Idle Allocated GPUs show No Data RUN-5478 # of GPUs is higher than existing GPUs in the cluster RUN-5444 MIG doesn't work on A100 - 80GB RUN-5424 Deployment GPUs tab shows all the GPUs on the node instead of the ones in use by the deployment RUN-5370 Can submit job with the same node port + imagePullpolicy RUN-5226 MIG job can't see device after submitting a different mig job RUN-4869 S3 jobs run forever with NotReady state RUN-4244 Run:ai Alertmanager shows false positive errors on Agent","title":"Version 2.9"},{"location":"home/whats-new-2-9/#runai-version-29","text":"","title":"Run:ai Version 2.9"},{"location":"home/whats-new-2-9/#release-date","text":"February 2023","title":"Release Date"},{"location":"home/whats-new-2-9/#release-content","text":"","title":"Release Content"},{"location":"home/whats-new-2-9/#authentication","text":"Openshift groups Ability to manage access control through IDP groups declaration - groups are managed from the Openshift platform and integrated into Run:ai platform, as opposed to group management in vanilla k8s with SSO. Openshift doesn\u2019t need any additional configuration as this comes built-in with regular installation or the upgrade option. UID/GID for SSO users When running a workload through the UI the Run:ai platform now automatically injects the UID and GID into the created container. This has changed from previous versions where the user would enter data in these fields manually. This is designed for environments where UIDs and GIDs are managed in an SSO server, and Run:ai is configured with SSO. SSO: block access to Run:ai When configuring SSO in the Run:ai platform all users are assigned a new default role. It means an SSO user will not have any access to the Run:ai platform unless a manager explicitly assigns additional roles via the user or group management areas. Run CPU over-quota workloads Added support for CPU workloads to support over-quota - CPU resources fairness was added to the Run:ai scheduler in addition to the GPU fairness that is already supported. The updated fairness algorithm takes into account all resource types (GPU, CPU compute and CPU memory) and is supported regardless of node pool configuration.","title":"Authentication"},{"location":"home/whats-new-2-9/#runai-workspaces","text":"A Run:ai workspace is a simplified, efficient tool for researchers to conduct their experiments, build AI models, access standard MLOps tools, and collaborate with their peers. Run:ai workspaces abstract complex concepts related to running containerized workloads in a Kubernetes environment, such as networking, storage, and secrets, and are built from predefined abstracted setups, that ease and streamline the researcher AI models development. A workspace consists of container images, data sets, resource requests, and all the required tools for the research. They are quickly created with the workspace wizard. For more information see Workspaces .","title":"Run:ai Workspaces"},{"location":"home/whats-new-2-9/#new-supported-tools-for-researchers","text":"As part of the introduction of Run:ai workspaces a few new development and research tools were added. The new supported tools are: RStudio, Visual Studio Code, Matlab and Weights and Biases (see full details). This is an addition to adding already supported tools, such as JupyterNotebook and TensorBoard to Run:ai workspaces. Weight and Biases Weights and Biases is a commercial tool that provides experiment tracking, model visualization, and collaboration for machine learning projects. It helps researchers and developers keep track of their experiments, visualize their results, and compare different models to make informed decisions. This integration provides data researchers with connectivity between the running Workspace in Run:ai and the relevant project for experiment tracking. For more information, see Weights and Biases Node pools enhancements Added additional support to multi-node pools. This new capability allows the researcher to specify a prioritized list of node pools for the Run:ai scheduler to use. Researchers now gain the flexibility to use multiple resource types and maximize the utilization of the system\u2019s GPU and CPU resources. Administrators now have the option to set a default Project (namespace) level with a prioritized list of node pools that a workload will use if the researcher did not set its own priorities. New nodes and node pools Screens Run:ai has revised the nodes table, adding new information fields and graphs. It is now easier to assess how resources are allocated and utilized. Run:ai has also added a new \u2018node pools\u2019 table where Administrators can add a new node pool, update, and delete an existing node pool. In addition, the node pools table presents a large number of metrics and details about each of the node pools. A set of graphs reflect the node pools\u2019 resource status over time according to different criteria. Consumption Dashboard Added a \u201cConsumption\u201d dashboard. When enabled by the \u201cShow Consumption Dashboard\u201d alpha flag under \u201cSettings\u201d, this dashboard allows the admin to review consumption patterns for GPUs, CPUs and RAM over time. You can segregate consumption by over or in-quota allocation in the project or department level. Event History (Audit Log UI) Added the option for Administrators to view the system\u2019s Audit Log via the Run:ai user interface. Configuration changes and other administrative operations (login/logout etc) are saved in an Audit Log facility. Administrators can browse through the Admin Log (Event History), download as a JSON or CSV, filter specific date periods, set multiple criteria filters, and decide which information fields to view. Idle jobs timeout policy Added an option \u2018Editor\u2019 so that Administrators can terminate idle workloads by setting the criteria of \u2018idle time\u2019 per project so that the editor can identify and terminate idle Training and Interactive (build) workloads. This is used for maximizing and maintaining system sanitation.","title":"New supported tools for researchers"},{"location":"home/whats-new-2-9/#installation-enhancements","text":"","title":"Installation Enhancements"},{"location":"home/whats-new-2-9/#cluster-upgrade","text":"Cluster upgrade to 2.9 requires uninstalling and then installing. No data is lost during the process. For more information see cluster upgrade . Using an IP address for a cluster URL is no longer available in this version. You must use a domain name .","title":"Cluster Upgrade"},{"location":"home/whats-new-2-9/#cluster-prerequisites","text":"Prometheus is no longer installed together with Run:ai. You must install the Prometheus stack before installing Run:ai. This is designed for organizations that already have Prometheus installed in the cluster. The Run:ai installation configures the existing Prometheus with a custom set of rules designed to extract metrics from the cluster. NGINX is no longer installed together with Run:ai. You must install an Ingress controller before installing Run:ai. This is designed for organizations that already have an ingress controller installed. The Run:ai installation creates NGINX rules to work with the controller. List of Run:ai installation Prerequisites can be found here . The Run:ai installation now performs a series of checks to verify the installation's validity. When the installation is complete, verify by reviewing the following in the log file: Are all mandatory prerequisites met? Are optional prerequisites met? Does the cluster have connectivity to the Run:ai control plane? Does Run:ai support the underlying Kubernetes version?","title":"Cluster Prerequisites"},{"location":"home/whats-new-2-9/#control-plane-upgrade","text":"A special process is required to upgrade the control-plane to version 2.9.","title":"Control Plane Upgrade"},{"location":"home/whats-new-2-9/#control-plane-prerequisites","text":"Run:ai control plane installation no longer installs NGINX. You must pre-install an ingress controller. The default persistent storage is now a default storage class preconfigured in Kubernetes rather than the older NFS assumptions. NFS flags in runai-adm generate-values still exist for backward compatibility.","title":"Control plane Prerequisites"},{"location":"home/whats-new-2-9/#other","text":"Cluster Wizard has been simplified for environments with multiple clusters in a self-hosted configuration. Clusters are now easier to configure. Choose a cluster location: Same as Control Plane Remote to Control Plane.","title":"Other"},{"location":"home/whats-new-2-9/#new-supported-software","text":"Run:ai now supports Kubernetes 1.25 and 1.26. Run:ai now supports Openshift 4.11 Run:ai now supports Dynamic MIG with NVIDIA H100 hardware The Run:ai command-line interface now supports Microsoft Windows. See Install the Run:ai Command-line Interface .","title":"New Supported Software"},{"location":"home/whats-new-2-9/#known-issues","text":"Internal ID Description Workaround RUN-7874 When a project is not connected to a namespace - new job returns \"malformed URL\" None RUN-7617 Cannot delete Node affinity from project after it was created Remove it using the API.","title":"Known Issues"},{"location":"home/whats-new-2-9/#fixed-issues","text":"Internal ID Description RUN-7776 user does not exist in the UI due to pagination limitation RUN-6995 Group Mapping from SSO Group to Researcher Manager Role no working RUN-6460 S3 Fail (read/write in Jupyter notebook) RUN-6445 Project can be created with deleted node pool RUN-6400 EKS - Every command response in runai CLI starts with an error. No functionality harm RUN-6399 Requested GPU is always 0 for MPI jobs, making also other metrics wrong RUN-6359 Job gets UnexpectedAdmissionError race condition with Kubelet RUN-6272 runai pod which owner is not RunaiJob - Do not allow deletion, suspension, cloning RUN-6218 When installing Run:ai on openshift a second time, oauth client secret is incorrect/not updated RUN-6216 Multi cluster: allocated GPU is wrong as a result of metric not counting jobs in error RUN-6029 CLI Submit git sync severe bug RUN-6027 [Security Issue] Job submitted with github sync -- Password is displayed in the UI RUN-5822 Environment Variables in the UI do not honor the \"canRemove:false\" attribute in Policy RUN-5676 Security issue with \"Clone Job\" functionality RUN-5527 Metrics (MIG - OCP): GPU Idle Allocated GPUs show No Data RUN-5478 # of GPUs is higher than existing GPUs in the cluster RUN-5444 MIG doesn't work on A100 - 80GB RUN-5424 Deployment GPUs tab shows all the GPUs on the node instead of the ones in use by the deployment RUN-5370 Can submit job with the same node port + imagePullpolicy RUN-5226 MIG job can't see device after submitting a different mig job RUN-4869 S3 jobs run forever with NotReady state RUN-4244 Run:ai Alertmanager shows false positive errors on Agent","title":"Fixed Issues"},{"location":"home/whats-new-2020/","text":"December 28th, 2020 \u00b6 It is now possible to allocate a specific amount of GPU memory rather than use the fraction syntax. Use --gpu-memory=5G . December 15th, 2020 \u00b6 Project and Departments can now be set to not allocate resources beyond the assigned GPUs. This is useful for budget-conscious Projects/Departments. December 1st, 2020 \u00b6 New integration documents: Apache Airflow TensorBoard November 25th, 2020 \u00b6 Syntax changes in CLI: runai <object> list has been replaced by runai list <object> . runai get has been replaced by runai describe job . runai <object> set has been replaced by runai config <object> . The older style will still work with a deprecation notice. runai top node has been revamped. November 12th, 2020 \u00b6 An Admin can now create templates for the Command-line interface. Both a default template and specific templates, that can be used with the --template flag. The new templates allow for mandatory values, defaults, and run-time environment variable resolution. It is now also possible to pass Secrets to Job. see here November 2nd, 2020 \u00b6 Several changes and additions to the Command-line interface: Passing a command and arguments is now done docker-style by adding -- at the end of the command You no longer need to provide a Job name . If you don't, a Job name will be generated automatically. You can also control the job-name prefix using an additional flag. New --image-pull-policy flag, allowing Researcher support for updating images without tagging. For further information see runai submit September 6th, 2020 \u00b6 We released a module that helps the Researcher perform Hyperparameter optimization (HPO). HPO is about running many smaller experiments with varying parameters to help determine the optimal parameter set Hyperparameter Optimization Quickstart September 3rd, 2020 \u00b6 GPU Fractions now run in training and not only interactive. GPU Fractions training Job can be preempted, bin-packed and consolidated like any integer Job. See Run:ai Scheduler Fraction for more. August 10th, 2020 \u00b6 Run:ai Now supports Distributed Training and Gang Scheduling . For further information, see the Launch Distributed Training Workloads quickstart. August 4th, 2020 \u00b6 There is now an optional second level of Project hierarchy called Departments . For further information on how to configure and use Departments, see Working with Departments July 28th, 2020 \u00b6 You can now enforce a cluster-wise setting that mandates all containers running using the Run:ai CLI to run as non root . For further information, see Enforce non-root Containers July 21th, 2020 \u00b6 It is now possible to mount a Persistent Storage Claim using the Run:ai CLI. See the --pvc flag in the runai submit CLI flag June 13th, 2020 \u00b6 New Settings for the Allocation of CPU and Memory \u00b6 It is now possible to set limits for CPU and memory as well as to establish defaults based on the ratio of GPU to CPU and GPU to memory. For further information see: Allocation of CPU and Memory June 3rd, 2020 \u00b6 Node Group Affinity \u00b6 Projects now support Node Affinity. This feature allows the Administrator to assign specific Projects to run only on specific nodes (machines). Example use cases: The Project team needs specialized hardware (e.g. with enough memory) The Project team is the owner of specific hardware which was acquired with a specialized budget We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes For further information see: Working with Projects Limit Duration of Interactive Jobs \u00b6 Researchers frequently forget to close Interactive Job. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive Job and close them automatically. For further information on how to set up duration limits see: Working with Projects May 24th, 2020 \u00b6 Kubernetes Operators \u00b6 Cluster installation now works with Kubernetes Operators . Operators make it easy to install, update, and delete a Run:ai cluster. For further information see: Upgrading a Run:ai Cluster Installation and Deleting a a Run:ai Cluster Installation March 3rd, 2020 \u00b6 Admin Overview Dashboard \u00b6 A new admin overview dashboard that shows a more holistic view of multiple clusters. Applicable for customers with more than one cluster.","title":"Whats New 2020"},{"location":"home/whats-new-2020/#december-28th-2020","text":"It is now possible to allocate a specific amount of GPU memory rather than use the fraction syntax. Use --gpu-memory=5G .","title":"December 28th, 2020"},{"location":"home/whats-new-2020/#december-15th-2020","text":"Project and Departments can now be set to not allocate resources beyond the assigned GPUs. This is useful for budget-conscious Projects/Departments.","title":"December 15th, 2020"},{"location":"home/whats-new-2020/#december-1st-2020","text":"New integration documents: Apache Airflow TensorBoard","title":"December 1st, 2020"},{"location":"home/whats-new-2020/#november-25th-2020","text":"Syntax changes in CLI: runai <object> list has been replaced by runai list <object> . runai get has been replaced by runai describe job . runai <object> set has been replaced by runai config <object> . The older style will still work with a deprecation notice. runai top node has been revamped.","title":"November 25th, 2020"},{"location":"home/whats-new-2020/#november-12th-2020","text":"An Admin can now create templates for the Command-line interface. Both a default template and specific templates, that can be used with the --template flag. The new templates allow for mandatory values, defaults, and run-time environment variable resolution. It is now also possible to pass Secrets to Job. see here","title":"November 12th, 2020"},{"location":"home/whats-new-2020/#november-2nd-2020","text":"Several changes and additions to the Command-line interface: Passing a command and arguments is now done docker-style by adding -- at the end of the command You no longer need to provide a Job name . If you don't, a Job name will be generated automatically. You can also control the job-name prefix using an additional flag. New --image-pull-policy flag, allowing Researcher support for updating images without tagging. For further information see runai submit","title":"November 2nd, 2020"},{"location":"home/whats-new-2020/#september-6th-2020","text":"We released a module that helps the Researcher perform Hyperparameter optimization (HPO). HPO is about running many smaller experiments with varying parameters to help determine the optimal parameter set Hyperparameter Optimization Quickstart","title":"September 6th, 2020"},{"location":"home/whats-new-2020/#september-3rd-2020","text":"GPU Fractions now run in training and not only interactive. GPU Fractions training Job can be preempted, bin-packed and consolidated like any integer Job. See Run:ai Scheduler Fraction for more.","title":"September 3rd, 2020"},{"location":"home/whats-new-2020/#august-10th-2020","text":"Run:ai Now supports Distributed Training and Gang Scheduling . For further information, see the Launch Distributed Training Workloads quickstart.","title":"August 10th, 2020"},{"location":"home/whats-new-2020/#august-4th-2020","text":"There is now an optional second level of Project hierarchy called Departments . For further information on how to configure and use Departments, see Working with Departments","title":"August 4th, 2020"},{"location":"home/whats-new-2020/#july-28th-2020","text":"You can now enforce a cluster-wise setting that mandates all containers running using the Run:ai CLI to run as non root . For further information, see Enforce non-root Containers","title":"July 28th, 2020"},{"location":"home/whats-new-2020/#july-21th-2020","text":"It is now possible to mount a Persistent Storage Claim using the Run:ai CLI. See the --pvc flag in the runai submit CLI flag","title":"July 21th, 2020"},{"location":"home/whats-new-2020/#june-13th-2020","text":"","title":"June 13th, 2020"},{"location":"home/whats-new-2020/#new-settings-for-the-allocation-of-cpu-and-memory","text":"It is now possible to set limits for CPU and memory as well as to establish defaults based on the ratio of GPU to CPU and GPU to memory. For further information see: Allocation of CPU and Memory","title":"New Settings for the Allocation of CPU and Memory"},{"location":"home/whats-new-2020/#june-3rd-2020","text":"","title":"June 3rd, 2020"},{"location":"home/whats-new-2020/#node-group-affinity","text":"Projects now support Node Affinity. This feature allows the Administrator to assign specific Projects to run only on specific nodes (machines). Example use cases: The Project team needs specialized hardware (e.g. with enough memory) The Project team is the owner of specific hardware which was acquired with a specialized budget We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes For further information see: Working with Projects","title":"Node Group Affinity"},{"location":"home/whats-new-2020/#limit-duration-of-interactive-jobs","text":"Researchers frequently forget to close Interactive Job. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive Job and close them automatically. For further information on how to set up duration limits see: Working with Projects","title":"Limit Duration of Interactive Jobs"},{"location":"home/whats-new-2020/#may-24th-2020","text":"","title":"May 24th, 2020"},{"location":"home/whats-new-2020/#kubernetes-operators","text":"Cluster installation now works with Kubernetes Operators . Operators make it easy to install, update, and delete a Run:ai cluster. For further information see: Upgrading a Run:ai Cluster Installation and Deleting a a Run:ai Cluster Installation","title":"Kubernetes Operators"},{"location":"home/whats-new-2020/#march-3rd-2020","text":"","title":"March 3rd, 2020"},{"location":"home/whats-new-2020/#admin-overview-dashboard","text":"A new admin overview dashboard that shows a more holistic view of multiple clusters. Applicable for customers with more than one cluster.","title":"Admin Overview Dashboard"},{"location":"home/whats-new-2021/","text":"December 8 th 2021 \u00b6 To comply with organizational policies and enhance the Run:ai platform security, Run:ai now supports Single Sign-On (SSO). This functionality is currently in beta and is available for new customer tenants only. For further details on SSO see Single Sign-On . To optimize resource management and utilization of Nvidia GPUs based on Ampere architecture, such as A100, Run:ai now supports dynamic creation and allocation of MIG partitions. This functionality is currently in beta. For further details on the dynamic allocation of MIG partitions see Dynamic MIG . Run:ai now supports AI workloads running in containerized clusters based on the VMWare Tanzu orchestrator. For further details on supported orchestrators see the prerequisites document. Supportability enhancements: A new \"Status History\" tab has been added to the job details view. The new tab shows the details of each status change of each job and allows researchers to analyze how to improve experiments as well as equip administrators with a tool to analyze running and historical jobs. In addition, the details of the reason a job is in the current status are available when hovering over the job status on the jobs table. To improve the ability to monitor the Run:ai environment, Run:ai components now expose alerts indicating whether the component is running. For further details on cluster monitoring see Cluster Monitoring User Experience (UX) enhancements: Run:ai cluster version is now available in the clusters list. Researchers can now submit and integrate with Git directly from the user interface. October 29 th 2021 \u00b6 The Run:ai cluster now enforces the access definitions of the user and lists only jobs under permitted projects. For example, runai list jobs will only show jobs from projects to which the researcher has access to. The Run:ai CLI runai list projects option now displays the quota definitions of each project. The Run:ai CLI port forwarding option now supports any IP address. The Run:ai CLI binary download is now signed with a checksum, to allow customers to validate the origin of the CLI and align with security best practices and standards. The Run:ai Researcher User Interface now supports setting GPU Memory as well as volumes in NFS servers. The Run:ai metrics used in the Dashboards are now officially documented and can be accessed via APIs as documented here . Run:ai now officially supports integration with Seldon Core. For more details read here . Run:ai now support VMWare Tanzu Kubernetes. August 30 th 2021 \u00b6 Run:ai now supports a self-hosted installation. With the self-hosted installation the Run:ai control-plane which typically resides on the cloud, is deployed at the customer's data center. For further details on supported installation types see Installation Types . Note The Run:ai self-hosted installation requires a dedicated license, and has different pricing than the SaaS installation. For more details contact your Run:ai account manager. NFS volumes can now be mounted directly to containers run by Run:ai while submitting jobs via Run:ai. See the --nfs-server flag of runai submit . To ease the manageability of user templates, Run:ai now supports global user templates . Global user templates are user templates that are managed by the Run:ai administrator and are available for all the projects within a specific cluster. The purpose of global user templates is to help define and enforce cross-organization resource policies. To simplify researchers' job submission via the Run:ai Researcher User Interface (UI), the UI now supports autocomplete, which is based on pre-defined values, as configured by the Administrator using the administrative templates. Run:ai extended the usage of Cluster name, as defined by the Administrator while configuring clusters at Run:ai. The Cluster name is now populated to the Run:ai dashboards as well as the Researcher UI. The original command line, which was used for running a Job, is now shown under the Job details under the General tab. August 4 th 2021 \u00b6 Researcher User Interface (UI) enhancements: Revised user interface and user experience Researchers can create templates for the ease of jobs submission. Templates can be saved and used at the project level Researchers can be easily re-submit jobs from the Submit page or directly from the jobs list on the Jobs page Administrators can create administrative templates which set cluster-wide defaults, constraints, and defaults for the submission of Jobs. Different teams can collaborate and share templates by exporting and importing templates in the Submit screen Researcher Command Line Interface (CLI) enhancements: Jobs can be manually suspended and resumed using the new commands: runai suspend & runai resume A new command was added: runai top job Kubeflow integration is now supported. The new integration allows building ML pipelines in Kubeflow Pipelines as well as Kubeflow Notebooks and run the workloads via the Run:ai platform. For further details see Integrate Run:ai with Kubeflow . Mlflow integration is now supported. For further details see Integrate Run:ai with MLflow . Run:ai Projects are implemented as Kubernetes namespaces. Run:ai now supports customizable namespace names. For further details see Manual Creation of Namespaces . May 10 th 2021 \u00b6 Usability improvements of Run:ai Command-line interface (CLI). The CLI now supports autocomplete for all options and parameters. Usability improvements of the Administration user interface navigation menu now allow for easier navigation. Run:ai can be installed when Kubernetes has Pod Security Policy (PSP) enabled. April 20 th 2021 \u00b6 Job List and Node list now show the GPU type (e.g. v-100). April 18 th , 2021 \u00b6 Inference workloads are now supported. For further details see Inference Overview . JupyterHub integration is now supported. For further details see JupyterHub Integration NVIDIA MIG is now supported. You can use the NVIDIA MIG technology to partition A-100 GPUs. Each partition will be considered as a single GPU by the Run:ai system and all the Run:ai functionality is supported in the partition level, including GPU Fractions. April 1 st , 2021 \u00b6 Run:ai now supports Kubernetes 1.20 March 24th 2021 \u00b6 Job List and Node list now show CPU utilization and CPU memory utilization. February 14th, 2021 \u00b6 The Job list now shows per-Job graphs for GPU utilization, GPU memory. The Node list now shows per-Node graphs for GPU utilization, GPU memory. January 22 nd , 2021 \u00b6 New Analytics dashboard with emphasis on CPU, CPU Memory, GPU, and GPU Memory. Allows better diagnostics of resource misuse. January 15th, 2021 \u00b6 A new developer documentation area has been created . In it: New documentation for Researcher REST API . New documentation for Administration Rest API . Kubernetes-based API for job creation . January 9th 2021 \u00b6 A new Researcher user interface is now available. January 2nd, 2021 \u00b6 Run:ai Clusters now support Azure Managed Kubernetes Service (AKS)","title":"Whats New 2021"},{"location":"home/whats-new-2021/#december-8th-2021","text":"To comply with organizational policies and enhance the Run:ai platform security, Run:ai now supports Single Sign-On (SSO). This functionality is currently in beta and is available for new customer tenants only. For further details on SSO see Single Sign-On . To optimize resource management and utilization of Nvidia GPUs based on Ampere architecture, such as A100, Run:ai now supports dynamic creation and allocation of MIG partitions. This functionality is currently in beta. For further details on the dynamic allocation of MIG partitions see Dynamic MIG . Run:ai now supports AI workloads running in containerized clusters based on the VMWare Tanzu orchestrator. For further details on supported orchestrators see the prerequisites document. Supportability enhancements: A new \"Status History\" tab has been added to the job details view. The new tab shows the details of each status change of each job and allows researchers to analyze how to improve experiments as well as equip administrators with a tool to analyze running and historical jobs. In addition, the details of the reason a job is in the current status are available when hovering over the job status on the jobs table. To improve the ability to monitor the Run:ai environment, Run:ai components now expose alerts indicating whether the component is running. For further details on cluster monitoring see Cluster Monitoring User Experience (UX) enhancements: Run:ai cluster version is now available in the clusters list. Researchers can now submit and integrate with Git directly from the user interface.","title":"December 8th 2021"},{"location":"home/whats-new-2021/#october-29th-2021","text":"The Run:ai cluster now enforces the access definitions of the user and lists only jobs under permitted projects. For example, runai list jobs will only show jobs from projects to which the researcher has access to. The Run:ai CLI runai list projects option now displays the quota definitions of each project. The Run:ai CLI port forwarding option now supports any IP address. The Run:ai CLI binary download is now signed with a checksum, to allow customers to validate the origin of the CLI and align with security best practices and standards. The Run:ai Researcher User Interface now supports setting GPU Memory as well as volumes in NFS servers. The Run:ai metrics used in the Dashboards are now officially documented and can be accessed via APIs as documented here . Run:ai now officially supports integration with Seldon Core. For more details read here . Run:ai now support VMWare Tanzu Kubernetes.","title":"October 29th 2021"},{"location":"home/whats-new-2021/#august-30th-2021","text":"Run:ai now supports a self-hosted installation. With the self-hosted installation the Run:ai control-plane which typically resides on the cloud, is deployed at the customer's data center. For further details on supported installation types see Installation Types . Note The Run:ai self-hosted installation requires a dedicated license, and has different pricing than the SaaS installation. For more details contact your Run:ai account manager. NFS volumes can now be mounted directly to containers run by Run:ai while submitting jobs via Run:ai. See the --nfs-server flag of runai submit . To ease the manageability of user templates, Run:ai now supports global user templates . Global user templates are user templates that are managed by the Run:ai administrator and are available for all the projects within a specific cluster. The purpose of global user templates is to help define and enforce cross-organization resource policies. To simplify researchers' job submission via the Run:ai Researcher User Interface (UI), the UI now supports autocomplete, which is based on pre-defined values, as configured by the Administrator using the administrative templates. Run:ai extended the usage of Cluster name, as defined by the Administrator while configuring clusters at Run:ai. The Cluster name is now populated to the Run:ai dashboards as well as the Researcher UI. The original command line, which was used for running a Job, is now shown under the Job details under the General tab.","title":"August 30th 2021"},{"location":"home/whats-new-2021/#august-4th-2021","text":"Researcher User Interface (UI) enhancements: Revised user interface and user experience Researchers can create templates for the ease of jobs submission. Templates can be saved and used at the project level Researchers can be easily re-submit jobs from the Submit page or directly from the jobs list on the Jobs page Administrators can create administrative templates which set cluster-wide defaults, constraints, and defaults for the submission of Jobs. Different teams can collaborate and share templates by exporting and importing templates in the Submit screen Researcher Command Line Interface (CLI) enhancements: Jobs can be manually suspended and resumed using the new commands: runai suspend & runai resume A new command was added: runai top job Kubeflow integration is now supported. The new integration allows building ML pipelines in Kubeflow Pipelines as well as Kubeflow Notebooks and run the workloads via the Run:ai platform. For further details see Integrate Run:ai with Kubeflow . Mlflow integration is now supported. For further details see Integrate Run:ai with MLflow . Run:ai Projects are implemented as Kubernetes namespaces. Run:ai now supports customizable namespace names. For further details see Manual Creation of Namespaces .","title":"August 4th 2021"},{"location":"home/whats-new-2021/#may-10th-2021","text":"Usability improvements of Run:ai Command-line interface (CLI). The CLI now supports autocomplete for all options and parameters. Usability improvements of the Administration user interface navigation menu now allow for easier navigation. Run:ai can be installed when Kubernetes has Pod Security Policy (PSP) enabled.","title":"May 10th 2021"},{"location":"home/whats-new-2021/#april-20th-2021","text":"Job List and Node list now show the GPU type (e.g. v-100).","title":"April 20th 2021"},{"location":"home/whats-new-2021/#april-18th-2021","text":"Inference workloads are now supported. For further details see Inference Overview . JupyterHub integration is now supported. For further details see JupyterHub Integration NVIDIA MIG is now supported. You can use the NVIDIA MIG technology to partition A-100 GPUs. Each partition will be considered as a single GPU by the Run:ai system and all the Run:ai functionality is supported in the partition level, including GPU Fractions.","title":"April 18th, 2021"},{"location":"home/whats-new-2021/#april-1st-2021","text":"Run:ai now supports Kubernetes 1.20","title":"April 1st, 2021"},{"location":"home/whats-new-2021/#march-24th-2021","text":"Job List and Node list now show CPU utilization and CPU memory utilization.","title":"March 24th 2021"},{"location":"home/whats-new-2021/#february-14th-2021","text":"The Job list now shows per-Job graphs for GPU utilization, GPU memory. The Node list now shows per-Node graphs for GPU utilization, GPU memory.","title":"February 14th, 2021"},{"location":"home/whats-new-2021/#january-22nd-2021","text":"New Analytics dashboard with emphasis on CPU, CPU Memory, GPU, and GPU Memory. Allows better diagnostics of resource misuse.","title":"January 22nd, 2021"},{"location":"home/whats-new-2021/#january-15th-2021","text":"A new developer documentation area has been created . In it: New documentation for Researcher REST API . New documentation for Administration Rest API . Kubernetes-based API for job creation .","title":"January 15th, 2021"},{"location":"home/whats-new-2021/#january-9th-2021","text":"A new Researcher user interface is now available.","title":"January 9th 2021"},{"location":"home/whats-new-2021/#january-2nd-2021","text":"Run:ai Clusters now support Azure Managed Kubernetes Service (AKS)","title":"January 2nd, 2021"},{"location":"home/whats-new-2022/","text":"July 2022 Run:ai Version 2.7 \u00b6 New Audit Log API is now available. The last login indication is now showing at the bottom left of the screen for single-sign-on users as well as regular users. Built-in Tensorboard support in the Run:ai user interface. You can now submit a Job and allocate Extended Kubernetes Resources . Extended resources are third-party devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that you want to allocate to your Job. The third-party vendor has extended Kubernetes using a Device Plugin . Run:ai now allows the allocation of these resources via the Run:ai user interface Job form as well as the Run:ai Workload API. You can now submit a job with additional Linux Capabilities . Linux capabilities allow the researcher to give the Job additional permissions without actually giving the Job root access to the node. Run:ai allows adding such capabilities to the Job via the Run:ai user interface Job form as well as the Run:ai Workload API. June 2022 Run:ai Version 2.6 (Cloud update only) \u00b6 The login screen now provides the capability to recover a password. With single-sign-on, you can now (optionally) map the user's first and last name from the organizational directory. See single-sign-on prerequisites A new user role of ML Engineer . The role allows the user to view and manage inference deployments and cluster resources. Clearer documentation on how to perform port-forwarding when accessing the Run:ai cluster from Windows. Using the Run:ai user interface it is now possible to clone an existing Job. The clone operation will open a Job form and allow you to change parameters before re-submitting. May 2022 Run:ai Version 2.5 \u00b6 Command-line interface installation The command-line interface utility is no longer a separate install. Instead is now installed by logging into the control plane and downloading the utility which matches the cluster's version. Warning The command-line interface utility for version 2.3 is not compatible with a cluster version of 2.5 or later. If you upgrade the cluster, you must also upgrade the command-line interface. Inference . Run:ai inference offering has been overhauled with the ability to submit deployments via the user interface and a new and consistent API. For more information see Inference overview . To enable the new inference module call by Run:ai customer support. CPU and CPU memory quotas can now be configured for projects and departments. These are hard quotas which means that the total amount of the requested resource for all workloads associated with a project/department cannot exceed the set limit. To enable this feature please call Run:ai customer support. Workloads . We have revamped the way Run:ai submits Jobs. Run:ai now submits Workloads . The change includes: New Cluster API . The older API has been deprecated and remains for backward compatibility. The API creates all the resources required for the run, including volumes, services, and the like. It also deletes all resources when the workload itself is deleted. Administrative templates have been replaced with Policies . Policies apply across all ways to submit jobs: command-line, API, and user interface. runai delete has been changed in favor of runai delete job Self-hosted installation: The default Openshift installation is now set to work with a configured Openshift IdP. See creation of backend values for more information. In addition, the default for OpenShift is now HTTPS. To send logs to Run:ai customer support there is a utility to package all logs into one tar file. Version 2.5 brings a new method that automatically sends all new logs to Run:ai support servers for a set amount of time. See collecting logs for more information. It is now possible to mount an S3 bucket into a Run:ai Job. The option is only available via the command-line interface. For more information see runai submit . User interface improvements: The top navigation bar of the Run:ai user interface has been improved and now allows users to easily access everything related to the account, as well as multiple helpful links to the product documentation, CLI and APIs. Researcher Authentication configuration is now mandatory. Newly Supported Versions \u00b6 Run:ai now supports Kubernetes 1.24 Run:ai now supports Openshift 4.10 Distributed training now supports MPI version 0.3. Support for older versions of MPI has been removed. April 2022 Run:ai Version 2.4 (Controlled Release only) \u00b6 Important Upgrade Note \u00b6 This version contains a significant change in the way that Run:ai uses and installs NVIDIA pre-requisites. Prior to this version, Run:ai has installed its own variants of two NVIDIA components: NVIDIA device plugin and NVIDIA DCGM Exporter . As these two variants are no longer needed, Run:ai now uses the standard NVIDIA installation which makes the Run:ai installation experience simpler. It does however require non-trivial changes when upgrading from older versions of Run:ai. Going forward, we also mandate the usage of the NVIDIA GPU Operator version 1.9. The Operator easies the installation of all NVIDIA software. Drivers and Kubernetes components alike. For further information see the Run:ai NVIDIA prerequisites as well as the Run:ai cluster upgrade . Dynamic MIG Support \u00b6 Run:ai now supports the dynamic allocation of NVIDIA MIG slices. For further information see the document on fractions as well as the dynamic MIG quickstart . Other features: Run:ai now support fractions on GKE. GKE has a different software stack for NVIDIA. To install Run:ai on GKE please contact Run:ai customer support. March 2022 Run:ai Version 2.3 \u00b6 Important Upgrade Note \u00b6 To upgrade to version 2.3 cluster from earlier versions, you must uninstall version 2.2 or earlier and only then install version 2.3. For detailed information see cluster upgrade . Unified User Interface \u00b6 The Researcher user interface and the Administrator user interface have been unified into a single unified Run:ai user interface . The new user interface is served from https://<company-name>.run.ai . The user interface capabilities are subject to the role of the individual user. See instructions on how to set up the unified user interface. See user interface Jobs area for a description of how to submit, view and delete Jobs from the unified user interface. Other features: Additional information about scheduler decisions can now be found as part of the Job's status. View the Job status by running runai describe job or selecting a Job in the user interface and clicking Status History . Run:ai now support Charmed Kubernetes . Run:ai now supports orchestration of containerized virtual machines via Kubevirt . For more information see kubevirt support . Run:ai now supports Openshift 4.9, Kubernetes 1.22, and 1.23. February 2022 Run:ai Version 2.2 (Cloud update only) \u00b6 When enabling Single-Sign, you can now use role groups . With groups, you no longer need to provide roles to individuals. Rather, you can create a group in the organization's directory and assign its members with specific Run:ai Roles such as Administrator, Researcher, and the like. For more information see single-sign-on . REST API has changed. The new API relies on Applications . See Calling REST APIs for more information. Added a new user role Research Manager . The role automatically assigns the user as a Researcher to all projects, including future projects. January 2022 Run:ai Version 2.0 \u00b6 We have now stabilized on a single version numbering system for all Run:ai artifacts: Run:ai Control plane. Run:ai Cluster. Run:ai Command-line interface. Run:ai Administrator Command-line interface. Future versions will be numbered using 2 digits (2.0, 2.1, 2.2, etc.). The numbering for the different artifacts will vary at the third digit as we provide patches to customers. As such, in the future, the control plane can be tagged as 2.1.0 while the cluster tagged as 2.1.1. Release Contents \u00b6 To allow for better control over resource allocation, the Run:ai platform now provides the ability to define different over-quota priorities for projects. For full details see Controlling over-quota behavior . To help review and track resource consumption per department, the Department object was added to multiple dashboard metrics. Supportability enhancements: A new tool was added, to allow IT administrators to validate cluster and control-plane installation prerequisites. For full details see cluster installation prerequisites , Kubernetes self-hosted prerequisites or Openshift self-hosted prerequisites . To better analyze scheduling issues, the node name was added to multiple scheduler log events.","title":"Whats New 2022"},{"location":"home/whats-new-2022/#july-2022-runai-version-27","text":"New Audit Log API is now available. The last login indication is now showing at the bottom left of the screen for single-sign-on users as well as regular users. Built-in Tensorboard support in the Run:ai user interface. You can now submit a Job and allocate Extended Kubernetes Resources . Extended resources are third-party devices (such as high-performance NICs, FPGAs, or InfiniBand adapters) that you want to allocate to your Job. The third-party vendor has extended Kubernetes using a Device Plugin . Run:ai now allows the allocation of these resources via the Run:ai user interface Job form as well as the Run:ai Workload API. You can now submit a job with additional Linux Capabilities . Linux capabilities allow the researcher to give the Job additional permissions without actually giving the Job root access to the node. Run:ai allows adding such capabilities to the Job via the Run:ai user interface Job form as well as the Run:ai Workload API.","title":"July 2022 Run:ai Version 2.7"},{"location":"home/whats-new-2022/#june-2022-runai-version-26-cloud-update-only","text":"The login screen now provides the capability to recover a password. With single-sign-on, you can now (optionally) map the user's first and last name from the organizational directory. See single-sign-on prerequisites A new user role of ML Engineer . The role allows the user to view and manage inference deployments and cluster resources. Clearer documentation on how to perform port-forwarding when accessing the Run:ai cluster from Windows. Using the Run:ai user interface it is now possible to clone an existing Job. The clone operation will open a Job form and allow you to change parameters before re-submitting.","title":"June 2022 Run:ai Version 2.6 (Cloud update only)"},{"location":"home/whats-new-2022/#may-2022-runai-version-25","text":"Command-line interface installation The command-line interface utility is no longer a separate install. Instead is now installed by logging into the control plane and downloading the utility which matches the cluster's version. Warning The command-line interface utility for version 2.3 is not compatible with a cluster version of 2.5 or later. If you upgrade the cluster, you must also upgrade the command-line interface. Inference . Run:ai inference offering has been overhauled with the ability to submit deployments via the user interface and a new and consistent API. For more information see Inference overview . To enable the new inference module call by Run:ai customer support. CPU and CPU memory quotas can now be configured for projects and departments. These are hard quotas which means that the total amount of the requested resource for all workloads associated with a project/department cannot exceed the set limit. To enable this feature please call Run:ai customer support. Workloads . We have revamped the way Run:ai submits Jobs. Run:ai now submits Workloads . The change includes: New Cluster API . The older API has been deprecated and remains for backward compatibility. The API creates all the resources required for the run, including volumes, services, and the like. It also deletes all resources when the workload itself is deleted. Administrative templates have been replaced with Policies . Policies apply across all ways to submit jobs: command-line, API, and user interface. runai delete has been changed in favor of runai delete job Self-hosted installation: The default Openshift installation is now set to work with a configured Openshift IdP. See creation of backend values for more information. In addition, the default for OpenShift is now HTTPS. To send logs to Run:ai customer support there is a utility to package all logs into one tar file. Version 2.5 brings a new method that automatically sends all new logs to Run:ai support servers for a set amount of time. See collecting logs for more information. It is now possible to mount an S3 bucket into a Run:ai Job. The option is only available via the command-line interface. For more information see runai submit . User interface improvements: The top navigation bar of the Run:ai user interface has been improved and now allows users to easily access everything related to the account, as well as multiple helpful links to the product documentation, CLI and APIs. Researcher Authentication configuration is now mandatory.","title":"May 2022 Run:ai Version 2.5"},{"location":"home/whats-new-2022/#newly-supported-versions","text":"Run:ai now supports Kubernetes 1.24 Run:ai now supports Openshift 4.10 Distributed training now supports MPI version 0.3. Support for older versions of MPI has been removed.","title":"Newly Supported Versions"},{"location":"home/whats-new-2022/#april-2022-runai-version-24-controlled-release-only","text":"","title":"April 2022 Run:ai Version 2.4 (Controlled Release only)"},{"location":"home/whats-new-2022/#important-upgrade-note","text":"This version contains a significant change in the way that Run:ai uses and installs NVIDIA pre-requisites. Prior to this version, Run:ai has installed its own variants of two NVIDIA components: NVIDIA device plugin and NVIDIA DCGM Exporter . As these two variants are no longer needed, Run:ai now uses the standard NVIDIA installation which makes the Run:ai installation experience simpler. It does however require non-trivial changes when upgrading from older versions of Run:ai. Going forward, we also mandate the usage of the NVIDIA GPU Operator version 1.9. The Operator easies the installation of all NVIDIA software. Drivers and Kubernetes components alike. For further information see the Run:ai NVIDIA prerequisites as well as the Run:ai cluster upgrade .","title":"Important Upgrade Note"},{"location":"home/whats-new-2022/#dynamic-mig-support","text":"Run:ai now supports the dynamic allocation of NVIDIA MIG slices. For further information see the document on fractions as well as the dynamic MIG quickstart . Other features: Run:ai now support fractions on GKE. GKE has a different software stack for NVIDIA. To install Run:ai on GKE please contact Run:ai customer support.","title":"Dynamic MIG Support"},{"location":"home/whats-new-2022/#march-2022-runai-version-23","text":"","title":"March 2022 Run:ai Version 2.3"},{"location":"home/whats-new-2022/#important-upgrade-note_1","text":"To upgrade to version 2.3 cluster from earlier versions, you must uninstall version 2.2 or earlier and only then install version 2.3. For detailed information see cluster upgrade .","title":"Important Upgrade Note"},{"location":"home/whats-new-2022/#unified-user-interface","text":"The Researcher user interface and the Administrator user interface have been unified into a single unified Run:ai user interface . The new user interface is served from https://<company-name>.run.ai . The user interface capabilities are subject to the role of the individual user. See instructions on how to set up the unified user interface. See user interface Jobs area for a description of how to submit, view and delete Jobs from the unified user interface. Other features: Additional information about scheduler decisions can now be found as part of the Job's status. View the Job status by running runai describe job or selecting a Job in the user interface and clicking Status History . Run:ai now support Charmed Kubernetes . Run:ai now supports orchestration of containerized virtual machines via Kubevirt . For more information see kubevirt support . Run:ai now supports Openshift 4.9, Kubernetes 1.22, and 1.23.","title":"Unified User Interface"},{"location":"home/whats-new-2022/#february-2022-runai-version-22-cloud-update-only","text":"When enabling Single-Sign, you can now use role groups . With groups, you no longer need to provide roles to individuals. Rather, you can create a group in the organization's directory and assign its members with specific Run:ai Roles such as Administrator, Researcher, and the like. For more information see single-sign-on . REST API has changed. The new API relies on Applications . See Calling REST APIs for more information. Added a new user role Research Manager . The role automatically assigns the user as a Researcher to all projects, including future projects.","title":"February 2022 Run:ai Version 2.2 (Cloud update only)"},{"location":"home/whats-new-2022/#january-2022-runai-version-20","text":"We have now stabilized on a single version numbering system for all Run:ai artifacts: Run:ai Control plane. Run:ai Cluster. Run:ai Command-line interface. Run:ai Administrator Command-line interface. Future versions will be numbered using 2 digits (2.0, 2.1, 2.2, etc.). The numbering for the different artifacts will vary at the third digit as we provide patches to customers. As such, in the future, the control plane can be tagged as 2.1.0 while the cluster tagged as 2.1.1.","title":"January 2022 Run:ai Version 2.0"},{"location":"home/whats-new-2022/#release-contents","text":"To allow for better control over resource allocation, the Run:ai platform now provides the ability to define different over-quota priorities for projects. For full details see Controlling over-quota behavior . To help review and track resource consumption per department, the Department object was added to multiple dashboard metrics. Supportability enhancements: A new tool was added, to allow IT administrators to validate cluster and control-plane installation prerequisites. For full details see cluster installation prerequisites , Kubernetes self-hosted prerequisites or Openshift self-hosted prerequisites . To better analyze scheduling issues, the node name was added to multiple scheduler log events.","title":"Release Contents"}]}